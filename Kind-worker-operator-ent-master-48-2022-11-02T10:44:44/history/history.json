{"Worker Suite:Worker Suite#[It] Slice Config Negative Scenario tests Slice Config Negative Scenario Update Should fail while updating sliceconfig with invalid clusters":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":3,"unknown":0,"total":3},"items":[{"uid":"70608bcd413bb58f","status":"passed","time":{"start":1666674621000,"stop":1666674622072,"duration":1072}},{"uid":"84f10c4ed72fb9e5","status":"passed","time":{"start":1666674212000,"stop":1666674212776,"duration":776}},{"uid":"c35034f26287d252","status":"passed","time":{"start":1666665366000,"stop":1666665366788,"duration":788}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting worker-operator pod Check ping between iperf-server and iperf-client after worker-operator pod restart":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":1,"unknown":0,"total":3},"items":[{"uid":"76bf3d51ba29f3bb","status":"passed","time":{"start":1666674621000,"stop":1666674625726,"duration":4726}},{"uid":"b39d901e5b0c6ed5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674212000,"stop":1666674212000,"duration":0}},{"uid":"4898b2e530b709f2","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666665366000,"stop":1666665366000,"duration":0}}]},"Worker Suite:Worker Suite#[It]  QosProfile tests QosProfile Create validation Creates qosprofile":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":3,"unknown":0,"total":3},"items":[{"uid":"55f47c2336ca7fe1","status":"passed","time":{"start":1666674621000,"stop":1666674622956,"duration":1956}},{"uid":"c55b727e64a02ab8","status":"passed","time":{"start":1666674212000,"stop":1666674212718,"duration":718}},{"uid":"cb11b99efdd6e4e5","status":"passed","time":{"start":1666665366000,"stop":1666665366761,"duration":761}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting vl3 pod Check ping between iperf-server and iperf-client after vl3 pod restart":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":1,"unknown":0,"total":3},"items":[{"uid":"2c3c72791df4490e","status":"passed","time":{"start":1666674621000,"stop":1666674625862,"duration":4862}},{"uid":"d7528fabee8bf84","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674212000,"stop":1666674212000,"duration":0}},{"uid":"905687ccaf481ca4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666665366000,"stop":1666665366000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Hub netpol scenarios: Slice Configs Netpol scenarios Should create slice for valid namespace and valid clusters in applicationNamespaces of sliceconfigs manifest":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":3,"unknown":0,"total":3},"items":[{"uid":"fb69bba7b63ac860","status":"passed","time":{"start":1666674621000,"stop":1666674632231,"duration":11231}},{"uid":"ed8bcb05e1df738f","status":"passed","time":{"start":1666674212000,"stop":1666674223824,"duration":11824}},{"uid":"4106eeab77272c94","status":"passed","time":{"start":1666665366000,"stop":1666665366641,"duration":641}}]},"Worker Suite:Worker Suite#[It] Slice Config Negative Scenario tests Slice Config Negative Scenario Creation Should fail while creating sliceconfig with invalid clusters":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":3,"unknown":0,"total":3},"items":[{"uid":"22e006fa38119f89","status":"passed","time":{"start":1666674621000,"stop":1666674621465,"duration":465}},{"uid":"5d9d84c9ea40b9d4","status":"passed","time":{"start":1666674212000,"stop":1666674212669,"duration":669}},{"uid":"ff63a7543d631c85","status":"passed","time":{"start":1666665366000,"stop":1666665366514,"duration":514}}]},"Intracluster Suite:Intracluster Suite#[BeforeSuite]":{"statistic":{"failed":21,"broken":0,"skipped":0,"passed":3,"unknown":0,"total":24},"items":[{"uid":"d0a6d08155d31cba","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00035c228>: {\n        Underlying: <*exec.ExitError | 0xc0006cc000>{\n            ProcessState: {\n                pid: 6188,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 2, Usec: 584824},\n                    Stime: {Sec: 0, Usec: 363553},\n                    Maxrss: 98984,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 21999,\n                    Majflt: 9,\n                    Nswap: 0,\n                    Inblock: 1464,\n                    Oublock: 20872,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 6820,\n                    Nivcsw: 6683,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"install.go:192: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.4.4.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:165: [debug] Clearing discovery cache\",\n                        \"wait.go:66: [debug] beginning wait for 7 resources with timeout of 1m0s\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:310: [debug] Starting delete for \\\"kubeslice-preinstall\\\" ServiceAccount\",\n                        \"client.go:339: [debug] serviceaccounts \\\"kubeslice-preinstall\\\" not found\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:310: [debug] Starting delete for \\\"kubeslice-preinstall-role\\\" ClusterRole\",\n                        \"client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \\\"kubeslice-preinstall-role\\\" not found\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:310: [debug] Starting delete for \\\"kubeslice-preinstall-rolebinding\\\" ClusterRoleBinding\",\n                        \"client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \\\"kubeslice-preinstall-rolebinding\\\" not found\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:310: [debug] Starting delete for \\\"kubeslice-worker-preinstall-configmap\\\" ConfigMap\",\n                        \"client.go:339: [debug] configmaps \\\"kubeslice-worker-preinstall-configmap\\\" not found\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:310: [debug] Starting delete for \\\"kubeslice-worker-preinstall-job\\\" Job\",\n                        \"client.go:339: [debug] jobs.batch \\\"kubeslice-worker-preinstall-job\\\" not found\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:540: [debug] Watching for changes to Job kubeslice-worker-preinstall-job with timeout of 5m0s\",\n                        \"client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: ADDED\",\n                        \"client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 0, jobs failed: 0, jobs succeeded: 0\",\n                        \"client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\",\n                        \"c...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; install.go:192: [debug] Original chart version: \"\"\n    install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.4.4.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:165: [debug] Clearing discovery cache\n    wait.go:66: [debug] beginning wait for 7 resources with timeout of 1m0s\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:310: [debug] Starting delete for \"kubeslice-preinstall\" ServiceAccount\n    client.go:339: [debug] serviceaccounts \"kubeslice-preinstall\" not found\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:310: [debug] Starting delete for \"kubeslice-preinstall-role\" ClusterRole\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-preinstall-role\" not found\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:310: [debug] Starting delete for \"kubeslice-preinstall-rolebinding\" ClusterRoleBinding\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-preinstall-rolebinding\" not found\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:310: [debug] Starting delete for \"kubeslice-worker-preinstall-configmap\" ConfigMap\n    client.go:339: [debug] configmaps \"kubeslice-worker-preinstall-configmap\" not found\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:310: [debug] Starting delete for \"kubeslice-worker-preinstall-job\" Job\n    client.go:339: [debug] jobs.batch \"kubeslice-worker-preinstall-job\" not found\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:540: [debug] Watching for changes to Job kubeslice-worker-preinstall-job with timeout of 5m0s\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: ADDED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 0, jobs failed: 0, jobs succeeded: 0\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 1, jobs succeeded: 0\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 2, jobs succeeded: 0\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 3, jobs succeeded: 0\n    client.go:310: [debug] Starting delete for \"kubeslice-worker-preinstall-job\" Job\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:310: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"jaeger\" Service\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-svc\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:339: [debug] services \"kubeslice-webhook-service\" not found\n    client.go:339: [debug] services \"jaeger\" not found\n    client.go:339: [debug] services \"nsm-admission-webhook-svc\" not found\n    client.go:339: [debug] services \"kubeslice-dns\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook\" Deployment\n    client.go:310: [debug] Starting delete for \"jaeger\" Deployment\n    client.go:310: [debug] Starting delete for \"prefix-service\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:339: [debug] deployments.apps \"kubeslice-operator\" not found\n    client.go:339: [debug] deployments.apps \"nsm-admission-webhook\" not found\n    client.go:339: [debug] deployments.apps \"jaeger\" not found\n    client.go:339: [debug] deployments.apps \"kubeslice-dns\" not found\n    client.go:339: [debug] deployments.apps \"prefix-service\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsm-kernel-forwarder\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:339: [debug] daemonsets.apps \"nsmgr\" not found\n    client.go:339: [debug] daemonsets.apps \"kubeslice-netop\" not found\n    client.go:339: [debug] daemonsets.apps \"nsm-kernel-forwarder\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:339: [debug] rolebindings.rbac.authorization.k8s.io \"kubeslice-leader-election-rolebinding\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:339: [debug] roles.rbac.authorization.k8s.io \"kubeslice-leader-election-role\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-kubernetes-dashboard\" not found\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-dns-rolebinding\" not found\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-proxy-rolebinding\" not found\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"nsm-role-binding\" not found\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-manager-rolebinding\" not found\n    client.go:310: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"nsm-role\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"aggregate-network-services-view\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-metrics-reader\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-proxy-role\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-kubernetes-dashboard\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-manager-role\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-dns-role\" not found\n    client.go:310: [debug] Starting delete for \"networkservices.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkserviceendpoints.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservicemanagers.networkservicemesh.io\" CustomResourceDefinition\n    client.go:339: [debug] customresourcedefinitions.apiextensions.k8s.io \"networkserviceendpoints.networkservicemesh.io\" not found\n    client.go:339: [debug] customresourcedefinitions.apiextensions.k8s.io \"networkservicemanagers.networkservicemesh.io\" not found\n    client.go:339: [debug] customresourcedefinitions.apiextensions.k8s.io \"networkservices.networkservicemesh.io\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:339: [debug] configmaps \"kubeslice-manager-config\" not found\n    client.go:339: [debug] configmaps \"nsm-config\" not found\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-image-pull-secret\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:339: [debug] secrets \"kubeslice-hub\" not found\n    client.go:339: [debug] secrets \"kubeslice-image-pull-secret\" not found\n    client.go:339: [debug] secrets \"nsm-admission-webhook-certs\" not found\n    client.go:339: [debug] secrets \"kubeslice-admission-webhook-certs\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:339: [debug] serviceaccounts \"kubeslice-controller-manager\" not found\n    client.go:339: [debug] serviceaccounts \"nsmgr-acc\" not found\n    client.go:339: [debug] serviceaccounts \"kubeslice-kubernetes-dashboard\" not found\n    client.go:339: [debug] serviceaccounts \"kubeslice-dns\" not found\n    client.go:339: [debug] serviceaccounts \"nsc-acc\" not found\n    client.go:339: [debug] serviceaccounts \"nse-acc\" not found\n    client.go:339: [debug] serviceaccounts \"forward-plane-acc\" not found\n    client.go:339: [debug] serviceaccounts \"vpn-gateway-client\" not found\n    client.go:339: [debug] serviceaccounts \"vpn-gateway-server\" not found\n    client.go:339: [debug] serviceaccounts \"slice-router\" not found\n    client.go:339: [debug] serviceaccounts \"kubeslice-netop\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-cfg\" MutatingWebhookConfiguration\n    client.go:339: [debug] mutatingwebhookconfigurations.admissionregistration.k8s.io \"kubeslice-mutating-webhook-configuration\" not found\n    client.go:339: [debug] mutatingwebhookconfigurations.admissionregistration.k8s.io \"nsm-admission-webhook-cfg\" not found\n    uninstall.go:148: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: failed pre-install: timed out waiting for the condition\n    helm.go:84: [debug] failed pre-install: timed out waiting for the condition\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:361\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:141\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\noccurred","time":{"start":1667385281000,"stop":1667385652563,"duration":371563}},{"uid":"c284804455be4a69","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000128090>: {\n        Underlying: <*exec.ExitError | 0xc00072c000>{\n            ProcessState: {\n                pid: 6185,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 2, Usec: 52700},\n                    Stime: {Sec: 0, Usec: 244745},\n                    Maxrss: 104004,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 16755,\n                    Majflt: 2,\n                    Nswap: 0,\n                    Inblock: 240,\n                    Oublock: 20864,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 5019,\n                    Nivcsw: 5350,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"install.go:192: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.4.4.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:165: [debug] Clearing discovery cache\",\n                        \"wait.go:66: [debug] beginning wait for 7 resources with timeout of 1m0s\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:310: [debug] Starting delete for \\\"kubeslice-preinstall\\\" ServiceAccount\",\n                        \"client.go:339: [debug] serviceaccounts \\\"kubeslice-preinstall\\\" not found\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:310: [debug] Starting delete for \\\"kubeslice-preinstall-role\\\" ClusterRole\",\n                        \"client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \\\"kubeslice-preinstall-role\\\" not found\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:310: [debug] Starting delete for \\\"kubeslice-preinstall-rolebinding\\\" ClusterRoleBinding\",\n                        \"client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \\\"kubeslice-preinstall-rolebinding\\\" not found\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:310: [debug] Starting delete for \\\"kubeslice-worker-preinstall-configmap\\\" ConfigMap\",\n                        \"client.go:339: [debug] configmaps \\\"kubeslice-worker-preinstall-configmap\\\" not found\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:310: [debug] Starting delete for \\\"kubeslice-worker-preinstall-job\\\" Job\",\n                        \"client.go:339: [debug] jobs.batch \\\"kubeslice-worker-preinstall-job\\\" not found\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:540: [debug] Watching for changes to Job kubeslice-worker-preinstall-job with timeout of 5m0s\",\n                        \"client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: ADDED\",\n                        \"client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 0, jobs failed: 0, jobs succeeded: 0\",\n                        \"client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\",\n                        \"cl...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; install.go:192: [debug] Original chart version: \"\"\n    install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.4.4.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:165: [debug] Clearing discovery cache\n    wait.go:66: [debug] beginning wait for 7 resources with timeout of 1m0s\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:310: [debug] Starting delete for \"kubeslice-preinstall\" ServiceAccount\n    client.go:339: [debug] serviceaccounts \"kubeslice-preinstall\" not found\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:310: [debug] Starting delete for \"kubeslice-preinstall-role\" ClusterRole\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-preinstall-role\" not found\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:310: [debug] Starting delete for \"kubeslice-preinstall-rolebinding\" ClusterRoleBinding\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-preinstall-rolebinding\" not found\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:310: [debug] Starting delete for \"kubeslice-worker-preinstall-configmap\" ConfigMap\n    client.go:339: [debug] configmaps \"kubeslice-worker-preinstall-configmap\" not found\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:310: [debug] Starting delete for \"kubeslice-worker-preinstall-job\" Job\n    client.go:339: [debug] jobs.batch \"kubeslice-worker-preinstall-job\" not found\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:540: [debug] Watching for changes to Job kubeslice-worker-preinstall-job with timeout of 5m0s\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: ADDED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 0, jobs failed: 0, jobs succeeded: 0\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 1, jobs succeeded: 0\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 2, jobs succeeded: 0\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 3, jobs succeeded: 0\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 4, jobs succeeded: 0\n    client.go:310: [debug] Starting delete for \"kubeslice-worker-preinstall-job\" Job\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:310: [debug] Starting delete for \"jaeger\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-svc\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:339: [debug] services \"kubeslice-webhook-service\" not found\n    client.go:339: [debug] services \"nsm-admission-webhook-svc\" not found\n    client.go:339: [debug] services \"jaeger\" not found\n    client.go:339: [debug] services \"kubeslice-dns\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook\" Deployment\n    client.go:310: [debug] Starting delete for \"jaeger\" Deployment\n    client.go:310: [debug] Starting delete for \"prefix-service\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:339: [debug] deployments.apps \"kubeslice-operator\" not found\n    client.go:339: [debug] deployments.apps \"nsm-admission-webhook\" not found\n    client.go:339: [debug] deployments.apps \"kubeslice-dns\" not found\n    client.go:339: [debug] deployments.apps \"prefix-service\" not found\n    client.go:339: [debug] deployments.apps \"jaeger\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsm-kernel-forwarder\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:339: [debug] daemonsets.apps \"nsm-kernel-forwarder\" not found\n    client.go:339: [debug] daemonsets.apps \"kubeslice-netop\" not found\n    client.go:339: [debug] daemonsets.apps \"nsmgr\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:339: [debug] rolebindings.rbac.authorization.k8s.io \"kubeslice-leader-election-rolebinding\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:339: [debug] roles.rbac.authorization.k8s.io \"kubeslice-leader-election-role\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-kubernetes-dashboard\" not found\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"nsm-role-binding\" not found\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-proxy-rolebinding\" not found\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-manager-rolebinding\" not found\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-dns-rolebinding\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRole\n    client.go:310: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-kubernetes-dashboard\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-proxy-role\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-manager-role\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"nsm-role\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"aggregate-network-services-view\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-dns-role\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-metrics-reader\" not found\n    client.go:310: [debug] Starting delete for \"networkservices.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkserviceendpoints.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservicemanagers.networkservicemesh.io\" CustomResourceDefinition\n    client.go:339: [debug] customresourcedefinitions.apiextensions.k8s.io \"networkserviceendpoints.networkservicemesh.io\" not found\n    client.go:339: [debug] customresourcedefinitions.apiextensions.k8s.io \"networkservicemanagers.networkservicemesh.io\" not found\n    client.go:339: [debug] customresourcedefinitions.apiextensions.k8s.io \"networkservices.networkservicemesh.io\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:339: [debug] configmaps \"kubeslice-manager-config\" not found\n    client.go:339: [debug] configmaps \"nsm-config\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-image-pull-secret\" Secret\n    client.go:339: [debug] secrets \"kubeslice-admission-webhook-certs\" not found\n    client.go:339: [debug] secrets \"kubeslice-hub\" not found\n    client.go:339: [debug] secrets \"nsm-admission-webhook-certs\" not found\n    client.go:339: [debug] secrets \"kubeslice-image-pull-secret\" not found\n    client.go:310: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:339: [debug] serviceaccounts \"nse-acc\" not found\n    client.go:339: [debug] serviceaccounts \"nsc-acc\" not found\n    client.go:339: [debug] serviceaccounts \"kubeslice-kubernetes-dashboard\" not found\n    client.go:339: [debug] serviceaccounts \"kubeslice-controller-manager\" not found\n    client.go:339: [debug] serviceaccounts \"nsmgr-acc\" not found\n    client.go:339: [debug] serviceaccounts \"forward-plane-acc\" not found\n    client.go:339: [debug] serviceaccounts \"vpn-gateway-server\" not found\n    client.go:339: [debug] serviceaccounts \"kubeslice-dns\" not found\n    client.go:339: [debug] serviceaccounts \"vpn-gateway-client\" not found\n    client.go:339: [debug] serviceaccounts \"kubeslice-netop\" not found\n    client.go:339: [debug] serviceaccounts \"slice-router\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-cfg\" MutatingWebhookConfiguration\n    client.go:339: [debug] mutatingwebhookconfigurations.admissionregistration.k8s.io \"kubeslice-mutating-webhook-configuration\" not found\n    client.go:339: [debug] mutatingwebhookconfigurations.admissionregistration.k8s.io \"nsm-admission-webhook-cfg\" not found\n    uninstall.go:148: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: failed pre-install: timed out waiting for the condition\n    helm.go:84: [debug] failed pre-install: timed out waiting for the condition\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:361\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:141\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\noccurred","time":{"start":1667293744000,"stop":1667294105909,"duration":361909}},{"uid":"af00253644872206","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004a2060>: {\n        Underlying: <*exec.ExitError | 0xc0007a4000>{\n            ProcessState: {\n                pid: 6342,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 2, Usec: 896775},\n                    Stime: {Sec: 0, Usec: 226880},\n                    Maxrss: 87640,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 19350,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14816,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 5380,\n                    Nivcsw: 3685,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"install.go:192: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.5.6.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:165: [debug] Clearing discovery cache\",\n                        \"wait.go:66: [debug] beginning wait for 5 resources with timeout of 1m0s\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:310: [debug] Starting delete for \\\"kubeslice-preinstall\\\" ServiceAccount\",\n                        \"client.go:339: [debug] serviceaccounts \\\"kubeslice-preinstall\\\" not found\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:310: [debug] Starting delete for \\\"kubeslice-preinstall-role\\\" ClusterRole\",\n                        \"client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \\\"kubeslice-preinstall-role\\\" not found\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:310: [debug] Starting delete for \\\"kubeslice-preinstall-rolebinding\\\" ClusterRoleBinding\",\n                        \"client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \\\"kubeslice-preinstall-rolebinding\\\" not found\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:310: [debug] Starting delete for \\\"kubeslice-worker-preinstall-configmap\\\" ConfigMap\",\n                        \"client.go:339: [debug] configmaps \\\"kubeslice-worker-preinstall-configmap\\\" not found\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:310: [debug] Starting delete for \\\"kubeslice-worker-preinstall-job\\\" Job\",\n                        \"client.go:339: [debug] jobs.batch \\\"kubeslice-worker-preinstall-job\\\" not found\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:540: [debug] Watching for changes to Job kubeslice-worker-preinstall-job with timeout of 5m0s\",\n                        \"client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: ADDED\",\n                        \"client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 0, jobs failed: 0, jobs succeeded: 0\",\n                        \"client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\",\n                        \"client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\",\n                        \"client.go:568: [d...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; install.go:192: [debug] Original chart version: \"\"\n    install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.5.6.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:165: [debug] Clearing discovery cache\n    wait.go:66: [debug] beginning wait for 5 resources with timeout of 1m0s\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:310: [debug] Starting delete for \"kubeslice-preinstall\" ServiceAccount\n    client.go:339: [debug] serviceaccounts \"kubeslice-preinstall\" not found\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:310: [debug] Starting delete for \"kubeslice-preinstall-role\" ClusterRole\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-preinstall-role\" not found\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:310: [debug] Starting delete for \"kubeslice-preinstall-rolebinding\" ClusterRoleBinding\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-preinstall-rolebinding\" not found\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:310: [debug] Starting delete for \"kubeslice-worker-preinstall-configmap\" ConfigMap\n    client.go:339: [debug] configmaps \"kubeslice-worker-preinstall-configmap\" not found\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:310: [debug] Starting delete for \"kubeslice-worker-preinstall-job\" Job\n    client.go:339: [debug] jobs.batch \"kubeslice-worker-preinstall-job\" not found\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:540: [debug] Watching for changes to Job kubeslice-worker-preinstall-job with timeout of 5m0s\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: ADDED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 0, jobs failed: 0, jobs succeeded: 0\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 1, jobs succeeded: 0\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 2, jobs succeeded: 0\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 3, jobs succeeded: 0\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 4, jobs succeeded: 0\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 5, jobs succeeded: 0\n    client.go:310: [debug] Starting delete for \"kubeslice-worker-preinstall-job\" Job\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:310: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-svc\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:310: [debug] Starting delete for \"jaeger\" Service\n    client.go:339: [debug] services \"kubeslice-webhook-service\" not found\n    client.go:339: [debug] services \"kubeslice-dns\" not found\n    client.go:339: [debug] services \"jaeger\" not found\n    client.go:339: [debug] services \"nsm-admission-webhook-svc\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook\" Deployment\n    client.go:310: [debug] Starting delete for \"prefix-service\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:310: [debug] Starting delete for \"jaeger\" Deployment\n    client.go:339: [debug] deployments.apps \"kubeslice-operator\" not found\n    client.go:339: [debug] deployments.apps \"kubeslice-dns\" not found\n    client.go:339: [debug] deployments.apps \"jaeger\" not found\n    client.go:339: [debug] deployments.apps \"prefix-service\" not found\n    client.go:339: [debug] deployments.apps \"nsm-admission-webhook\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsm-kernel-forwarder\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:339: [debug] daemonsets.apps \"kubeslice-netop\" not found\n    client.go:339: [debug] daemonsets.apps \"nsmgr\" not found\n    client.go:339: [debug] daemonsets.apps \"nsm-kernel-forwarder\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:339: [debug] rolebindings.rbac.authorization.k8s.io \"kubeslice-leader-election-rolebinding\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:339: [debug] roles.rbac.authorization.k8s.io \"kubeslice-leader-election-role\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"nsm-role-binding\" not found\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-dns-rolebinding\" not found\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-proxy-rolebinding\" not found\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-kubernetes-dashboard\" not found\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-manager-rolebinding\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRole\n    client.go:310: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-proxy-role\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-kubernetes-dashboard\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-metrics-reader\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-manager-role\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"aggregate-network-services-view\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"nsm-role\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-dns-role\" not found\n    client.go:310: [debug] Starting delete for \"networkservices.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservicemanagers.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkserviceendpoints.networkservicemesh.io\" CustomResourceDefinition\n    client.go:339: [debug] customresourcedefinitions.apiextensions.k8s.io \"networkservicemanagers.networkservicemesh.io\" not found\n    client.go:339: [debug] customresourcedefinitions.apiextensions.k8s.io \"networkserviceendpoints.networkservicemesh.io\" not found\n    client.go:339: [debug] customresourcedefinitions.apiextensions.k8s.io \"networkservices.networkservicemesh.io\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:339: [debug] configmaps \"kubeslice-manager-config\" not found\n    client.go:339: [debug] configmaps \"nsm-config\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-image-pull-secret\" Secret\n    client.go:339: [debug] secrets \"nsm-admission-webhook-certs\" not found\n    client.go:339: [debug] secrets \"kubeslice-image-pull-secret\" not found\n    client.go:339: [debug] secrets \"kubeslice-admission-webhook-certs\" not found\n    client.go:339: [debug] secrets \"kubeslice-hub\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ServiceAccount\n    client.go:339: [debug] serviceaccounts \"kubeslice-controller-manager\" not found\n    client.go:339: [debug] serviceaccounts \"vpn-gateway-client\" not found\n    client.go:339: [debug] serviceaccounts \"kubeslice-kubernetes-dashboard\" not found\n    client.go:339: [debug] serviceaccounts \"forward-plane-acc\" not found\n    client.go:339: [debug] serviceaccounts \"kubeslice-dns\" not found\n    client.go:339: [debug] serviceaccounts \"slice-router\" not found\n    client.go:339: [debug] serviceaccounts \"nse-acc\" not found\n    client.go:339: [debug] serviceaccounts \"nsc-acc\" not found\n    client.go:339: [debug] serviceaccounts \"vpn-gateway-server\" not found\n    client.go:339: [debug] serviceaccounts \"kubeslice-netop\" not found\n    client.go:339: [debug] serviceaccounts \"nsmgr-acc\" not found\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-cfg\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:339: [debug] mutatingwebhookconfigurations.admissionregistration.k8s.io \"nsm-admission-webhook-cfg\" not found\n    client.go:339: [debug] mutatingwebhookconfigurations.admissionregistration.k8s.io \"kubeslice-mutating-webhook-configuration\" not found\n    uninstall.go:148: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: failed pre-install: timed out waiting for the condition\n    helm.go:84: [debug] failed pre-install: timed out waiting for the condition\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:361\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:141\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\noccurred","time":{"start":1666883986000,"stop":1666884409835,"duration":423835}},{"uid":"68bd6ea327dbc9bf","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004be7b0>: {\n        Underlying: <*exec.ExitError | 0xc0004a4640>{\n            ProcessState: {\n                pid: 6261,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 51520},\n                    Stime: {Sec: 0, Usec: 13738},\n                    Maxrss: 40936,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 1954,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 192,\n                    Nivcsw: 102,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"install.go:192: [debug] Original chart version: \\\"\\\"\",\n                        \"Error: INSTALLATION FAILED: looks like \\\"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages\\\" is not a valid chart repository or cannot be reached: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\",\n                        \"helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\",\n                        \"helm.sh/helm/v3/pkg/repo.(*ChartRepository).DownloadIndexFile\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:128\",\n                        \"helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:252\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:723\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:204\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:139\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:872\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:990\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:918\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:250\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1571\",\n                        \"looks like \\\"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages\\\" is not a valid chart repository or cannot be reached\",\n                        \"helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:254\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:723\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:204\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:139\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n           ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; install.go:192: [debug] Original chart version: \"\"\n    Error: INSTALLATION FAILED: looks like \"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages\" is not a valid chart repository or cannot be reached: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\n    helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\n    helm.sh/helm/v3/pkg/repo.(*ChartRepository).DownloadIndexFile\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:128\n    helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:252\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:723\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:204\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:139\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    looks like \"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages\" is not a valid chart repository or cannot be reached\n    helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:254\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:723\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:204\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:139\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:141\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\noccurred","time":{"start":1666754211000,"stop":1666754280721,"duration":69721}},{"uid":"e6dcdc36b131c9c7","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000455788>: {\n        Underlying: <*exec.ExitError | 0xc0000f3b40>{\n            ProcessState: {\n                pid: 6289,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 53323},\n                    Stime: {Sec: 0, Usec: 3808},\n                    Maxrss: 40404,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2433,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 181,\n                    Nivcsw: 77,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"install.go:192: [debug] Original chart version: \\\"\\\"\",\n                        \"Error: INSTALLATION FAILED: looks like \\\"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/\\\" is not a valid chart repository or cannot be reached: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\",\n                        \"helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\",\n                        \"helm.sh/helm/v3/pkg/repo.(*ChartRepository).DownloadIndexFile\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:128\",\n                        \"helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:252\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:723\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:204\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:139\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:872\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:990\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:918\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:250\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1571\",\n                        \"looks like \\\"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/\\\" is not a valid chart repository or cannot be reached\",\n                        \"helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:254\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:723\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:204\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:139\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n           ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; install.go:192: [debug] Original chart version: \"\"\n    Error: INSTALLATION FAILED: looks like \"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/\" is not a valid chart repository or cannot be reached: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\n    helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\n    helm.sh/helm/v3/pkg/repo.(*ChartRepository).DownloadIndexFile\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:128\n    helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:252\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:723\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:204\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:139\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    looks like \"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/\" is not a valid chart repository or cannot be reached\n    helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:254\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:723\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:204\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:139\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:141\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\noccurred","time":{"start":1666750446000,"stop":1666750520761,"duration":74761}},{"uid":"34769ef9da8aa68a","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004a8ff0>: {\n        Underlying: <*exec.ExitError | 0xc00044b500>{\n            ProcessState: {\n                pid: 6288,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 44185},\n                    Stime: {Sec: 0, Usec: 17674},\n                    Maxrss: 41000,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 1967,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 154,\n                    Nivcsw: 338,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"install.go:192: [debug] Original chart version: \\\"\\\"\",\n                        \"Error: INSTALLATION FAILED: looks like \\\"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/\\\" is not a valid chart repository or cannot be reached: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\",\n                        \"helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\",\n                        \"helm.sh/helm/v3/pkg/repo.(*ChartRepository).DownloadIndexFile\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:128\",\n                        \"helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:252\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:723\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:204\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:139\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:872\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:990\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:918\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:250\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1571\",\n                        \"looks like \\\"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/\\\" is not a valid chart repository or cannot be reached\",\n                        \"helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:254\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:723\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:204\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:139\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n         ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; install.go:192: [debug] Original chart version: \"\"\n    Error: INSTALLATION FAILED: looks like \"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/\" is not a valid chart repository or cannot be reached: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\n    helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\n    helm.sh/helm/v3/pkg/repo.(*ChartRepository).DownloadIndexFile\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:128\n    helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:252\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:723\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:204\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:139\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    looks like \"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/\" is not a valid chart repository or cannot be reached\n    helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:254\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:723\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:204\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:139\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:141\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\noccurred","time":{"start":1666747897000,"stop":1666747977149,"duration":80149}},{"uid":"b2c1c13abd69afd7","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000e0a8>: {\n        Underlying: <*exec.ExitError | 0xc0007a6040>{\n            ProcessState: {\n                pid: 6348,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 2, Usec: 87409},\n                    Stime: {Sec: 0, Usec: 218027},\n                    Maxrss: 83220,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 15533,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14816,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 4470,\n                    Nivcsw: 3063,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"install.go:192: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.5.6.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:165: [debug] Clearing discovery cache\",\n                        \"wait.go:66: [debug] beginning wait for 5 resources with timeout of 1m0s\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:310: [debug] Starting delete for \\\"kubeslice-preinstall\\\" ServiceAccount\",\n                        \"client.go:339: [debug] serviceaccounts \\\"kubeslice-preinstall\\\" not found\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:310: [debug] Starting delete for \\\"kubeslice-preinstall-role\\\" ClusterRole\",\n                        \"client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \\\"kubeslice-preinstall-role\\\" not found\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:310: [debug] Starting delete for \\\"kubeslice-preinstall-rolebinding\\\" ClusterRoleBinding\",\n                        \"client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \\\"kubeslice-preinstall-rolebinding\\\" not found\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:310: [debug] Starting delete for \\\"kubeslice-worker-preinstall-configmap\\\" ConfigMap\",\n                        \"client.go:339: [debug] configmaps \\\"kubeslice-worker-preinstall-configmap\\\" not found\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:310: [debug] Starting delete for \\\"kubeslice-worker-preinstall-job\\\" Job\",\n                        \"client.go:339: [debug] jobs.batch \\\"kubeslice-worker-preinstall-job\\\" not found\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:540: [debug] Watching for changes to Job kubeslice-worker-preinstall-job with timeout of 5m0s\",\n                        \"client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: ADDED\",\n                        \"client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 0, jobs failed: 0, jobs succeeded: 0\",\n                        \"client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\",\n                        \"client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\",\n                        \"client.go:568: [de...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; install.go:192: [debug] Original chart version: \"\"\n    install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.5.6.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:165: [debug] Clearing discovery cache\n    wait.go:66: [debug] beginning wait for 5 resources with timeout of 1m0s\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:310: [debug] Starting delete for \"kubeslice-preinstall\" ServiceAccount\n    client.go:339: [debug] serviceaccounts \"kubeslice-preinstall\" not found\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:310: [debug] Starting delete for \"kubeslice-preinstall-role\" ClusterRole\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-preinstall-role\" not found\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:310: [debug] Starting delete for \"kubeslice-preinstall-rolebinding\" ClusterRoleBinding\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-preinstall-rolebinding\" not found\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:310: [debug] Starting delete for \"kubeslice-worker-preinstall-configmap\" ConfigMap\n    client.go:339: [debug] configmaps \"kubeslice-worker-preinstall-configmap\" not found\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:310: [debug] Starting delete for \"kubeslice-worker-preinstall-job\" Job\n    client.go:339: [debug] jobs.batch \"kubeslice-worker-preinstall-job\" not found\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:540: [debug] Watching for changes to Job kubeslice-worker-preinstall-job with timeout of 5m0s\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: ADDED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 0, jobs failed: 0, jobs succeeded: 0\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 1, jobs succeeded: 0\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 2, jobs succeeded: 0\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 3, jobs succeeded: 0\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 4, jobs succeeded: 0\n    client.go:310: [debug] Starting delete for \"kubeslice-worker-preinstall-job\" Job\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:310: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"jaeger\" Service\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-svc\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:339: [debug] services \"kubeslice-webhook-service\" not found\n    client.go:339: [debug] services \"jaeger\" not found\n    client.go:339: [debug] services \"kubeslice-dns\" not found\n    client.go:339: [debug] services \"nsm-admission-webhook-svc\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook\" Deployment\n    client.go:310: [debug] Starting delete for \"jaeger\" Deployment\n    client.go:310: [debug] Starting delete for \"prefix-service\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:339: [debug] deployments.apps \"kubeslice-operator\" not found\n    client.go:339: [debug] deployments.apps \"kubeslice-dns\" not found\n    client.go:339: [debug] deployments.apps \"nsm-admission-webhook\" not found\n    client.go:339: [debug] deployments.apps \"jaeger\" not found\n    client.go:339: [debug] deployments.apps \"prefix-service\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsm-kernel-forwarder\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:339: [debug] daemonsets.apps \"kubeslice-netop\" not found\n    client.go:339: [debug] daemonsets.apps \"nsmgr\" not found\n    client.go:339: [debug] daemonsets.apps \"nsm-kernel-forwarder\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:339: [debug] rolebindings.rbac.authorization.k8s.io \"kubeslice-leader-election-rolebinding\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:339: [debug] roles.rbac.authorization.k8s.io \"kubeslice-leader-election-role\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-dns-rolebinding\" not found\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-kubernetes-dashboard\" not found\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-manager-rolebinding\" not found\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"nsm-role-binding\" not found\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-proxy-rolebinding\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"aggregate-network-services-view\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-dns-role\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-manager-role\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-kubernetes-dashboard\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-proxy-role\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"nsm-role\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-metrics-reader\" not found\n    client.go:310: [debug] Starting delete for \"networkservices.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkserviceendpoints.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservicemanagers.networkservicemesh.io\" CustomResourceDefinition\n    client.go:339: [debug] customresourcedefinitions.apiextensions.k8s.io \"networkserviceendpoints.networkservicemesh.io\" not found\n    client.go:339: [debug] customresourcedefinitions.apiextensions.k8s.io \"networkservices.networkservicemesh.io\" not found\n    client.go:339: [debug] customresourcedefinitions.apiextensions.k8s.io \"networkservicemanagers.networkservicemesh.io\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:339: [debug] configmaps \"nsm-config\" not found\n    client.go:339: [debug] configmaps \"kubeslice-manager-config\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-image-pull-secret\" Secret\n    client.go:339: [debug] secrets \"kubeslice-admission-webhook-certs\" not found\n    client.go:339: [debug] secrets \"kubeslice-image-pull-secret\" not found\n    client.go:339: [debug] secrets \"nsm-admission-webhook-certs\" not found\n    client.go:339: [debug] secrets \"kubeslice-hub\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:339: [debug] serviceaccounts \"slice-router\" not found\n    client.go:339: [debug] serviceaccounts \"nse-acc\" not found\n    client.go:339: [debug] serviceaccounts \"kubeslice-controller-manager\" not found\n    client.go:339: [debug] serviceaccounts \"nsc-acc\" not found\n    client.go:339: [debug] serviceaccounts \"kubeslice-kubernetes-dashboard\" not found\n    client.go:339: [debug] serviceaccounts \"kubeslice-netop\" not found\n    client.go:339: [debug] serviceaccounts \"vpn-gateway-server\" not found\n    client.go:339: [debug] serviceaccounts \"forward-plane-acc\" not found\n    client.go:339: [debug] serviceaccounts \"nsmgr-acc\" not found\n    client.go:339: [debug] serviceaccounts \"kubeslice-dns\" not found\n    client.go:339: [debug] serviceaccounts \"vpn-gateway-client\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-cfg\" MutatingWebhookConfiguration\n    client.go:339: [debug] mutatingwebhookconfigurations.admissionregistration.k8s.io \"nsm-admission-webhook-cfg\" not found\n    client.go:339: [debug] mutatingwebhookconfigurations.admissionregistration.k8s.io \"kubeslice-mutating-webhook-configuration\" not found\n    uninstall.go:148: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: failed pre-install: timed out waiting for the condition\n    helm.go:84: [debug] failed pre-install: timed out waiting for the condition\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:361\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:141\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\noccurred","time":{"start":1666706731000,"stop":1666707135483,"duration":404483}},{"uid":"dddf74b986714d21","status":"passed","time":{"start":1666674484000,"stop":1666674589437,"duration":105437}},{"uid":"757001286de8d90","status":"passed","time":{"start":1666673362000,"stop":1666673442347,"duration":80347}},{"uid":"341390dc3d91fc4","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000e0f0>: {\n        Underlying: <*exec.ExitError | 0xc0001668a0>{\n            ProcessState: {\n                pid: 6250,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 45106},\n                    Stime: {Sec: 0, Usec: 15035},\n                    Maxrss: 39928,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2308,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 253,\n                    Nivcsw: 246,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:192: [debug] Original chart version: \\\"\\\"\",\n                        \"Error: INSTALLATION FAILED: looks like \\\"https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/\\\" is not a valid chart repository or cannot be reached: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/index.yaml : 404 Not Found\",\n                        \"helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/index.yaml : 404 Not Found\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\",\n                        \"helm.sh/helm/v3/pkg/repo.(*ChartRepository).DownloadIndexFile\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:128\",\n                        \"helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:252\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:723\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:204\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:139\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:872\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:990\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:918\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:250\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1571\",\n                        \"looks like \\\"https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/\\\" is not a valid chart repository or cannot be reached\",\n                        \"helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:254\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:723\",\n                        \"main...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:192: [debug] Original chart version: \"\"\n    Error: INSTALLATION FAILED: looks like \"https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/\" is not a valid chart repository or cannot be reached: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/index.yaml : 404 Not Found\n    helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/index.yaml : 404 Not Found\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\n    helm.sh/helm/v3/pkg/repo.(*ChartRepository).DownloadIndexFile\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:128\n    helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:252\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:723\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:204\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:139\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    looks like \"https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/\" is not a valid chart repository or cannot be reached\n    helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:254\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:723\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:204\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:139\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:141\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\noccurred","time":{"start":1666668696000,"stop":1666668782021,"duration":86021}},{"uid":"c91e427d762d191b","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000128090>: {\n        Underlying: <*exec.ExitError | 0xc000774000>{\n            ProcessState: {\n                pid: 6107,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 3, Usec: 550293},\n                    Stime: {Sec: 0, Usec: 328211},\n                    Maxrss: 97520,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 15599,\n                    Majflt: 7,\n                    Nswap: 0,\n                    Inblock: 512,\n                    Oublock: 20168,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 5960,\n                    Nivcsw: 9372,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"install.go:192: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.4.3.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:165: [debug] Clearing discovery cache\",\n                        \"wait.go:66: [debug] beginning wait for 6 resources with timeout of 1m0s\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:310: [debug] Starting delete for \\\"kubeslice-preinstall\\\" ServiceAccount\",\n                        \"client.go:339: [debug] serviceaccounts \\\"kubeslice-preinstall\\\" not found\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:310: [debug] Starting delete for \\\"kubeslice-preinstall-role\\\" ClusterRole\",\n                        \"client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \\\"kubeslice-preinstall-role\\\" not found\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:310: [debug] Starting delete for \\\"kubeslice-preinstall-rolebinding\\\" ClusterRoleBinding\",\n                        \"client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \\\"kubeslice-preinstall-rolebinding\\\" not found\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:310: [debug] Starting delete for \\\"kubeslice-worker-preinstall-configmap\\\" ConfigMap\",\n                        \"client.go:339: [debug] configmaps \\\"kubeslice-worker-preinstall-configmap\\\" not found\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:310: [debug] Starting delete for \\\"kubeslice-worker-preinstall-job\\\" Job\",\n                        \"client.go:339: [debug] jobs.batch \\\"kubeslice-worker-preinstall-job\\\" not found\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:540: [debug] Watching for changes to Job kubeslice-worker-preinstall-job with timeout of 5m0s\",\n                        \"client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: ADDED\",\n                        \"client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 0, jobs failed: 0, jobs succeeded: 0\",\n                        \"client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\",\n                        \"client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, job...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; install.go:192: [debug] Original chart version: \"\"\n    install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.4.3.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:165: [debug] Clearing discovery cache\n    wait.go:66: [debug] beginning wait for 6 resources with timeout of 1m0s\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:310: [debug] Starting delete for \"kubeslice-preinstall\" ServiceAccount\n    client.go:339: [debug] serviceaccounts \"kubeslice-preinstall\" not found\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:310: [debug] Starting delete for \"kubeslice-preinstall-role\" ClusterRole\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-preinstall-role\" not found\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:310: [debug] Starting delete for \"kubeslice-preinstall-rolebinding\" ClusterRoleBinding\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-preinstall-rolebinding\" not found\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:310: [debug] Starting delete for \"kubeslice-worker-preinstall-configmap\" ConfigMap\n    client.go:339: [debug] configmaps \"kubeslice-worker-preinstall-configmap\" not found\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:310: [debug] Starting delete for \"kubeslice-worker-preinstall-job\" Job\n    client.go:339: [debug] jobs.batch \"kubeslice-worker-preinstall-job\" not found\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:540: [debug] Watching for changes to Job kubeslice-worker-preinstall-job with timeout of 5m0s\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: ADDED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 0, jobs failed: 0, jobs succeeded: 0\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 1, jobs succeeded: 0\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 2, jobs succeeded: 0\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 3, jobs succeeded: 0\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 4, jobs succeeded: 0\n    client.go:310: [debug] Starting delete for \"kubeslice-worker-preinstall-job\" Job\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:310: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-svc\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:310: [debug] Starting delete for \"jaeger\" Service\n    client.go:339: [debug] services \"kubeslice-webhook-service\" not found\n    client.go:339: [debug] services \"kubeslice-dns\" not found\n    client.go:339: [debug] services \"nsm-admission-webhook-svc\" not found\n    client.go:339: [debug] services \"jaeger\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook\" Deployment\n    client.go:310: [debug] Starting delete for \"jaeger\" Deployment\n    client.go:310: [debug] Starting delete for \"prefix-service\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:339: [debug] deployments.apps \"kubeslice-operator\" not found\n    client.go:339: [debug] deployments.apps \"jaeger\" not found\n    client.go:339: [debug] deployments.apps \"nsm-admission-webhook\" not found\n    client.go:339: [debug] deployments.apps \"prefix-service\" not found\n    client.go:339: [debug] deployments.apps \"kubeslice-dns\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsm-kernel-forwarder\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:339: [debug] daemonsets.apps \"kubeslice-netop\" not found\n    client.go:339: [debug] daemonsets.apps \"nsmgr\" not found\n    client.go:339: [debug] daemonsets.apps \"nsm-kernel-forwarder\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:339: [debug] rolebindings.rbac.authorization.k8s.io \"kubeslice-leader-election-rolebinding\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:339: [debug] roles.rbac.authorization.k8s.io \"kubeslice-leader-election-role\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"nsm-role-binding\" not found\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-dns-rolebinding\" not found\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-proxy-rolebinding\" not found\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-manager-rolebinding\" not found\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-kubernetes-dashboard\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRole\n    client.go:310: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-metrics-reader\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"nsm-role\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-kubernetes-dashboard\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"aggregate-network-services-view\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-dns-role\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-proxy-role\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-manager-role\" not found\n    client.go:310: [debug] Starting delete for \"networkservices.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkserviceendpoints.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservicemanagers.networkservicemesh.io\" CustomResourceDefinition\n    client.go:339: [debug] customresourcedefinitions.apiextensions.k8s.io \"networkservicemanagers.networkservicemesh.io\" not found\n    client.go:339: [debug] customresourcedefinitions.apiextensions.k8s.io \"networkserviceendpoints.networkservicemesh.io\" not found\n    client.go:339: [debug] customresourcedefinitions.apiextensions.k8s.io \"networkservices.networkservicemesh.io\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:339: [debug] configmaps \"kubeslice-manager-config\" not found\n    client.go:339: [debug] configmaps \"nsm-config\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-image-pull-secret\" Secret\n    client.go:339: [debug] secrets \"nsm-admission-webhook-certs\" not found\n    client.go:339: [debug] secrets \"kubeslice-hub\" not found\n    client.go:339: [debug] secrets \"kubeslice-admission-webhook-certs\" not found\n    client.go:339: [debug] secrets \"kubeslice-image-pull-secret\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:339: [debug] serviceaccounts \"kubeslice-controller-manager\" not found\n    client.go:339: [debug] serviceaccounts \"nse-acc\" not found\n    client.go:339: [debug] serviceaccounts \"kubeslice-netop\" not found\n    client.go:339: [debug] serviceaccounts \"nsc-acc\" not found\n    client.go:339: [debug] serviceaccounts \"kubeslice-kubernetes-dashboard\" not found\n    client.go:339: [debug] serviceaccounts \"vpn-gateway-server\" not found\n    client.go:339: [debug] serviceaccounts \"nsmgr-acc\" not found\n    client.go:339: [debug] serviceaccounts \"kubeslice-dns\" not found\n    client.go:339: [debug] serviceaccounts \"forward-plane-acc\" not found\n    client.go:339: [debug] serviceaccounts \"slice-router\" not found\n    client.go:339: [debug] serviceaccounts \"vpn-gateway-client\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-cfg\" MutatingWebhookConfiguration\n    client.go:339: [debug] mutatingwebhookconfigurations.admissionregistration.k8s.io \"nsm-admission-webhook-cfg\" not found\n    client.go:339: [debug] mutatingwebhookconfigurations.admissionregistration.k8s.io \"kubeslice-mutating-webhook-configuration\" not found\n    uninstall.go:148: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: failed pre-install: timed out waiting for the condition\n    helm.go:84: [debug] failed pre-install: timed out waiting for the condition\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:361\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:141\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\noccurred","time":{"start":1666667460000,"stop":1666667845764,"duration":385764}},{"uid":"a28b1f12333c838d","status":"passed","time":{"start":1666664909000,"stop":1666665007521,"duration":98521}},{"uid":"8bf37e4b8a17dd45","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000ef18>: {\n        Underlying: <*exec.ExitError | 0xc00015e920>{\n            ProcessState: {\n                pid: 6111,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 66684},\n                    Stime: {Sec: 0, Usec: 16671},\n                    Maxrss: 40824,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 1963,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 2240,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 851,\n                    Nivcsw: 783,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"install.go:192: [debug] Original chart version: \\\"1.7.0\\\"\",\n                        \"Error: INSTALLATION FAILED: looks like \\\"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/\\\" is not a valid chart repository or cannot be reached: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\",\n                        \"helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\",\n                        \"helm.sh/helm/v3/pkg/repo.(*ChartRepository).DownloadIndexFile\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:128\",\n                        \"helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:252\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:723\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:204\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:139\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:872\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:990\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:918\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:250\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1571\",\n                        \"looks like \\\"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/\\\" is not a valid chart repository or cannot be reached\",\n                        \"helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:254\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:723\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:204\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:139\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; install.go:192: [debug] Original chart version: \"1.7.0\"\n    Error: INSTALLATION FAILED: looks like \"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/\" is not a valid chart repository or cannot be reached: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\n    helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\n    helm.sh/helm/v3/pkg/repo.(*ChartRepository).DownloadIndexFile\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:128\n    helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:252\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:723\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:204\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:139\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    looks like \"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/\" is not a valid chart repository or cannot be reached\n    helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:254\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:723\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:204\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:139\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:141\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\noccurred","time":{"start":1666664766000,"stop":1666664766570,"duration":570}},{"uid":"645a236f3a2d9ff7","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000e750>: {\n        Underlying: <*exec.ExitError | 0xc0004530e0>{\n            ProcessState: {\n                pid: 6000,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 82558},\n                    Stime: {Sec: 0, Usec: 36692},\n                    Maxrss: 40852,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2453,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 2240,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 1098,\n                    Nivcsw: 725,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"install.go:192: [debug] Original chart version: \\\"1.7.0\\\"\",\n                        \"Error: INSTALLATION FAILED: looks like \\\"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/\\\" is not a valid chart repository or cannot be reached: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\",\n                        \"helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\",\n                        \"helm.sh/helm/v3/pkg/repo.(*ChartRepository).DownloadIndexFile\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:128\",\n                        \"helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:252\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:723\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:204\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:139\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:872\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:990\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:918\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:250\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1571\",\n                        \"looks like \\\"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/\\\" is not a valid chart repository or cannot be reached\",\n                        \"helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:254\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:723\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:204\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:139\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; install.go:192: [debug] Original chart version: \"1.7.0\"\n    Error: INSTALLATION FAILED: looks like \"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/\" is not a valid chart repository or cannot be reached: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\n    helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\n    helm.sh/helm/v3/pkg/repo.(*ChartRepository).DownloadIndexFile\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:128\n    helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:252\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:723\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:204\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:139\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    looks like \"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/\" is not a valid chart repository or cannot be reached\n    helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:254\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:723\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:204\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:139\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:141\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\noccurred","time":{"start":1666663248000,"stop":1666663248894,"duration":894}},{"uid":"c1ff94863976f176","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000e0c0>: {\n        Underlying: <*exec.ExitError | 0xc0009ea000>{\n            ProcessState: {\n                pid: 6173,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 4, Usec: 33369},\n                    Stime: {Sec: 0, Usec: 869942},\n                    Maxrss: 110972,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 8135,\n                    Majflt: 8,\n                    Nswap: 0,\n                    Inblock: 1304,\n                    Oublock: 19504,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 50842,\n                    Nivcsw: 13581,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:192: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.4.6.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:165: [debug] Clearing discovery cache\",\n                        \"wait.go:66: [debug] beginning wait for 4 resources with timeout of 1m0s\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 47 resource(s)\",\n                        \"wait.go:66: [debug] beginning wait for 47 resources with timeout of 5m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admiss...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:192: [debug] Original chart version: \"\"\n    install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.4.6.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:165: [debug] Clearing discovery cache\n    wait.go:66: [debug] beginning wait for 4 resources with timeout of 1m0s\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 47 resource(s)\n    wait.go:66: [debug] beginning wait for 47 resources with timeout of 5m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-svc\" Service\n    client.go:310: [debug] Starting delete for \"jaeger\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:310: [debug] Starting delete for \"jaeger\" Deployment\n    client.go:310: [debug] Starting delete for \"prefix-service\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsm-kernel-forwarder\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRole\n    client.go:310: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"networkservices.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkserviceendpoints.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservicemanagers.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-cfg\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    uninstall.go:148: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:141\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\noccurred","time":{"start":1666423616000,"stop":1666423979092,"duration":363092}},{"uid":"79cc1bc06ad67c4","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004b82a0>: {\n        Underlying: <*exec.ExitError | 0xc00049e200>{\n            ProcessState: {\n                pid: 6236,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 43305},\n                    Stime: {Sec: 0, Usec: 14435},\n                    Maxrss: 40652,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 1962,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 248,\n                    Nivcsw: 145,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:192: [debug] Original chart version: \\\"\\\"\",\n                        \"Error: INSTALLATION FAILED: looks like \\\"https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/\\\" is not a valid chart repository or cannot be reached: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/index.yaml : 404 Not Found\",\n                        \"helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/index.yaml : 404 Not Found\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\",\n                        \"helm.sh/helm/v3/pkg/repo.(*ChartRepository).DownloadIndexFile\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:128\",\n                        \"helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:252\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:723\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:204\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:139\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:872\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:990\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:918\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:250\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1571\",\n                        \"looks like \\\"https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/\\\" is not a valid chart repository or cannot be reached\",\n                        \"helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:254\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:723\",\n                        \"main...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:192: [debug] Original chart version: \"\"\n    Error: INSTALLATION FAILED: looks like \"https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/\" is not a valid chart repository or cannot be reached: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/index.yaml : 404 Not Found\n    helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/index.yaml : 404 Not Found\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\n    helm.sh/helm/v3/pkg/repo.(*ChartRepository).DownloadIndexFile\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:128\n    helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:252\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:723\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:204\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:139\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    looks like \"https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/\" is not a valid chart repository or cannot be reached\n    helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:254\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:723\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:204\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:139\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:141\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\noccurred","time":{"start":1666419243000,"stop":1666419329633,"duration":86633}},{"uid":"d78f6105072d48dc","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00027e048>: {\n        Underlying: <*exec.ExitError | 0xc0006a2000>{\n            ProcessState: {\n                pid: 6218,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 4, Usec: 894819},\n                    Stime: {Sec: 0, Usec: 741351},\n                    Maxrss: 99340,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 8093,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 17224,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 52372,\n                    Nivcsw: 12679,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:192: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.4.6.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:165: [debug] Clearing discovery cache\",\n                        \"wait.go:66: [debug] beginning wait for 4 resources with timeout of 1m0s\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 47 resource(s)\",\n                        \"wait.go:66: [debug] beginning wait for 47 resources with timeout of 5m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:192: [debug] Original chart version: \"\"\n    install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.4.6.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:165: [debug] Clearing discovery cache\n    wait.go:66: [debug] beginning wait for 4 resources with timeout of 1m0s\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 47 resource(s)\n    wait.go:66: [debug] beginning wait for 47 resources with timeout of 5m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:310: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"jaeger\" Service\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-svc\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:310: [debug] Starting delete for \"jaeger\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook\" Deployment\n    client.go:310: [debug] Starting delete for \"prefix-service\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsm-kernel-forwarder\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRole\n    client.go:310: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"networkservices.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkserviceendpoints.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservicemanagers.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-cfg\" MutatingWebhookConfiguration\n    uninstall.go:148: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:141\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\noccurred","time":{"start":1666338675000,"stop":1666339034374,"duration":359374}},{"uid":"8a4b9c0c2778f969","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004bc1b0>: {\n        Underlying: <*exec.ExitError | 0xc00036c200>{\n            ProcessState: {\n                pid: 6222,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 39984},\n                    Stime: {Sec: 0, Usec: 15993},\n                    Maxrss: 40564,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 1951,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 249,\n                    Nivcsw: 76,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:192: [debug] Original chart version: \\\"\\\"\",\n                        \"Error: INSTALLATION FAILED: looks like \\\"https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/\\\" is not a valid chart repository or cannot be reached: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/index.yaml : 404 Not Found\",\n                        \"helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/index.yaml : 404 Not Found\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\",\n                        \"helm.sh/helm/v3/pkg/repo.(*ChartRepository).DownloadIndexFile\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:128\",\n                        \"helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:252\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:723\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:204\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:139\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:872\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:990\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:918\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:250\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1571\",\n                        \"looks like \\\"https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/\\\" is not a valid chart repository or cannot be reached\",\n                        \"helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:254\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:723\",\n                        \"main....\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:192: [debug] Original chart version: \"\"\n    Error: INSTALLATION FAILED: looks like \"https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/\" is not a valid chart repository or cannot be reached: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/index.yaml : 404 Not Found\n    helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/index.yaml : 404 Not Found\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\n    helm.sh/helm/v3/pkg/repo.(*ChartRepository).DownloadIndexFile\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:128\n    helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:252\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:723\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:204\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:139\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    looks like \"https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/\" is not a valid chart repository or cannot be reached\n    helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:254\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:723\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:204\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:139\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:141\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\noccurred","time":{"start":1666258902000,"stop":1666258920741,"duration":18741}},{"uid":"44c003500147408","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000e0a8>: {\n        Underlying: <*exec.ExitError | 0xc00048a020>{\n            ProcessState: {\n                pid: 6211,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 4, Usec: 545914},\n                    Stime: {Sec: 0, Usec: 919607},\n                    Maxrss: 94296,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 6044,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 17224,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 52638,\n                    Nivcsw: 11833,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:192: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.4.6.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:165: [debug] Clearing discovery cache\",\n                        \"wait.go:66: [debug] beginning wait for 4 resources with timeout of 1m0s\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 47 resource(s)\",\n                        \"wait.go:66: [debug] beginning wait for 47 resources with timeout of 5m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:192: [debug] Original chart version: \"\"\n    install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.4.6.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:165: [debug] Clearing discovery cache\n    wait.go:66: [debug] beginning wait for 4 resources with timeout of 1m0s\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 47 resource(s)\n    wait.go:66: [debug] beginning wait for 47 resources with timeout of 5m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:310: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:310: [debug] Starting delete for \"jaeger\" Service\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-svc\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:310: [debug] Starting delete for \"jaeger\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook\" Deployment\n    client.go:310: [debug] Starting delete for \"prefix-service\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsm-kernel-forwarder\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"networkservices.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkserviceendpoints.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservicemanagers.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-cfg\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    uninstall.go:148: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:141\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\noccurred","time":{"start":1666253957000,"stop":1666254315500,"duration":358500}},{"uid":"e70d15778b37824c","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004bc120>: {\n        Underlying: <*exec.ExitError | 0xc00036a200>{\n            ProcessState: {\n                pid: 6199,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 62575},\n                    Stime: {Sec: 0, Usec: 8343},\n                    Maxrss: 42636,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 1968,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 32,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 264,\n                    Nivcsw: 104,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:192: [debug] Original chart version: \\\"\\\"\",\n                        \"Error: INSTALLATION FAILED: looks like \\\"https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/\\\" is not a valid chart repository or cannot be reached: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/index.yaml : 404 Not Found\",\n                        \"helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/index.yaml : 404 Not Found\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\",\n                        \"helm.sh/helm/v3/pkg/repo.(*ChartRepository).DownloadIndexFile\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:128\",\n                        \"helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:252\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:723\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:204\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:139\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:872\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:990\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:918\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:250\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1571\",\n                        \"looks like \\\"https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/\\\" is not a valid chart repository or cannot be reached\",\n                        \"helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:254\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:723\",\n                        \"main...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:192: [debug] Original chart version: \"\"\n    Error: INSTALLATION FAILED: looks like \"https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/\" is not a valid chart repository or cannot be reached: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/index.yaml : 404 Not Found\n    helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/index.yaml : 404 Not Found\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\n    helm.sh/helm/v3/pkg/repo.(*ChartRepository).DownloadIndexFile\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:128\n    helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:252\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:723\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:204\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:139\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    looks like \"https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/\" is not a valid chart repository or cannot be reached\n    helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:254\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:723\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:204\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:139\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:141\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\noccurred","time":{"start":1666253081000,"stop":1666253106310,"duration":25310}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests Iperf App connectivity Verify Connectivity between iperf-server and iperf-client":{"statistic":{"failed":0,"broken":0,"skipped":3,"passed":0,"unknown":0,"total":3},"items":[{"uid":"8af4dab6d6f947","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674621000,"stop":1666674621000,"duration":0}},{"uid":"8753cd3b08bcb7af","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674212000,"stop":1666674212000,"duration":0}},{"uid":"7805020f88b3f0f7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666665366000,"stop":1666665366000,"duration":0}}]},"Empty Suite:Empty Suite#[It] Hub Creation tests Hub Installtion Test Senarios Should fail if Hub is installed again in the same cluster":{"statistic":{"failed":0,"broken":0,"skipped":3,"passed":8,"unknown":0,"total":11},"items":[{"uid":"e792a36d7fcf78cf","status":"passed","time":{"start":1666883811000,"stop":1666883814977,"duration":3977}},{"uid":"dcaf77d6b45865ae","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666754109000,"stop":1666754109000,"duration":0}},{"uid":"47878eddb234efd6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666750338000,"stop":1666750338000,"duration":0}},{"uid":"6b273de3091448f7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666747791000,"stop":1666747791000,"duration":0}},{"uid":"6a4771d068040c66","status":"passed","time":{"start":1666706575000,"stop":1666706577594,"duration":2594}},{"uid":"f786878ad4cc8479","status":"passed","time":{"start":1666077791000,"stop":1666077794637,"duration":3637}},{"uid":"4b68e88e3ae83824","status":"passed","time":{"start":1665944068000,"stop":1665944070736,"duration":2736}},{"uid":"77b9b280c0736c51","status":"passed","time":{"start":1665665806000,"stop":1665665809254,"duration":3254}},{"uid":"2a1a418d7648ae4e","status":"passed","time":{"start":1665665864000,"stop":1665665868992,"duration":4992}},{"uid":"7e4d26d002f77153","status":"passed","time":{"start":1665552534000,"stop":1665552537501,"duration":3501}},{"uid":"c68491335cba1a0a","status":"passed","time":{"start":1664798682000,"stop":1664798687418,"duration":5418}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests slice with netpol Should contain application namespaces":{"statistic":{"failed":0,"broken":0,"skipped":3,"passed":0,"unknown":0,"total":3},"items":[{"uid":"e55dbe285f60aa89","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674621000,"stop":1666674621000,"duration":0}},{"uid":"f2abe3b2b1676f3b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674212000,"stop":1666674212000,"duration":0}},{"uid":"c862e7d3fb3e3d7c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666665366000,"stop":1666665366000,"duration":0}}]},"Intracluster Suite:Intracluster Suite#[It] Intracluster slice tests Iperf connectivity Verify Iperf traffic":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":1,"unknown":0,"total":3},"items":[{"uid":"b009f60bf683ac0a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674484000,"stop":1666674484000,"duration":0}},{"uid":"15546daac538cbb6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666673362000,"stop":1666673362000,"duration":0}},{"uid":"248a82cfbcff4d6","status":"passed","time":{"start":1666664909000,"stop":1666664978880,"duration":69880}}]},"6d3ae00abbd534b9ffed53e43b8ae06d":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"d0e00832a80fa9e2","status":"passed","time":{"start":1664356969024,"stop":1664357288256,"duration":319232}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests Iperf App connectivity should have gateway pod running":{"statistic":{"failed":2,"broken":0,"skipped":1,"passed":0,"unknown":0,"total":3},"items":[{"uid":"86dc75443902fa08","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1666674621000,"stop":1666674802755,"duration":181755}},{"uid":"bab3a388a5ecb9bd","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674212000,"stop":1666674212000,"duration":0}},{"uid":"4e0bc44d3f73fee","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1666665366000,"stop":1666665546702,"duration":180702}}]},"Worker Suite:Worker Suite#[It] Slice Config Negative Scenario tests Slice Config Negative Scenario Update Should fail while updating sliceconfig with invalid values on slice subnet":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":3,"unknown":0,"total":3},"items":[{"uid":"cfa480eca67c09bd","status":"passed","time":{"start":1666674621000,"stop":1666674622259,"duration":1259}},{"uid":"748d0f2ea2f4eac0","status":"passed","time":{"start":1666674212000,"stop":1666674212766,"duration":766}},{"uid":"6e8fcd08ba97a742","status":"passed","time":{"start":1666665366000,"stop":1666665366986,"duration":986}}]},"Worker Suite:Worker Suite#[It] Slice Config Negative Scenario tests Slice Config Negative Scenario Creation Should fail while creating sliceconfig with with invalid DSCP class in qosprofiledetails":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":3,"unknown":0,"total":3},"items":[{"uid":"e9bd73558e3feca3","status":"passed","time":{"start":1666674621000,"stop":1666674621516,"duration":516}},{"uid":"a6c11be7e96cf2fc","status":"passed","time":{"start":1666674212000,"stop":1666674212493,"duration":493}},{"uid":"eac4bdbdc0152d87","status":"passed","time":{"start":1666665366000,"stop":1666665366352,"duration":352}}]},"Worker Suite:Worker Suite#[It] Slice creation scenarios: when slice red is installed install slice blue workers should have vl3 routers from both slices":{"statistic":{"failed":0,"broken":0,"skipped":3,"passed":0,"unknown":0,"total":3},"items":[{"uid":"a005e805a4b9d56b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674621000,"stop":1666674621000,"duration":0}},{"uid":"9177072624ebe51d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674212000,"stop":1666674212000,"duration":0}},{"uid":"a451ea5f6a43dee8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666665366000,"stop":1666665366000,"duration":0}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests Iperf App connectivity Should install iperf-sleep on client cluster":{"statistic":{"failed":0,"broken":0,"skipped":3,"passed":0,"unknown":0,"total":3},"items":[{"uid":"ad4b4288cc9f5e38","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674621000,"stop":1666674621000,"duration":0}},{"uid":"2099b56525520370","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674212000,"stop":1666674212000,"duration":0}},{"uid":"8d82d4dbf82bc739","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666665366000,"stop":1666665366000,"duration":0}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Update Should fail when Project is applied with service account name as blank in Write users":{"statistic":{"failed":2,"broken":0,"skipped":0,"passed":0,"unknown":0,"total":2},"items":[{"uid":"66f00ba1c6d81b44","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0001cf248>: {\n        Underlying: <*exec.ExitError | 0xc000829280>{\n            ProcessState: {\n                pid: 7667,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 201845},\n                    Stime: {Sec: 0, Usec: 60553},\n                    Maxrss: 77900,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 7913,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13760,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 445,\n                    Nivcsw: 917,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20applied%20with%20service%20account%20name%20as%20blank%20in%20Write%20users1600498833\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.94.3:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20applied%20with%20service%20account%20name%20as%20blank%20in%20Write%20users1600498833\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.94.3:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20applied%20with%20service%20account%20name%20as%20blank%20in%20Write%20users1600498833\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.94.3:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20applied%20with%20service%20account%20name%20as%20blank%20in%20Write%20users1600498833\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.94.3:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20applied%20with%20service%20account%20name%20as%20blank%20in%20Write%20users1600498833\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.94.3:443: connect: connection refused\noccurred","time":{"start":1665665955000,"stop":1665665955329,"duration":329}},{"uid":"c7904797d3391969","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000535c50>: {\n        Underlying: <*exec.ExitError | 0xc00077bc20>{\n            ProcessState: {\n                pid: 7243,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 219874},\n                    Stime: {Sec: 0, Usec: 82971},\n                    Maxrss: 89736,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 5453,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13760,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 348,\n                    Nivcsw: 284,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20applied%20with%20service%20account%20name%20as%20blank%20in%20Write%20users428461408\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.116.233:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20applied%20with%20service%20account%20name%20as%20blank%20in%20Write%20users428461408\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.116.233:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20applied%20with%20service%20account%20name%20as%20blank%20in%20Write%20users428461408\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.116.233:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20applied%20with%20service%20account%20name%20as%20blank%20in%20Write%20users428461408\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.116.233:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20applied%20with%20service%20account%20name%20as%20blank%20in%20Write%20users428461408\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.116.233:443: connect: connection refused\noccurred","time":{"start":1665666027000,"stop":1665666027386,"duration":386}}]},"13ebe318b750e99b58ecccc097a6ac75":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"3ed4c898ac054102","status":"passed","time":{"start":1664357313667,"stop":1664357345958,"duration":32291}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests slice with netpol should contain the allowed namespaces":{"statistic":{"failed":0,"broken":0,"skipped":3,"passed":0,"unknown":0,"total":3},"items":[{"uid":"9a8f311edefb50a5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674621000,"stop":1666674621000,"duration":0}},{"uid":"485aeafe466aec00","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674212000,"stop":1666674212000,"duration":0}},{"uid":"6c6fa4606e59af11","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666665366000,"stop":1666665366000,"duration":0}}]},"Empty Suite:Empty Suite#[It] Hub Creation tests Hub Installtion Test Senarios Should successfully pass if Cert is installed first and then Hub":{"statistic":{"failed":3,"broken":0,"skipped":0,"passed":8,"unknown":0,"total":11},"items":[{"uid":"96125eb2607d263f","status":"passed","time":{"start":1666883811000,"stop":1666883860192,"duration":49192}},{"uid":"c023445878a526ee","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000e858>: {\n        Underlying: <*exec.ExitError | 0xc0000784c0>{\n            ProcessState: {\n                pid: 6219,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 55605},\n                    Stime: {Sec: 0, Usec: 3971},\n                    Maxrss: 40616,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 1979,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 171,\n                    Nivcsw: 51,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"install.go:192: [debug] Original chart version: \\\"\\\"\",\n                        \"Error: INSTALLATION FAILED: looks like \\\"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages\\\" is not a valid chart repository or cannot be reached: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\",\n                        \"helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\",\n                        \"helm.sh/helm/v3/pkg/repo.(*ChartRepository).DownloadIndexFile\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:128\",\n                        \"helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:252\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:723\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:204\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:139\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:872\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:990\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:918\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:250\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1571\",\n                        \"looks like \\\"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages\\\" is not a valid chart repository or cannot be reached\",\n                        \"helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:254\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:723\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:204\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:139\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n             ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; install.go:192: [debug] Original chart version: \"\"\n    Error: INSTALLATION FAILED: looks like \"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages\" is not a valid chart repository or cannot be reached: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\n    helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\n    helm.sh/helm/v3/pkg/repo.(*ChartRepository).DownloadIndexFile\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:128\n    helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:252\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:723\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:204\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:139\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    looks like \"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages\" is not a valid chart repository or cannot be reached\n    helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:254\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:723\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:204\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:139\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:141\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\noccurred","time":{"start":1666754109000,"stop":1666754182119,"duration":73119}},{"uid":"db203ea6a32bf38e","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000fba8>: {\n        Underlying: <*exec.ExitError | 0xc00049eae0>{\n            ProcessState: {\n                pid: 6249,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 53756},\n                    Stime: {Sec: 0, Usec: 3839},\n                    Maxrss: 40884,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 1953,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 317,\n                    Nivcsw: 100,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"install.go:192: [debug] Original chart version: \\\"\\\"\",\n                        \"Error: INSTALLATION FAILED: looks like \\\"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/\\\" is not a valid chart repository or cannot be reached: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\",\n                        \"helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\",\n                        \"helm.sh/helm/v3/pkg/repo.(*ChartRepository).DownloadIndexFile\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:128\",\n                        \"helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:252\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:723\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:204\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:139\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:872\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:990\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:918\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:250\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1571\",\n                        \"looks like \\\"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/\\\" is not a valid chart repository or cannot be reached\",\n                        \"helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:254\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:723\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:204\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:139\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n          ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; install.go:192: [debug] Original chart version: \"\"\n    Error: INSTALLATION FAILED: looks like \"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/\" is not a valid chart repository or cannot be reached: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\n    helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\n    helm.sh/helm/v3/pkg/repo.(*ChartRepository).DownloadIndexFile\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:128\n    helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:252\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:723\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:204\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:139\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    looks like \"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/\" is not a valid chart repository or cannot be reached\n    helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:254\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:723\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:204\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:139\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:141\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\noccurred","time":{"start":1666750338000,"stop":1666750420125,"duration":82125}},{"uid":"879cdb92d0b023c4","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004ea030>: {\n        Underlying: <*exec.ExitError | 0xc0004fc040>{\n            ProcessState: {\n                pid: 6249,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 32423},\n                    Stime: {Sec: 0, Usec: 32423},\n                    Maxrss: 42520,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 1887,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 234,\n                    Nivcsw: 223,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"install.go:192: [debug] Original chart version: \\\"\\\"\",\n                        \"Error: INSTALLATION FAILED: looks like \\\"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/\\\" is not a valid chart repository or cannot be reached: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\",\n                        \"helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\",\n                        \"helm.sh/helm/v3/pkg/repo.(*ChartRepository).DownloadIndexFile\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:128\",\n                        \"helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:252\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:723\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:204\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:139\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:872\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:990\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:918\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:250\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1571\",\n                        \"looks like \\\"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/\\\" is not a valid chart repository or cannot be reached\",\n                        \"helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:254\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:723\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:204\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:139\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n         ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; install.go:192: [debug] Original chart version: \"\"\n    Error: INSTALLATION FAILED: looks like \"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/\" is not a valid chart repository or cannot be reached: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\n    helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\n    helm.sh/helm/v3/pkg/repo.(*ChartRepository).DownloadIndexFile\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:128\n    helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:252\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:723\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:204\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:139\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    looks like \"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/\" is not a valid chart repository or cannot be reached\n    helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:254\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:723\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:204\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:139\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:141\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\noccurred","time":{"start":1666747791000,"stop":1666747873444,"duration":82444}},{"uid":"cdda694b72ba4cea","status":"passed","time":{"start":1666706575000,"stop":1666706616181,"duration":41181}},{"uid":"fc8d3acd092c2a38","status":"passed","time":{"start":1666077791000,"stop":1666077840157,"duration":49157}},{"uid":"4447fd1392ccf9e0","status":"passed","time":{"start":1665944068000,"stop":1665944101892,"duration":33892}},{"uid":"87e6803e8f374867","status":"passed","time":{"start":1665665806000,"stop":1665665897869,"duration":91869}},{"uid":"6b893f602b35a010","status":"passed","time":{"start":1665665864000,"stop":1665665953615,"duration":89615}},{"uid":"f900bca47743b0b6","status":"passed","time":{"start":1665552534000,"stop":1665552625973,"duration":91973}},{"uid":"bac5ee86c5d7e1f8","status":"passed","time":{"start":1664798682000,"stop":1664798770633,"duration":88633}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Deletion Should fail when deleting a project that does not exist":{"statistic":{"failed":2,"broken":0,"skipped":0,"passed":0,"unknown":0,"total":2},"items":[{"uid":"c5733e7e403a8e21","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0003880d8>: {\n        Underlying: <*exec.ExitError | 0xc000894000>{\n            ProcessState: {\n                pid: 7677,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 175839},\n                    Stime: {Sec: 0, Usec: 54426},\n                    Maxrss: 89052,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 11105,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13760,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 363,\n                    Nivcsw: 263,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Deletion%20Should%20fail%20when%20deleting%20a%20project%20that%20does%20not%20exist3953096897\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.94.3:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Deletion%20Should%20fail%20when%20deleting%20a%20project%20that%20does%20not%20exist3953096897\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.94.3:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Deletion%20Should%20fail%20when%20deleting%20a%20project%20that%20does%20not%20exist3953096897\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.94.3:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Deletion%20Should%20fail%20when%20deleting%20a%20project%20that%20does%20not%20exist3953096897\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.94.3:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Deletion%20Should%20fail%20when%20deleting%20a%20project%20that%20does%20not%20exist3953096897\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.94.3:443: connect: connection refused\noccurred","time":{"start":1665665955000,"stop":1665665955272,"duration":272}},{"uid":"b939f92de8e1b585","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <bool>: false\nto be true","time":{"start":1665666027000,"stop":1665666104854,"duration":77854}}]},"Worker Suite:Worker Suite#[It] Hub netpol scenarios: Slice Configs Netpol scenarios Should successfully deploy slice for valid namespace and valid clusters in allowedNamespaces of sliceconfigs manifest":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":3,"unknown":0,"total":3},"items":[{"uid":"33b93a599a242e16","status":"passed","time":{"start":1666674621000,"stop":1666674622190,"duration":1190}},{"uid":"a24b9dd2853d10d1","status":"passed","time":{"start":1666674212000,"stop":1666674213183,"duration":1183}},{"uid":"c8cdc8892617b1ae","status":"passed","time":{"start":1666665366000,"stop":1666665367174,"duration":1174}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Creation Should create Multiple Projects in controller using valid manifest":{"statistic":{"failed":2,"broken":0,"skipped":0,"passed":0,"unknown":0,"total":2},"items":[{"uid":"18a00444b0304a98","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <bool>: false\nto be true","time":{"start":1665665955000,"stop":1665666022315,"duration":67315}},{"uid":"bf3055a3cef5b761","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0005346c0>: {\n        Underlying: <*exec.ExitError | 0xc0006a2640>{\n            ProcessState: {\n                pid: 7137,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 240403},\n                    Stime: {Sec: 0, Usec: 80134},\n                    Maxrss: 83008,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 9342,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13760,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 454,\n                    Nivcsw: 440,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20create%20Multiple%20Projects%20in%20controller%20using%20valid%20manifest3011383695\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.116.233:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20create%20Multiple%20Projects%20in%20controller%20using%20valid%20manifest3011383695\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.116.233:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20create%20Multiple%20Projects%20in%20controller%20using%20valid%20manifest3011383695\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.116.233:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20create%20Multiple%20Projects%20in%20controller%20using%20valid%20manifest3011383695\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.116.233:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20create%20Multiple%20Projects%20in%20controller%20using%20valid%20manifest3011383695\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.116.233:443: connect: connection refused\noccurred","time":{"start":1665666027000,"stop":1665666027378,"duration":378}}]},"Worker Suite:Worker Suite#[It] Slice Config Negative Scenario tests Slice Config Negative Scenario Creation Should fail while creating sliceconfig with invalid values on slice subnet":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":3,"unknown":0,"total":3},"items":[{"uid":"bcecfa51f4e7d1d7","status":"passed","time":{"start":1666674621000,"stop":1666674621799,"duration":799}},{"uid":"9a88a7fc19149015","status":"passed","time":{"start":1666674212000,"stop":1666674212546,"duration":546}},{"uid":"3f5d64f31b148fcf","status":"passed","time":{"start":1666665366000,"stop":1666665366729,"duration":729}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Update Should fail when Project is Applied with service account name as combination of special characters in Write users":{"statistic":{"failed":2,"broken":0,"skipped":0,"passed":0,"unknown":0,"total":2},"items":[{"uid":"ab859194a1b3a2c3","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0006c3fc8>: {\n        Underlying: <*exec.ExitError | 0xc000529d00>{\n            ProcessState: {\n                pid: 7658,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 185920},\n                    Stime: {Sec: 0, Usec: 71203},\n                    Maxrss: 81272,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 7903,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13760,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 332,\n                    Nivcsw: 308,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Write%20users1908415316\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.94.3:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Write%20users1908415316\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.94.3:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Write%20users1908415316\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.94.3:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Write%20users1908415316\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.94.3:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Write%20users1908415316\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.94.3:443: connect: connection refused\noccurred","time":{"start":1665665955000,"stop":1665665955291,"duration":291}},{"uid":"a68bb7ac415a5254","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00061c060>: {\n        Underlying: <*exec.ExitError | 0xc0007a6980>{\n            ProcessState: {\n                pid: 7234,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 261896},\n                    Stime: {Sec: 0, Usec: 74827},\n                    Maxrss: 87332,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 6065,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13760,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 345,\n                    Nivcsw: 386,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Write%20users1169169401\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.116.233:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Write%20users1169169401\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.116.233:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Write%20users1169169401\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.116.233:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Write%20users1169169401\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.116.233:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Write%20users1169169401\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.116.233:443: connect: connection refused\noccurred","time":{"start":1665666027000,"stop":1665666027429,"duration":429}}]},"Worker Suite:Worker Suite#[It] Attach/Deattach Scenario Attach cluster from slice  when Deattach cluster from slice  should have vl3 router running on attached cluster":{"statistic":{"failed":0,"broken":0,"skipped":3,"passed":0,"unknown":0,"total":3},"items":[{"uid":"3b6ad7c9fbced854","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674621000,"stop":1666674621000,"duration":0}},{"uid":"9f016eb4ad998901","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674212000,"stop":1666674212000,"duration":0}},{"uid":"7658f4a34a207a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666665366000,"stop":1666665366000,"duration":0}}]},"Intracluster Suite:Intracluster Suite#[It] Intracluster slice tests Slice with netpol should reconcile netpol in app NS":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":1,"unknown":0,"total":3},"items":[{"uid":"6e51e580580a3c8f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674484000,"stop":1666674484000,"duration":0}},{"uid":"cfc2715f0ae3ab33","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666673362000,"stop":1666673362000,"duration":0}},{"uid":"fc14330795da52d2","status":"passed","time":{"start":1666664909000,"stop":1666664909161,"duration":161}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Deletion Should Delete an existing project successfully":{"statistic":{"failed":2,"broken":0,"skipped":0,"passed":0,"unknown":0,"total":2},"items":[{"uid":"969fe0e2722e37b5","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0001cf2d8>: {\n        Underlying: <*exec.ExitError | 0xc0003db1e0>{\n            ProcessState: {\n                pid: 7686,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 151589},\n                    Stime: {Sec: 0, Usec: 60635},\n                    Maxrss: 89192,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 10071,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13760,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 488,\n                    Nivcsw: 245,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Deletion%20Should%20Delete%20an%20existing%20project%20successfully865657031\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.94.3:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Deletion%20Should%20Delete%20an%20existing%20project%20successfully865657031\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.94.3:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Deletion%20Should%20Delete%20an%20existing%20project%20successfully865657031\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.94.3:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Deletion%20Should%20Delete%20an%20existing%20project%20successfully865657031\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.94.3:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Deletion%20Should%20Delete%20an%20existing%20project%20successfully865657031\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.94.3:443: connect: connection refused\noccurred","time":{"start":1665665955000,"stop":1665665955271,"duration":271}},{"uid":"bfac7adfb3190871","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <bool>: false\nto be true","time":{"start":1665666027000,"stop":1665666104746,"duration":77746}}]},"Worker Suite:Worker Suite#[It] Slice Config Negative Scenario tests Slice Config Negative Scenario Update Should update while deploying sliceconfig with existing slice name":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":3,"unknown":0,"total":3},"items":[{"uid":"f79ee2283ca092b1","status":"passed","time":{"start":1666674621000,"stop":1666674622443,"duration":1443}},{"uid":"178c98571fc69a57","status":"passed","time":{"start":1666674212000,"stop":1666674213413,"duration":1413}},{"uid":"8070e92cd6447500","status":"passed","time":{"start":1666665366000,"stop":1666665367387,"duration":1387}}]},"Empty Suite:Empty Suite#[AfterSuite]":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":11,"unknown":0,"total":11},"items":[{"uid":"ed634703411033aa","status":"passed","time":{"start":1666883811000,"stop":1666883811000,"duration":0}},{"uid":"250910c86a48a626","status":"passed","time":{"start":1666754109000,"stop":1666754109000,"duration":0}},{"uid":"71d9d83711a7efc1","status":"passed","time":{"start":1666750338000,"stop":1666750338000,"duration":0}},{"uid":"8da04d8f1951558a","status":"passed","time":{"start":1666747791000,"stop":1666747791000,"duration":0}},{"uid":"15d4290774dff69b","status":"passed","time":{"start":1666706575000,"stop":1666706575000,"duration":0}},{"uid":"2eb8e75b5539d3c7","status":"passed","time":{"start":1666077791000,"stop":1666077791000,"duration":0}},{"uid":"a3426ccb20790311","status":"passed","time":{"start":1665944068000,"stop":1665944068000,"duration":0}},{"uid":"1e701ed935903b9c","status":"passed","time":{"start":1665665806000,"stop":1665665806000,"duration":0}},{"uid":"3b132a545dd9db2","status":"passed","time":{"start":1665665864000,"stop":1665665864000,"duration":0}},{"uid":"6089fc4ee091501d","status":"passed","time":{"start":1665552534000,"stop":1665552534000,"duration":0}},{"uid":"e5a9c1b71dd6c71b","status":"passed","time":{"start":1664798682000,"stop":1664798682000,"duration":0}}]},"Worker Suite:Worker Suite#[AfterSuite]":{"statistic":{"failed":3,"broken":0,"skipped":0,"passed":21,"unknown":0,"total":24},"items":[{"uid":"9cf5a02d978aa9bf","status":"passed","time":{"start":1667385863000,"stop":1667385869060,"duration":6060}},{"uid":"f6f29bba03090134","status":"passed","time":{"start":1667294317000,"stop":1667294321983,"duration":4983}},{"uid":"182ccf2017693005","status":"passed","time":{"start":1666884621000,"stop":1666884625250,"duration":4250}},{"uid":"8f4eab8ab998faa9","status":"passed","time":{"start":1666754282000,"stop":1666754283893,"duration":1893}},{"uid":"c2bd4749c08ab204","status":"passed","time":{"start":1666750523000,"stop":1666750524707,"duration":1707}},{"uid":"8f35eae975e02d03","status":"passed","time":{"start":1666748075000,"stop":1666748076856,"duration":1856}},{"uid":"565cfc9d121db39a","status":"passed","time":{"start":1666707346000,"stop":1666707348666,"duration":2666}},{"uid":"394ae2b485f31b63","status":"failed","statusDetails":"Timed out after 210.009s.\nExpected an error to have occurred.  Got:\n    <nil>: nil","time":{"start":1666674621000,"stop":1666674845751,"duration":224751}},{"uid":"f96c321e28521757","status":"failed","statusDetails":"Timed out after 210.009s.\nExpected an error to have occurred.  Got:\n    <nil>: nil","time":{"start":1666674212000,"stop":1666674434604,"duration":222604}},{"uid":"9d259b2c55d864ae","status":"passed","time":{"start":1666668056000,"stop":1666668065763,"duration":9763}},{"uid":"bbb1de599147948d","status":"failed","statusDetails":"Timed out after 210.009s.\nExpected an error to have occurred.  Got:\n    <nil>: nil","time":{"start":1666665366000,"stop":1666665583801,"duration":217801}},{"uid":"b5814da42e4d2ebb","status":"passed","time":{"start":1666664774000,"stop":1666664774555,"duration":555}},{"uid":"3ead8258b7c85249","status":"passed","time":{"start":1666663260000,"stop":1666663260746,"duration":746}},{"uid":"c568429435f492d0","status":"passed","time":{"start":1666424006000,"stop":1666424033462,"duration":27462}},{"uid":"8ef2e4321858275b","status":"passed","time":{"start":1666339060000,"stop":1666339086111,"duration":26111}},{"uid":"a9ef0671caf56fac","status":"passed","time":{"start":1666258922000,"stop":1666258923658,"duration":1658}},{"uid":"fda4bb75451d1f3f","status":"passed","time":{"start":1666254342000,"stop":1666254368134,"duration":26134}},{"uid":"4d3d7a350f618539","status":"passed","time":{"start":1666253108000,"stop":1666253110451,"duration":2451}},{"uid":"da4a1c8577a9f54","status":"passed","time":{"start":1666250774000,"stop":1666250800453,"duration":26453}},{"uid":"90824f7a7a06290b","status":"passed","time":{"start":1666243891000,"stop":1666243915881,"duration":24881}}]},"Worker Suite:Worker Suite#[It]  QosProfile tests QosProfile delete validation Delete qos profile - negative scenario":{"statistic":{"failed":1,"broken":0,"skipped":0,"passed":2,"unknown":0,"total":3},"items":[{"uid":"cbffd0e9e52ef374","status":"passed","time":{"start":1666674621000,"stop":1666674625946,"duration":4946}},{"uid":"4b4ddb288b571f42","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1666674212000,"stop":1666674393217,"duration":181217}},{"uid":"f337dcbbf9532b4f","status":"passed","time":{"start":1666665366000,"stop":1666665384537,"duration":18537}}]},"Worker Suite:Worker Suite#[It] Attach/Deattach Scenario Attach cluster from slice  when Deattach cluster from slice  should have gateway pod deleted from deattached cluster":{"statistic":{"failed":0,"broken":0,"skipped":3,"passed":0,"unknown":0,"total":3},"items":[{"uid":"5e5e35003ef16f39","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674621000,"stop":1666674621000,"duration":0}},{"uid":"67757b5e779af27a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674212000,"stop":1666674212000,"duration":0}},{"uid":"b54032c048100938","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666665366000,"stop":1666665366000,"duration":0}}]},"Intracluster Suite:Intracluster Suite#[It] Intracluster slice tests Slice with netpol should label app ns with kubeslice label":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":1,"unknown":0,"total":3},"items":[{"uid":"bb79fed2ac699425","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674484000,"stop":1666674484000,"duration":0}},{"uid":"5ada287d7b42dc39","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666673362000,"stop":1666673362000,"duration":0}},{"uid":"9c8cc9655051d6b0","status":"passed","time":{"start":1666664909000,"stop":1666664909069,"duration":69}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests Iperf App connectivity Should install iperf-server on server cluster":{"statistic":{"failed":0,"broken":0,"skipped":3,"passed":0,"unknown":0,"total":3},"items":[{"uid":"ce39ab00c7139db8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674621000,"stop":1666674621000,"duration":0}},{"uid":"4f2e6a84f35172b1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674212000,"stop":1666674212000,"duration":0}},{"uid":"e13de42a2b3cee8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666665366000,"stop":1666665366000,"duration":0}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests slice with netpol should remove network policy resource from application namespace once the isolationEnabled is set to false":{"statistic":{"failed":0,"broken":0,"skipped":3,"passed":0,"unknown":0,"total":3},"items":[{"uid":"bda93b900e2eda52","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674621000,"stop":1666674621000,"duration":0}},{"uid":"2150362fda73467c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674212000,"stop":1666674212000,"duration":0}},{"uid":"9976778184a80502","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666665366000,"stop":1666665366000,"duration":0}}]},"Empty Suite:Empty Suite#[It] Hub Creation tests Hub Installtion Test Senarios Should fail if Hub is installed before installing Cert":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":11,"unknown":0,"total":11},"items":[{"uid":"4e300ac6dbd64a3d","status":"passed","time":{"start":1666883811000,"stop":1666883815793,"duration":4793}},{"uid":"1a1a62f8873062dc","status":"passed","time":{"start":1666754109000,"stop":1666754109127,"duration":127}},{"uid":"79e38ad599cebaea","status":"passed","time":{"start":1666750338000,"stop":1666750338199,"duration":199}},{"uid":"d6665c7282b614d4","status":"passed","time":{"start":1666747791000,"stop":1666747791119,"duration":119}},{"uid":"c35498da5817c514","status":"passed","time":{"start":1666706575000,"stop":1666706577377,"duration":2377}},{"uid":"a1074016246462e0","status":"passed","time":{"start":1666077791000,"stop":1666077793178,"duration":2178}},{"uid":"fbb081059b6bfa36","status":"passed","time":{"start":1665944068000,"stop":1665944069639,"duration":1639}},{"uid":"1d85de33c3e4740e","status":"passed","time":{"start":1665665806000,"stop":1665665807706,"duration":1706}},{"uid":"21d2a7a6d80aaa8a","status":"passed","time":{"start":1665665864000,"stop":1665665866652,"duration":2652}},{"uid":"1865cca59b350f33","status":"passed","time":{"start":1665552534000,"stop":1665552536076,"duration":2076}},{"uid":"23d69bd3d795b0b5","status":"passed","time":{"start":1664798682000,"stop":1664798684713,"duration":2713}}]},"Worker Suite:Worker Suite#[It] Slice creation scenarios: when slice red is installed workers should get attached to slice":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":3,"unknown":0,"total":3},"items":[{"uid":"e412274f0fb2f9ab","status":"passed","time":{"start":1666674621000,"stop":1666674621949,"duration":949}},{"uid":"be442b6438ae3f99","status":"passed","time":{"start":1666674212000,"stop":1666674213252,"duration":1252}},{"uid":"d9bf91241ac31196","status":"passed","time":{"start":1666665366000,"stop":1666665367572,"duration":1572}}]},"Worker Suite:Worker Suite#[It] Namespace reconcile test scenarios: Offboard slice get detached from app ns in cluster objects":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":3,"unknown":0,"total":3},"items":[{"uid":"92609a3466218b08","status":"passed","time":{"start":1666674621000,"stop":1666674621247,"duration":247}},{"uid":"633e652e2d2109ae","status":"passed","time":{"start":1666674212000,"stop":1666674212236,"duration":236}},{"uid":"d11d50f51f780330","status":"passed","time":{"start":1666665366000,"stop":1666665366360,"duration":360}}]},"Hub Suite:Hub Suite#[It] Cluster negative tests Worker Cluster Registration with Wrong endpoint":{"statistic":{"failed":2,"broken":0,"skipped":0,"passed":0,"unknown":0,"total":2},"items":[{"uid":"1ac94a7e446ca4ba","status":"failed","statusDetails":"Timed out after 120.001s.\nExpected an error to have occurred.  Got:\n    <nil>: nil","time":{"start":1665665955000,"stop":1665666604859,"duration":649859}},{"uid":"d1b51cdd7eea210b","status":"failed","statusDetails":"Timed out after 120.000s.\nExpected an error to have occurred.  Got:\n    <nil>: nil","time":{"start":1665666027000,"stop":1665666762034,"duration":735034}}]},"Worker Suite:Worker Suite#[It] Slice Creation scenarios Slice with LOW QOS should reconcile low qos slice":{"statistic":{"failed":0,"broken":0,"skipped":3,"passed":0,"unknown":0,"total":3},"items":[{"uid":"ae432458b80bb5c7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674621000,"stop":1666674621000,"duration":0}},{"uid":"a542914546ca85c8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674212000,"stop":1666674212000,"duration":0}},{"uid":"d19a6f77a628bfd2","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666665366000,"stop":1666665366000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Hub netpol scenarios: Slice Configs Netpol scenarios Should successfully deploy for valid namespace creating clusters with * in both applicationNamespace and allowedNamespace":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":3,"unknown":0,"total":3},"items":[{"uid":"c2dcf50e3f16241a","status":"passed","time":{"start":1666674621000,"stop":1666674622235,"duration":1235}},{"uid":"bd9b235ca64e1ccb","status":"passed","time":{"start":1666674212000,"stop":1666674213004,"duration":1004}},{"uid":"8997a52ff303e9b4","status":"passed","time":{"start":1666665366000,"stop":1666665367138,"duration":1138}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Creation Should update Project while applying valid manifest with existing Project name":{"statistic":{"failed":2,"broken":0,"skipped":0,"passed":0,"unknown":0,"total":2},"items":[{"uid":"a27b21d15828dbff","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <bool>: false\nto be true","time":{"start":1665665955000,"stop":1665666031643,"duration":76643}},{"uid":"16fbcb1f534144ff","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00061c030>: {\n        Underlying: <*exec.ExitError | 0xc0007ac0a0>{\n            ProcessState: {\n                pid: 7127,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 280122},\n                    Stime: {Sec: 0, Usec: 62249},\n                    Maxrss: 87676,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 10855,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13760,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 476,\n                    Nivcsw: 544,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20update%20Project%20while%20applying%20valid%20manifest%20with%20existing%20Project%20name2294658713\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.116.233:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20update%20Project%20while%20applying%20valid%20manifest%20with%20existing%20Project%20name2294658713\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.116.233:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20update%20Project%20while%20applying%20valid%20manifest%20with%20existing%20Project%20name2294658713\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.116.233:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20update%20Project%20while%20applying%20valid%20manifest%20with%20existing%20Project%20name2294658713\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.116.233:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20update%20Project%20while%20applying%20valid%20manifest%20with%20existing%20Project%20name2294658713\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.116.233:443: connect: connection refused\noccurred","time":{"start":1665666027000,"stop":1665666027409,"duration":409}}]},"Worker Suite:Worker Suite#[BeforeSuite]":{"statistic":{"failed":21,"broken":0,"skipped":0,"passed":3,"unknown":0,"total":24},"items":[{"uid":"e5cfe4903620e0e8","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004f8120>: {\n        Underlying: <*exec.ExitError | 0xc000492200>{\n            ProcessState: {\n                pid: 6234,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 99789},\n                    Stime: {Sec: 0, Usec: 29105},\n                    Maxrss: 48972,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2570,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 168,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 524,\n                    Nivcsw: 316,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"install.go:192: [debug] Original chart version: \\\"1.7.0\\\"\",\n                        \"install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.7.0.tgz\",\n                        \"\",\n                        \"Error: INSTALLATION FAILED: cannot re-use a name that is still in use\",\n                        \"helm.go:84: [debug] cannot re-use a name that is still in use\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).availableName\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:483\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:197\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:278\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:139\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:872\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:990\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:918\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:250\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1571\",\n                        \"INSTALLATION FAILED\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:141\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:872\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:990\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:918\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:250\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1571\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"install.go:192: [debug] Original chart version: \\\"1.7.0\\\"\",\n                    \"install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.7.0.tgz\",\n                    \"\",\n                    \"Error: INSTALLATION FAILED: cannot re-use a name that is still in use\",\n                    \"helm.go:84: [debug] cannot re-use a name that is still in use...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; install.go:192: [debug] Original chart version: \"1.7.0\"\n    install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.7.0.tgz\n    \n    Error: INSTALLATION FAILED: cannot re-use a name that is still in use\n    helm.go:84: [debug] cannot re-use a name that is still in use\n    helm.sh/helm/v3/pkg/action.(*Install).availableName\n    \thelm.sh/helm/v3/pkg/action/install.go:483\n    helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\n    \thelm.sh/helm/v3/pkg/action/install.go:197\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:278\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:139\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:141\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\noccurred","time":{"start":1667385863000,"stop":1667385863638,"duration":638}},{"uid":"55f92fa72945e284","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0001132c0>: {\n        Underlying: <*exec.ExitError | 0xc000158080>{\n            ProcessState: {\n                pid: 6231,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 87334},\n                    Stime: {Sec: 0, Usec: 26580},\n                    Maxrss: 49340,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2140,\n                    Majflt: 1,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 160,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 637,\n                    Nivcsw: 287,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"install.go:192: [debug] Original chart version: \\\"1.7.0\\\"\",\n                        \"install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.7.0.tgz\",\n                        \"\",\n                        \"Error: INSTALLATION FAILED: cannot re-use a name that is still in use\",\n                        \"helm.go:84: [debug] cannot re-use a name that is still in use\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).availableName\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:483\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:197\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:278\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:139\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:872\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:990\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:918\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:250\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1571\",\n                        \"INSTALLATION FAILED\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:141\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:872\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:990\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:918\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:250\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1571\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"install.go:192: [debug] Original chart version: \\\"1.7.0\\\"\",\n                    \"install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.7.0.tgz\",\n                    \"\",\n                    \"Error: INSTALLATION FAILED: cannot re-use a name that is still in use\",\n                    \"helm.go:84: [debug] cannot re-use a name that is still in use...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; install.go:192: [debug] Original chart version: \"1.7.0\"\n    install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.7.0.tgz\n    \n    Error: INSTALLATION FAILED: cannot re-use a name that is still in use\n    helm.go:84: [debug] cannot re-use a name that is still in use\n    helm.sh/helm/v3/pkg/action.(*Install).availableName\n    \thelm.sh/helm/v3/pkg/action/install.go:483\n    helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\n    \thelm.sh/helm/v3/pkg/action/install.go:197\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:278\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:139\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:141\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\noccurred","time":{"start":1667294317000,"stop":1667294317406,"duration":406}},{"uid":"e6054492f59d7280","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000480b88>: {\n        Underlying: <*exec.ExitError | 0xc0004965a0>{\n            ProcessState: {\n                pid: 6415,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 168560},\n                    Stime: {Sec: 0, Usec: 29498},\n                    Maxrss: 51724,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2679,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 360,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 655,\n                    Nivcsw: 377,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"install.go:192: [debug] Original chart version: \\\"v1.8.0\\\"\",\n                        \"install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\",\n                        \"\",\n                        \"Error: INSTALLATION FAILED: cannot re-use a name that is still in use\",\n                        \"helm.go:84: [debug] cannot re-use a name that is still in use\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).availableName\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:483\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:197\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:278\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:139\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:872\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:990\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:918\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:250\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1571\",\n                        \"INSTALLATION FAILED\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:141\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:872\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:990\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:918\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:250\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1571\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"install.go:192: [debug] Original chart version: \\\"v1.8.0\\\"\",\n                    \"install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\",\n                    \"\",\n                    \"Error: INSTALLATION FAILED: cannot re-use a name that is still in use\",\n                    \"helm.go:84: [debug] cannot re-use a name that is still in ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; install.go:192: [debug] Original chart version: \"v1.8.0\"\n    install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\n    \n    Error: INSTALLATION FAILED: cannot re-use a name that is still in use\n    helm.go:84: [debug] cannot re-use a name that is still in use\n    helm.sh/helm/v3/pkg/action.(*Install).availableName\n    \thelm.sh/helm/v3/pkg/action/install.go:483\n    helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\n    \thelm.sh/helm/v3/pkg/action/install.go:197\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:278\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:139\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:141\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\noccurred","time":{"start":1666884621000,"stop":1666884621383,"duration":383}},{"uid":"42478527bc4f81ce","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00047a0a8>: {\n        Underlying: <*exec.ExitError | 0xc00014c040>{\n            ProcessState: {\n                pid: 6314,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 46205},\n                    Stime: {Sec: 0, Usec: 14216},\n                    Maxrss: 40728,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 1480,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 256,\n                    Nivcsw: 124,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"install.go:192: [debug] Original chart version: \\\"\\\"\",\n                        \"Error: INSTALLATION FAILED: looks like \\\"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages\\\" is not a valid chart repository or cannot be reached: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\",\n                        \"helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\",\n                        \"helm.sh/helm/v3/pkg/repo.(*ChartRepository).DownloadIndexFile\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:128\",\n                        \"helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:252\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:723\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:204\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:139\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:872\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:990\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:918\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:250\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1571\",\n                        \"looks like \\\"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages\\\" is not a valid chart repository or cannot be reached\",\n                        \"helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:254\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:723\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:204\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:139\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n           ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; install.go:192: [debug] Original chart version: \"\"\n    Error: INSTALLATION FAILED: looks like \"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages\" is not a valid chart repository or cannot be reached: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\n    helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\n    helm.sh/helm/v3/pkg/repo.(*ChartRepository).DownloadIndexFile\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:128\n    helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:252\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:723\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:204\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:139\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    looks like \"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages\" is not a valid chart repository or cannot be reached\n    helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:254\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:723\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:204\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:139\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:141\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\noccurred","time":{"start":1666754282000,"stop":1666754357231,"duration":75231}},{"uid":"86aac3dc8de50f84","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0000aa360>: {\n        Underlying: <*exec.ExitError | 0xc0000ae060>{\n            ProcessState: {\n                pid: 6343,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 40540},\n                    Stime: {Sec: 0, Usec: 12162},\n                    Maxrss: 40580,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 1955,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 285,\n                    Nivcsw: 120,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"install.go:192: [debug] Original chart version: \\\"\\\"\",\n                        \"Error: INSTALLATION FAILED: looks like \\\"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/\\\" is not a valid chart repository or cannot be reached: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\",\n                        \"helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\",\n                        \"helm.sh/helm/v3/pkg/repo.(*ChartRepository).DownloadIndexFile\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:128\",\n                        \"helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:252\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:723\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:204\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:139\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:872\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:990\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:918\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:250\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1571\",\n                        \"looks like \\\"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/\\\" is not a valid chart repository or cannot be reached\",\n                        \"helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:254\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:723\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:204\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:139\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n         ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; install.go:192: [debug] Original chart version: \"\"\n    Error: INSTALLATION FAILED: looks like \"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/\" is not a valid chart repository or cannot be reached: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\n    helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\n    helm.sh/helm/v3/pkg/repo.(*ChartRepository).DownloadIndexFile\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:128\n    helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:252\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:723\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:204\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:139\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    looks like \"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/\" is not a valid chart repository or cannot be reached\n    helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:254\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:723\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:204\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:139\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:141\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\noccurred","time":{"start":1666750523000,"stop":1666750614389,"duration":91389}},{"uid":"eef7da4dbff2758e","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0000b83d8>: {\n        Underlying: <*exec.ExitError | 0xc0004c05c0>{\n            ProcessState: {\n                pid: 6363,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 129403},\n                    Stime: {Sec: 0, Usec: 29578},\n                    Maxrss: 51804,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2152,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 360,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 726,\n                    Nivcsw: 358,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"install.go:192: [debug] Original chart version: \\\"v1.8.0\\\"\",\n                        \"install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\",\n                        \"\",\n                        \"Error: INSTALLATION FAILED: cannot re-use a name that is still in use\",\n                        \"helm.go:84: [debug] cannot re-use a name that is still in use\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).availableName\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:483\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:197\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:278\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:139\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:872\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:990\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:918\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:250\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1571\",\n                        \"INSTALLATION FAILED\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:141\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:872\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:990\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:918\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:250\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1571\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"install.go:192: [debug] Original chart version: \\\"v1.8.0\\\"\",\n                    \"install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\",\n                    \"\",\n                    \"Error: INSTALLATION FAILED: cannot re-use a name that is still in use\",\n                    \"helm.go:84: [debug] cannot re-use a name that is still in ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; install.go:192: [debug] Original chart version: \"v1.8.0\"\n    install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\n    \n    Error: INSTALLATION FAILED: cannot re-use a name that is still in use\n    helm.go:84: [debug] cannot re-use a name that is still in use\n    helm.sh/helm/v3/pkg/action.(*Install).availableName\n    \thelm.sh/helm/v3/pkg/action/install.go:483\n    helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\n    \thelm.sh/helm/v3/pkg/action/install.go:197\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:278\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:139\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:141\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\noccurred","time":{"start":1666748075000,"stop":1666748075368,"duration":368}},{"uid":"90e4924138da3b7c","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000e3c0>: {\n        Underlying: <*exec.ExitError | 0xc000474e60>{\n            ProcessState: {\n                pid: 6417,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 117546},\n                    Stime: {Sec: 0, Usec: 26121},\n                    Maxrss: 51636,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2614,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 360,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 677,\n                    Nivcsw: 254,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"install.go:192: [debug] Original chart version: \\\"v1.8.0\\\"\",\n                        \"install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\",\n                        \"\",\n                        \"Error: INSTALLATION FAILED: cannot re-use a name that is still in use\",\n                        \"helm.go:84: [debug] cannot re-use a name that is still in use\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).availableName\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:483\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:197\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:278\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:139\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:872\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:990\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:918\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:250\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1571\",\n                        \"INSTALLATION FAILED\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:141\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:872\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:990\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:918\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:250\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1571\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"install.go:192: [debug] Original chart version: \\\"v1.8.0\\\"\",\n                    \"install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\",\n                    \"\",\n                    \"Error: INSTALLATION FAILED: cannot re-use a name that is still in use\",\n                    \"helm.go:84: [debug] cannot re-use a name that is still in ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; install.go:192: [debug] Original chart version: \"v1.8.0\"\n    install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\n    \n    Error: INSTALLATION FAILED: cannot re-use a name that is still in use\n    helm.go:84: [debug] cannot re-use a name that is still in use\n    helm.sh/helm/v3/pkg/action.(*Install).availableName\n    \thelm.sh/helm/v3/pkg/action/install.go:483\n    helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\n    \thelm.sh/helm/v3/pkg/action/install.go:197\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:278\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:139\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:141\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\noccurred","time":{"start":1666707346000,"stop":1666707346358,"duration":358}},{"uid":"7827bcf93de0a345","status":"passed","time":{"start":1666674621000,"stop":1666674807597,"duration":186597}},{"uid":"e2dd5779703a4c85","status":"passed","time":{"start":1666674212000,"stop":1666674379616,"duration":167616}},{"uid":"2ba107e5c0943fc8","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004eb728>: {\n        Underlying: <*exec.ExitError | 0xc000429200>{\n            ProcessState: {\n                pid: 6152,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 124937},\n                    Stime: {Sec: 0, Usec: 50755},\n                    Maxrss: 51060,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2646,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 536,\n                    Oublock: 152,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 964,\n                    Nivcsw: 433,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"install.go:192: [debug] Original chart version: \\\"1.7.0\\\"\",\n                        \"install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.7.0.tgz\",\n                        \"\",\n                        \"Error: INSTALLATION FAILED: cannot re-use a name that is still in use\",\n                        \"helm.go:84: [debug] cannot re-use a name that is still in use\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).availableName\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:483\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:197\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:278\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:139\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:872\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:990\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:918\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:250\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1571\",\n                        \"INSTALLATION FAILED\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:141\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:872\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:990\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:918\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:250\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1571\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"install.go:192: [debug] Original chart version: \\\"1.7.0\\\"\",\n                    \"install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.7.0.tgz\",\n                    \"\",\n                    \"Error: INSTALLATION FAILED: cannot re-use a name that is still in use\",\n                    \"helm.go:84: [debug] cannot re-use a name that is still in ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; install.go:192: [debug] Original chart version: \"1.7.0\"\n    install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.7.0.tgz\n    \n    Error: INSTALLATION FAILED: cannot re-use a name that is still in use\n    helm.go:84: [debug] cannot re-use a name that is still in use\n    helm.sh/helm/v3/pkg/action.(*Install).availableName\n    \thelm.sh/helm/v3/pkg/action/install.go:483\n    helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\n    \thelm.sh/helm/v3/pkg/action/install.go:197\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:278\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:139\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:141\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\noccurred","time":{"start":1666668056000,"stop":1666668056766,"duration":766}},{"uid":"68466ef777034499","status":"passed","time":{"start":1666665366000,"stop":1666665547982,"duration":181982}},{"uid":"49b373df43d85d4a","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000f6c8>: {\n        Underlying: <*exec.ExitError | 0xc0004b48e0>{\n            ProcessState: {\n                pid: 6265,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 47955},\n                    Stime: {Sec: 0, Usec: 17438},\n                    Maxrss: 40172,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2886,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 235,\n                    Nivcsw: 149,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"install.go:192: [debug] Original chart version: \\\"1.7.0\\\"\",\n                        \"Error: INSTALLATION FAILED: looks like \\\"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/\\\" is not a valid chart repository or cannot be reached: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\",\n                        \"helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\",\n                        \"helm.sh/helm/v3/pkg/repo.(*ChartRepository).DownloadIndexFile\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:128\",\n                        \"helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:252\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:723\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:204\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:139\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:872\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:990\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:918\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:250\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1571\",\n                        \"looks like \\\"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/\\\" is not a valid chart repository or cannot be reached\",\n                        \"helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:254\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:723\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:204\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:139\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n    ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; install.go:192: [debug] Original chart version: \"1.7.0\"\n    Error: INSTALLATION FAILED: looks like \"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/\" is not a valid chart repository or cannot be reached: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\n    helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\n    helm.sh/helm/v3/pkg/repo.(*ChartRepository).DownloadIndexFile\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:128\n    helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:252\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:723\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:204\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:139\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    looks like \"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/\" is not a valid chart repository or cannot be reached\n    helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:254\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:723\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:204\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:139\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:141\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\noccurred","time":{"start":1666664774000,"stop":1666664774170,"duration":170}},{"uid":"e6b474ec0802df07","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000184408>: {\n        Underlying: <*exec.ExitError | 0xc000079680>{\n            ProcessState: {\n                pid: 6156,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 49927},\n                    Stime: {Sec: 0, Usec: 16642},\n                    Maxrss: 40812,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 1468,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 175,\n                    Nivcsw: 97,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"install.go:192: [debug] Original chart version: \\\"1.7.0\\\"\",\n                        \"Error: INSTALLATION FAILED: looks like \\\"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/\\\" is not a valid chart repository or cannot be reached: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\",\n                        \"helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\",\n                        \"helm.sh/helm/v3/pkg/repo.(*ChartRepository).DownloadIndexFile\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:128\",\n                        \"helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:252\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:723\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:204\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:139\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:872\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:990\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:918\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:250\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1571\",\n                        \"looks like \\\"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/\\\" is not a valid chart repository or cannot be reached\",\n                        \"helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:254\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:723\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:204\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:139\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n     ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; install.go:192: [debug] Original chart version: \"1.7.0\"\n    Error: INSTALLATION FAILED: looks like \"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/\" is not a valid chart repository or cannot be reached: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\n    helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\n    helm.sh/helm/v3/pkg/repo.(*ChartRepository).DownloadIndexFile\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:128\n    helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:252\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:723\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:204\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:139\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    looks like \"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/\" is not a valid chart repository or cannot be reached\n    helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:254\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:723\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:204\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:139\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:141\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\noccurred","time":{"start":1666663260000,"stop":1666663260150,"duration":150}},{"uid":"56e96faf9a5608a2","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00078c018>: {\n        Underlying: <*exec.ExitError | 0xc000a04000>{\n            ProcessState: {\n                pid: 6291,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 4, Usec: 674956},\n                    Stime: {Sec: 0, Usec: 748453},\n                    Maxrss: 101112,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 12638,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 19888,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 49900,\n                    Nivcsw: 15036,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:192: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.4.6.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD serviceexports.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD serviceimports.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD slice.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD slicegateways.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 47 resource(s)\",\n                        \"wait.go:66: [debug] beginning wait for 47 resources with timeout of 5m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:192: [debug] Original chart version: \"\"\n    install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.4.6.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD serviceexports.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD serviceimports.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD slice.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD slicegateways.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 47 resource(s)\n    wait.go:66: [debug] beginning wait for 47 resources with timeout of 5m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:310: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"jaeger\" Service\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-svc\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:310: [debug] Starting delete for \"jaeger\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook\" Deployment\n    client.go:310: [debug] Starting delete for \"prefix-service\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsm-kernel-forwarder\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRole\n    client.go:310: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"networkservices.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkserviceendpoints.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservicemanagers.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-cfg\" MutatingWebhookConfiguration\n    uninstall.go:148: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: client rate limiter Wait returned an error: rate: Wait(n=1) would exceed context deadline\n    helm.go:84: [debug] client rate limiter Wait returned an error: rate: Wait(n=1) would exceed context deadline\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:141\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\noccurred","time":{"start":1666424006000,"stop":1666424417672,"duration":411672}},{"uid":"5d8a7d913ed0e91","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0006ee018>: {\n        Underlying: <*exec.ExitError | 0xc000992000>{\n            ProcessState: {\n                pid: 6330,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 4, Usec: 976974},\n                    Stime: {Sec: 0, Usec: 815571},\n                    Maxrss: 104896,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 5424,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 17624,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 52200,\n                    Nivcsw: 12303,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:192: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.4.6.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD serviceexports.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD serviceimports.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD slice.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD slicegateways.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 47 resource(s)\",\n                        \"wait.go:66: [debug] beginning wait for 47 resources with timeout of 5m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:192: [debug] Original chart version: \"\"\n    install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.4.6.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD serviceexports.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD serviceimports.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD slice.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD slicegateways.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 47 resource(s)\n    wait.go:66: [debug] beginning wait for 47 resources with timeout of 5m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:310: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-svc\" Service\n    client.go:310: [debug] Starting delete for \"jaeger\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:310: [debug] Starting delete for \"jaeger\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook\" Deployment\n    client.go:310: [debug] Starting delete for \"prefix-service\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsm-kernel-forwarder\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRole\n    client.go:310: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"networkservices.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkserviceendpoints.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservicemanagers.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:310: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-cfg\" MutatingWebhookConfiguration\n    uninstall.go:148: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:141\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\noccurred","time":{"start":1666339060000,"stop":1666339460405,"duration":400405}},{"uid":"7e2d6f06a60b9a87","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004a8b40>: {\n        Underlying: <*exec.ExitError | 0xc0000e07a0>{\n            ProcessState: {\n                pid: 6275,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 60049},\n                    Stime: {Sec: 0, Usec: 4619},\n                    Maxrss: 41840,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 1960,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 258,\n                    Nivcsw: 125,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:192: [debug] Original chart version: \\\"\\\"\",\n                        \"Error: INSTALLATION FAILED: looks like \\\"https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/\\\" is not a valid chart repository or cannot be reached: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/index.yaml : 404 Not Found\",\n                        \"helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/index.yaml : 404 Not Found\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\",\n                        \"helm.sh/helm/v3/pkg/repo.(*ChartRepository).DownloadIndexFile\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:128\",\n                        \"helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:252\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:723\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:204\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:139\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:872\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:990\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:918\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:250\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1571\",\n                        \"looks like \\\"https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/\\\" is not a valid chart repository or cannot be reached\",\n                        \"helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:254\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:723\",\n                        \"main....\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:192: [debug] Original chart version: \"\"\n    Error: INSTALLATION FAILED: looks like \"https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/\" is not a valid chart repository or cannot be reached: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/index.yaml : 404 Not Found\n    helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/index.yaml : 404 Not Found\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\n    helm.sh/helm/v3/pkg/repo.(*ChartRepository).DownloadIndexFile\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:128\n    helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:252\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:723\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:204\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:139\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    looks like \"https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/\" is not a valid chart repository or cannot be reached\n    helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:254\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:723\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:204\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:139\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:141\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\noccurred","time":{"start":1666258922000,"stop":1666258997418,"duration":75418}},{"uid":"cbe986104ecdabeb","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004cc048>: {\n        Underlying: <*exec.ExitError | 0xc0006d8000>{\n            ProcessState: {\n                pid: 6324,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 4, Usec: 985808},\n                    Stime: {Sec: 1, Usec: 11675},\n                    Maxrss: 107232,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 6430,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 17624,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 51789,\n                    Nivcsw: 11479,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:192: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.4.6.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD serviceexports.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD serviceimports.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD slice.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD slicegateways.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 47 resource(s)\",\n                        \"wait.go:66: [debug] beginning wait for 47 resources with timeout of 5m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debu...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:192: [debug] Original chart version: \"\"\n    install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.4.6.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD serviceexports.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD serviceimports.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD slice.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD slicegateways.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 47 resource(s)\n    wait.go:66: [debug] beginning wait for 47 resources with timeout of 5m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:310: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-svc\" Service\n    client.go:310: [debug] Starting delete for \"jaeger\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:310: [debug] Starting delete for \"jaeger\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook\" Deployment\n    client.go:310: [debug] Starting delete for \"prefix-service\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsm-kernel-forwarder\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRole\n    client.go:310: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:310: [debug] Starting delete for \"networkservices.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkserviceendpoints.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservicemanagers.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-cfg\" MutatingWebhookConfiguration\n    uninstall.go:148: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: client rate limiter Wait returned an error: rate: Wait(n=1) would exceed context deadline\n    helm.go:84: [debug] client rate limiter Wait returned an error: rate: Wait(n=1) would exceed context deadline\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:141\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\noccurred","time":{"start":1666254342000,"stop":1666254744592,"duration":402592}},{"uid":"74b22a1779152ca8","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000128750>: {\n        Underlying: <*exec.ExitError | 0xc00049e240>{\n            ProcessState: {\n                pid: 6252,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 47972},\n                    Stime: {Sec: 0, Usec: 22141},\n                    Maxrss: 42756,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 1950,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 227,\n                    Nivcsw: 60,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:192: [debug] Original chart version: \\\"\\\"\",\n                        \"Error: INSTALLATION FAILED: looks like \\\"https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/\\\" is not a valid chart repository or cannot be reached: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/index.yaml : 404 Not Found\",\n                        \"helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/index.yaml : 404 Not Found\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\",\n                        \"helm.sh/helm/v3/pkg/repo.(*ChartRepository).DownloadIndexFile\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:128\",\n                        \"helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:252\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:723\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:204\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:139\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:872\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:990\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:918\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:250\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1571\",\n                        \"looks like \\\"https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/\\\" is not a valid chart repository or cannot be reached\",\n                        \"helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:254\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:723\",\n                        \"main....\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:192: [debug] Original chart version: \"\"\n    Error: INSTALLATION FAILED: looks like \"https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/\" is not a valid chart repository or cannot be reached: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/index.yaml : 404 Not Found\n    helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/index.yaml : 404 Not Found\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\n    helm.sh/helm/v3/pkg/repo.(*ChartRepository).DownloadIndexFile\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:128\n    helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:252\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:723\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:204\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:139\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    looks like \"https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/\" is not a valid chart repository or cannot be reached\n    helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:254\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:723\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:204\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:139\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:141\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\noccurred","time":{"start":1666253108000,"stop":1666253204260,"duration":96260}},{"uid":"a121168ee92c5c88","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000a0e018>: {\n        Underlying: <*exec.ExitError | 0xc000a54000>{\n            ProcessState: {\n                pid: 6312,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 5, Usec: 56943},\n                    Stime: {Sec: 0, Usec: 983570},\n                    Maxrss: 94016,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 14464,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 17624,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 53634,\n                    Nivcsw: 12000,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:192: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.4.6.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD serviceexports.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD serviceimports.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD slice.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD slicegateways.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 47 resource(s)\",\n                        \"wait.go:66: [debug] beginning wait for 47 resources with timeout of 5m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debu...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:192: [debug] Original chart version: \"\"\n    install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.4.6.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD serviceexports.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD serviceimports.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD slice.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD slicegateways.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 47 resource(s)\n    wait.go:66: [debug] beginning wait for 47 resources with timeout of 5m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:310: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"jaeger\" Service\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-svc\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:310: [debug] Starting delete for \"jaeger\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook\" Deployment\n    client.go:310: [debug] Starting delete for \"prefix-service\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsm-kernel-forwarder\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"networkservices.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkserviceendpoints.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservicemanagers.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-cfg\" MutatingWebhookConfiguration\n    uninstall.go:148: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:141\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\noccurred","time":{"start":1666250774000,"stop":1666251197776,"duration":423776}},{"uid":"45fdf211406491c2","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00087e018>: {\n        Underlying: <*exec.ExitError | 0xc00017e000>{\n            ProcessState: {\n                pid: 6353,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 4, Usec: 763912},\n                    Stime: {Sec: 0, Usec: 574203},\n                    Maxrss: 98516,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 7507,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 17624,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 48487,\n                    Nivcsw: 10212,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:192: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.4.6.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD serviceexports.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD serviceimports.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD slice.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD slicegateways.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 47 resource(s)\",\n                        \"wait.go:66: [debug] beginning wait for 47 resources with timeout of 5m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment i...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:192: [debug] Original chart version: \"\"\n    install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.4.6.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD serviceexports.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD serviceimports.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD slice.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD slicegateways.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 47 resource(s)\n    wait.go:66: [debug] beginning wait for 47 resources with timeout of 5m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:310: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-svc\" Service\n    client.go:310: [debug] Starting delete for \"jaeger\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:310: [debug] Starting delete for \"jaeger\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook\" Deployment\n    client.go:310: [debug] Starting delete for \"prefix-service\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsm-kernel-forwarder\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRole\n    client.go:310: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"networkservices.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkserviceendpoints.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservicemanagers.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-cfg\" MutatingWebhookConfiguration\n    uninstall.go:148: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:141\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\noccurred","time":{"start":1666243891000,"stop":1666244305448,"duration":414448}}]},"Worker Suite:Worker Suite#[It] Slice creation scenarios: when slice red is installed workers should have gateway pod running":{"statistic":{"failed":2,"broken":0,"skipped":1,"passed":0,"unknown":0,"total":3},"items":[{"uid":"5ba607b19aee5a44","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1666674621000,"stop":1666674801289,"duration":180289}},{"uid":"62395ab83a88eb26","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674212000,"stop":1666674212000,"duration":0}},{"uid":"a2d4d7b1db13c5c6","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1666665366000,"stop":1666665546268,"duration":180268}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting iperf-server pod Check ping between iperf-server and iperf-client after iperf-server pod restart":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":1,"unknown":0,"total":3},"items":[{"uid":"e02d61d62900444","status":"passed","time":{"start":1666674621000,"stop":1666674640845,"duration":19845}},{"uid":"d0538f9e6780eb4d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674212000,"stop":1666674212000,"duration":0}},{"uid":"27cdd0b237858abb","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666665366000,"stop":1666665366000,"duration":0}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Update Should update successfully when Project Applied while removing a service account name in Read users":{"statistic":{"failed":2,"broken":0,"skipped":0,"passed":0,"unknown":0,"total":2},"items":[{"uid":"dd294f9cb0ee0bf6","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000e3d8>: {\n        Underlying: <*exec.ExitError | 0xc000749920>{\n            ProcessState: {\n                pid: 7621,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 181810},\n                    Stime: {Sec: 0, Usec: 69260},\n                    Maxrss: 81532,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 8120,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13760,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 481,\n                    Nivcsw: 364,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Read%20users2994065585\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.94.3:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Read%20users2994065585\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.94.3:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Read%20users2994065585\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.94.3:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Read%20users2994065585\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.94.3:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Read%20users2994065585\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.94.3:443: connect: connection refused\noccurred","time":{"start":1665665955000,"stop":1665665955314,"duration":314}},{"uid":"5c3b6c0b44b85332","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0005340a8>: {\n        Underlying: <*exec.ExitError | 0xc00088c8e0>{\n            ProcessState: {\n                pid: 7194,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 272305},\n                    Stime: {Sec: 0, Usec: 54461},\n                    Maxrss: 76384,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 6138,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13760,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 293,\n                    Nivcsw: 407,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Read%20users2184618495\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.116.233:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Read%20users2184618495\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.116.233:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Read%20users2184618495\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.116.233:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Read%20users2184618495\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.116.233:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Read%20users2184618495\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.116.233:443: connect: connection refused\noccurred","time":{"start":1665666027000,"stop":1665666027417,"duration":417}}]},"Worker Suite:Worker Suite#[It] NodeRestartSlice Testing node restart should re-establish connection on node restart":{"statistic":{"failed":0,"broken":0,"skipped":3,"passed":0,"unknown":0,"total":3},"items":[{"uid":"23530269308a5a66","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674621000,"stop":1666674621000,"duration":0}},{"uid":"afb1deea97f77557","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674212000,"stop":1666674212000,"duration":0}},{"uid":"863e91df5fd13522","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666665366000,"stop":1666665366000,"duration":0}}]},"Worker Suite:Worker Suite#[It]  QosProfile tests QosProfile delete validation should remove gateway pod from spoke":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":2,"unknown":0,"total":3},"items":[{"uid":"7eaf0620d59e96ac","status":"passed","time":{"start":1666674621000,"stop":1666674621005,"duration":5}},{"uid":"bba4d4cc33518b8d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674212000,"stop":1666674212000,"duration":0}},{"uid":"c65d1b30856f0561","status":"passed","time":{"start":1666665366000,"stop":1666665366007,"duration":7}}]},"Worker Suite:Worker Suite#[It] Hub netpol scenarios: Slice Configs Netpol scenarios Should fail while Deleting Slice without removing the namespaceIsolationProfile part in slice configs":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":3,"unknown":0,"total":3},"items":[{"uid":"743d14dad1ce46e6","status":"passed","time":{"start":1666674621000,"stop":1666674621123,"duration":123}},{"uid":"518eecbaf5d57e51","status":"passed","time":{"start":1666674212000,"stop":1666674212127,"duration":127}},{"uid":"ae82511a404d76ea","status":"passed","time":{"start":1666665366000,"stop":1666665366072,"duration":72}}]},"Intracluster Suite:Intracluster Suite#[It] Intracluster slice tests Iperf connectivity Verify Connectivity between iperf-server and iperf-client":{"statistic":{"failed":1,"broken":0,"skipped":1,"passed":1,"unknown":0,"total":3},"items":[{"uid":"728112fd2bc3c4e9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674484000,"stop":1666674484000,"duration":0}},{"uid":"fcc62dcd713f4eb5","status":"failed","statusDetails":"Timed out after 240.001s.\nExpected\n    <bool>: false\nto be true","time":{"start":1666673362000,"stop":1666673610529,"duration":248529}},{"uid":"d66eaf4cd5c83125","status":"passed","time":{"start":1666664909000,"stop":1666664930257,"duration":21257}}]},"Worker Suite:Worker Suite#[It] Slice deletion tests: Slice is installed & validated Slice object is deleted from hub should remove vl3 router from spoke":{"statistic":{"failed":3,"broken":0,"skipped":0,"passed":0,"unknown":0,"total":3},"items":[{"uid":"bf752ce981917883","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1666674621000,"stop":1666674807506,"duration":186506}},{"uid":"315eb90523ec71e9","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1666674212000,"stop":1666674394862,"duration":182862}},{"uid":"9e0f64da63ebebd6","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1666665366000,"stop":1666665551829,"duration":185829}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests slice with netpol should have gateway pod running":{"statistic":{"failed":2,"broken":0,"skipped":1,"passed":0,"unknown":0,"total":3},"items":[{"uid":"52d3f76573dd67c9","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1666674621000,"stop":1666674814549,"duration":193549}},{"uid":"6e0670b861134228","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674212000,"stop":1666674212000,"duration":0}},{"uid":"c28104908256c621","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1666665366000,"stop":1666665633328,"duration":267328}}]},"Worker Suite:Worker Suite#[It] Slice Config Negative Scenario tests Slice Config Negative Scenario Update Should fail while updating sliceconfig with with invalid DSCP class in qosprofiledetails":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":3,"unknown":0,"total":3},"items":[{"uid":"93efa331b6fd9539","status":"passed","time":{"start":1666674621000,"stop":1666674622750,"duration":1750}},{"uid":"bb073809c7b7117c","status":"passed","time":{"start":1666674212000,"stop":1666674213045,"duration":1045}},{"uid":"2b98eb78128d016f","status":"passed","time":{"start":1666665366000,"stop":1666665366659,"duration":659}}]},"Empty Suite:Empty Suite#[It] Hub Deletion tests Hub Uninstalltion Test Scenarios Should pass if project is uninstalled first and then hub":{"statistic":{"failed":3,"broken":0,"skipped":0,"passed":8,"unknown":0,"total":11},"items":[{"uid":"1fe2d47ee0023794","status":"passed","time":{"start":1666883811000,"stop":1666883928221,"duration":117221}},{"uid":"66de5df2f5972e23","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000f6c8>: {\n        Underlying: <*exec.ExitError | 0xc0000788e0>{\n            ProcessState: {\n                pid: 6191,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 49732},\n                    Stime: {Sec: 0, Usec: 13563},\n                    Maxrss: 40736,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 1959,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 282,\n                    Nivcsw: 110,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"install.go:192: [debug] Original chart version: \\\"\\\"\",\n                        \"Error: INSTALLATION FAILED: looks like \\\"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages\\\" is not a valid chart repository or cannot be reached: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\",\n                        \"helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\",\n                        \"helm.sh/helm/v3/pkg/repo.(*ChartRepository).DownloadIndexFile\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:128\",\n                        \"helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:252\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:723\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:204\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:139\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:872\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:990\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:918\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:250\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1571\",\n                        \"looks like \\\"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages\\\" is not a valid chart repository or cannot be reached\",\n                        \"helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:254\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:723\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:204\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:139\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n           ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; install.go:192: [debug] Original chart version: \"\"\n    Error: INSTALLATION FAILED: looks like \"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages\" is not a valid chart repository or cannot be reached: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\n    helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\n    helm.sh/helm/v3/pkg/repo.(*ChartRepository).DownloadIndexFile\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:128\n    helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:252\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:723\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:204\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:139\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    looks like \"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages\" is not a valid chart repository or cannot be reached\n    helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:254\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:723\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:204\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:139\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:141\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\noccurred","time":{"start":1666754109000,"stop":1666754137162,"duration":28162}},{"uid":"5f434379a6eed139","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000e720>: {\n        Underlying: <*exec.ExitError | 0xc00049e060>{\n            ProcessState: {\n                pid: 6221,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 41391},\n                    Stime: {Sec: 0, Usec: 16556},\n                    Maxrss: 40680,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 1982,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 328,\n                    Nivcsw: 119,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"install.go:192: [debug] Original chart version: \\\"\\\"\",\n                        \"Error: INSTALLATION FAILED: looks like \\\"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/\\\" is not a valid chart repository or cannot be reached: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\",\n                        \"helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\",\n                        \"helm.sh/helm/v3/pkg/repo.(*ChartRepository).DownloadIndexFile\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:128\",\n                        \"helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:252\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:723\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:204\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:139\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:872\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:990\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:918\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:250\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1571\",\n                        \"looks like \\\"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/\\\" is not a valid chart repository or cannot be reached\",\n                        \"helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:254\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:723\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:204\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:139\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n         ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; install.go:192: [debug] Original chart version: \"\"\n    Error: INSTALLATION FAILED: looks like \"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/\" is not a valid chart repository or cannot be reached: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\n    helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\n    helm.sh/helm/v3/pkg/repo.(*ChartRepository).DownloadIndexFile\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:128\n    helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:252\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:723\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:204\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:139\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    looks like \"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/\" is not a valid chart repository or cannot be reached\n    helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:254\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:723\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:204\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:139\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:141\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\noccurred","time":{"start":1666750338000,"stop":1666750364302,"duration":26302}},{"uid":"d549f70f77d90c0d","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000e3a8>: {\n        Underlying: <*exec.ExitError | 0xc0005227e0>{\n            ProcessState: {\n                pid: 6220,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 35149},\n                    Stime: {Sec: 0, Usec: 24604},\n                    Maxrss: 40492,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 1942,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 236,\n                    Nivcsw: 49,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"install.go:192: [debug] Original chart version: \\\"\\\"\",\n                        \"Error: INSTALLATION FAILED: looks like \\\"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/\\\" is not a valid chart repository or cannot be reached: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\",\n                        \"helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\",\n                        \"helm.sh/helm/v3/pkg/repo.(*ChartRepository).DownloadIndexFile\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:128\",\n                        \"helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:252\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:723\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:204\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:139\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:872\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:990\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:918\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:250\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1571\",\n                        \"looks like \\\"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/\\\" is not a valid chart repository or cannot be reached\",\n                        \"helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:254\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:723\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:204\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:139\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n          ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; install.go:192: [debug] Original chart version: \"\"\n    Error: INSTALLATION FAILED: looks like \"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/\" is not a valid chart repository or cannot be reached: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\n    helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\n    helm.sh/helm/v3/pkg/repo.(*ChartRepository).DownloadIndexFile\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:128\n    helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:252\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:723\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:204\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:139\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    looks like \"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/\" is not a valid chart repository or cannot be reached\n    helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:254\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:723\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:204\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:139\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:141\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\noccurred","time":{"start":1666747791000,"stop":1666747814524,"duration":23524}},{"uid":"2127ac8734f2f1aa","status":"passed","time":{"start":1666706575000,"stop":1666706684410,"duration":109410}},{"uid":"887a3add36672719","status":"passed","time":{"start":1666077791000,"stop":1666077906721,"duration":115721}},{"uid":"90d2ff014c6fd9a4","status":"passed","time":{"start":1665944068000,"stop":1665944170151,"duration":102151}},{"uid":"3c3e4f3a1ced3ca6","status":"passed","time":{"start":1665665806000,"stop":1665665858357,"duration":52357}},{"uid":"4c03dc2cef90ec98","status":"passed","time":{"start":1665665864000,"stop":1665665929453,"duration":65453}},{"uid":"a8aa711dae237e","status":"passed","time":{"start":1665552534000,"stop":1665552596081,"duration":62081}},{"uid":"9bf04857a45b9492","status":"passed","time":{"start":1664798682000,"stop":1664798747095,"duration":65095}}]},"Worker Suite:Worker Suite#[It] Hub netpol scenarios: Slice Configs Netpol scenarios Should fail when deploying with invalid clusters in both applicationNamespace and allowedNamespace":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":3,"unknown":0,"total":3},"items":[{"uid":"6b81fb4b54c12076","status":"passed","time":{"start":1666674621000,"stop":1666674622591,"duration":1591}},{"uid":"374a98a82b77e6c0","status":"passed","time":{"start":1666674212000,"stop":1666674213970,"duration":1970}},{"uid":"edd5d107323ba0dd","status":"passed","time":{"start":1666665366000,"stop":1666665367477,"duration":1477}}]},"Worker Suite:Worker Suite#[It]  QosProfile tests QosProfile Create validation Create qos profile- validation of BandwidthCeilingKbps > BandwidthGuaranteedKbps":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":3,"unknown":0,"total":3},"items":[{"uid":"9671b111380c118","status":"passed","time":{"start":1666674621000,"stop":1666674621857,"duration":857}},{"uid":"cf2b5923f1b10871","status":"passed","time":{"start":1666674212000,"stop":1666674212445,"duration":445}},{"uid":"5b41accb7aae5cdb","status":"passed","time":{"start":1666665366000,"stop":1666665366479,"duration":479}}]},"Worker Suite:Worker Suite#[It] Hub netpol scenarios: Slice Configs Netpol scenarios Should fail when deploying with invalid namespace in both applicationNamespace and allowedNamespace":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":3,"unknown":0,"total":3},"items":[{"uid":"3aefd471e51691b8","status":"passed","time":{"start":1666674621000,"stop":1666674622116,"duration":1116}},{"uid":"fa09d3209c896077","status":"passed","time":{"start":1666674212000,"stop":1666674213180,"duration":1180}},{"uid":"e5ae6dfbfc0f153b","status":"passed","time":{"start":1666665366000,"stop":1666665367310,"duration":1310}}]},"Hub Suite:Hub Suite#[AfterSuite]":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":4,"unknown":0,"total":4},"items":[{"uid":"93cfa4ef0cf37672","status":"passed","time":{"start":1666668667000,"stop":1666668669056,"duration":2056}},{"uid":"7ae6eb20bb60067d","status":"passed","time":{"start":1666419212000,"stop":1666419214361,"duration":2361}},{"uid":"e98a559a225fcce3","status":"passed","time":{"start":1665665955000,"stop":1665666257388,"duration":302388}},{"uid":"8bbbfaab38a08bb","status":"passed","time":{"start":1665666027000,"stop":1665666031529,"duration":4529}}]},"Hub Suite:Hub Suite#[It] Cluster CR tests Cluster CR validation Creates cluster secrets":{"statistic":{"failed":2,"broken":0,"skipped":0,"passed":0,"unknown":0,"total":2},"items":[{"uid":"316bf71cda515d92","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0006c22e8>: {\n        Underlying: <*exec.ExitError | 0xc0005feac0>{\n            ProcessState: {\n                pid: 7591,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 202287},\n                    Stime: {Sec: 0, Usec: 47597},\n                    Maxrss: 82156,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 13136,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13760,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 540,\n                    Nivcsw: 485,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Cluster%20CR%20tests%20Cluster%20CR%20validation%20Creates%20cluster%20secrets61365964\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.94.3:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Cluster%20CR%20tests%20Cluster%20CR%20validation%20Creates%20cluster%20secrets61365964\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.94.3:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Cluster%20CR%20tests%20Cluster%20CR%20validation%20Creates%20cluster%20secrets61365964\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.94.3:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Cluster%20CR%20tests%20Cluster%20CR%20validation%20Creates%20cluster%20secrets61365964\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.94.3:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Cluster%20CR%20tests%20Cluster%20CR%20validation%20Creates%20cluster%20secrets61365964\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.94.3:443: connect: connection refused\noccurred","time":{"start":1665665955000,"stop":1665665955341,"duration":341}},{"uid":"25ff6d4442713bdd","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000535008>: {\n        Underlying: <*exec.ExitError | 0xc0006af1e0>{\n            ProcessState: {\n                pid: 7108,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 282963},\n                    Stime: {Sec: 0, Usec: 59126},\n                    Maxrss: 73396,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 6185,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13760,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 705,\n                    Nivcsw: 648,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Cluster%20CR%20tests%20Cluster%20CR%20validation%20Creates%20cluster%20secrets3063442492\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.116.233:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Cluster%20CR%20tests%20Cluster%20CR%20validation%20Creates%20cluster%20secrets3063442492\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.116.233:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Cluster%20CR%20tests%20Cluster%20CR%20validation%20Creates%20cluster%20secrets3063442492\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.116.233:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Cluster%20CR%20tests%20Cluster%20CR%20validation%20Creates%20cluster%20secrets3063442492\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.116.233:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Cluster%20CR%20tests%20Cluster%20CR%20validation%20Creates%20cluster%20secrets3063442492\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.116.233:443: connect: connection refused\noccurred","time":{"start":1665666027000,"stop":1665666027478,"duration":478}}]},"Worker Suite:Worker Suite#[It] Hub netpol scenarios: Slice Configs Netpol scenarios Should remove successfully while Deleting Slice after removing the applicationNamespace in namespaceIsolationProfile part in slice configs":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":3,"unknown":0,"total":3},"items":[{"uid":"72f5821dc3c138ef","status":"passed","time":{"start":1666674621000,"stop":1666674631986,"duration":10986}},{"uid":"80b0945b01f4e470","status":"passed","time":{"start":1666674212000,"stop":1666674223466,"duration":11466}},{"uid":"db37d1839d1fe2e1","status":"passed","time":{"start":1666665366000,"stop":1666665376901,"duration":10901}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Iperf connectivity should onboard deployment in application Namespaces without slice annotations":{"statistic":{"failed":2,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":3},"items":[{"uid":"d07fb62c0db03ebe","status":"passed","time":{"start":1666674621000,"stop":1666674639275,"duration":18275}},{"uid":"c0582ded4629581e","status":"failed","statusDetails":"Unexpected error:\n    <*errors.StatusError | 0xc0008fa1e0>: {\n        ErrStatus: {\n            TypeMeta: {Kind: \"\", APIVersion: \"\"},\n            ListMeta: {\n                SelfLink: \"\",\n                ResourceVersion: \"\",\n                Continue: \"\",\n                RemainingItemCount: nil,\n            },\n            Status: \"Failure\",\n            Message: \"namespaces \\\"iperf\\\" already exists\",\n            Reason: \"AlreadyExists\",\n            Details: {Name: \"iperf\", Group: \"\", Kind: \"namespaces\", UID: \"\", Causes: nil, RetryAfterSeconds: 0},\n            Code: 409,\n        },\n    }\n    namespaces \"iperf\" already exists\noccurred","time":{"start":1666674212000,"stop":1666674216632,"duration":4632}},{"uid":"ddf8427c2f145b9a","status":"failed","statusDetails":"Unexpected error:\n    <*errors.StatusError | 0xc0007dca00>: {\n        ErrStatus: {\n            TypeMeta: {Kind: \"\", APIVersion: \"\"},\n            ListMeta: {\n                SelfLink: \"\",\n                ResourceVersion: \"\",\n                Continue: \"\",\n                RemainingItemCount: nil,\n            },\n            Status: \"Failure\",\n            Message: \"namespaces \\\"iperf\\\" already exists\",\n            Reason: \"AlreadyExists\",\n            Details: {Name: \"iperf\", Group: \"\", Kind: \"namespaces\", UID: \"\", Causes: nil, RetryAfterSeconds: 0},\n            Code: 409,\n        },\n    }\n    namespaces \"iperf\" already exists\noccurred","time":{"start":1666665366000,"stop":1666665367449,"duration":1449}}]},"Worker Suite:Worker Suite#[It] Namespace reconcile test scenarios: Offboard remove label from app ns":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":3,"unknown":0,"total":3},"items":[{"uid":"1c8d5d6edc84399e","status":"passed","time":{"start":1666674621000,"stop":1666674621254,"duration":254}},{"uid":"3ee9b2c994816320","status":"passed","time":{"start":1666674212000,"stop":1666674212206,"duration":206}},{"uid":"d4d8cf18c2977cf9","status":"passed","time":{"start":1666665366000,"stop":1666665366351,"duration":351}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting nsm-kernel-forwarder pod Check ping between iperf-server and iperf-client after nsm-kernel-forwarder pod restart":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":1,"unknown":0,"total":3},"items":[{"uid":"b7834886f14f67c7","status":"passed","time":{"start":1666674621000,"stop":1666674638563,"duration":17563}},{"uid":"5501783c25343284","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674212000,"stop":1666674212000,"duration":0}},{"uid":"45e80585dafa0e71","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666665366000,"stop":1666665366000,"duration":0}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Creation Should fail while creating Project with  project name as blank":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":2,"unknown":0,"total":2},"items":[{"uid":"5f480c4f8ff748c4","status":"passed","time":{"start":1665665955000,"stop":1665665955295,"duration":295}},{"uid":"d0471f0f8c91f192","status":"passed","time":{"start":1665666027000,"stop":1665666027370,"duration":370}}]},"Istio Suite:Istio Suite#[AfterSuite]":{"statistic":{"failed":6,"broken":0,"skipped":0,"passed":0,"unknown":0,"total":6},"items":[{"uid":"ebb3d6d8b92be2b","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0006e6690>: {\n        Underlying: <*exec.ExitError | 0xc0000b20c0>{\n            ProcessState: {\n                pid: 6400,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 40920},\n                    Stime: {Sec: 0, Usec: 13640},\n                    Maxrss: 41460,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 1852,\n                    Majflt: 1,\n                    Nswap: 0,\n                    Inblock: 128,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 188,\n                    Nivcsw: 68,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error: uninstall: Release not loaded: istiod: release: not found\noccurred","time":{"start":1666884621000,"stop":1666884621262,"duration":262}},{"uid":"dab26f55a74be52e","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004f5e00>: {\n        Underlying: <*exec.ExitError | 0xc00068fd60>{\n            ProcessState: {\n                pid: 6348,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 43436},\n                    Stime: {Sec: 0, Usec: 4343},\n                    Maxrss: 39780,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 1840,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 239,\n                    Nivcsw: 79,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error: uninstall: Release not loaded: istiod: release: not found\noccurred","time":{"start":1666747979000,"stop":1666747979193,"duration":193}},{"uid":"695120cec797eec","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000e288>: {\n        Underlying: <*exec.ExitError | 0xc00014eda0>{\n            ProcessState: {\n                pid: 6401,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 31761},\n                    Stime: {Sec: 0, Usec: 9074},\n                    Maxrss: 40464,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 1863,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 172,\n                    Nivcsw: 39,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error: uninstall: Release not loaded: istiod: release: not found\noccurred","time":{"start":1666707346000,"stop":1666707346156,"duration":156}},{"uid":"5e8ad4941508d071","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004e87c8>: {\n        Underlying: <*exec.ExitError | 0xc000b82b80>{\n            ProcessState: {\n                pid: 6477,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 46324},\n                    Stime: {Sec: 0, Usec: 12633},\n                    Maxrss: 44792,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 1890,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 258,\n                    Nivcsw: 256,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    Error: uninstall: Release not loaded: istiod: release: not found\noccurred","time":{"start":1665944636000,"stop":1665944637564,"duration":1564}},{"uid":"9f2405fb1ee1ac56","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00035ca50>: {\n        Underlying: <*exec.ExitError | 0xc00045ec60>{\n            ProcessState: {\n                pid: 6141,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 50829},\n                    Stime: {Sec: 0, Usec: 23104},\n                    Maxrss: 42920,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2355,\n                    Majflt: 3,\n                    Nswap: 0,\n                    Inblock: 744,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 240,\n                    Nivcsw: 120,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    Error: uninstall: Release not loaded: istiod: release: not found\noccurred","time":{"start":1665552694000,"stop":1665552695741,"duration":1741}},{"uid":"b75c881d493b2392","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000fc98>: {\n        Underlying: <*exec.ExitError | 0xc00066c3a0>{\n            ProcessState: {\n                pid: 6121,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 39076},\n                    Stime: {Sec: 0, Usec: 21709},\n                    Maxrss: 41528,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 1876,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 231,\n                    Nivcsw: 82,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error: uninstall: Release not loaded: istiod: release: not found\noccurred","time":{"start":1664798844000,"stop":1664798844352,"duration":352}}]},"Worker Suite:Worker Suite#[It] Slice Creation scenarios Slice with MEDIUM QOS should have gateway pod running":{"statistic":{"failed":2,"broken":0,"skipped":1,"passed":0,"unknown":0,"total":3},"items":[{"uid":"666eeac12bb122a5","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1666674621000,"stop":1666674801316,"duration":180316}},{"uid":"63e3a30eccfe903b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674212000,"stop":1666674212000,"duration":0}},{"uid":"a1bc22d70c959661","status":"failed","statusDetails":"Timed out after 180.003s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1666665366000,"stop":1666665711545,"duration":345545}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests Iperf App connectivity Verify Iperf traffic":{"statistic":{"failed":0,"broken":0,"skipped":3,"passed":0,"unknown":0,"total":3},"items":[{"uid":"5887988a96c33f2d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674621000,"stop":1666674621000,"duration":0}},{"uid":"4b2cfbea69e94410","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674212000,"stop":1666674212000,"duration":0}},{"uid":"2cb3e5edae769987","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666665366000,"stop":1666665366000,"duration":0}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests slice with netpol Should label all application namespaces with kubeslice namespace":{"statistic":{"failed":0,"broken":0,"skipped":3,"passed":0,"unknown":0,"total":3},"items":[{"uid":"e9d9f2c9875ebf75","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674621000,"stop":1666674621000,"duration":0}},{"uid":"6879cada6e4a679","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674212000,"stop":1666674212000,"duration":0}},{"uid":"3235e2c9beb61035","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666665366000,"stop":1666665366000,"duration":0}}]},"Worker Suite:Worker Suite#[It] NodeRestartSlice Testing node restart should have vl3 router running":{"statistic":{"failed":2,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":3},"items":[{"uid":"4aee7225483306fd","status":"passed","time":{"start":1666674621000,"stop":1666674658994,"duration":37994}},{"uid":"adbe46e736e25e06","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1666674212000,"stop":1666674395919,"duration":183919}},{"uid":"73cdf85f03daf134","status":"failed","statusDetails":"Expected success, but got an error:\n    <*errors.StatusError | 0xc000a7c280>: {\n        ErrStatus: {\n            TypeMeta: {Kind: \"\", APIVersion: \"\"},\n            ListMeta: {\n                SelfLink: \"\",\n                ResourceVersion: \"\",\n                Continue: \"\",\n                RemainingItemCount: nil,\n            },\n            Status: \"Failure\",\n            Message: \"namespaces \\\"iperf\\\" already exists\",\n            Reason: \"AlreadyExists\",\n            Details: {Name: \"iperf\", Group: \"\", Kind: \"namespaces\", UID: \"\", Causes: nil, RetryAfterSeconds: 0},\n            Code: 409,\n        },\n    }\n    namespaces \"iperf\" already exists","time":{"start":1666665366000,"stop":1666665369025,"duration":3025}}]},"Worker Suite:Worker Suite#[It] Slice Creation scenarios Slice with MEDIUM QOS should reconcile medium qos slice":{"statistic":{"failed":0,"broken":0,"skipped":3,"passed":0,"unknown":0,"total":3},"items":[{"uid":"beaa45a83cf70abc","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674621000,"stop":1666674621000,"duration":0}},{"uid":"45b24f5667fc8084","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674212000,"stop":1666674212000,"duration":0}},{"uid":"f6a951ff7df2ca28","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666665366000,"stop":1666665366000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Iperf connectivity Verify Connectivity between iperf-server and iperf-client":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":1,"unknown":0,"total":3},"items":[{"uid":"747276eef9474554","status":"passed","time":{"start":1666674621000,"stop":1666674641477,"duration":20477}},{"uid":"94aeb7ae13deafed","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674212000,"stop":1666674212000,"duration":0}},{"uid":"499b43cd310d1e3d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666665366000,"stop":1666665366000,"duration":0}}]},"Intracluster Suite:Intracluster Suite#[It] Intracluster slice tests Slice with netpol should have vl3 router running":{"statistic":{"failed":2,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":3},"items":[{"uid":"98c0dc18170eddb2","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00098e930>: {\n        Underlying: <*exec.ExitError | 0xc0009bada0>{\n            ProcessState: {\n                pid: 6163,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 339794},\n                    Stime: {Sec: 0, Usec: 226529},\n                    Maxrss: 104624,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 12818,\n                    Majflt: 2,\n                    Nswap: 0,\n                    Inblock: 232,\n                    Oublock: 19152,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 922,\n                    Nivcsw: 1706,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1666674484000,"stop":1666674486433,"duration":2433}},{"uid":"efb4067803ceb3fd","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1666673362000,"stop":1666673544862,"duration":182862}},{"uid":"88b6b18ca036e926","status":"passed","time":{"start":1666664909000,"stop":1666664927991,"duration":18991}}]},"Worker Suite:Worker Suite#[It] NodeRestartSlice Testing node restart iperf connectivity across multi cluster":{"statistic":{"failed":1,"broken":0,"skipped":2,"passed":0,"unknown":0,"total":3},"items":[{"uid":"9124d72031fd7cd8","status":"failed","statusDetails":"Timed out after 271.905s.\nExpected\n    <bool>: false\nto equal\n    <bool>: true","time":{"start":1666674621000,"stop":1666675082895,"duration":461895}},{"uid":"1eb79d9a76cba23d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674212000,"stop":1666674212000,"duration":0}},{"uid":"23b366779109dbbc","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666665366000,"stop":1666665366000,"duration":0}}]},"Worker Suite:Worker Suite#[It]  QosProfile tests QosProfile delete validation should remove vl3 router from spoke":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":2,"unknown":0,"total":3},"items":[{"uid":"5f455b19d468b248","status":"passed","time":{"start":1666674621000,"stop":1666674624030,"duration":3030}},{"uid":"379d60762346e10c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674212000,"stop":1666674212000,"duration":0}},{"uid":"95037a4e90452294","status":"passed","time":{"start":1666665366000,"stop":1666665369053,"duration":3053}}]},"Hub Suite:Hub Suite#[It] Cluster negative tests Worker Cluster Registration with Wrong token":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":0,"unknown":0,"total":2},"items":[{"uid":"5871eb0969debaeb","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1665665955000,"stop":1665665955000,"duration":0}},{"uid":"3f4ee082fcfa2773","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1665666027000,"stop":1665666027000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice Creation scenarios Slice with MEDIUM QOS should have vl3 router running":{"statistic":{"failed":1,"broken":0,"skipped":0,"passed":2,"unknown":0,"total":3},"items":[{"uid":"98b50f35c1c0cd85","status":"passed","time":{"start":1666674621000,"stop":1666674628968,"duration":7968}},{"uid":"7a97c764f0e1c175","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1666674212000,"stop":1666674393343,"duration":181343}},{"uid":"f6bc733bdd5c0180","status":"passed","time":{"start":1666665366000,"stop":1666665371877,"duration":5877}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Iperf connectivity Verify Iperf traffic":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":1,"unknown":0,"total":3},"items":[{"uid":"6743e34473b8ec6b","status":"passed","time":{"start":1666674621000,"stop":1666674683569,"duration":62569}},{"uid":"c93629510176a4c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674212000,"stop":1666674212000,"duration":0}},{"uid":"9990e928d55743a3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666665366000,"stop":1666665366000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Attach/Deattach Scenario Attach cluster from slice  when Deattach cluster from slice  should have vl3 router deleted from deattach cluster":{"statistic":{"failed":3,"broken":0,"skipped":0,"passed":0,"unknown":0,"total":3},"items":[{"uid":"6790aa0b69862346","status":"failed","statusDetails":"Timed out after 180.004s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1666674621000,"stop":1666674808118,"duration":187118}},{"uid":"22afc2fa8a4c50ac","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1666674212000,"stop":1666674393851,"duration":181851}},{"uid":"a286c88e5be4c754","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1666665366000,"stop":1666665595804,"duration":229804}}]},"Hub Suite:Hub Suite#[BeforeSuite]":{"statistic":{"failed":2,"broken":0,"skipped":0,"passed":2,"unknown":0,"total":4},"items":[{"uid":"121eeb38b5d77fb7","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000f770>: {\n        Underlying: <*exec.ExitError | 0xc000078580>{\n            ProcessState: {\n                pid: 6216,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 43392},\n                    Stime: {Sec: 0, Usec: 20027},\n                    Maxrss: 40456,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2447,\n                    Majflt: 7,\n                    Nswap: 0,\n                    Inblock: 1576,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 373,\n                    Nivcsw: 135,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:192: [debug] Original chart version: \\\"\\\"\",\n                        \"Error: INSTALLATION FAILED: looks like \\\"https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/\\\" is not a valid chart repository or cannot be reached: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/index.yaml : 404 Not Found\",\n                        \"helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/index.yaml : 404 Not Found\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\",\n                        \"helm.sh/helm/v3/pkg/repo.(*ChartRepository).DownloadIndexFile\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:128\",\n                        \"helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:252\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:723\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:204\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:139\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:872\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:990\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:918\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:250\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1571\",\n                        \"looks like \\\"https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/\\\" is not a valid chart repository or cannot be reached\",\n                        \"helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:254\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:723\",\n                        \"m...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:192: [debug] Original chart version: \"\"\n    Error: INSTALLATION FAILED: looks like \"https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/\" is not a valid chart repository or cannot be reached: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/index.yaml : 404 Not Found\n    helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/index.yaml : 404 Not Found\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\n    helm.sh/helm/v3/pkg/repo.(*ChartRepository).DownloadIndexFile\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:128\n    helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:252\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:723\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:204\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:139\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    looks like \"https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/\" is not a valid chart repository or cannot be reached\n    helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:254\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:723\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:204\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:139\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:141\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\noccurred","time":{"start":1666668667000,"stop":1666668694019,"duration":27019}},{"uid":"3e541c77607c27a0","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000361020>: {\n        Underlying: <*exec.ExitError | 0xc0000b4420>{\n            ProcessState: {\n                pid: 6200,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 40289},\n                    Stime: {Sec: 0, Usec: 14389},\n                    Maxrss: 40516,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 1457,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 16,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 224,\n                    Nivcsw: 210,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:192: [debug] Original chart version: \\\"\\\"\",\n                        \"Error: INSTALLATION FAILED: looks like \\\"https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/\\\" is not a valid chart repository or cannot be reached: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/index.yaml : 404 Not Found\",\n                        \"helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/index.yaml : 404 Not Found\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\",\n                        \"helm.sh/helm/v3/pkg/repo.(*ChartRepository).DownloadIndexFile\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:128\",\n                        \"helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:252\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:723\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:204\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:139\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:872\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:990\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:918\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:250\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1571\",\n                        \"looks like \\\"https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/\\\" is not a valid chart repository or cannot be reached\",\n                        \"helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:254\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:723\",\n                        \"mai...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:192: [debug] Original chart version: \"\"\n    Error: INSTALLATION FAILED: looks like \"https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/\" is not a valid chart repository or cannot be reached: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/index.yaml : 404 Not Found\n    helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/index.yaml : 404 Not Found\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\n    helm.sh/helm/v3/pkg/repo.(*ChartRepository).DownloadIndexFile\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:128\n    helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:252\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:723\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:204\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:139\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    looks like \"https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/\" is not a valid chart repository or cannot be reached\n    helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:254\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:723\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:204\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:139\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:141\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\noccurred","time":{"start":1666419212000,"stop":1666419239715,"duration":27715}},{"uid":"818d51499a0ff4b","status":"passed","time":{"start":1665665955000,"stop":1665666052660,"duration":97660}},{"uid":"871ebd3fd7df8e48","status":"passed","time":{"start":1665666027000,"stop":1665666126756,"duration":99756}}]},"Worker Suite:Worker Suite#[It] Slice creation scenarios: when slice red is installed workers should have vl3 router pods running":{"statistic":{"failed":1,"broken":0,"skipped":0,"passed":2,"unknown":0,"total":3},"items":[{"uid":"a96cae28fd8f6de","status":"passed","time":{"start":1666674621000,"stop":1666674625080,"duration":4080}},{"uid":"b0e1d9fbdbfedfef","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1666674212000,"stop":1666674392286,"duration":180286}},{"uid":"159d9a6b5efbb829","status":"passed","time":{"start":1666665366000,"stop":1666665366079,"duration":79}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests Iperf App connectivity Application namespaces should be isolated":{"statistic":{"failed":0,"broken":0,"skipped":3,"passed":0,"unknown":0,"total":3},"items":[{"uid":"4f82d386c9db9523","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674621000,"stop":1666674621000,"duration":0}},{"uid":"7465603fff4565c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674212000,"stop":1666674212000,"duration":0}},{"uid":"f62db318d3c0396c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666665366000,"stop":1666665366000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice deletion tests: Slice is installed & validated Slice object is deleted from hub should remove gateway pod from spoke":{"statistic":{"failed":0,"broken":0,"skipped":3,"passed":0,"unknown":0,"total":3},"items":[{"uid":"ac8356a8d805ccb2","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674621000,"stop":1666674621000,"duration":0}},{"uid":"96f4e754da2dd82d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674212000,"stop":1666674212000,"duration":0}},{"uid":"a8621fd1814b0e8f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666665366000,"stop":1666665366000,"duration":0}}]},"Hub Suite:Hub Suite#[It] Cluster negative tests Worker Cluster Registration with Wrong ca.cert":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":0,"unknown":0,"total":2},"items":[{"uid":"dc025b001ebe1660","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1665665955000,"stop":1665665955000,"duration":0}},{"uid":"b77399730a713ae4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1665666027000,"stop":1665666027000,"duration":0}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Creation Should fail while creating service accounts as combination of special characters":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":2,"unknown":0,"total":2},"items":[{"uid":"8fa10de8639757a4","status":"passed","time":{"start":1665665955000,"stop":1665665956631,"duration":1631}},{"uid":"60eb76f15b503998","status":"passed","time":{"start":1665666027000,"stop":1665666027381,"duration":381}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting nsm-manager pod Check ping between iperf-server and iperf-client after nsm-manager pod restart":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":1,"unknown":0,"total":3},"items":[{"uid":"f1207f9e73d89d9b","status":"passed","time":{"start":1666674621000,"stop":1666674625708,"duration":4708}},{"uid":"a7b071c77f2df46d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674212000,"stop":1666674212000,"duration":0}},{"uid":"226443f6860d9d22","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666665366000,"stop":1666665366000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting nsm-kernel-forwarder pod Should restart nsm-kernel-forwarder pod":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":1,"unknown":0,"total":3},"items":[{"uid":"7dc02fd6d532bdab","status":"passed","time":{"start":1666674621000,"stop":1666674642362,"duration":21362}},{"uid":"14e4112042d871d1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674212000,"stop":1666674212000,"duration":0}},{"uid":"5e46fe7a3387993a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666665366000,"stop":1666665366000,"duration":0}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Creation Should fail while creating Project with project name as combination of special characters":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":2,"unknown":0,"total":2},"items":[{"uid":"a212a4bd2972ea3f","status":"passed","time":{"start":1665665955000,"stop":1665665955271,"duration":271}},{"uid":"c09aa1ec9b1fdf56","status":"passed","time":{"start":1665666027000,"stop":1665666027398,"duration":398}}]},"Worker Suite:Worker Suite#[It] Hub netpol scenarios: Slice Configs Netpol scenarios Should fail while creating namespaces with same name in both allowedNamespace and applicationNamespace":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":3,"unknown":0,"total":3},"items":[{"uid":"6f4256c7e807e45f","status":"passed","time":{"start":1666674621000,"stop":1666674621532,"duration":532}},{"uid":"43da8a7d4a93be05","status":"passed","time":{"start":1666674212000,"stop":1666674212563,"duration":563}},{"uid":"24a30c1be75ce70d","status":"passed","time":{"start":1666665366000,"stop":1666665366526,"duration":526}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting iperf-server pod Should restart iperf-server pod":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":1,"unknown":0,"total":3},"items":[{"uid":"9ea533a13ebfe64f","status":"passed","time":{"start":1666674621000,"stop":1666674652477,"duration":31477}},{"uid":"872f6ddd85cb8ed8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674212000,"stop":1666674212000,"duration":0}},{"uid":"5fc874abecddad35","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666665366000,"stop":1666665366000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting worker-operator pod Should restart worker-operator pod":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":1,"unknown":0,"total":3},"items":[{"uid":"b7509070cb74cbf8","status":"passed","time":{"start":1666674621000,"stop":1666674643216,"duration":22216}},{"uid":"f71193e860bca40b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674212000,"stop":1666674212000,"duration":0}},{"uid":"a0b320a56a2f42c3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666665366000,"stop":1666665366000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice Creation scenarios Slice with LOW QOS should have vl3 router running":{"statistic":{"failed":1,"broken":0,"skipped":0,"passed":2,"unknown":0,"total":3},"items":[{"uid":"3e6209431d484280","status":"passed","time":{"start":1666674621000,"stop":1666674627356,"duration":6356}},{"uid":"344f8969a594d284","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1666674212000,"stop":1666674392690,"duration":180690}},{"uid":"e2b1aef43bd99b87","status":"passed","time":{"start":1666665366000,"stop":1666665370530,"duration":4530}}]},"Worker Suite:Worker Suite#[It]  QosProfile tests QosProfile delete validation Delete qosProfile positive scenario":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":2,"unknown":0,"total":3},"items":[{"uid":"c94780116c11541b","status":"passed","time":{"start":1666674621000,"stop":1666674621619,"duration":619}},{"uid":"34e1c340a6b1ba67","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674212000,"stop":1666674212000,"duration":0}},{"uid":"f8c96d4446700153","status":"passed","time":{"start":1666665366000,"stop":1666665366600,"duration":600}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests slice with netpol should install slice on each worker cluster with correct namespaceisolationprofile":{"statistic":{"failed":0,"broken":0,"skipped":3,"passed":0,"unknown":0,"total":3},"items":[{"uid":"5afe915ec4ef443","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674621000,"stop":1666674621000,"duration":0}},{"uid":"7a40e03716975881","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674212000,"stop":1666674212000,"duration":0}},{"uid":"9953240102436e6c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666665366000,"stop":1666665366000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Namespace reconcile test scenarios: Delete deboarded app ns gets deleted":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":3,"unknown":0,"total":3},"items":[{"uid":"c20f35687316643a","status":"passed","time":{"start":1666674621000,"stop":1666674631907,"duration":10907}},{"uid":"7a71374e3d7029d7","status":"passed","time":{"start":1666674212000,"stop":1666674222865,"duration":10865}},{"uid":"f0ae2683397d6f12","status":"passed","time":{"start":1666665366000,"stop":1666665376691,"duration":10691}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting mesh-dns pod Check ping between iperf-server and iperf-client after mesh-dns pod restart":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":1,"unknown":0,"total":3},"items":[{"uid":"795604882163a276","status":"passed","time":{"start":1666674621000,"stop":1666674625979,"duration":4979}},{"uid":"5fc987fdc73cb652","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674212000,"stop":1666674212000,"duration":0}},{"uid":"3126fbb1c95819d3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666665366000,"stop":1666665366000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice creation scenarios: when slice red is installed install slice blue workers should get attached to slice blue":{"statistic":{"failed":0,"broken":0,"skipped":3,"passed":0,"unknown":0,"total":3},"items":[{"uid":"623b6283143d6cb9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674621000,"stop":1666674621000,"duration":0}},{"uid":"ccc53c3bbcf15049","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674212000,"stop":1666674212000,"duration":0}},{"uid":"6a21071cef435bc3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666665366000,"stop":1666665366000,"duration":0}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests slice with netpol should have vl3 router running":{"statistic":{"failed":1,"broken":0,"skipped":0,"passed":2,"unknown":0,"total":3},"items":[{"uid":"ca083833bbd2c51b","status":"passed","time":{"start":1666674621000,"stop":1666674630435,"duration":9435}},{"uid":"7bce7e2e0613f981","status":"failed","statusDetails":"Unexpected error:\n    <*errors.StatusError | 0xc000620d20>: {\n        ErrStatus: {\n            TypeMeta: {Kind: \"\", APIVersion: \"\"},\n            ListMeta: {\n                SelfLink: \"\",\n                ResourceVersion: \"\",\n                Continue: \"\",\n                RemainingItemCount: nil,\n            },\n            Status: \"Failure\",\n            Message: \"namespaces \\\"iperf\\\" already exists\",\n            Reason: \"AlreadyExists\",\n            Details: {Name: \"iperf\", Group: \"\", Kind: \"namespaces\", UID: \"\", Causes: nil, RetryAfterSeconds: 0},\n            Code: 409,\n        },\n    }\n    namespaces \"iperf\" already exists\noccurred","time":{"start":1666674212000,"stop":1666674227920,"duration":15920}},{"uid":"bf8ec6b81b7b173c","status":"passed","time":{"start":1666665366000,"stop":1666665390279,"duration":24279}}]},"Intracluster Suite:Intracluster Suite#[AfterSuite]":{"statistic":{"failed":5,"broken":0,"skipped":0,"passed":19,"unknown":0,"total":24},"items":[{"uid":"1d03fa2ccf97a621","status":"failed","statusDetails":"Timed out after 210.009s.\nExpected an error to have occurred.  Got:\n    <nil>: nil","time":{"start":1667385281000,"stop":1667385491521,"duration":210521}},{"uid":"1513134f22bebd78","status":"failed","statusDetails":"Timed out after 210.009s.\nExpected an error to have occurred.  Got:\n    <nil>: nil","time":{"start":1667293744000,"stop":1667293954508,"duration":210508}},{"uid":"c3c47eaeae3d9d21","status":"failed","statusDetails":"Timed out after 210.009s.\nExpected an error to have occurred.  Got:\n    <nil>: nil","time":{"start":1666883986000,"stop":1666884196589,"duration":210589}},{"uid":"97ee1cbc244fb1bb","status":"passed","time":{"start":1666754211000,"stop":1666754212807,"duration":1807}},{"uid":"7bdc992f5ad72069","status":"passed","time":{"start":1666750446000,"stop":1666750447645,"duration":1645}},{"uid":"4bba68cf13487624","status":"passed","time":{"start":1666747897000,"stop":1666747898936,"duration":1936}},{"uid":"daf83020bac04de3","status":"failed","statusDetails":"Timed out after 210.010s.\nExpected an error to have occurred.  Got:\n    <nil>: nil","time":{"start":1666706731000,"stop":1666706941464,"duration":210464}},{"uid":"55e1507f521d9efa","status":"passed","time":{"start":1666674484000,"stop":1666674512118,"duration":28118}},{"uid":"d915bc794f9d7c72","status":"passed","time":{"start":1666673362000,"stop":1666673548752,"duration":186752}},{"uid":"ca3635dd15d53390","status":"passed","time":{"start":1666668696000,"stop":1666668698208,"duration":2208}},{"uid":"38543d1808ce6315","status":"failed","statusDetails":"Timed out after 210.008s.\nExpected an error to have occurred.  Got:\n    <nil>: nil","time":{"start":1666667460000,"stop":1666667670705,"duration":210705}},{"uid":"b67e1956960b2662","status":"passed","time":{"start":1666664909000,"stop":1666665094865,"duration":185865}},{"uid":"885a5e878e87f36","status":"passed","time":{"start":1666664766000,"stop":1666664767161,"duration":1161}},{"uid":"b33ecf433d5b3f0a","status":"passed","time":{"start":1666663248000,"stop":1666663249494,"duration":1494}},{"uid":"db4d8f02e6fa7849","status":"passed","time":{"start":1666423616000,"stop":1666423642847,"duration":26847}},{"uid":"60be839a65868056","status":"passed","time":{"start":1666419243000,"stop":1666419245822,"duration":2822}},{"uid":"e6cd9679bc94a2c0","status":"passed","time":{"start":1666338675000,"stop":1666338700065,"duration":25065}},{"uid":"da5cffb871f366af","status":"passed","time":{"start":1666258902000,"stop":1666258903653,"duration":1653}},{"uid":"844fd1c62d5765c4","status":"passed","time":{"start":1666253957000,"stop":1666253983407,"duration":26407}},{"uid":"7eab5702f4ecda53","status":"passed","time":{"start":1666253081000,"stop":1666253082867,"duration":1867}}]},"Empty Suite:Empty Suite#[BeforeSuite]":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":11,"unknown":0,"total":11},"items":[{"uid":"5bfccb4ef5b4d40a","status":"passed","time":{"start":1666883811000,"stop":1666883811000,"duration":0}},{"uid":"c02da9839f226eb6","status":"passed","time":{"start":1666754109000,"stop":1666754109000,"duration":0}},{"uid":"73307383b3a2ce0e","status":"passed","time":{"start":1666750338000,"stop":1666750338000,"duration":0}},{"uid":"95e2e9774a76b04a","status":"passed","time":{"start":1666747791000,"stop":1666747791000,"duration":0}},{"uid":"88dbcfb7b5612b32","status":"passed","time":{"start":1666706575000,"stop":1666706575000,"duration":0}},{"uid":"a5f573464ee8bfec","status":"passed","time":{"start":1666077791000,"stop":1666077791000,"duration":0}},{"uid":"a1f151e7e4d89ce","status":"passed","time":{"start":1665944068000,"stop":1665944068000,"duration":0}},{"uid":"263b687c417bf2c4","status":"passed","time":{"start":1665665806000,"stop":1665665806000,"duration":0}},{"uid":"db912ffd987e6511","status":"passed","time":{"start":1665665864000,"stop":1665665864000,"duration":0}},{"uid":"9846427ee6b049a","status":"passed","time":{"start":1665552534000,"stop":1665552534000,"duration":0}},{"uid":"ae06bc63f9c946bf","status":"passed","time":{"start":1664798682000,"stop":1664798682000,"duration":0}}]},"Hub Suite:Hub Suite#[It] Cluster negative tests Worker Cluster Registration with Wrong clustername":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":0,"unknown":0,"total":2},"items":[{"uid":"a768a6dadd8b6676","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1665665955000,"stop":1665665955000,"duration":0}},{"uid":"76fe2cb00584df6a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1665666027000,"stop":1665666027000,"duration":0}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Update Should fail when Project is Applied with service account name as blank in Read users":{"statistic":{"failed":2,"broken":0,"skipped":0,"passed":0,"unknown":0,"total":2},"items":[{"uid":"4e017b39e0e50605","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000ec90>: {\n        Underlying: <*exec.ExitError | 0xc0006be860>{\n            ProcessState: {\n                pid: 7649,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 176035},\n                    Stime: {Sec: 0, Usec: 58678},\n                    Maxrss: 84564,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 10099,\n                    Majflt: 1,\n                    Nswap: 0,\n                    Inblock: 232,\n                    Oublock: 13760,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 401,\n                    Nivcsw: 339,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20blank%20in%20Read%20users1275205082\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.94.3:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20blank%20in%20Read%20users1275205082\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.94.3:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20blank%20in%20Read%20users1275205082\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.94.3:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20blank%20in%20Read%20users1275205082\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.94.3:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20blank%20in%20Read%20users1275205082\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.94.3:443: connect: connection refused\noccurred","time":{"start":1665665955000,"stop":1665665955295,"duration":295}},{"uid":"fffc98573e9a4dea","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0001a5698>: {\n        Underlying: <*exec.ExitError | 0xc0007a6140>{\n            ProcessState: {\n                pid: 7224,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 236514},\n                    Stime: {Sec: 0, Usec: 81556},\n                    Maxrss: 86132,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 6310,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13760,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 501,\n                    Nivcsw: 493,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20blank%20in%20Read%20users137744948\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.116.233:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20blank%20in%20Read%20users137744948\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.116.233:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20blank%20in%20Read%20users137744948\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.116.233:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20blank%20in%20Read%20users137744948\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.116.233:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20blank%20in%20Read%20users137744948\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.116.233:443: connect: connection refused\noccurred","time":{"start":1665666027000,"stop":1665666027486,"duration":486}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting mesh-dns pod Should restart mesh-dns pod":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":1,"unknown":0,"total":3},"items":[{"uid":"aa3187964e003665","status":"passed","time":{"start":1666674621000,"stop":1666674632214,"duration":11214}},{"uid":"92096330ef90d831","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674212000,"stop":1666674212000,"duration":0}},{"uid":"f32accac86600d0b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666665366000,"stop":1666665366000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Namespace reconcile test scenarios: Delete deleted app ns entry should get removed from cluster objs":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":3,"unknown":0,"total":3},"items":[{"uid":"a2429bb1269b6565","status":"passed","time":{"start":1666674621000,"stop":1666674632581,"duration":11581}},{"uid":"9ea2e74fafb33f2d","status":"passed","time":{"start":1666674212000,"stop":1666674223479,"duration":11479}},{"uid":"2918e9e1d827a43d","status":"passed","time":{"start":1666665366000,"stop":1666665377246,"duration":11246}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting nsm-manager pod Should restart nsm-manager pod":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":1,"unknown":0,"total":3},"items":[{"uid":"23059a6586789f86","status":"passed","time":{"start":1666674621000,"stop":1666674652344,"duration":31344}},{"uid":"840e42421136972a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674212000,"stop":1666674212000,"duration":0}},{"uid":"711680b9e167e1dc","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666665366000,"stop":1666665366000,"duration":0}}]},"Empty Suite:Empty Suite#[It] Hub Creation tests Hub Installtion Test Senarios Should fail if Cert is installed again in the same cluster":{"statistic":{"failed":0,"broken":0,"skipped":3,"passed":8,"unknown":0,"total":11},"items":[{"uid":"81b3bd7d97fb5ed7","status":"passed","time":{"start":1666883811000,"stop":1666883811309,"duration":309}},{"uid":"f7dc1aaaa73f047b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666754109000,"stop":1666754109000,"duration":0}},{"uid":"aa7d9fbc6ee10cf9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666750338000,"stop":1666750338000,"duration":0}},{"uid":"538dd2490b295a64","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666747791000,"stop":1666747791000,"duration":0}},{"uid":"ebf087016e9d1d6f","status":"passed","time":{"start":1666706575000,"stop":1666706575360,"duration":360}},{"uid":"832ff072c9ead5c8","status":"passed","time":{"start":1666077791000,"stop":1666077791293,"duration":293}},{"uid":"da31357bb6b8729b","status":"passed","time":{"start":1665944068000,"stop":1665944068319,"duration":319}},{"uid":"2136aec3c7584a18","status":"passed","time":{"start":1665665806000,"stop":1665665806326,"duration":326}},{"uid":"50a28c6e8cd5f2b","status":"passed","time":{"start":1665665864000,"stop":1665665864306,"duration":306}},{"uid":"1acade2afe20c6ea","status":"passed","time":{"start":1665552534000,"stop":1665552534405,"duration":405}},{"uid":"a9628127a3b7968f","status":"passed","time":{"start":1664798682000,"stop":1664798682296,"duration":296}}]},"36e5cc2aa1b3fe54ab1d2ed6d47adaa2":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"93867d9c48a2e27c","status":"passed","time":{"start":1664357350099,"stop":1664357352298,"duration":2199}}]},"Intracluster Suite:Intracluster Suite#[It] Intracluster slice tests Slice with netpol should remove network policy resource from application namespace once the isolationEnabled is set to false":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":1,"unknown":0,"total":3},"items":[{"uid":"45d160ebb6bd26e8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674484000,"stop":1666674484000,"duration":0}},{"uid":"2c13d4b5b15275f1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666673362000,"stop":1666673362000,"duration":0}},{"uid":"4c01f25e6fd89cb2","status":"passed","time":{"start":1666664909000,"stop":1666664916324,"duration":7324}}]},"Intracluster Suite:Intracluster Suite#[It] Intracluster slice tests Iperf connectivity should onboard deployment in application Namespaces without slice annotations":{"statistic":{"failed":1,"broken":0,"skipped":0,"passed":2,"unknown":0,"total":3},"items":[{"uid":"86e64c68bb6fafe9","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00098e018>: {\n        Underlying: <*exec.ExitError | 0xc0009ba040>{\n            ProcessState: {\n                pid: 6173,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 239894},\n                    Stime: {Sec: 0, Usec: 110720},\n                    Maxrss: 108920,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 12618,\n                    Majflt: 3,\n                    Nswap: 0,\n                    Inblock: 448,\n                    Oublock: 19152,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 527,\n                    Nivcsw: 677,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1666674484000,"stop":1666674484731,"duration":731}},{"uid":"54a20ccea13007ee","status":"passed","time":{"start":1666673362000,"stop":1666673513871,"duration":151871}},{"uid":"237cb8d32ad34257","status":"passed","time":{"start":1666664909000,"stop":1666664955391,"duration":46391}}]},"Intracluster Suite:Intracluster Suite#[It] Intracluster slice tests Slice with netpol should reconcile netpol config":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":1,"unknown":0,"total":3},"items":[{"uid":"46a72b00f117dc2e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674484000,"stop":1666674484000,"duration":0}},{"uid":"a6f9755d28e3c8e4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666673362000,"stop":1666673362000,"duration":0}},{"uid":"5af15f7d7437ec3f","status":"passed","time":{"start":1666664909000,"stop":1666664917518,"duration":8518}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting iperf-client pod Check ping between iperf-server and iperf-client after iperf-client pod restart":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":1,"unknown":0,"total":3},"items":[{"uid":"37a3970ca3e3d958","status":"passed","time":{"start":1666674621000,"stop":1666674640806,"duration":19806}},{"uid":"131e56b48f1142c3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674212000,"stop":1666674212000,"duration":0}},{"uid":"c467e55dfbe9bd2e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666665366000,"stop":1666665366000,"duration":0}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests slice with netpol Should install networkpolicies in all application namespaces":{"statistic":{"failed":0,"broken":0,"skipped":3,"passed":0,"unknown":0,"total":3},"items":[{"uid":"aa0b085c594c305e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674621000,"stop":1666674621000,"duration":0}},{"uid":"a4f35d06f14f6c6b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674212000,"stop":1666674212000,"duration":0}},{"uid":"6bf9603fb3166cba","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666665366000,"stop":1666665366000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice Creation scenarios Slice with LOW QOS should have gateway pod running":{"statistic":{"failed":2,"broken":0,"skipped":1,"passed":0,"unknown":0,"total":3},"items":[{"uid":"22998905a43b780b","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1666674621000,"stop":1666674801318,"duration":180318}},{"uid":"9cbe6c65d6e1fe78","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674212000,"stop":1666674212000,"duration":0}},{"uid":"c06d0109c7a7aa0f","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1666665366000,"stop":1666665546332,"duration":180332}}]},"Worker Suite:Worker Suite#[It] Hub netpol scenarios: Slice Configs Netpol scenarios Should fail when deploying clusters in namespaceisolationprofile with * and a cluster name in both applicationNamespace and allowedNamespace":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":3,"unknown":0,"total":3},"items":[{"uid":"21728e0aaddf2122","status":"passed","time":{"start":1666674621000,"stop":1666674621879,"duration":879}},{"uid":"779e8a84065c7cab","status":"passed","time":{"start":1666674212000,"stop":1666674212710,"duration":710}},{"uid":"8eb2c1751fc46a5e","status":"passed","time":{"start":1666665366000,"stop":1666665366953,"duration":953}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Update Should update successfully when Project applied with valid service account name in Write users":{"statistic":{"failed":2,"broken":0,"skipped":0,"passed":0,"unknown":0,"total":2},"items":[{"uid":"eedaa7cb93302999","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000e030>: {\n        Underlying: <*exec.ExitError | 0xc0007493e0>{\n            ProcessState: {\n                pid: 7612,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 245108},\n                    Stime: {Sec: 0, Usec: 60334},\n                    Maxrss: 90508,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 11054,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13760,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 486,\n                    Nivcsw: 428,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20applied%20with%20valid%20service%20account%20name%20in%20Write%20users3736952544\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.94.3:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20applied%20with%20valid%20service%20account%20name%20in%20Write%20users3736952544\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.94.3:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20applied%20with%20valid%20service%20account%20name%20in%20Write%20users3736952544\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.94.3:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20applied%20with%20valid%20service%20account%20name%20in%20Write%20users3736952544\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.94.3:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20applied%20with%20valid%20service%20account%20name%20in%20Write%20users3736952544\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.94.3:443: connect: connection refused\noccurred","time":{"start":1665665955000,"stop":1665665955335,"duration":335}},{"uid":"92a1cf6290c73497","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000534bb8>: {\n        Underlying: <*exec.ExitError | 0xc0006a3980>{\n            ProcessState: {\n                pid: 7184,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 242814},\n                    Stime: {Sec: 0, Usec: 80938},\n                    Maxrss: 85764,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 9341,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13760,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 503,\n                    Nivcsw: 352,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20applied%20with%20valid%20service%20account%20name%20in%20Write%20users1327815431\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.116.233:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20applied%20with%20valid%20service%20account%20name%20in%20Write%20users1327815431\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.116.233:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20applied%20with%20valid%20service%20account%20name%20in%20Write%20users1327815431\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.116.233:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20applied%20with%20valid%20service%20account%20name%20in%20Write%20users1327815431\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.116.233:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20applied%20with%20valid%20service%20account%20name%20in%20Write%20users1327815431\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.116.233:443: connect: connection refused\noccurred","time":{"start":1665666027000,"stop":1665666027431,"duration":431}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Update Should fail when Project is Applied with service account name as combination of special characters in Read users":{"statistic":{"failed":2,"broken":0,"skipped":0,"passed":0,"unknown":0,"total":2},"items":[{"uid":"e5f6a8f076b4ecb2","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000e990>: {\n        Underlying: <*exec.ExitError | 0xc0006be320>{\n            ProcessState: {\n                pid: 7640,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 202709},\n                    Stime: {Sec: 0, Usec: 38982},\n                    Maxrss: 84408,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 10340,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13760,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 632,\n                    Nivcsw: 379,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Read%20users647910301\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.94.3:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Read%20users647910301\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.94.3:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Read%20users647910301\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.94.3:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Read%20users647910301\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.94.3:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Read%20users647910301\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.94.3:443: connect: connection refused\noccurred","time":{"start":1665665955000,"stop":1665665955277,"duration":277}},{"uid":"42bade3689220f35","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0001a52d8>: {\n        Underlying: <*exec.ExitError | 0xc0004d9b40>{\n            ProcessState: {\n                pid: 7214,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 256881},\n                    Stime: {Sec: 0, Usec: 58956},\n                    Maxrss: 79860,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 7483,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13760,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 338,\n                    Nivcsw: 443,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Read%20users286678817\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.116.233:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Read%20users286678817\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.116.233:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Read%20users286678817\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.116.233:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Read%20users286678817\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.116.233:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Read%20users286678817\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.116.233:443: connect: connection refused\noccurred","time":{"start":1665666027000,"stop":1665666027384,"duration":384}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Creation Should create Project while using valid manifest":{"statistic":{"failed":2,"broken":0,"skipped":0,"passed":0,"unknown":0,"total":2},"items":[{"uid":"aca79bd4327c5a13","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <bool>: false\nto be true","time":{"start":1665665955000,"stop":1665666032287,"duration":77287}},{"uid":"e467bfa0d24c5ff8","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000535140>: {\n        Underlying: <*exec.ExitError | 0xc0006af720>{\n            ProcessState: {\n                pid: 7118,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 253446},\n                    Stime: {Sec: 0, Usec: 76436},\n                    Maxrss: 85476,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 9921,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13760,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 353,\n                    Nivcsw: 488,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20create%20Project%20while%20using%20valid%20manifest672581222\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.116.233:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20create%20Project%20while%20using%20valid%20manifest672581222\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.116.233:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20create%20Project%20while%20using%20valid%20manifest672581222\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.116.233:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20create%20Project%20while%20using%20valid%20manifest672581222\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.116.233:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20create%20Project%20while%20using%20valid%20manifest672581222\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.116.233:443: connect: connection refused\noccurred","time":{"start":1665666027000,"stop":1665666027426,"duration":426}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting iperf-client pod Should restart iperf-client pod":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":1,"unknown":0,"total":3},"items":[{"uid":"791a66f069e235d8","status":"passed","time":{"start":1666674621000,"stop":1666674652327,"duration":31327}},{"uid":"3db089b9132eea90","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674212000,"stop":1666674212000,"duration":0}},{"uid":"91e74953dc450680","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666665366000,"stop":1666665366000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Namespace reconcile test scenarios: Onboard cluster objs should have app ns & attached slice entry":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":3,"unknown":0,"total":3},"items":[{"uid":"cd301e835b009d37","status":"passed","time":{"start":1666674621000,"stop":1666674621306,"duration":306}},{"uid":"9c5c3913635f3cd5","status":"passed","time":{"start":1666674212000,"stop":1666674212207,"duration":207}},{"uid":"94b5cbfcb105208a","status":"passed","time":{"start":1666665366000,"stop":1666665366286,"duration":286}}]},"c3e9b1aabfe279c6b2b3003fd9939e51":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"227fdc514f111a0","status":"passed","time":{"start":1664357346059,"stop":1664357350079,"duration":4020}}]},"Worker Suite:Worker Suite#[It] Namespace reconcile test scenarios: Onboard label app ns on workers":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":3,"unknown":0,"total":3},"items":[{"uid":"cb9225af274f22f6","status":"passed","time":{"start":1666674621000,"stop":1666674622425,"duration":1425}},{"uid":"57556dc69643b8de","status":"passed","time":{"start":1666674212000,"stop":1666674212960,"duration":960}},{"uid":"212b3a8a57a83a6a","status":"passed","time":{"start":1666665366000,"stop":1666665367771,"duration":1771}}]},"Worker Suite:Worker Suite#[It] Slice creation scenarios: when slice red is installed install slice blue workers should have gateway pods from both slices":{"statistic":{"failed":0,"broken":0,"skipped":3,"passed":0,"unknown":0,"total":3},"items":[{"uid":"c67b5a27f94ff67e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674621000,"stop":1666674621000,"duration":0}},{"uid":"3402f9fda3978c9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674212000,"stop":1666674212000,"duration":0}},{"uid":"bedc8846a08107c9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666665366000,"stop":1666665366000,"duration":0}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests Iperf App connectivity should have vl3 router running":{"statistic":{"failed":1,"broken":0,"skipped":0,"passed":2,"unknown":0,"total":3},"items":[{"uid":"2b17f3ce2005aff0","status":"passed","time":{"start":1666674621000,"stop":1666674622652,"duration":1652}},{"uid":"317698ebeb157f11","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1666674212000,"stop":1666674396467,"duration":184467}},{"uid":"9fd549b7e93f3113","status":"passed","time":{"start":1666665366000,"stop":1666665366968,"duration":968}}]},"Istio Suite:Istio Suite#[BeforeSuite]":{"statistic":{"failed":6,"broken":0,"skipped":0,"passed":0,"unknown":0,"total":6},"items":[{"uid":"5614166770d5903d","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0001db1b8>: {\n        Underlying: <*exec.ExitError | 0xc000488540>{\n            ProcessState: {\n                pid: 6384,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 167887},\n                    Stime: {Sec: 0, Usec: 18149},\n                    Maxrss: 50604,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2828,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 360,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 550,\n                    Nivcsw: 178,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"install.go:192: [debug] Original chart version: \\\"v1.8.0\\\"\",\n                        \"install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\",\n                        \"\",\n                        \"Error: INSTALLATION FAILED: cannot re-use a name that is still in use\",\n                        \"helm.go:84: [debug] cannot re-use a name that is still in use\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).availableName\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:483\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:197\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:278\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:139\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:872\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:990\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:918\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:250\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1571\",\n                        \"INSTALLATION FAILED\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:141\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:872\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:990\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:918\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:250\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1571\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"install.go:192: [debug] Original chart version: \\\"v1.8.0\\\"\",\n                    \"install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\",\n                    \"\",\n                    \"Error: INSTALLATION FAILED: cannot re-use a name that is still in use\",\n                    \"helm.go:84: [debug] cannot re-use a name that is still in ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; install.go:192: [debug] Original chart version: \"v1.8.0\"\n    install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\n    \n    Error: INSTALLATION FAILED: cannot re-use a name that is still in use\n    helm.go:84: [debug] cannot re-use a name that is still in use\n    helm.sh/helm/v3/pkg/action.(*Install).availableName\n    \thelm.sh/helm/v3/pkg/action/install.go:483\n    helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\n    \thelm.sh/helm/v3/pkg/action/install.go:197\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:278\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:139\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:141\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\noccurred","time":{"start":1666884621000,"stop":1666884621480,"duration":480}},{"uid":"d80bc857d1050672","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004789c0>: {\n        Underlying: <*exec.ExitError | 0xc00048b660>{\n            ProcessState: {\n                pid: 6336,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 57756},\n                    Stime: {Sec: 0, Usec: 13328},\n                    Maxrss: 40484,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 1941,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 250,\n                    Nivcsw: 278,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"install.go:192: [debug] Original chart version: \\\"\\\"\",\n                        \"Error: INSTALLATION FAILED: looks like \\\"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/\\\" is not a valid chart repository or cannot be reached: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\",\n                        \"helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\",\n                        \"helm.sh/helm/v3/pkg/repo.(*ChartRepository).DownloadIndexFile\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:128\",\n                        \"helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:252\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:723\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:204\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:139\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:872\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:990\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:918\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:250\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1571\",\n                        \"looks like \\\"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/\\\" is not a valid chart repository or cannot be reached\",\n                        \"helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:254\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:723\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:204\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:139\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n         ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; install.go:192: [debug] Original chart version: \"\"\n    Error: INSTALLATION FAILED: looks like \"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/\" is not a valid chart repository or cannot be reached: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\n    helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\n    helm.sh/helm/v3/pkg/repo.(*ChartRepository).DownloadIndexFile\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:128\n    helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:252\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:723\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:204\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:139\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    looks like \"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/\" is not a valid chart repository or cannot be reached\n    helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:254\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:723\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:204\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:139\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:141\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\noccurred","time":{"start":1666747979000,"stop":1666748074368,"duration":95368}},{"uid":"1cde8c1efc6df270","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004dc318>: {\n        Underlying: <*exec.ExitError | 0xc0004f0220>{\n            ProcessState: {\n                pid: 6386,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 122377},\n                    Stime: {Sec: 0, Usec: 16317},\n                    Maxrss: 51728,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2121,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 24,\n                    Oublock: 360,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 688,\n                    Nivcsw: 222,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"install.go:192: [debug] Original chart version: \\\"v1.8.0\\\"\",\n                        \"install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\",\n                        \"\",\n                        \"Error: INSTALLATION FAILED: cannot re-use a name that is still in use\",\n                        \"helm.go:84: [debug] cannot re-use a name that is still in use\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).availableName\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:483\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:197\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:278\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:139\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:872\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:990\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:918\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:250\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1571\",\n                        \"INSTALLATION FAILED\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:141\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:872\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:990\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:918\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:250\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1571\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"install.go:192: [debug] Original chart version: \\\"v1.8.0\\\"\",\n                    \"install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\",\n                    \"\",\n                    \"Error: INSTALLATION FAILED: cannot re-use a name that is still in use\",\n                    \"helm.go:84: [debug] cannot re-use a name that is still in...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; install.go:192: [debug] Original chart version: \"v1.8.0\"\n    install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\n    \n    Error: INSTALLATION FAILED: cannot re-use a name that is still in use\n    helm.go:84: [debug] cannot re-use a name that is still in use\n    helm.sh/helm/v3/pkg/action.(*Install).availableName\n    \thelm.sh/helm/v3/pkg/action/install.go:483\n    helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\n    \thelm.sh/helm/v3/pkg/action/install.go:197\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:278\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:139\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:141\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\noccurred","time":{"start":1666707346000,"stop":1666707346388,"duration":388}},{"uid":"ead93f6718764b08","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004e8060>: {\n        Underlying: <*exec.ExitError | 0xc0001c0000>{\n            ProcessState: {\n                pid: 6437,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 4, Usec: 39204},\n                    Stime: {Sec: 0, Usec: 608010},\n                    Maxrss: 90856,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 5495,\n                    Majflt: 1,\n                    Nswap: 0,\n                    Inblock: 160,\n                    Oublock: 14288,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 49764,\n                    Nivcsw: 9910,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:192: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.4.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD serviceexports.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD serviceimports.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD slice.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD slicegateways.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 47 resource(s)\",\n                        \"wait.go:66: [debug] beginning wait for 47 resources with timeout of 5m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debu...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:192: [debug] Original chart version: \"\"\n    install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.4.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD serviceexports.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD serviceimports.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD slice.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD slicegateways.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 47 resource(s)\n    wait.go:66: [debug] beginning wait for 47 resources with timeout of 5m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:310: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"jaeger\" Service\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-svc\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook\" Deployment\n    client.go:310: [debug] Starting delete for \"jaeger\" Deployment\n    client.go:310: [debug] Starting delete for \"prefix-service\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsm-kernel-forwarder\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRole\n    client.go:310: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"networkserviceendpoints.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservices.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservicemanagers.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-cfg\" MutatingWebhookConfiguration\n    uninstall.go:148: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:141\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\noccurred","time":{"start":1665944636000,"stop":1665945054369,"duration":418369}},{"uid":"4177292bb4210012","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000132450>: {\n        Underlying: <*exec.ExitError | 0xc00014e000>{\n            ProcessState: {\n                pid: 6098,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 4, Usec: 887993},\n                    Stime: {Sec: 0, Usec: 813208},\n                    Maxrss: 81628,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 9418,\n                    Majflt: 4,\n                    Nswap: 0,\n                    Inblock: 720,\n                    Oublock: 13888,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 52079,\n                    Nivcsw: 10073,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.3.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:165: [debug] Clearing discovery cache\",\n                        \"wait.go:48: [debug] beginning wait for 4 resources with timeout of 1m0s\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 44 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 44 resources with timeout of 5m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-sys...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.3.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:165: [debug] Clearing discovery cache\n    wait.go:48: [debug] beginning wait for 4 resources with timeout of 1m0s\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 44 resource(s)\n    wait.go:48: [debug] beginning wait for 44 resources with timeout of 5m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:310: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-svc\" Service\n    client.go:310: [debug] Starting delete for \"jaeger\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:310: [debug] Starting delete for \"jaeger\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook\" Deployment\n    client.go:310: [debug] Starting delete for \"prefix-service\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsm-kernel-forwarder\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:310: [debug] Starting delete for \"networkservices.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkserviceendpoints.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservicemanagers.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-cfg\" MutatingWebhookConfiguration\n    uninstall.go:144: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: client rate limiter Wait returned an error: rate: Wait(n=1) would exceed context deadline\n    helm.go:84: [debug] client rate limiter Wait returned an error: rate: Wait(n=1) would exceed context deadline\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1665552694000,"stop":1665553100419,"duration":406419}},{"uid":"42fef9a0774140ea","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00034e030>: {\n        Underlying: <*exec.ExitError | 0xc000422000>{\n            ProcessState: {\n                pid: 6102,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 2, Usec: 819215},\n                    Stime: {Sec: 0, Usec: 383113},\n                    Maxrss: 86208,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 16079,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14208,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 13521,\n                    Nivcsw: 4184,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.5.5.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:165: [debug] Clearing discovery cache\",\n                        \"wait.go:48: [debug] beginning wait for 5 resources with timeout of 1m0s\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:310: [debug] Starting delete for \\\"kubeslice-preinstall\\\" ServiceAccount\",\n                        \"client.go:339: [debug] serviceaccounts \\\"kubeslice-preinstall\\\" not found\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:310: [debug] Starting delete for \\\"kubeslice-preinstall-role\\\" ClusterRole\",\n                        \"client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \\\"kubeslice-preinstall-role\\\" not found\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:310: [debug] Starting delete for \\\"kubeslice-preinstall-rolebinding\\\" ClusterRoleBinding\",\n                        \"client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \\\"kubeslice-preinstall-rolebinding\\\" not found\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:310: [debug] Starting delete for \\\"kubeslice-worker-preinstall-configmap\\\" ConfigMap\",\n                        \"client.go:339: [debug] configmaps \\\"kubeslice-worker-preinstall-configmap\\\" not found\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:310: [debug] Starting delete for \\\"kubeslice-worker-preinstall-job\\\" Job\",\n                        \"client.go:339: [debug] jobs.batch \\\"kubeslice-worker-preinstall-job\\\" not found\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:540: [debug] Watching for changes to Job kubeslice-worker-preinstall-job with timeout of 5m0s\",\n                        \"client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: ADDED\",\n                        \"client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 0, jobs failed: 0, jobs succeeded: 0\",\n                        \"client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\",\n                        \"client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\",\n                        \"client.go:568: [...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.5.5.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:165: [debug] Clearing discovery cache\n    wait.go:48: [debug] beginning wait for 5 resources with timeout of 1m0s\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:310: [debug] Starting delete for \"kubeslice-preinstall\" ServiceAccount\n    client.go:339: [debug] serviceaccounts \"kubeslice-preinstall\" not found\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:310: [debug] Starting delete for \"kubeslice-preinstall-role\" ClusterRole\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-preinstall-role\" not found\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:310: [debug] Starting delete for \"kubeslice-preinstall-rolebinding\" ClusterRoleBinding\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-preinstall-rolebinding\" not found\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:310: [debug] Starting delete for \"kubeslice-worker-preinstall-configmap\" ConfigMap\n    client.go:339: [debug] configmaps \"kubeslice-worker-preinstall-configmap\" not found\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:310: [debug] Starting delete for \"kubeslice-worker-preinstall-job\" Job\n    client.go:339: [debug] jobs.batch \"kubeslice-worker-preinstall-job\" not found\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:540: [debug] Watching for changes to Job kubeslice-worker-preinstall-job with timeout of 5m0s\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: ADDED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 0, jobs failed: 0, jobs succeeded: 0\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 1, jobs succeeded: 0\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 2, jobs succeeded: 0\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 3, jobs succeeded: 0\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 4, jobs succeeded: 0\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 5, jobs succeeded: 0\n    client.go:310: [debug] Starting delete for \"kubeslice-worker-preinstall-job\" Job\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:310: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-svc\" Service\n    client.go:310: [debug] Starting delete for \"jaeger\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:339: [debug] services \"kubeslice-webhook-service\" not found\n    client.go:339: [debug] services \"nsm-admission-webhook-svc\" not found\n    client.go:339: [debug] services \"kubeslice-dns\" not found\n    client.go:339: [debug] services \"jaeger\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook\" Deployment\n    client.go:310: [debug] Starting delete for \"jaeger\" Deployment\n    client.go:310: [debug] Starting delete for \"prefix-service\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:339: [debug] deployments.apps \"jaeger\" not found\n    client.go:339: [debug] deployments.apps \"kubeslice-dns\" not found\n    client.go:339: [debug] deployments.apps \"prefix-service\" not found\n    client.go:339: [debug] deployments.apps \"kubeslice-operator\" not found\n    client.go:339: [debug] deployments.apps \"nsm-admission-webhook\" not found\n    client.go:310: [debug] Starting delete for \"nsm-kernel-forwarder\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:339: [debug] daemonsets.apps \"kubeslice-netop\" not found\n    client.go:339: [debug] daemonsets.apps \"nsm-kernel-forwarder\" not found\n    client.go:339: [debug] daemonsets.apps \"nsmgr\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:339: [debug] rolebindings.rbac.authorization.k8s.io \"kubeslice-leader-election-rolebinding\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:339: [debug] roles.rbac.authorization.k8s.io \"kubeslice-leader-election-role\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-manager-rolebinding\" not found\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-kubernetes-dashboard\" not found\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-dns-rolebinding\" not found\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-proxy-rolebinding\" not found\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"nsm-role-binding\" not found\n    client.go:310: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-manager-role\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-proxy-role\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"nsm-role\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-metrics-reader\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-kubernetes-dashboard\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"aggregate-network-services-view\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-dns-role\" not found\n    client.go:310: [debug] Starting delete for \"networkservices.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkserviceendpoints.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservicemanagers.networkservicemesh.io\" CustomResourceDefinition\n    client.go:339: [debug] customresourcedefinitions.apiextensions.k8s.io \"networkserviceendpoints.networkservicemesh.io\" not found\n    client.go:339: [debug] customresourcedefinitions.apiextensions.k8s.io \"networkservices.networkservicemesh.io\" not found\n    client.go:339: [debug] customresourcedefinitions.apiextensions.k8s.io \"networkservicemanagers.networkservicemesh.io\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:339: [debug] configmaps \"nsm-config\" not found\n    client.go:339: [debug] configmaps \"kubeslice-manager-config\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-image-pull-secret\" Secret\n    client.go:339: [debug] secrets \"kubeslice-hub\" not found\n    client.go:339: [debug] secrets \"nsm-admission-webhook-certs\" not found\n    client.go:339: [debug] secrets \"kubeslice-admission-webhook-certs\" not found\n    client.go:339: [debug] secrets \"kubeslice-image-pull-secret\" not found\n    client.go:310: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:339: [debug] serviceaccounts \"kubeslice-controller-manager\" not found\n    client.go:339: [debug] serviceaccounts \"slice-router\" not found\n    client.go:339: [debug] serviceaccounts \"vpn-gateway-client\" not found\n    client.go:339: [debug] serviceaccounts \"forward-plane-acc\" not found\n    client.go:339: [debug] serviceaccounts \"nsc-acc\" not found\n    client.go:339: [debug] serviceaccounts \"kubeslice-kubernetes-dashboard\" not found\n    client.go:339: [debug] serviceaccounts \"vpn-gateway-server\" not found\n    client.go:339: [debug] serviceaccounts \"nsmgr-acc\" not found\n    client.go:339: [debug] serviceaccounts \"nse-acc\" not found\n    client.go:339: [debug] serviceaccounts \"kubeslice-netop\" not found\n    client.go:339: [debug] serviceaccounts \"kubeslice-dns\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-cfg\" MutatingWebhookConfiguration\n    client.go:339: [debug] mutatingwebhookconfigurations.admissionregistration.k8s.io \"kubeslice-mutating-webhook-configuration\" not found\n    client.go:339: [debug] mutatingwebhookconfigurations.admissionregistration.k8s.io \"nsm-admission-webhook-cfg\" not found\n    uninstall.go:144: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: failed pre-install: timed out waiting for the condition\n    helm.go:84: [debug] failed pre-install: timed out waiting for the condition\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:361\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1664798844000,"stop":1664799263439,"duration":419439}}]},"Worker Suite:Worker Suite#[It] Slice Config Negative Scenario tests Slice Config Negative Scenario Update Should fail while updating sliceconfig with bandwithguaranteedkbps more than bandwidthceilingkbps in qosprofiledetailsnvalid ":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":3,"unknown":0,"total":3},"items":[{"uid":"18754e6bb183d749","status":"passed","time":{"start":1666674621000,"stop":1666674621849,"duration":849}},{"uid":"e89b4c70ea2e1335","status":"passed","time":{"start":1666674212000,"stop":1666674212838,"duration":838}},{"uid":"e8eaaf620a90cba8","status":"passed","time":{"start":1666665366000,"stop":1666665366722,"duration":722}}]},"80cad514533480e0d3e6606cdbfcee2c":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"414ad2a49316c72e","status":"passed","time":{"start":1664357288257,"stop":1664357426728,"duration":138471}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Update Should update successfully when Project Applied while removing a service account name in Write users":{"statistic":{"failed":2,"broken":0,"skipped":0,"passed":0,"unknown":0,"total":2},"items":[{"uid":"c0ab472777ffde5d","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000e678>: {\n        Underlying: <*exec.ExitError | 0xc000749de0>{\n            ProcessState: {\n                pid: 7631,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 180466},\n                    Stime: {Sec: 0, Usec: 52636},\n                    Maxrss: 78780,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 7687,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13760,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 268,\n                    Nivcsw: 299,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Write%20users2716222324\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.94.3:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Write%20users2716222324\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.94.3:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Write%20users2716222324\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.94.3:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Write%20users2716222324\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.94.3:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Write%20users2716222324\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.94.3:443: connect: connection refused\noccurred","time":{"start":1665665955000,"stop":1665665955282,"duration":282}},{"uid":"7ff32a597a002292","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0001a4e40>: {\n        Underlying: <*exec.ExitError | 0xc0004d9440>{\n            ProcessState: {\n                pid: 7204,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 281212},\n                    Stime: {Sec: 0, Usec: 44190},\n                    Maxrss: 76648,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 6306,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13760,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 278,\n                    Nivcsw: 417,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Write%20users807127199\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.116.233:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Write%20users807127199\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.116.233:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Write%20users807127199\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.116.233:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Write%20users807127199\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.116.233:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Write%20users807127199\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.116.233:443: connect: connection refused\noccurred","time":{"start":1665666027000,"stop":1665666027427,"duration":427}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Update Should Update successfullyy when Project is Applied with valid service account name in Read users":{"statistic":{"failed":2,"broken":0,"skipped":0,"passed":0,"unknown":0,"total":2},"items":[{"uid":"6389cde96aed0647","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0006c2468>: {\n        Underlying: <*exec.ExitError | 0xc0005ff000>{\n            ProcessState: {\n                pid: 7602,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 184388},\n                    Stime: {Sec: 0, Usec: 52682},\n                    Maxrss: 83012,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 10255,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13760,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 339,\n                    Nivcsw: 487,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20Update%20successfullyy%20when%20Project%20is%20Applied%20with%20valid%20service%20account%20name%20in%20Read%20users3082760587\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.94.3:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20Update%20successfullyy%20when%20Project%20is%20Applied%20with%20valid%20service%20account%20name%20in%20Read%20users3082760587\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.94.3:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20Update%20successfullyy%20when%20Project%20is%20Applied%20with%20valid%20service%20account%20name%20in%20Read%20users3082760587\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.94.3:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20Update%20successfullyy%20when%20Project%20is%20Applied%20with%20valid%20service%20account%20name%20in%20Read%20users3082760587\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.94.3:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Update%20Should%20Update%20successfullyy%20when%20Project%20is%20Applied%20with%20valid%20service%20account%20name%20in%20Read%20users3082760587\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.94.3:443: connect: connection refused\noccurred","time":{"start":1665665955000,"stop":1665665955287,"duration":287}},{"uid":"3bb7dbc920d9f1a9","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000534a98>: {\n        Underlying: <*exec.ExitError | 0xc0006a3440>{\n            ProcessState: {\n                pid: 7174,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 245374},\n                    Stime: {Sec: 0, Usec: 78073},\n                    Maxrss: 83984,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 9060,\n                    Majflt: 1,\n                    Nswap: 0,\n                    Inblock: 256,\n                    Oublock: 13760,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 490,\n                    Nivcsw: 392,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20Update%20successfullyy%20when%20Project%20is%20Applied%20with%20valid%20service%20account%20name%20in%20Read%20users3706711320\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.116.233:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20Update%20successfullyy%20when%20Project%20is%20Applied%20with%20valid%20service%20account%20name%20in%20Read%20users3706711320\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.116.233:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20Update%20successfullyy%20when%20Project%20is%20Applied%20with%20valid%20service%20account%20name%20in%20Read%20users3706711320\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.116.233:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20Update%20successfullyy%20when%20Project%20is%20Applied%20with%20valid%20service%20account%20name%20in%20Read%20users3706711320\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.116.233:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Update%20Should%20Update%20successfullyy%20when%20Project%20is%20Applied%20with%20valid%20service%20account%20name%20in%20Read%20users3706711320\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.116.233:443: connect: connection refused\noccurred","time":{"start":1665666027000,"stop":1665666027426,"duration":426}}]},"Worker Suite:Worker Suite#[It] Attach/Deattach Scenario Attach cluster from slice  when Deattach cluster from slice  should have gateway pod running on attached cluster":{"statistic":{"failed":0,"broken":0,"skipped":3,"passed":0,"unknown":0,"total":3},"items":[{"uid":"31867e62064ae7da","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674621000,"stop":1666674621000,"duration":0}},{"uid":"687de1346083d599","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674212000,"stop":1666674212000,"duration":0}},{"uid":"490422f1f91df45a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666665366000,"stop":1666665366000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting vl3 pod Should restart vl3 pod":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":1,"unknown":0,"total":3},"items":[{"uid":"8ff721e33bb67d1d","status":"passed","time":{"start":1666674621000,"stop":1666674633462,"duration":12462}},{"uid":"4708c4f2fe47db98","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674212000,"stop":1666674212000,"duration":0}},{"uid":"41a75758cd238df9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666665366000,"stop":1666665366000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice Config Negative Scenario tests Slice Config Negative Scenario Creation Should fail while creating sliceconfig with bandwithguaranteedkbps more than bandwidthceilingkbps in qosprofiledetailsnvalid ":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":3,"unknown":0,"total":3},"items":[{"uid":"62103481a17cebb2","status":"passed","time":{"start":1666674621000,"stop":1666674621392,"duration":392}},{"uid":"d9c5d1edbaa0c22a","status":"passed","time":{"start":1666674212000,"stop":1666674212000,"duration":0}},{"uid":"4e93f2484f1bdf25","status":"passed","time":{"start":1666665366000,"stop":1666665366385,"duration":385}}]},"Hub Suite:Hub Suite#[It] Cluster CR tests Cluster CR validation Register worker cluster":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":0,"unknown":0,"total":2},"items":[{"uid":"8f887bf1c7923eb3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1665665955000,"stop":1665665955000,"duration":0}},{"uid":"428962ac3a190439","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1665666027000,"stop":1665666027000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice Config Negative Scenario tests Slice Config Negative Scenario Creation Should fail while creating sliceconfig with invalid values on slice name":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":3,"unknown":0,"total":3},"items":[{"uid":"d037f9d8da359aa","status":"passed","time":{"start":1666674621000,"stop":1666674623116,"duration":2116}},{"uid":"d235284037950804","status":"passed","time":{"start":1666674212000,"stop":1666674213728,"duration":1728}},{"uid":"98b1e77f602283ff","status":"passed","time":{"start":1666665366000,"stop":1666665367446,"duration":1446}}]}}