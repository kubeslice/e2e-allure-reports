{"Worker Suite:Worker Suite#[It] Slice Config Negative Scenario tests Slice Config Negative Scenario Update Should fail while updating sliceconfig with invalid clusters":{"statistic":{"failed":0,"broken":0,"skipped":14,"passed":37,"unknown":0,"total":51},"items":[{"uid":"13852af469deaca6","status":"passed","time":{"start":1663044723000,"stop":1663044723443,"duration":443}},{"uid":"45dea2f773d31f12","status":"skipped","statusDetails":"skipped","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"b2edaebc986239fa","status":"passed","time":{"start":1662348908000,"stop":1662348908558,"duration":558}},{"uid":"2a31082fe0d5e535","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"23b9f719d57ac655","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"7a5647d17e0b5bcb","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"b9b379f4416b5d59","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"f34754622a0b6abc","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"b537556db97f6321","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"de078acf3821df6d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"5a354b8d9b3fbc3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"ecece9e3a4a054e4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"46651dd944544c23","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"a1fcb0e25a7117c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"657df4a50e7fe701","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"23817cfa9d4d7d62","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"91f8c07fc6364c3f","status":"passed","time":{"start":1660113628000,"stop":1660113628215,"duration":215}},{"uid":"5db4ff86031bf856","status":"passed","time":{"start":1660104898000,"stop":1660104898207,"duration":207}},{"uid":"89d6070f92bcb80d","status":"passed","time":{"start":1660067259000,"stop":1660067259386,"duration":386}},{"uid":"cd0bbf108d6b54a2","status":"passed","time":{"start":1660047551000,"stop":1660047551502,"duration":502}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting worker-operator pod Check ping between iperf-server and iperf-client after worker-operator pod restart":{"statistic":{"failed":0,"broken":0,"skipped":51,"passed":0,"unknown":0,"total":51},"items":[{"uid":"28c3e32ce483dd53","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"4ac7ebb2c9f20f3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"2251bd97a582a2d9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662348908000,"stop":1662348908000,"duration":0}},{"uid":"923ed96ab662e4c4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"49a9906fd903410e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"69b83bd378cb9215","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"f902726b3d94801d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"720408ddc6745501","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"1fcad47399f6f8c1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"876447118287556b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"6e44cd66bd515231","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"b2ae4122e2d79852","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"83c8f056812fe9f2","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"a4b18b22b1de744d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"dd95e720072bb5ba","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"96cf94ab4b822d32","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"287b484dbf2fa5ea","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"9c287adab578aaf5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"f2226ffe7a4c9f89","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"26f9a0d42688abd0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice validation Slice with LOW QOS should reconcile low qos slice":{"statistic":{"failed":2,"broken":0,"skipped":3,"passed":4,"unknown":0,"total":9},"items":[{"uid":"163950f3c0e6d910","status":"passed","time":{"start":1659168739000,"stop":1659168739395,"duration":395}},{"uid":"de409261aa8cd036","status":"passed","time":{"start":1659164084000,"stop":1659164084315,"duration":315}},{"uid":"e4c1a18074ae344e","status":"passed","time":{"start":1659160188000,"stop":1659160188292,"duration":292}},{"uid":"34602a2dfa4121a2","status":"passed","time":{"start":1659119724000,"stop":1659119724310,"duration":310}},{"uid":"1e46ed45be6838ba","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000130ac8>: {\n        Underlying: <*exec.ExitError | 0xc00079e000>{\n            ProcessState: {\n                pid: 7527,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 50928},\n                    Stime: {Sec: 0, Usec: 18188},\n                    Maxrss: 45792,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2561,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 184,\n                    Nivcsw: 163,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicelowqos\\\" not found\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicelowqos\\\" not found\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicelowqos\\\" not found\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicelowqos\\\" not found\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (NotFound): slice.networking.kubeslice.io \"slicelowqos\" not found\noccurred","time":{"start":1659116511000,"stop":1659116511891,"duration":891}},{"uid":"4d3617c2bce05dfb","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00088d680>: {\n        Underlying: <*exec.ExitError | 0xc0007e2400>{\n            ProcessState: {\n                pid: 7608,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 61643},\n                    Stime: {Sec: 0, Usec: 14225},\n                    Maxrss: 41880,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2500,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 187,\n                    Nivcsw: 280,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicelowqos\\\" not found\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicelowqos\\\" not found\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicelowqos\\\" not found\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicelowqos\\\" not found\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (NotFound): slice.networking.kubeslice.io \"slicelowqos\" not found\noccurred","time":{"start":1659109470000,"stop":1659109470829,"duration":829}},{"uid":"70e2f9f641c12986","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659106836000,"stop":1659106836000,"duration":0}},{"uid":"5d48e97edd82140c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659101522000,"stop":1659101522000,"duration":0}},{"uid":"758a38f347a57677","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659082816000,"stop":1659082816000,"duration":0}}]},"Worker Suite:Worker Suite#[It]  QosProfile tests QosProfile Create validation Creates qosprofile":{"statistic":{"failed":8,"broken":0,"skipped":2,"passed":1,"unknown":0,"total":11},"items":[{"uid":"92537cc13c9aa34a","status":"skipped","statusDetails":"skipped","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"42f8a951596158bc","status":"skipped","statusDetails":"skipped","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"6d23ffbe2b9b82a1","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004a8fc0>: {\n        Underlying: <*exec.ExitError | 0xc0006c02e0>{\n            ProcessState: {\n                pid: 7624,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 220498},\n                    Stime: {Sec: 0, Usec: 64852},\n                    Maxrss: 84356,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4524,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 17168,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 882,\n                    Nivcsw: 622,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile14237864\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile14237864\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile14237864\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile14237864\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile14237864\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1662348908000,"stop":1662348909308,"duration":1308}},{"uid":"10b2ffe525c8069d","status":"passed","time":{"start":1661844205000,"stop":1661844206973,"duration":1973}},{"uid":"56377b85ade8e235","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000734c90>: {\n        Underlying: <*exec.ExitError | 0xc00017f4e0>{\n            ProcessState: {\n                pid: 7231,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 194309},\n                    Stime: {Sec: 0, Usec: 28939},\n                    Maxrss: 84624,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4130,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 448,\n                    Nivcsw: 513,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile409231599\\\": Internal error occurred: failed calling webhook \\\"msliceqosconfig.kb.io\\\": failed to call webhook: the server could not find the requested resource\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile409231599\\\": Internal error occurred: failed calling webhook \\\"msliceqosconfig.kb.io\\\": failed to call webhook: the server could not find the requested resource\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile409231599\\\": Internal error occurred: failed calling webhook \\\"msliceqosconfig.kb.io\\\": failed to call webhook: the server could not find the requested resource\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile409231599\\\": Internal error occurred: failed calling webhook \\\"msliceqosconfig.kb.io\\\": failed to call webhook: the server could not find the requested resource\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile409231599\": Internal error occurred: failed calling webhook \"msliceqosconfig.kb.io\": failed to call webhook: the server could not find the requested resource\noccurred","time":{"start":1661754870000,"stop":1661754870227,"duration":227}},{"uid":"cdfccedb32805a6d","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0008ce8d0>: {\n        Underlying: <*exec.ExitError | 0xc000075160>{\n            ProcessState: {\n                pid: 7126,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 186229},\n                    Stime: {Sec: 0, Usec: 50789},\n                    Maxrss: 83332,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3346,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 311,\n                    Nivcsw: 468,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile478452098\\\": Internal error occurred: failed calling webhook \\\"msliceqosconfig.kb.io\\\": failed to call webhook: the server could not find the requested resource\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile478452098\\\": Internal error occurred: failed calling webhook \\\"msliceqosconfig.kb.io\\\": failed to call webhook: the server could not find the requested resource\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile478452098\\\": Internal error occurred: failed calling webhook \\\"msliceqosconfig.kb.io\\\": failed to call webhook: the server could not find the requested resource\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile478452098\\\": Internal error occurred: failed calling webhook \\\"msliceqosconfig.kb.io\\\": failed to call webhook: the server could not find the requested resource\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile478452098\": Internal error occurred: failed calling webhook \"msliceqosconfig.kb.io\": failed to call webhook: the server could not find the requested resource\noccurred","time":{"start":1661611962000,"stop":1661611962282,"duration":282}},{"uid":"39dff07005bfd391","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00090d2c0>: {\n        Underlying: <*exec.ExitError | 0xc00096b7e0>{\n            ProcessState: {\n                pid: 7253,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 203666},\n                    Stime: {Sec: 0, Usec: 16972},\n                    Maxrss: 73652,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3787,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 243,\n                    Nivcsw: 541,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1956553570\\\": Internal error occurred: failed calling webhook \\\"msliceqosconfig.kb.io\\\": failed to call webhook: the server could not find the requested resource\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1956553570\\\": Internal error occurred: failed calling webhook \\\"msliceqosconfig.kb.io\\\": failed to call webhook: the server could not find the requested resource\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1956553570\\\": Internal error occurred: failed calling webhook \\\"msliceqosconfig.kb.io\\\": failed to call webhook: the server could not find the requested resource\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1956553570\\\": Internal error occurred: failed calling webhook \\\"msliceqosconfig.kb.io\\\": failed to call webhook: the server could not find the requested resource\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1956553570\": Internal error occurred: failed calling webhook \"msliceqosconfig.kb.io\": failed to call webhook: the server could not find the requested resource\noccurred","time":{"start":1661585077000,"stop":1661585077211,"duration":211}},{"uid":"7bc68a0e88891c85","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000e2b8>: {\n        Underlying: <*exec.ExitError | 0xc0004982a0>{\n            ProcessState: {\n                pid: 6256,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 209933},\n                    Stime: {Sec: 0, Usec: 28814},\n                    Maxrss: 76176,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3487,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 450,\n                    Nivcsw: 699,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1035375976\\\": Internal error occurred: failed calling webhook \\\"msliceqosconfig.kb.io\\\": failed to call webhook: the server could not find the requested resource\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1035375976\\\": Internal error occurred: failed calling webhook \\\"msliceqosconfig.kb.io\\\": failed to call webhook: the server could not find the requested resource\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1035375976\\\": Internal error occurred: failed calling webhook \\\"msliceqosconfig.kb.io\\\": failed to call webhook: the server could not find the requested resource\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1035375976\\\": Internal error occurred: failed calling webhook \\\"msliceqosconfig.kb.io\\\": failed to call webhook: the server could not find the requested resource\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1035375976\": Internal error occurred: failed calling webhook \"msliceqosconfig.kb.io\": failed to call webhook: the server could not find the requested resource\noccurred","time":{"start":1661584447000,"stop":1661584447271,"duration":271}},{"uid":"430de593b8f536f7","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0008947f8>: {\n        Underlying: <*exec.ExitError | 0xc00066f4c0>{\n            ProcessState: {\n                pid: 7218,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 179043},\n                    Stime: {Sec: 0, Usec: 40691},\n                    Maxrss: 84332,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4528,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 305,\n                    Nivcsw: 496,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile2172885303\\\": Internal error occurred: failed calling webhook \\\"msliceqosconfig.kb.io\\\": failed to call webhook: the server could not find the requested resource\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile2172885303\\\": Internal error occurred: failed calling webhook \\\"msliceqosconfig.kb.io\\\": failed to call webhook: the server could not find the requested resource\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile2172885303\\\": Internal error occurred: failed calling webhook \\\"msliceqosconfig.kb.io\\\": failed to call webhook: the server could not find the requested resource\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile2172885303\\\": Internal error occurred: failed calling webhook \\\"msliceqosconfig.kb.io\\\": failed to call webhook: the server could not find the requested resource\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile2172885303\": Internal error occurred: failed calling webhook \"msliceqosconfig.kb.io\": failed to call webhook: the server could not find the requested resource\noccurred","time":{"start":1661580045000,"stop":1661580045231,"duration":231}},{"uid":"d7e26e5ec18d1799","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000866e10>: {\n        Underlying: <*exec.ExitError | 0xc0001705a0>{\n            ProcessState: {\n                pid: 7215,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 283655},\n                    Stime: {Sec: 0, Usec: 147820},\n                    Maxrss: 88092,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 7354,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 608,\n                    Nivcsw: 769,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile644429197\\\": Internal error occurred: failed calling webhook \\\"msliceqosconfig.kb.io\\\": failed to call webhook: the server could not find the requested resource\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile644429197\\\": Internal error occurred: failed calling webhook \\\"msliceqosconfig.kb.io\\\": failed to call webhook: the server could not find the requested resource\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile644429197\\\": Internal error occurred: failed calling webhook \\\"msliceqosconfig.kb.io\\\": failed to call webhook: the server could not find the requested resource\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile644429197\\\": Internal error occurred: failed calling webhook \\\"msliceqosconfig.kb.io\\\": failed to call webhook: the server could not find the requested resource\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile644429197\": Internal error occurred: failed calling webhook \"msliceqosconfig.kb.io\": failed to call webhook: the server could not find the requested resource\noccurred","time":{"start":1661578505000,"stop":1661578505425,"duration":425}},{"uid":"b2526d8eff8749cd","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00083b320>: {\n        Underlying: <*exec.ExitError | 0xc000680760>{\n            ProcessState: {\n                pid: 7326,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 246465},\n                    Stime: {Sec: 0, Usec: 53400},\n                    Maxrss: 88600,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3743,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 421,\n                    Nivcsw: 411,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile491100803\\\": Internal error occurred: failed calling webhook \\\"msliceqosconfig.kb.io\\\": failed to call webhook: the server could not find the requested resource\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile491100803\\\": Internal error occurred: failed calling webhook \\\"msliceqosconfig.kb.io\\\": failed to call webhook: the server could not find the requested resource\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile491100803\\\": Internal error occurred: failed calling webhook \\\"msliceqosconfig.kb.io\\\": failed to call webhook: the server could not find the requested resource\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile491100803\\\": Internal error occurred: failed calling webhook \\\"msliceqosconfig.kb.io\\\": failed to call webhook: the server could not find the requested resource\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile491100803\": Internal error occurred: failed calling webhook \"msliceqosconfig.kb.io\": failed to call webhook: the server could not find the requested resource\noccurred","time":{"start":1661572479000,"stop":1661572479300,"duration":300}}]},"Worker Suite:Worker Suite#[It] Intracluster slice tests Slice with netpol should reconcile netpol in app NS":{"statistic":{"failed":0,"broken":0,"skipped":35,"passed":16,"unknown":0,"total":51},"items":[{"uid":"22a11f59cafafc60","status":"skipped","statusDetails":"skipped","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"b631f5383a1bd584","status":"passed","time":{"start":1663044069000,"stop":1663044069201,"duration":201}},{"uid":"5caab0f7781479db","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662348908000,"stop":1662348908000,"duration":0}},{"uid":"9621a14826a8aeec","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"d06ab1fbe660b229","status":"skipped","statusDetails":"skipped - Spec skipped because Skip() was called in BeforeAll","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"6ee4a737597466b4","status":"skipped","statusDetails":"skipped - Spec skipped because Skip() was called in BeforeAll","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"1c3e680e1d9aa172","status":"skipped","statusDetails":"skipped - Spec skipped because Skip() was called in BeforeAll","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"e1d87546b2d5eca0","status":"skipped","statusDetails":"skipped - Spec skipped because Skip() was called in BeforeAll","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"6766d870f32f34ee","status":"skipped","statusDetails":"skipped - Spec skipped because Skip() was called in BeforeAll","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"aa67149fc0ee5d8a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"e907dbece23f7b0f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"20c128e3003afe08","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"31b11f2d72313b84","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"73e92772cb87ca71","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"21edcf4724cf6993","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"3ebbdaf318e2157","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"625cff30c35f4ecf","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"516b5a95f9a8362b","status":"passed","time":{"start":1660104898000,"stop":1660104898190,"duration":190}},{"uid":"9519a31c10379b4e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"d318c1fe9592a06d","status":"passed","time":{"start":1660047551000,"stop":1660047551147,"duration":147}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting vl3 pod Check ping between iperf-server and iperf-client after vl3 pod restart":{"statistic":{"failed":0,"broken":0,"skipped":51,"passed":0,"unknown":0,"total":51},"items":[{"uid":"f505cb950b4c7dde","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"aacab27b5d1e76ac","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"dc477ac1860562ef","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662348908000,"stop":1662348908000,"duration":0}},{"uid":"59ffecf5ba3eb0fc","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"e20bfccadd2af6c8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"4ab01c6c4960b411","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"a9321f81cd35af60","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"559110264e8bcfea","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"2f252e80b9e4f527","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"bca6a30e9f22bc29","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"2f7684c8fab6cb68","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"9a4ba359b3243e0e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"ace3bab5c7d41de5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"8e342b171cb4e98f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"65957c53148a92f2","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"2ac206fc9fbd64f8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"2c49fbb5f5ddb3c0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"e3852ee003573a86","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"b11735feb6c8e0fe","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"9c06e997af9ed3e4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Hub netpol scenarios: Slice Configs Netpol scenarios Should create slice for valid namespace and valid clusters in applicationNamespaces of sliceconfigs manifest":{"statistic":{"failed":13,"broken":0,"skipped":1,"passed":37,"unknown":0,"total":51},"items":[{"uid":"c74214d8e8a11343","status":"skipped","statusDetails":"skipped","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"16fc69378bea4996","status":"passed","time":{"start":1663044069000,"stop":1663044079698,"duration":10698}},{"uid":"3f594a5a7689de26","status":"passed","time":{"start":1662348908000,"stop":1662348918504,"duration":10504}},{"uid":"8770428b79a3720a","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000468048>: {\n        Underlying: <*exec.ExitError | 0xc000708040>{\n            ProcessState: {\n                pid: 7148,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 245000},\n                    Stime: {Sec: 0, Usec: 73889},\n                    Maxrss: 83972,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 6636,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 393,\n                    Nivcsw: 455,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"fireredslice\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\noccurred","time":{"start":1661844205000,"stop":1661844205290,"duration":290}},{"uid":"70b6d88d01a62d61","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004bfcc8>: {\n        Underlying: <*exec.ExitError | 0xc000658fc0>{\n            ProcessState: {\n                pid: 7298,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 183810},\n                    Stime: {Sec: 0, Usec: 40846},\n                    Maxrss: 85124,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 5461,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 309,\n                    Nivcsw: 549,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"fireredslice\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661754870000,"stop":1661754870248,"duration":248}},{"uid":"830da7dbfa65ed7f","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000ed08>: {\n        Underlying: <*exec.ExitError | 0xc0007b06e0>{\n            ProcessState: {\n                pid: 7131,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 170259},\n                    Stime: {Sec: 0, Usec: 46434},\n                    Maxrss: 87412,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4166,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 345,\n                    Nivcsw: 521,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"fireredslice\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661611962000,"stop":1661611962258,"duration":258}},{"uid":"4ed4f0488ece8e10","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00090cc90>: {\n        Underlying: <*exec.ExitError | 0xc00096a6e0>{\n            ProcessState: {\n                pid: 7232,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 196262},\n                    Stime: {Sec: 0, Usec: 53526},\n                    Maxrss: 80964,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 5010,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 447,\n                    Nivcsw: 846,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"fireredslice\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661585077000,"stop":1661585077302,"duration":302}},{"uid":"3410fee95ffc57cc","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00013b7b8>: {\n        Underlying: <*exec.ExitError | 0xc000697c20>{\n            ProcessState: {\n                pid: 6013,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 222498},\n                    Stime: {Sec: 0, Usec: 27324},\n                    Maxrss: 81192,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3698,\n                    Majflt: 8,\n                    Nswap: 0,\n                    Inblock: 1736,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 480,\n                    Nivcsw: 510,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"fireredslice\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661584447000,"stop":1661584447239,"duration":239}},{"uid":"f6e37700b8473264","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0006d28a0>: {\n        Underlying: <*exec.ExitError | 0xc0003f02a0>{\n            ProcessState: {\n                pid: 7303,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 179564},\n                    Stime: {Sec: 0, Usec: 36729},\n                    Maxrss: 84344,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2913,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 399,\n                    Nivcsw: 549,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"fireredslice\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661580045000,"stop":1661580045224,"duration":224}},{"uid":"daff666b8e978b98","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0000bfe48>: {\n        Underlying: <*exec.ExitError | 0xc00051d320>{\n            ProcessState: {\n                pid: 7230,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 228929},\n                    Stime: {Sec: 0, Usec: 57232},\n                    Maxrss: 95092,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4156,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 399,\n                    Nivcsw: 333,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"fireredslice\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661578505000,"stop":1661578505295,"duration":295}},{"uid":"f1f1c741ba4bbb05","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0003d2d38>: {\n        Underlying: <*exec.ExitError | 0xc0006b7b00>{\n            ProcessState: {\n                pid: 7331,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 270476},\n                    Stime: {Sec: 0, Usec: 35798},\n                    Maxrss: 91452,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 6170,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 342,\n                    Nivcsw: 427,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"fireredslice\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661572479000,"stop":1661572479279,"duration":279}},{"uid":"a8726f27b2ffb762","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0006eb380>: {\n        Underlying: <*exec.ExitError | 0xc0007e26c0>{\n            ProcessState: {\n                pid: 7111,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 184641},\n                    Stime: {Sec: 0, Usec: 34620},\n                    Maxrss: 79596,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 6028,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 508,\n                    Nivcsw: 294,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"fireredslice\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660832281000,"stop":1660832281229,"duration":229}},{"uid":"3637107585add953","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0007f13b0>: {\n        Underlying: <*exec.ExitError | 0xc0006623a0>{\n            ProcessState: {\n                pid: 7029,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 163457},\n                    Stime: {Sec: 0, Usec: 66161},\n                    Maxrss: 85640,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 7899,\n                    Majflt: 1,\n                    Nswap: 0,\n                    Inblock: 128,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 441,\n                    Nivcsw: 329,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"fireredslice\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660808797000,"stop":1660808797220,"duration":220}},{"uid":"ed59468a101d0f76","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00066d5d8>: {\n        Underlying: <*exec.ExitError | 0xc000917b20>{\n            ProcessState: {\n                pid: 7232,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 136938},\n                    Stime: {Sec: 0, Usec: 41842},\n                    Maxrss: 88508,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 5791,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 322,\n                    Nivcsw: 296,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"fireredslice\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660299182000,"stop":1660299182176,"duration":176}},{"uid":"2fd8ed7ea4ab8a5e","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0000cf9e0>: {\n        Underlying: <*exec.ExitError | 0xc00078bf40>{\n            ProcessState: {\n                pid: 7185,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 110522},\n                    Stime: {Sec: 0, Usec: 55261},\n                    Maxrss: 84484,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3732,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 271,\n                    Nivcsw: 231,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"fireredslice\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660295740000,"stop":1660295740156,"duration":156}},{"uid":"8b1c3530302b945a","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00084c228>: {\n        Underlying: <*exec.ExitError | 0xc0008a05e0>{\n            ProcessState: {\n                pid: 7213,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 158752},\n                    Stime: {Sec: 0, Usec: 48846},\n                    Maxrss: 74180,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4579,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 395,\n                    Nivcsw: 412,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"fireredslice\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660293657000,"stop":1660293657205,"duration":205}},{"uid":"b0cd5625a8c65951","status":"passed","time":{"start":1660113628000,"stop":1660113638508,"duration":10508}},{"uid":"6fb85775730300cf","status":"passed","time":{"start":1660104898000,"stop":1660104898482,"duration":482}},{"uid":"347b84b841b98233","status":"passed","time":{"start":1660067259000,"stop":1660067269513,"duration":10513}},{"uid":"c7a92ac4d03e8b95","status":"passed","time":{"start":1660047551000,"stop":1660047561644,"duration":10644}}]},"Istio Suite:Istio Suite#[It] Bookinfo tests Bookinfo Scenario 3 without egress and with ingress Should verify if Ping works successfully from Productpage to details endpoint":{"statistic":{"failed":5,"broken":0,"skipped":48,"passed":4,"unknown":0,"total":57},"items":[{"uid":"f7e50f9fd5b5058","status":"failed","statusDetails":"Unexpected non-nil/non-zero argument at index 1:\n\t<*shell.ErrWithCmdOutput>: &shell.ErrWithCmdOutput{Underlying:(*exec.ExitError)(0xc000395c40), Output:(*shell.output)(0xc000490b58)}","time":{"start":1663043831000,"stop":1663043897534,"duration":66534}},{"uid":"708190d7c2db8755","status":"failed","statusDetails":"Unexpected non-nil/non-zero argument at index 1:\n\t<*shell.ErrWithCmdOutput>: &shell.ErrWithCmdOutput{Underlying:(*exec.ExitError)(0xc0005e2660), Output:(*shell.output)(0xc0006c4450)}","time":{"start":1663043075000,"stop":1663043142569,"duration":67569}},{"uid":"b68f9024cd4a6053","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663040338000,"stop":1663040338000,"duration":0}},{"uid":"6a335db38bd597bb","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662819574000,"stop":1662819574000,"duration":0}},{"uid":"db4f1f5a4e4bb248","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662795310000,"stop":1662795310000,"duration":0}},{"uid":"ccc9501ff297d87e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662792773000,"stop":1662792773000,"duration":0}},{"uid":"ab7dbee727a0cd90","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662620110000,"stop":1662620110000,"duration":0}},{"uid":"e96fc6f8891532d3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662617640000,"stop":1662617640000,"duration":0}},{"uid":"f17f0e788558bfbf","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662610377000,"stop":1662610377000,"duration":0}},{"uid":"802deb28792b4ba","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662557911000,"stop":1662557911000,"duration":0}},{"uid":"8169b1192d87fbb5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662553747000,"stop":1662553747000,"duration":0}},{"uid":"36f92c08777c7fb6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662553327000,"stop":1662553327000,"duration":0}},{"uid":"440a154f0bad0853","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662381943000,"stop":1662381943000,"duration":0}},{"uid":"212a36790c41f6c5","status":"failed","statusDetails":"Unexpected non-nil/non-zero argument at index 1:\n\t<*shell.ErrWithCmdOutput>: &shell.ErrWithCmdOutput{Underlying:(*exec.ExitError)(0xc0004c6620), Output:(*shell.output)(0xc000916768)}","time":{"start":1662348103000,"stop":1662348159028,"duration":56028}},{"uid":"177356f146928cc2","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662049889000,"stop":1662049889000,"duration":0}},{"uid":"1ccca36adeaf4a1f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662049074000,"stop":1662049074000,"duration":0}},{"uid":"ef4d9f554f08e87a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662028179000,"stop":1662028179000,"duration":0}},{"uid":"410acefc06a71da3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661859205000,"stop":1661859205000,"duration":0}},{"uid":"f581c8ced5c5895e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661843950000,"stop":1661843950000,"duration":0}},{"uid":"381ec0baf3c4162","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754632000,"stop":1661754632000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice Config Negative Scenario tests Slice Config Negative Scenario Creation Should fail while creating sliceconfig with invalid clusters":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":50,"unknown":0,"total":51},"items":[{"uid":"503aa401e6a3399d","status":"passed","time":{"start":1663044723000,"stop":1663044723239,"duration":239}},{"uid":"98f2dc08ca2b4f76","status":"skipped","statusDetails":"skipped","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"f8847491d7582a17","status":"passed","time":{"start":1662348908000,"stop":1662348908238,"duration":238}},{"uid":"231f32628976e3d5","status":"passed","time":{"start":1661844205000,"stop":1661844205284,"duration":284}},{"uid":"56d1561dfa0b72d4","status":"passed","time":{"start":1661754870000,"stop":1661754870230,"duration":230}},{"uid":"a16adc13a7cc28a3","status":"passed","time":{"start":1661611962000,"stop":1661611962271,"duration":271}},{"uid":"2ef6ce84ec5bbed9","status":"passed","time":{"start":1661585077000,"stop":1661585077261,"duration":261}},{"uid":"77046303fe151842","status":"passed","time":{"start":1661584447000,"stop":1661584447240,"duration":240}},{"uid":"a468aabadf132e7a","status":"passed","time":{"start":1661580045000,"stop":1661580045259,"duration":259}},{"uid":"2c3fad03a8a08bb1","status":"passed","time":{"start":1661578505000,"stop":1661578505298,"duration":298}},{"uid":"9615f8008c9bca00","status":"passed","time":{"start":1661572479000,"stop":1661572479278,"duration":278}},{"uid":"f6df31f7c57c3084","status":"passed","time":{"start":1660832281000,"stop":1660832281227,"duration":227}},{"uid":"1c6fd09974e0dd56","status":"passed","time":{"start":1660808797000,"stop":1660808797210,"duration":210}},{"uid":"fd58156bdd2eeda9","status":"passed","time":{"start":1660299182000,"stop":1660299182184,"duration":184}},{"uid":"2a85415c006c6122","status":"passed","time":{"start":1660295740000,"stop":1660295740187,"duration":187}},{"uid":"ee7e944f1af25c9b","status":"passed","time":{"start":1660293657000,"stop":1660293657357,"duration":357}},{"uid":"da5cb7d723fd8f4f","status":"passed","time":{"start":1660113628000,"stop":1660113628291,"duration":291}},{"uid":"48badc49b55e6bcd","status":"passed","time":{"start":1660104898000,"stop":1660104898409,"duration":409}},{"uid":"b96df0ef959fd578","status":"passed","time":{"start":1660067259000,"stop":1660067259174,"duration":174}},{"uid":"b5cac23adb5289fe","status":"passed","time":{"start":1660047551000,"stop":1660047551264,"duration":264}}]},"Worker Suite:Worker Suite#[It] Intracluster slice tests Slice with netpol should remove network policy resource from application namespace once the isolationEnabled is set to false":{"statistic":{"failed":1,"broken":0,"skipped":35,"passed":15,"unknown":0,"total":51},"items":[{"uid":"8c393c519eab89f6","status":"skipped","statusDetails":"skipped","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"3cd59e898b9101d5","status":"passed","time":{"start":1663044069000,"stop":1663044078287,"duration":9287}},{"uid":"a84b851cccb1a4f8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662348908000,"stop":1662348908000,"duration":0}},{"uid":"feda9d1d72df3201","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"62129a23520222a8","status":"skipped","statusDetails":"skipped - Spec skipped because Skip() was called in BeforeAll","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"6ab576ccf1dd5409","status":"skipped","statusDetails":"skipped - Spec skipped because Skip() was called in BeforeAll","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"18fd6314ea755aae","status":"skipped","statusDetails":"skipped - Spec skipped because Skip() was called in BeforeAll","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"88b74ac5cb908b36","status":"skipped","statusDetails":"skipped - Spec skipped because Skip() was called in BeforeAll","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"ea10db5d245fdad4","status":"skipped","statusDetails":"skipped - Spec skipped because Skip() was called in BeforeAll","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"a91109949847603a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"b1627f18254eb1b3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"62485e10b5a8bbfd","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"7384bf6b99d34c2f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"db0298cc8d1fe4b6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"43c67000a11a2e4c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"364a094eb57ec9ff","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"e5efa63f8f60e74","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"8feb3246cec61f5a","status":"passed","time":{"start":1660104898000,"stop":1660104905082,"duration":7082}},{"uid":"10295f1f4963b982","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"b43b7d108b078b7d","status":"passed","time":{"start":1660047551000,"stop":1660047557836,"duration":6836}}]},"Empty Suite:Empty Suite#[It] Hub Creation tests Hub Installtion Test Senarios Should fail if Hub is installed again in the same cluster":{"statistic":{"failed":0,"broken":0,"skipped":32,"passed":137,"unknown":0,"total":169},"items":[{"uid":"8ff3e97305be5e43","status":"passed","time":{"start":1663052238000,"stop":1663052241903,"duration":3903}},{"uid":"d267d347b6fef562","status":"passed","time":{"start":1663051980000,"stop":1663051984109,"duration":4109}},{"uid":"8a76e7a7e496db25","status":"passed","time":{"start":1663050064000,"stop":1663050066486,"duration":2486}},{"uid":"5ed84f3dbbba2011","status":"passed","time":{"start":1663042347000,"stop":1663042349891,"duration":2891}},{"uid":"9ae3ed14d94be43d","status":"passed","time":{"start":1663041715000,"stop":1663041718784,"duration":3784}},{"uid":"32f17323cd150e8c","status":"passed","time":{"start":1663039756000,"stop":1663039758549,"duration":2549}},{"uid":"a9121b22eeafb906","status":"passed","time":{"start":1663040168000,"stop":1663040171987,"duration":3987}},{"uid":"b7c2b729cf0ab438","status":"passed","time":{"start":1663009296000,"stop":1663009298685,"duration":2685}},{"uid":"2f91d83f87287c4b","status":"passed","time":{"start":1663008270000,"stop":1663008273563,"duration":3563}},{"uid":"91558653970046f8","status":"passed","time":{"start":1662986456000,"stop":1662986458952,"duration":2952}},{"uid":"8578b3637fb7c3df","status":"passed","time":{"start":1662978394000,"stop":1662978398685,"duration":4685}},{"uid":"4790b9eedead310f","status":"passed","time":{"start":1662961730000,"stop":1662961732596,"duration":2596}},{"uid":"a14fff96bf21e2e5","status":"passed","time":{"start":1662960985000,"stop":1662960988952,"duration":3952}},{"uid":"11692929e1a8548f","status":"passed","time":{"start":1662960825000,"stop":1662960827421,"duration":2421}},{"uid":"4a738e1f926d87ea","status":"passed","time":{"start":1662824760000,"stop":1662824762756,"duration":2756}},{"uid":"33f757caab551236","status":"passed","time":{"start":1662819406000,"stop":1662819409472,"duration":3472}},{"uid":"d426bc513a03e186","status":"passed","time":{"start":1662795171000,"stop":1662795173738,"duration":2738}},{"uid":"ace53759c82b7c50","status":"passed","time":{"start":1662792593000,"stop":1662792596489,"duration":3489}},{"uid":"ac8eb79325756cca","status":"passed","time":{"start":1662791140000,"stop":1662791142961,"duration":2961}},{"uid":"27439fff86847d70","status":"passed","time":{"start":1662788408000,"stop":1662788410248,"duration":2248}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests Iperf App connectivity Verify Connectivity between iperf-server and iperf-client":{"statistic":{"failed":0,"broken":0,"skipped":44,"passed":7,"unknown":0,"total":51},"items":[{"uid":"29b66b9bf9bad3b3","status":"skipped","statusDetails":"skipped","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"4d47a848aac3d58d","status":"skipped","statusDetails":"skipped","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"18e2d2cc6e6ea028","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662348908000,"stop":1662348908000,"duration":0}},{"uid":"fae5d12f16cd3652","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"b8a8e0ce96f0487e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"f48912ef2ee99c0f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"ef2a27827a94d3b1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"d810e546e6225e38","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"7be46fc6a929e7bd","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"a3cdb33def2d44ef","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"fc8ab61f11a7ffa0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"e61c29d4c68c191e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"b2dd8250a0c30e23","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"ccb1ebe6dc37989","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"53f69498bc56e46a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"fdd33692af83777","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"c9e1f1608059fd00","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"cb80154ecd076dda","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"a7837f3cbb91d7ad","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"a7aac38e3105a7ba","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}}]},"Istio Suite:Istio Suite#[It] Bookinfo tests Bookinfo Scenario 2 with egress and without ingress Should verify if Ping works successfully from Productpage to details endpoint":{"statistic":{"failed":5,"broken":0,"skipped":47,"passed":5,"unknown":0,"total":57},"items":[{"uid":"5a21dee37df4e98c","status":"failed","statusDetails":"Unexpected non-nil/non-zero argument at index 1:\n\t<*shell.ErrWithCmdOutput>: &shell.ErrWithCmdOutput{Underlying:(*exec.ExitError)(0xc0006524a0), Output:(*shell.output)(0xc00088af30)}","time":{"start":1663043831000,"stop":1663043898138,"duration":67138}},{"uid":"327fdddf7fd8f36f","status":"failed","statusDetails":"Unexpected non-nil/non-zero argument at index 1:\n\t<*shell.ErrWithCmdOutput>: &shell.ErrWithCmdOutput{Underlying:(*exec.ExitError)(0xc00038a560), Output:(*shell.output)(0xc00000f290)}","time":{"start":1663043075000,"stop":1663043144017,"duration":69017}},{"uid":"dfb2d5e38f87df36","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663040338000,"stop":1663040338000,"duration":0}},{"uid":"59735254a5a81684","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662819574000,"stop":1662819574000,"duration":0}},{"uid":"1bbb9a677c1b2e0e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662795310000,"stop":1662795310000,"duration":0}},{"uid":"cd3eb7136ba510a5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662792773000,"stop":1662792773000,"duration":0}},{"uid":"74568b69d6041ed1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662620110000,"stop":1662620110000,"duration":0}},{"uid":"8fc0134f4607957","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662617640000,"stop":1662617640000,"duration":0}},{"uid":"34d164e0c4679692","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662610377000,"stop":1662610377000,"duration":0}},{"uid":"1d68cd04de572101","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662557911000,"stop":1662557911000,"duration":0}},{"uid":"1867883e7e9bca0e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662553747000,"stop":1662553747000,"duration":0}},{"uid":"38db5d066cdb41f4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662553327000,"stop":1662553327000,"duration":0}},{"uid":"c851986cb00057de","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662381943000,"stop":1662381943000,"duration":0}},{"uid":"a5f0384d51b1bb83","status":"failed","statusDetails":"Unexpected non-nil/non-zero argument at index 1:\n\t<*shell.ErrWithCmdOutput>: &shell.ErrWithCmdOutput{Underlying:(*exec.ExitError)(0xc00047c780), Output:(*shell.output)(0xc0004b3320)}","time":{"start":1662348103000,"stop":1662348156381,"duration":53381}},{"uid":"23cbefdd9a1af77d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662049889000,"stop":1662049889000,"duration":0}},{"uid":"2a829d608ad6018c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662049074000,"stop":1662049074000,"duration":0}},{"uid":"4c7fe332961fd44a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662028179000,"stop":1662028179000,"duration":0}},{"uid":"b71ee4126c942e09","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661859205000,"stop":1661859205000,"duration":0}},{"uid":"ef280a29039cc6e5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661843950000,"stop":1661843950000,"duration":0}},{"uid":"6bd06d1d4a016cbc","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754632000,"stop":1661754632000,"duration":0}}]},"Istio Suite:Istio Suite#[It] Bookinfo tests Bookinfo Scenario 4 with egress and ingress Should install Details, Reviews, Ratings and Serviceexport on worker cluster-2":{"statistic":{"failed":0,"broken":0,"skipped":50,"passed":7,"unknown":0,"total":57},"items":[{"uid":"ecef0b6ecfc27a75","status":"passed","time":{"start":1663043831000,"stop":1663043897538,"duration":66538}},{"uid":"991f3c9565593854","status":"passed","time":{"start":1663043075000,"stop":1663043155370,"duration":80370}},{"uid":"5c35609bb4a5925d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663040338000,"stop":1663040338000,"duration":0}},{"uid":"e341a76f02d611cc","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662819574000,"stop":1662819574000,"duration":0}},{"uid":"efd3b30d8efbce42","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662795310000,"stop":1662795310000,"duration":0}},{"uid":"4350ec9b9dc3905a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662792773000,"stop":1662792773000,"duration":0}},{"uid":"3d189a5467642f1a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662620110000,"stop":1662620110000,"duration":0}},{"uid":"4b8bf5b2d602b232","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662617640000,"stop":1662617640000,"duration":0}},{"uid":"2bce56d50d692c7c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662610377000,"stop":1662610377000,"duration":0}},{"uid":"f2f1b1a89c4551ee","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662557911000,"stop":1662557911000,"duration":0}},{"uid":"449ae6ccb842c462","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662553747000,"stop":1662553747000,"duration":0}},{"uid":"ee38b194631b87e8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662553327000,"stop":1662553327000,"duration":0}},{"uid":"ba2409da74cd9831","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662381943000,"stop":1662381943000,"duration":0}},{"uid":"cd3d7d321ad9320d","status":"passed","time":{"start":1662348103000,"stop":1662348171022,"duration":68022}},{"uid":"70f8c3a8493f50f1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662049889000,"stop":1662049889000,"duration":0}},{"uid":"e5237ef774b58dc7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662049074000,"stop":1662049074000,"duration":0}},{"uid":"d36ad1841835785d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662028179000,"stop":1662028179000,"duration":0}},{"uid":"e5415753698999f6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661859205000,"stop":1661859205000,"duration":0}},{"uid":"db7ceabd10cceb5f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661843950000,"stop":1661843950000,"duration":0}},{"uid":"ddec34cdcbfca3eb","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754632000,"stop":1661754632000,"duration":0}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests slice with netpol Should contain application namespaces":{"statistic":{"failed":0,"broken":0,"skipped":39,"passed":12,"unknown":0,"total":51},"items":[{"uid":"d8941230a94489b9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"cc1c801e3fb48ae3","status":"passed","time":{"start":1663044069000,"stop":1663044069220,"duration":220}},{"uid":"b7df453b70aa26d0","status":"passed","time":{"start":1662348908000,"stop":1662348908141,"duration":141}},{"uid":"a6bed29858a6cb51","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"4489cef890de3b4e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"629eafd2753eaff2","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"b39c10703061eaa8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"5212c02f4d2a78e6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"25c8ad5c60537495","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"5cb30bf242c79f42","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"7a60a5763445536f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"cfbfc9d00f6391ef","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"6aa2e3bb35b78e32","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"f196b21f6a4e329a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"35d37fa1d1af956a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"a826aa154151a134","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"df21d7568dd7599d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"c247d6ba54f52876","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"1d6194afce7554b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"1235243d85ca5cea","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests Iperf App connectivity should have gateway pod running":{"statistic":{"failed":1,"broken":0,"skipped":34,"passed":16,"unknown":0,"total":51},"items":[{"uid":"34bb4ac9b34dd92d","status":"skipped","statusDetails":"skipped","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"21938c37dba0ed11","status":"passed","time":{"start":1663044069000,"stop":1663044069070,"duration":70}},{"uid":"a4fb98d97ddd2b1b","status":"passed","time":{"start":1662348908000,"stop":1662348908076,"duration":76}},{"uid":"aef73c86444d0311","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"8c40594824340462","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"3fe05c6618b3fdb9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"9a2cb6accaac43b1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"5c5e8c2c989340c1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"1faa7a0077e2ec95","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"f7eb01f5623ae202","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"5e242b3a2c4ee07f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"8cbbb3185ae4282c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"ca752e72eaa1ba83","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"70a7d27a99b2b60b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"3eeb746fa3a2651","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"fbb2d3f303832d5e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"2861f06811efcc4a","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1660113628000,"stop":1660113810641,"duration":182641}},{"uid":"2435eab733dae1c3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"1255de5c12de14b1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"fd7704c54474e0f0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice Config Negative Scenario tests Slice Config Negative Scenario Update Should fail while updating sliceconfig with invalid values on slice subnet":{"statistic":{"failed":0,"broken":0,"skipped":14,"passed":37,"unknown":0,"total":51},"items":[{"uid":"1d592543be26787","status":"passed","time":{"start":1663044723000,"stop":1663044723499,"duration":499}},{"uid":"92370b2931d0cfab","status":"skipped","statusDetails":"skipped","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"e616e4871269f82a","status":"passed","time":{"start":1662348908000,"stop":1662348908577,"duration":577}},{"uid":"ef90a7afdcb180f8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"656699a1e1620303","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"deece24d36c06d5d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"328bd93562cd25d3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"b441e736a1f88468","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"9bf7322acb7d0e15","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"a8d90f527ae0fa1a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"e18b415d49c73d90","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"476e5d637b4b572a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"153893885e5371cd","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"e363da716206e0a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"59208380cb3fb644","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"ab82f83cf80120a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"a7c3da3595e9f755","status":"passed","time":{"start":1660113628000,"stop":1660113628214,"duration":214}},{"uid":"96793728a6e27a30","status":"passed","time":{"start":1660104898000,"stop":1660104898211,"duration":211}},{"uid":"dc3e813d284a352a","status":"passed","time":{"start":1660067259000,"stop":1660067259395,"duration":395}},{"uid":"356d03c3c897fa1b","status":"passed","time":{"start":1660047551000,"stop":1660047551432,"duration":432}}]},"Worker Suite:Worker Suite#[It] Slice Config Negative Scenario tests Slice Config Negative Scenario Creation Should fail while creating sliceconfig with with invalid DSCP class in qosprofiledetails":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":50,"unknown":0,"total":51},"items":[{"uid":"5e1737d0d13c93ea","status":"passed","time":{"start":1663044723000,"stop":1663044723338,"duration":338}},{"uid":"f2110e6a2eea2ed5","status":"skipped","statusDetails":"skipped","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"1104d3dd41d60e76","status":"passed","time":{"start":1662348908000,"stop":1662348908315,"duration":315}},{"uid":"af6e7e4e8814319c","status":"passed","time":{"start":1661844205000,"stop":1661844205383,"duration":383}},{"uid":"dacbda61d8c4a71f","status":"passed","time":{"start":1661754870000,"stop":1661754870260,"duration":260}},{"uid":"de203be1d7ef11a3","status":"passed","time":{"start":1661611962000,"stop":1661611962418,"duration":418}},{"uid":"20de9d1baf0c9638","status":"passed","time":{"start":1661585077000,"stop":1661585077301,"duration":301}},{"uid":"4d48ea733163a031","status":"passed","time":{"start":1661584447000,"stop":1661584447328,"duration":328}},{"uid":"c2293b4f43f1a99e","status":"passed","time":{"start":1661580045000,"stop":1661580045305,"duration":305}},{"uid":"b975d8a237252a11","status":"passed","time":{"start":1661578505000,"stop":1661578505350,"duration":350}},{"uid":"e61ba8e5ecc71d10","status":"passed","time":{"start":1661572479000,"stop":1661572479377,"duration":377}},{"uid":"dfd1b2b6dbbf63f6","status":"passed","time":{"start":1660832281000,"stop":1660832281320,"duration":320}},{"uid":"cd51e04c4119c18b","status":"passed","time":{"start":1660808797000,"stop":1660808797285,"duration":285}},{"uid":"5df03f75f30172d4","status":"passed","time":{"start":1660299182000,"stop":1660299182244,"duration":244}},{"uid":"6bc63fee86082900","status":"passed","time":{"start":1660295740000,"stop":1660295740239,"duration":239}},{"uid":"7eac490dc838f235","status":"passed","time":{"start":1660293657000,"stop":1660293657320,"duration":320}},{"uid":"32ed2535f1490246","status":"passed","time":{"start":1660113628000,"stop":1660113628295,"duration":295}},{"uid":"13e3eb7ce308bc4c","status":"passed","time":{"start":1660104898000,"stop":1660104898243,"duration":243}},{"uid":"e49f09d80140a40b","status":"passed","time":{"start":1660067259000,"stop":1660067259219,"duration":219}},{"uid":"49d64be485e17926","status":"passed","time":{"start":1660047551000,"stop":1660047551349,"duration":349}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests Iperf App connectivity Should install iperf-sleep on client cluster":{"statistic":{"failed":8,"broken":1,"skipped":35,"passed":7,"unknown":0,"total":51},"items":[{"uid":"99ffa04755aa24b9","status":"skipped","statusDetails":"skipped","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"dec3ce8d43219690","status":"broken","statusDetails":"interrupted","time":{"start":1663044069000,"stop":1663044094706,"duration":25706}},{"uid":"167b7af55a90ab12","status":"failed","statusDetails":"Timed out after 120.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1662348908000,"stop":1662349031619,"duration":123619}},{"uid":"3d81a38e3a5cfa76","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"f88578f67e630085","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"6d0932e2d42795ad","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"a41225ca20fbeb4d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"113db7e783a158bb","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"dad562b9b76a6df4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"3c5204700b0caf9a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"130587c55b300e70","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"9137f697eeb6eda9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"e3fa65532ee144ca","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"70d33360ebb66a96","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"fc7652fbc8491091","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"9f03ee2327d70c75","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"b29aed39f4f9ab9a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"ace7e813770a747f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"952e2e1421f79e7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"ae18ebba5777f587","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice creation scenarios: when slice red is installed install slice blue workers should have vl3 routers from both slices":{"statistic":{"failed":1,"broken":0,"skipped":37,"passed":13,"unknown":0,"total":51},"items":[{"uid":"826a455eaa5f5621","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"a35b18eec1a32c16","status":"passed","time":{"start":1663044069000,"stop":1663044072121,"duration":3121}},{"uid":"4a7d6badbfafc152","status":"passed","time":{"start":1662348908000,"stop":1662348911131,"duration":3131}},{"uid":"609e5605c9a70b66","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"71482a91ff625ecd","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"111e92d2ef60a9cd","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"d1ee453a6614a6d8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"cfa47d3b8ea4e4a4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"52e015bc0afdef9f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"60389ccaccd1a34e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"6624b2dd9231b57d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"c78e5b225a17f576","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"1d16c0e27c4410cb","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"8629b3ff1550e495","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"97f1ae79f85deec5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"9918f695e52359f3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"a18f7f1ce2d18999","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"93bff7dfa90cf20d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"4ff872de7cc98ab1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"f5ca1143b95130ca","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Update Should fail when Project is applied with service account name as blank in Write users":{"statistic":{"failed":16,"broken":0,"skipped":0,"passed":174,"unknown":0,"total":190},"items":[{"uid":"c651f3d637c7cec8","status":"passed","time":{"start":1663042493000,"stop":1663042497830,"duration":4830}},{"uid":"e11481fe86ee5280","status":"passed","time":{"start":1663041865000,"stop":1663041870148,"duration":5148}},{"uid":"3160d92bcc730075","status":"failed","statusDetails":"Timed out after 60.080s.\nExpected\n    <bool>: false\nto be true","time":{"start":1663039902000,"stop":1663040023373,"duration":121373}},{"uid":"a2d9111c2964fef1","status":"passed","time":{"start":1663009431000,"stop":1663009435919,"duration":4919}},{"uid":"e66fda0b1bbf789a","status":"passed","time":{"start":1662978562000,"stop":1662978567039,"duration":5039}},{"uid":"b7858d6c5c451df9","status":"passed","time":{"start":1662702362000,"stop":1662702367914,"duration":5914}},{"uid":"f6e04de0bdcf06ed","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <bool>: false\nto be true","time":{"start":1662553059000,"stop":1662553125658,"duration":66658}},{"uid":"fe366cd781aa3813","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <bool>: false\nto be true","time":{"start":1662553042000,"stop":1662553163287,"duration":121287}},{"uid":"9b173bb8996025b6","status":"passed","time":{"start":1662467486000,"stop":1662467491839,"duration":5839}},{"uid":"91c73e1b27706bb6","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <bool>: false\nto be true","time":{"start":1662374048000,"stop":1662374169269,"duration":121269}},{"uid":"a6b5450ce5fc026","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000352738>: {\n        Underlying: <*exec.ExitError | 0xc00062dc60>{\n            ProcessState: {\n                pid: 7207,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 238765},\n                    Stime: {Sec: 0, Usec: 37307},\n                    Maxrss: 89836,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4042,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13344,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 321,\n                    Nivcsw: 339,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20applied%20with%20service%20account%20name%20as%20blank%20in%20Write%20users2181394616\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.241.207:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20applied%20with%20service%20account%20name%20as%20blank%20in%20Write%20users2181394616\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.241.207:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20applied%20with%20service%20account%20name%20as%20blank%20in%20Write%20users2181394616\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.241.207:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20applied%20with%20service%20account%20name%20as%20blank%20in%20Write%20users2181394616\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.241.207:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20applied%20with%20service%20account%20name%20as%20blank%20in%20Write%20users2181394616\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.241.207:443: connect: connection refused\noccurred","time":{"start":1662374250000,"stop":1662374250339,"duration":339}},{"uid":"a0c8dc0ada13f844","status":"passed","time":{"start":1662361116000,"stop":1662361120890,"duration":4890}},{"uid":"81055fee4d5deed4","status":"passed","time":{"start":1662347286000,"stop":1662347291864,"duration":5864}},{"uid":"a688aa9ea52c7a97","status":"passed","time":{"start":1662049179000,"stop":1662049184238,"duration":5238}},{"uid":"8045b08733ae143f","status":"passed","time":{"start":1662048808000,"stop":1662048812988,"duration":4988}},{"uid":"9a128b715b1ba058","status":"passed","time":{"start":1662036991000,"stop":1662036995850,"duration":4850}},{"uid":"964ab09e0ab63164","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0006b5ba8>: {\n        Underlying: <*exec.ExitError | 0xc000827ce0>{\n            ProcessState: {\n                pid: 9627,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 193057},\n                    Stime: {Sec: 0, Usec: 35459},\n                    Maxrss: 87608,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3772,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13344,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 347,\n                    Nivcsw: 337,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20applied%20with%20service%20account%20name%20as%20blank%20in%20Write%20users2294725442\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.231.22:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20applied%20with%20service%20account%20name%20as%20blank%20in%20Write%20users2294725442\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.231.22:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20applied%20with%20service%20account%20name%20as%20blank%20in%20Write%20users2294725442\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.231.22:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20applied%20with%20service%20account%20name%20as%20blank%20in%20Write%20users2294725442\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.231.22:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20applied%20with%20service%20account%20name%20as%20blank%20in%20Write%20users2294725442\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.231.22:443: connect: connection refused\noccurred","time":{"start":1662031654000,"stop":1662031654298,"duration":298}},{"uid":"ad9fe82bb641977e","status":"passed","time":{"start":1662027354000,"stop":1662027359095,"duration":5095}},{"uid":"53f158c233577abc","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000eb28>: {\n        Underlying: <*exec.ExitError | 0xc0003ae380>{\n            ProcessState: {\n                pid: 7747,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 186590},\n                    Stime: {Sec: 0, Usec: 37318},\n                    Maxrss: 82936,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 6620,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13344,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 341,\n                    Nivcsw: 223,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20applied%20with%20service%20account%20name%20as%20blank%20in%20Write%20users724569075\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.242.99:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20applied%20with%20service%20account%20name%20as%20blank%20in%20Write%20users724569075\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.242.99:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20applied%20with%20service%20account%20name%20as%20blank%20in%20Write%20users724569075\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.242.99:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20applied%20with%20service%20account%20name%20as%20blank%20in%20Write%20users724569075\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.242.99:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20applied%20with%20service%20account%20name%20as%20blank%20in%20Write%20users724569075\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.242.99:443: connect: connection refused\noccurred","time":{"start":1661952242000,"stop":1661952242287,"duration":287}},{"uid":"c19f4a923f02c3d5","status":"passed","time":{"start":1661863163000,"stop":1661863168134,"duration":5134}}]},"Empty Suite:Empty Suite#[It] Hub Creation tests Hub Installtion Test Senarios Should successfully pass if Cert is installed first and then Hub":{"statistic":{"failed":32,"broken":0,"skipped":0,"passed":137,"unknown":0,"total":169},"items":[{"uid":"5fb20ecd6cbaeafe","status":"passed","time":{"start":1663052238000,"stop":1663052327006,"duration":89006}},{"uid":"59c73bda5a3bfae9","status":"passed","time":{"start":1663051980000,"stop":1663052030537,"duration":50537}},{"uid":"44522876eda8dd72","status":"passed","time":{"start":1663050064000,"stop":1663050100656,"duration":36656}},{"uid":"702850fca0cf1e0c","status":"passed","time":{"start":1663042347000,"stop":1663042384343,"duration":37343}},{"uid":"f97a60d068582a1d","status":"passed","time":{"start":1663041715000,"stop":1663041755219,"duration":40219}},{"uid":"848a1c3e0e0e6dd1","status":"passed","time":{"start":1663039756000,"stop":1663039849456,"duration":93456}},{"uid":"6e426856e5a656be","status":"passed","time":{"start":1663040168000,"stop":1663040213276,"duration":45276}},{"uid":"a5551ca823be2c90","status":"passed","time":{"start":1663009296000,"stop":1663009377757,"duration":81757}},{"uid":"f7ff8cfc109f4801","status":"passed","time":{"start":1663008270000,"stop":1663008318858,"duration":48858}},{"uid":"dab6f68196c9511c","status":"passed","time":{"start":1662986456000,"stop":1662986497125,"duration":41125}},{"uid":"7ef88f226929baef","status":"passed","time":{"start":1662978394000,"stop":1662978488454,"duration":94454}},{"uid":"5043455d986171b7","status":"passed","time":{"start":1662961730000,"stop":1662961832635,"duration":102635}},{"uid":"b0aef0f1704fed8e","status":"passed","time":{"start":1662960985000,"stop":1662961066612,"duration":81612}},{"uid":"44547dec0a7ad44c","status":"passed","time":{"start":1662960825000,"stop":1662960924938,"duration":99938}},{"uid":"2e8edba630aea61c","status":"passed","time":{"start":1662824760000,"stop":1662824863490,"duration":103490}},{"uid":"2a7b638170904cbb","status":"passed","time":{"start":1662819406000,"stop":1662819499771,"duration":93771}},{"uid":"ca4e380bcb81b5dc","status":"passed","time":{"start":1662795171000,"stop":1662795201648,"duration":30648}},{"uid":"4798cf7d9fa6573a","status":"passed","time":{"start":1662792593000,"stop":1662792641579,"duration":48579}},{"uid":"efd91ff2c4ff6f14","status":"passed","time":{"start":1662791140000,"stop":1662791186960,"duration":46960}},{"uid":"4ef1744062dbf4c8","status":"passed","time":{"start":1662788408000,"stop":1662788493749,"duration":85749}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests slice with netpol should contain the allowed namespaces":{"statistic":{"failed":0,"broken":0,"skipped":39,"passed":12,"unknown":0,"total":51},"items":[{"uid":"4013226d88ce372e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"6e983497ff74d0d5","status":"passed","time":{"start":1663044069000,"stop":1663044069179,"duration":179}},{"uid":"ea6283f4634f9c55","status":"passed","time":{"start":1662348908000,"stop":1662348908145,"duration":145}},{"uid":"5e45c519c81dbbf9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"f84011b8bdca50d6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"b22d1b1b92508627","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"366c74132df5fa89","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"ce010680e723273f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"e555d589f63e76c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"e97b261df10aa18b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"63e519488d53d2","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"e500a602ac4a484a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"d7977b35335146da","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"fd77bbe4f1f4eb1b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"c92421f5075bac7c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"d9e6c0f77e63520","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"1ec5cbb3af9551d4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"a8cac207d479ac21","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"2189a2fd8853074e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"f1df71237ae45d4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Deletion Should fail when deleting a project that does not exist":{"statistic":{"failed":14,"broken":0,"skipped":0,"passed":176,"unknown":0,"total":190},"items":[{"uid":"9519960a7a27f8a7","status":"passed","time":{"start":1663042493000,"stop":1663042501547,"duration":8547}},{"uid":"110dbbb9d863f3f0","status":"passed","time":{"start":1663041865000,"stop":1663041873893,"duration":8893}},{"uid":"84fca0262c61a677","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000824630>: {\n        Underlying: <*exec.ExitError | 0xc0000753c0>{\n            ProcessState: {\n                pid: 10039,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 152010},\n                    Stime: {Sec: 0, Usec: 47774},\n                    Maxrss: 83776,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2875,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13344,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 349,\n                    Nivcsw: 250,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Deletion%20Should%20fail%20when%20deleting%20a%20project%20that%20does%20not%20exist2919910986\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.217.78:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Deletion%20Should%20fail%20when%20deleting%20a%20project%20that%20does%20not%20exist2919910986\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.217.78:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Deletion%20Should%20fail%20when%20deleting%20a%20project%20that%20does%20not%20exist2919910986\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.217.78:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Deletion%20Should%20fail%20when%20deleting%20a%20project%20that%20does%20not%20exist2919910986\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.217.78:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Deletion%20Should%20fail%20when%20deleting%20a%20project%20that%20does%20not%20exist2919910986\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.217.78:443: connect: connection refused\noccurred","time":{"start":1663039902000,"stop":1663039902919,"duration":919}},{"uid":"70b7b9dc5839d6a2","status":"passed","time":{"start":1663009431000,"stop":1663009438777,"duration":7777}},{"uid":"d7bf1f31379d82c0","status":"passed","time":{"start":1662978562000,"stop":1662978569875,"duration":7875}},{"uid":"32e1c25a28e8422a","status":"passed","time":{"start":1662702362000,"stop":1662702375612,"duration":13612}},{"uid":"e38cf9e6b0de96be","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <bool>: false\nto be true","time":{"start":1662553059000,"stop":1662553180326,"duration":121326}},{"uid":"36d8903a020b6fb0","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <bool>: false\nto be true","time":{"start":1662553042000,"stop":1662553110346,"duration":68346}},{"uid":"77c6b419d1fcff5d","status":"passed","time":{"start":1662467486000,"stop":1662467494577,"duration":8577}},{"uid":"9a463e75e0302766","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <bool>: false\nto be true","time":{"start":1662374048000,"stop":1662374115433,"duration":67433}},{"uid":"17a30ce2797719a","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <bool>: false\nto be true","time":{"start":1662374250000,"stop":1662374318489,"duration":68489}},{"uid":"a2d984f4553e909b","status":"passed","time":{"start":1662361116000,"stop":1662361123772,"duration":7772}},{"uid":"c269372c2f68aef9","status":"passed","time":{"start":1662347286000,"stop":1662347302673,"duration":16673}},{"uid":"3230a18e0e112803","status":"passed","time":{"start":1662049179000,"stop":1662049187059,"duration":8059}},{"uid":"7e93ef28ee3a486f","status":"passed","time":{"start":1662048808000,"stop":1662048816772,"duration":8772}},{"uid":"201a63dcb1182529","status":"passed","time":{"start":1662036991000,"stop":1662036999545,"duration":8545}},{"uid":"346031fb6a7c40be","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <bool>: false\nto be true","time":{"start":1662031654000,"stop":1662031722214,"duration":68214}},{"uid":"dd7a645b2aa22848","status":"passed","time":{"start":1662027354000,"stop":1662027361913,"duration":7913}},{"uid":"5718ccee0e6164","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000f3e0>: {\n        Underlying: <*exec.ExitError | 0xc0009050a0>{\n            ProcessState: {\n                pid: 7664,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 190446},\n                    Stime: {Sec: 0, Usec: 47611},\n                    Maxrss: 86168,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4835,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13344,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 264,\n                    Nivcsw: 271,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Deletion%20Should%20fail%20when%20deleting%20a%20project%20that%20does%20not%20exist4199133241\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.242.99:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Deletion%20Should%20fail%20when%20deleting%20a%20project%20that%20does%20not%20exist4199133241\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.242.99:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Deletion%20Should%20fail%20when%20deleting%20a%20project%20that%20does%20not%20exist4199133241\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.242.99:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Deletion%20Should%20fail%20when%20deleting%20a%20project%20that%20does%20not%20exist4199133241\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.242.99:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Deletion%20Should%20fail%20when%20deleting%20a%20project%20that%20does%20not%20exist4199133241\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.242.99:443: connect: connection refused\noccurred","time":{"start":1661952242000,"stop":1661952242268,"duration":268}},{"uid":"7485b6a989d9272b","status":"passed","time":{"start":1661863163000,"stop":1661863171208,"duration":8208}}]},"Worker Suite:Worker Suite#[It] Hub netpol scenarios: Slice Configs Netpol scenarios Should successfully deploy slice for valid namespace and valid clusters in allowedNamespaces of sliceconfigs manifest":{"statistic":{"failed":9,"broken":0,"skipped":14,"passed":28,"unknown":0,"total":51},"items":[{"uid":"528fbd531717aadd","status":"skipped","statusDetails":"skipped","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"c7a0e30610d4f762","status":"passed","time":{"start":1663044069000,"stop":1663044069783,"duration":783}},{"uid":"180178e2c5d1e752","status":"passed","time":{"start":1662348908000,"stop":1662348909101,"duration":1101}},{"uid":"66660a5e2a5eba5d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"5669a6e840dcf9e1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"de97ff44621bde80","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"a15a8fd746b67a09","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"358e25373fbebb63","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"48c7e95ac2ae349a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"c3bda7178d792c8d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"8820f3af6ec50079","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"49271970c0f4e59a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"78d9515ea7494cc9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"bda88e86652e4d4c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"400ce18d643c9e5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"ffce12e2a6be2727","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"531f2cd9e5077f58","status":"failed","statusDetails":"Expected\n    <[]string | len:0, cap:0>: nil\nto equal\n    <[]string | len:1, cap:1>: [\"kube-system\"]","time":{"start":1660113628000,"stop":1660113628535,"duration":535}},{"uid":"56aa6694efc8c246","status":"failed","statusDetails":"Expected\n    <[]string | len:0, cap:0>: nil\nto equal\n    <[]string | len:1, cap:1>: [\"kube-system\"]","time":{"start":1660104898000,"stop":1660104898627,"duration":627}},{"uid":"727e6e902469b4f9","status":"passed","time":{"start":1660067259000,"stop":1660067259539,"duration":539}},{"uid":"3ea312df00826f38","status":"passed","time":{"start":1660047551000,"stop":1660047551447,"duration":447}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Creation Should create Multiple Projects in controller using valid manifest":{"statistic":{"failed":14,"broken":0,"skipped":0,"passed":179,"unknown":0,"total":193},"items":[{"uid":"79f4e4df4e19bf9e","status":"passed","time":{"start":1663042493000,"stop":1663042500767,"duration":7767}},{"uid":"64b43833c6935e8","status":"passed","time":{"start":1663041865000,"stop":1663041872929,"duration":7929}},{"uid":"59c25d359a6c6ade","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0008252c0>: {\n        Underlying: <*exec.ExitError | 0xc000407160>{\n            ProcessState: {\n                pid: 10077,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 145326},\n                    Stime: {Sec: 0, Usec: 61653},\n                    Maxrss: 90656,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 5825,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13344,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 638,\n                    Nivcsw: 487,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20create%20Multiple%20Projects%20in%20controller%20using%20valid%20manifest226309137\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.217.78:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20create%20Multiple%20Projects%20in%20controller%20using%20valid%20manifest226309137\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.217.78:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20create%20Multiple%20Projects%20in%20controller%20using%20valid%20manifest226309137\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.217.78:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20create%20Multiple%20Projects%20in%20controller%20using%20valid%20manifest226309137\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.217.78:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20create%20Multiple%20Projects%20in%20controller%20using%20valid%20manifest226309137\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.217.78:443: connect: connection refused\noccurred","time":{"start":1663039902000,"stop":1663039902286,"duration":286}},{"uid":"541e3ddd689f2d87","status":"passed","time":{"start":1663009431000,"stop":1663009438783,"duration":7783}},{"uid":"473f2dbb8c145524","status":"passed","time":{"start":1662978562000,"stop":1662978569904,"duration":7904}},{"uid":"bd0746700f45d8a9","status":"passed","time":{"start":1662702362000,"stop":1662702369818,"duration":7818}},{"uid":"eb7a7e48c0be74bd","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <bool>: false\nto be true","time":{"start":1662553059000,"stop":1662553120316,"duration":61316}},{"uid":"9fd3a334df25f043","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000508018>: {\n        Underlying: <*exec.ExitError | 0xc00083a040>{\n            ProcessState: {\n                pid: 10946,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 187771},\n                    Stime: {Sec: 0, Usec: 42152},\n                    Maxrss: 84040,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 7391,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 412,\n                    Nivcsw: 483,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20create%20Multiple%20Projects%20in%20controller%20using%20valid%20manifest392837153\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.123.224:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20create%20Multiple%20Projects%20in%20controller%20using%20valid%20manifest392837153\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.123.224:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20create%20Multiple%20Projects%20in%20controller%20using%20valid%20manifest392837153\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.123.224:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20create%20Multiple%20Projects%20in%20controller%20using%20valid%20manifest392837153\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.123.224:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20create%20Multiple%20Projects%20in%20controller%20using%20valid%20manifest392837153\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.123.224:443: connect: connection refused\noccurred","time":{"start":1662553042000,"stop":1662553042302,"duration":302}},{"uid":"bca4f7fd1e36108a","status":"passed","time":{"start":1662467486000,"stop":1662467493715,"duration":7715}},{"uid":"c0d3109653d36ec","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000700870>: {\n        Underlying: <*exec.ExitError | 0xc000075c80>{\n            ProcessState: {\n                pid: 10989,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 254314},\n                    Stime: {Sec: 0, Usec: 55101},\n                    Maxrss: 76792,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3056,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 470,\n                    Nivcsw: 630,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20create%20Multiple%20Projects%20in%20controller%20using%20valid%20manifest592753406\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.14.145:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20create%20Multiple%20Projects%20in%20controller%20using%20valid%20manifest592753406\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.14.145:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20create%20Multiple%20Projects%20in%20controller%20using%20valid%20manifest592753406\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.14.145:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20create%20Multiple%20Projects%20in%20controller%20using%20valid%20manifest592753406\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.14.145:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20create%20Multiple%20Projects%20in%20controller%20using%20valid%20manifest592753406\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.14.145:443: connect: connection refused\noccurred","time":{"start":1662374048000,"stop":1662374048455,"duration":455}},{"uid":"cbed2a994f772841","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000352918>: {\n        Underlying: <*exec.ExitError | 0xc00096dbe0>{\n            ProcessState: {\n                pid: 7236,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 231652},\n                    Stime: {Sec: 0, Usec: 27484},\n                    Maxrss: 78780,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2630,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13344,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 271,\n                    Nivcsw: 300,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20create%20Multiple%20Projects%20in%20controller%20using%20valid%20manifest1900907285\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.241.207:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20create%20Multiple%20Projects%20in%20controller%20using%20valid%20manifest1900907285\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.241.207:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20create%20Multiple%20Projects%20in%20controller%20using%20valid%20manifest1900907285\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.241.207:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20create%20Multiple%20Projects%20in%20controller%20using%20valid%20manifest1900907285\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.241.207:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20create%20Multiple%20Projects%20in%20controller%20using%20valid%20manifest1900907285\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.241.207:443: connect: connection refused\noccurred","time":{"start":1662374250000,"stop":1662374250331,"duration":331}},{"uid":"b00fa8c70d9a8404","status":"passed","time":{"start":1662361116000,"stop":1662361123769,"duration":7769}},{"uid":"b46189e37b5565d2","status":"passed","time":{"start":1662347286000,"stop":1662347293775,"duration":7775}},{"uid":"5a166983b831ff4e","status":"passed","time":{"start":1662049179000,"stop":1662049187147,"duration":8147}},{"uid":"340ac71c9afc01b6","status":"passed","time":{"start":1662048808000,"stop":1662048815897,"duration":7897}},{"uid":"1d7e84bdc6ac2682","status":"passed","time":{"start":1662036991000,"stop":1662036998724,"duration":7724}},{"uid":"2f9076ea8c933234","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <bool>: false\nto be true","time":{"start":1662031654000,"stop":1662031715269,"duration":61269}},{"uid":"c53491d97fd545fb","status":"passed","time":{"start":1662027354000,"stop":1662027362000,"duration":8000}},{"uid":"85fc74f2b5690e00","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <bool>: false\nto be true","time":{"start":1661952242000,"stop":1661952303316,"duration":61316}},{"uid":"3726a6000c152915","status":"passed","time":{"start":1661863163000,"stop":1661863170976,"duration":7976}}]},"Worker Suite:Worker Suite#[It] Slice Config Negative Scenario tests Slice Config Negative Scenario Creation Should fail while creating sliceconfig with invalid values on slice subnet":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":50,"unknown":0,"total":51},"items":[{"uid":"5348512b5f543da5","status":"passed","time":{"start":1663044723000,"stop":1663044723368,"duration":368}},{"uid":"ad7a0432ba32cb10","status":"skipped","statusDetails":"skipped","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"4790c1e5f756fb76","status":"passed","time":{"start":1662348908000,"stop":1662348908246,"duration":246}},{"uid":"a8b5ce8ba7ccecf9","status":"passed","time":{"start":1661844205000,"stop":1661844205297,"duration":297}},{"uid":"54c70353f01fc3be","status":"passed","time":{"start":1661754870000,"stop":1661754870225,"duration":225}},{"uid":"b66ea16fc1017a14","status":"passed","time":{"start":1661611962000,"stop":1661611962274,"duration":274}},{"uid":"b098770a9900a552","status":"passed","time":{"start":1661585077000,"stop":1661585077230,"duration":230}},{"uid":"7c6b9313bdc0d52a","status":"passed","time":{"start":1661584447000,"stop":1661584447273,"duration":273}},{"uid":"382e9fe0fd744a70","status":"passed","time":{"start":1661580045000,"stop":1661580045236,"duration":236}},{"uid":"3d65d89c4db16d92","status":"passed","time":{"start":1661578505000,"stop":1661578505294,"duration":294}},{"uid":"92f8873c8a832dd9","status":"passed","time":{"start":1661572479000,"stop":1661572479338,"duration":338}},{"uid":"6fd7ca3f06a0a0f0","status":"passed","time":{"start":1660832281000,"stop":1660832281217,"duration":217}},{"uid":"4dc867951f6583de","status":"passed","time":{"start":1660808797000,"stop":1660808797206,"duration":206}},{"uid":"7911afcd026fa615","status":"passed","time":{"start":1660299182000,"stop":1660299182192,"duration":192}},{"uid":"f52baeb43e0e1058","status":"passed","time":{"start":1660295740000,"stop":1660295740187,"duration":187}},{"uid":"1e68498eb43c5a17","status":"passed","time":{"start":1660293657000,"stop":1660293657273,"duration":273}},{"uid":"c046516ad4b3191a","status":"passed","time":{"start":1660113628000,"stop":1660113628403,"duration":403}},{"uid":"2a6b0223922ffa5d","status":"passed","time":{"start":1660104898000,"stop":1660104898388,"duration":388}},{"uid":"420942bc34bbbd50","status":"passed","time":{"start":1660067259000,"stop":1660067259222,"duration":222}},{"uid":"23627fff15ec10a0","status":"passed","time":{"start":1660047551000,"stop":1660047551314,"duration":314}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Update Should fail when Project is Applied with service account name as combination of special characters in Write users":{"statistic":{"failed":16,"broken":0,"skipped":0,"passed":174,"unknown":0,"total":190},"items":[{"uid":"9e987bcfc14f0ac2","status":"passed","time":{"start":1663042493000,"stop":1663042498848,"duration":5848}},{"uid":"3dbaf710354b1717","status":"passed","time":{"start":1663041865000,"stop":1663041870127,"duration":5127}},{"uid":"ffb722c260af130a","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <bool>: false\nto be true","time":{"start":1663039902000,"stop":1663039968585,"duration":66585}},{"uid":"849ab66d2de6922f","status":"passed","time":{"start":1663009431000,"stop":1663009436005,"duration":5005}},{"uid":"302552c30cb81a42","status":"passed","time":{"start":1662978562000,"stop":1662978567042,"duration":5042}},{"uid":"2660d1395ce3ef60","status":"passed","time":{"start":1662702362000,"stop":1662702366876,"duration":4876}},{"uid":"f2cf220bf3dd46d","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <bool>: false\nto be true","time":{"start":1662553059000,"stop":1662553180312,"duration":121312}},{"uid":"c30afc5e8cf52a5b","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <bool>: false\nto be true","time":{"start":1662553042000,"stop":1662553108666,"duration":66666}},{"uid":"5660074127f5186c","status":"passed","time":{"start":1662467486000,"stop":1662467490916,"duration":4916}},{"uid":"218dda9f40bbe56d","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <bool>: false\nto be true","time":{"start":1662374048000,"stop":1662374115439,"duration":67439}},{"uid":"c816c1131ad42008","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00087df68>: {\n        Underlying: <*exec.ExitError | 0xc00062d440>{\n            ProcessState: {\n                pid: 7198,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 225653},\n                    Stime: {Sec: 0, Usec: 46833},\n                    Maxrss: 79288,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3635,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13344,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 345,\n                    Nivcsw: 284,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Write%20users3835299507\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.241.207:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Write%20users3835299507\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.241.207:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Write%20users3835299507\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.241.207:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Write%20users3835299507\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.241.207:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Write%20users3835299507\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.241.207:443: connect: connection refused\noccurred","time":{"start":1662374250000,"stop":1662374250333,"duration":333}},{"uid":"5f9df3998edf9edb","status":"passed","time":{"start":1662361116000,"stop":1662361120894,"duration":4894}},{"uid":"13b7a0f26613d644","status":"passed","time":{"start":1662347286000,"stop":1662347290882,"duration":4882}},{"uid":"509f25007cfa705c","status":"passed","time":{"start":1662049179000,"stop":1662049184258,"duration":5258}},{"uid":"b04e043f10d43351","status":"passed","time":{"start":1662048808000,"stop":1662048813991,"duration":5991}},{"uid":"32e099325fc1800b","status":"passed","time":{"start":1662036991000,"stop":1662036996874,"duration":5874}},{"uid":"a93c6ca68bb6ba6f","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0007bbe00>: {\n        Underlying: <*exec.ExitError | 0xc0006c0d40>{\n            ProcessState: {\n                pid: 9617,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 192570},\n                    Stime: {Sec: 0, Usec: 21882},\n                    Maxrss: 75772,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3342,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13344,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 229,\n                    Nivcsw: 238,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Write%20users1921983176\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.231.22:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Write%20users1921983176\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.231.22:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Write%20users1921983176\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.231.22:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Write%20users1921983176\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.231.22:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Write%20users1921983176\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.231.22:443: connect: connection refused\noccurred","time":{"start":1662031654000,"stop":1662031654259,"duration":259}},{"uid":"797a2ba0a4fc8c31","status":"passed","time":{"start":1662027354000,"stop":1662027359072,"duration":5072}},{"uid":"c3ad5ee4a3370786","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000e960>: {\n        Underlying: <*exec.ExitError | 0xc0006d9e40>{\n            ProcessState: {\n                pid: 7737,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 176058},\n                    Stime: {Sec: 0, Usec: 46948},\n                    Maxrss: 82400,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4092,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13344,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 382,\n                    Nivcsw: 306,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Write%20users3611966608\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.242.99:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Write%20users3611966608\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.242.99:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Write%20users3611966608\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.242.99:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Write%20users3611966608\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.242.99:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Write%20users3611966608\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.242.99:443: connect: connection refused\noccurred","time":{"start":1661952242000,"stop":1661952242262,"duration":262}},{"uid":"85f0e7afee675204","status":"passed","time":{"start":1661863163000,"stop":1661863168015,"duration":5015}}]},"Worker Suite:Worker Suite#[It] Slice validation Slice with MEDIUM QOS should have gateway pod running":{"statistic":{"failed":1,"broken":0,"skipped":3,"passed":5,"unknown":0,"total":9},"items":[{"uid":"b3b6e4a8fa0c91c1","status":"passed","time":{"start":1659168739000,"stop":1659168739027,"duration":27}},{"uid":"8230e5aa7484ed6b","status":"passed","time":{"start":1659164084000,"stop":1659164084038,"duration":38}},{"uid":"a6c5b4296cbc546a","status":"passed","time":{"start":1659160188000,"stop":1659160188098,"duration":98}},{"uid":"b68db7ec5e46e791","status":"passed","time":{"start":1659119724000,"stop":1659119724043,"duration":43}},{"uid":"c82a0259af00b654","status":"passed","time":{"start":1659116511000,"stop":1659116511028,"duration":28}},{"uid":"a7cc472df3f4a042","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659109470000,"stop":1659109470000,"duration":0}},{"uid":"1b5e1219b3c47024","status":"failed","statusDetails":"Timed out after 180.004s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1659106836000,"stop":1659107016302,"duration":180302}},{"uid":"da28f429be91085e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659101522000,"stop":1659101522000,"duration":0}},{"uid":"5fb9844f703d68ba","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659082816000,"stop":1659082816000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Attach/Deattach Scenario Attach cluster from slice  when Deattach cluster from slice  should have vl3 router running on attached cluster":{"statistic":{"failed":1,"broken":0,"skipped":35,"passed":6,"unknown":0,"total":42},"items":[{"uid":"94d207b6f487e105","status":"passed","time":{"start":1663044723000,"stop":1663044723009,"duration":9}},{"uid":"cbb8b6a95caaa145","status":"passed","time":{"start":1663044069000,"stop":1663044069009,"duration":9}},{"uid":"58ba3e9f0c7f348e","status":"passed","time":{"start":1662348908000,"stop":1662348908006,"duration":6}},{"uid":"3f5beb70fa50158d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"abbb52c640cd9fda","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"d1a2a2321264dcfb","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"e0e5dc964c87821b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"3344ea9a05620af8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"d530c90978bc3edb","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"2a3fd51afab1e235","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"cdae3da582e504d4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"9c595a5c928b322e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"2ec179450bf101f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"d15540ac1143177f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"5a0c683f56f7bd2b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"7d62d68a8eda47b0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"3289fe47f5013d48","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"735a5e166091b1a4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"d0f24a524844458e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"694071bc5a3889df","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting nsm-kernel-forwarder pod Iperf cleanup":{"statistic":{"failed":0,"broken":0,"skipped":49,"passed":0,"unknown":0,"total":49},"items":[{"uid":"da1d6dbb87696050","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662348908000,"stop":1662348908000,"duration":0}},{"uid":"8fec16d306a0857f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"a3532def6f931738","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"f576622de0198209","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"3b3ed979748cc93f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"578a0569b89d5be9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"64f13ff87aaa8a65","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"f28514e4e1d2d49f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"a1e45a0e4b3c3fd5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"5e74417ff37a92ee","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"b5f1e5596a991cf5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"44569541dcf147a3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"11fda8d81cec246a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"f731ca1e4707a20d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"f44600f22877f25","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"52f42cba52cf9c10","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"c69073c6064b5f56","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"5c14f502ae6b7d0f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}},{"uid":"76aba77214a9e69f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659982549000,"stop":1659982549000,"duration":0}},{"uid":"8dc20c5bb402487f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659970182000,"stop":1659970182000,"duration":0}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Deletion Should Delete an existing project successfully":{"statistic":{"failed":14,"broken":0,"skipped":0,"passed":176,"unknown":0,"total":190},"items":[{"uid":"ae4581a9caf0325f","status":"passed","time":{"start":1663042493000,"stop":1663042497750,"duration":4750}},{"uid":"1a261c008b15ac20","status":"passed","time":{"start":1663041865000,"stop":1663041869978,"duration":4978}},{"uid":"65acea237c805943","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000824978>: {\n        Underlying: <*exec.ExitError | 0xc00030c860>{\n            ProcessState: {\n                pid: 10048,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 169317},\n                    Stime: {Sec: 0, Usec: 39376},\n                    Maxrss: 87412,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3726,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13344,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 407,\n                    Nivcsw: 227,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Deletion%20Should%20Delete%20an%20existing%20project%20successfully1856069718\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.217.78:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Deletion%20Should%20Delete%20an%20existing%20project%20successfully1856069718\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.217.78:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Deletion%20Should%20Delete%20an%20existing%20project%20successfully1856069718\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.217.78:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Deletion%20Should%20Delete%20an%20existing%20project%20successfully1856069718\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.217.78:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Deletion%20Should%20Delete%20an%20existing%20project%20successfully1856069718\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.217.78:443: connect: connection refused\noccurred","time":{"start":1663039902000,"stop":1663039902258,"duration":258}},{"uid":"c6ef70586c3b9cb7","status":"passed","time":{"start":1663009431000,"stop":1663009435753,"duration":4753}},{"uid":"fa070d811e349dac","status":"passed","time":{"start":1662978562000,"stop":1662978566859,"duration":4859}},{"uid":"6bb10fde84c34767","status":"passed","time":{"start":1662702362000,"stop":1662702366809,"duration":4809}},{"uid":"9617866376689dff","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0000bf620>: {\n        Underlying: <*exec.ExitError | 0xc0003dd860>{\n            ProcessState: {\n                pid: 12028,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 181406},\n                    Stime: {Sec: 0, Usec: 51267},\n                    Maxrss: 80608,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4993,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13344,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 331,\n                    Nivcsw: 266,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when applying patch:\",\n                        \"{\\\"status\\\":{}}\",\n                        \"to:\",\n                        \"Resource: \\\"controller.kubeslice.io/v1alpha1, Resource=projects\\\", GroupVersionKind: \\\"controller.kubeslice.io/v1alpha1, Kind=Project\\\"\",\n                        \"Name: \\\"projectdeletetest\\\", Namespace: \\\"kubeslice-controller\\\"\",\n                        \"for: \\\"/tmp/Project%20CR%20test%20Project%20Deletion%20Should%20Delete%20an%20existing%20project%20successfully255208318\\\": error when patching \\\"/tmp/Project%20CR%20test%20Project%20Deletion%20Should%20Delete%20an%20existing%20project%20successfully255208318\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.67.103:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when applying patch:\",\n                    \"{\\\"status\\\":{}}\",\n                    \"to:\",\n                    \"Resource: \\\"controller.kubeslice.io/v1alpha1, Resource=projects\\\", GroupVersionKind: \\\"controller.kubeslice.io/v1alpha1, Kind=Project\\\"\",\n                    \"Name: \\\"projectdeletetest\\\", Namespace: \\\"kubeslice-controller\\\"\",\n                    \"for: \\\"/tmp/Project%20CR%20test%20Project%20Deletion%20Should%20Delete%20an%20existing%20project%20successfully255208318\\\": error when patching \\\"/tmp/Project%20CR%20test%20Project%20Deletion%20Should%20Delete%20an%20existing%20project%20successfully255208318\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.67.103:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when applying patch:\",\n                        \"{\\\"status\\\":{}}\",\n                        \"to:\",\n                        \"Resource: \\\"controller.kubeslice.io/v1alpha1, Resource=projects\\\", GroupVersionKind: \\\"controller.kubeslice.io/v1alpha1, Kind=Project\\\"\",\n                        \"Name: \\\"projectdeletetest\\\", Namespace: \\\"kubeslice-controller\\\"\",\n                        \"for: \\\"/tmp/Project%20CR%20test%20Project%20Deletion%20Should%20Delete%20an%20existing%20project%20successfully255208318\\\": error when patching \\\"/tmp/Project%20CR%20test%20Project%20Deletion%20Should%20Delete%20an%20existing%20project%20successfully255208318\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-s...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; Error from server (InternalError): error when applying patch:\n    {\"status\":{}}\n    to:\n    Resource: \"controller.kubeslice.io/v1alpha1, Resource=projects\", GroupVersionKind: \"controller.kubeslice.io/v1alpha1, Kind=Project\"\n    Name: \"projectdeletetest\", Namespace: \"kubeslice-controller\"\n    for: \"/tmp/Project%20CR%20test%20Project%20Deletion%20Should%20Delete%20an%20existing%20project%20successfully255208318\": error when patching \"/tmp/Project%20CR%20test%20Project%20Deletion%20Should%20Delete%20an%20existing%20project%20successfully255208318\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.67.103:443: connect: connection refused\noccurred","time":{"start":1662553059000,"stop":1662553119280,"duration":60280}},{"uid":"4e21baa47fe29761","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <bool>: false\nto be true","time":{"start":1662553042000,"stop":1662553109868,"duration":67868}},{"uid":"da2bae0060102e09","status":"passed","time":{"start":1662467486000,"stop":1662467490713,"duration":4713}},{"uid":"8898d0ae3c4caf21","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <bool>: false\nto be true","time":{"start":1662374048000,"stop":1662374169297,"duration":121297}},{"uid":"1efc7e658cd54d5d","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <bool>: false\nto be true","time":{"start":1662374250000,"stop":1662374371415,"duration":121415}},{"uid":"992c831c7706577","status":"passed","time":{"start":1662361116000,"stop":1662361120784,"duration":4784}},{"uid":"7e4d877721b920c5","status":"passed","time":{"start":1662347286000,"stop":1662347290795,"duration":4795}},{"uid":"ff7cc5686f655f9b","status":"passed","time":{"start":1662049179000,"stop":1662049184087,"duration":5087}},{"uid":"f22dc0ae655353fb","status":"passed","time":{"start":1662048808000,"stop":1662048812809,"duration":4809}},{"uid":"21a356ad26dade1a","status":"passed","time":{"start":1662036991000,"stop":1662036995773,"duration":4773}},{"uid":"cbbe3cc7f3cdfc9d","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <bool>: false\nto be true","time":{"start":1662031654000,"stop":1662031720525,"duration":66525}},{"uid":"53f3f622b19d9995","status":"passed","time":{"start":1662027354000,"stop":1662027358912,"duration":4912}},{"uid":"5e3d6889d3644b0d","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000f638>: {\n        Underlying: <*exec.ExitError | 0xc0009055e0>{\n            ProcessState: {\n                pid: 7673,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 180793},\n                    Stime: {Sec: 0, Usec: 60264},\n                    Maxrss: 78916,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 5514,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13344,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 317,\n                    Nivcsw: 333,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Deletion%20Should%20Delete%20an%20existing%20project%20successfully212922473\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.242.99:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Deletion%20Should%20Delete%20an%20existing%20project%20successfully212922473\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.242.99:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Deletion%20Should%20Delete%20an%20existing%20project%20successfully212922473\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.242.99:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Deletion%20Should%20Delete%20an%20existing%20project%20successfully212922473\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.242.99:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Deletion%20Should%20Delete%20an%20existing%20project%20successfully212922473\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.242.99:443: connect: connection refused\noccurred","time":{"start":1661952242000,"stop":1661952242279,"duration":279}},{"uid":"ae360fa1d5b75b8a","status":"passed","time":{"start":1661863163000,"stop":1661863167841,"duration":4841}}]},"Istio Suite:Istio Suite#[It] Bookinfo tests Bookinfo Scenario 4 with egress and ingress Should install Productpage on worker cluster-1":{"statistic":{"failed":50,"broken":0,"skipped":0,"passed":7,"unknown":0,"total":57},"items":[{"uid":"5c39febf89ac64c","status":"passed","time":{"start":1663043831000,"stop":1663043957524,"duration":126524}},{"uid":"db8ecdb426be7f6d","status":"passed","time":{"start":1663043075000,"stop":1663043205323,"duration":130323}},{"uid":"f40e7c4a5b0f8658","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1663040338000,"stop":1663040518316,"duration":180316}},{"uid":"bcb98b340a742b00","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1662819574000,"stop":1662819754373,"duration":180373}},{"uid":"ed427ba7d35a197d","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1662795310000,"stop":1662795490282,"duration":180282}},{"uid":"d8191af5de307ac7","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1662792773000,"stop":1662792953480,"duration":180480}},{"uid":"8163ae19a2e7a286","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1662620110000,"stop":1662620290309,"duration":180309}},{"uid":"276fa7040955e80a","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1662617640000,"stop":1662617820361,"duration":180361}},{"uid":"6df302f8de70e85e","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1662610377000,"stop":1662610557334,"duration":180334}},{"uid":"9f2eb0b4e7b03c54","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1662557911000,"stop":1662558091274,"duration":180274}},{"uid":"d65812ae628fdbd","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1662553747000,"stop":1662553927248,"duration":180248}},{"uid":"3f4c32cefaac28df","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1662553327000,"stop":1662553507355,"duration":180355}},{"uid":"921877c96f8b6a71","status":"failed","statusDetails":"Timed out after 180.002s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1662381943000,"stop":1662382123355,"duration":180355}},{"uid":"77cb65bf09eeb0d4","status":"passed","time":{"start":1662348103000,"stop":1662348203438,"duration":100438}},{"uid":"dfdd71bfbdc199ad","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1662049889000,"stop":1662050069402,"duration":180402}},{"uid":"7428872697a35ac6","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1662049074000,"stop":1662049254352,"duration":180352}},{"uid":"d29dc5d3168ea882","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1662028179000,"stop":1662028359339,"duration":180339}},{"uid":"6532fe513608a733","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1661859205000,"stop":1661859385431,"duration":180431}},{"uid":"ba3c09131eafdfd7","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004f2480>: {\n        Underlying: <*exec.ExitError | 0xc0006d40a0>{\n            ProcessState: {\n                pid: 6904,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 256904},\n                    Stime: {Sec: 0, Usec: 75095},\n                    Maxrss: 79272,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 8672,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 316,\n                    Nivcsw: 420,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicewithegressingress\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicewithegressingress\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicewithegressingress\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicewithegressingress\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicewithegressingress\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\noccurred","time":{"start":1661843950000,"stop":1661843950331,"duration":331}},{"uid":"7a2f640498860991","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0002bfda0>: {\n        Underlying: <*exec.ExitError | 0xc00030af00>{\n            ProcessState: {\n                pid: 6941,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 168346},\n                    Stime: {Sec: 0, Usec: 67338},\n                    Maxrss: 79996,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 10650,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 250,\n                    Nivcsw: 496,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicewithegressingress\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicewithegressingress\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicewithegressingress\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicewithegressingress\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicewithegressingress\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661754632000,"stop":1661754632242,"duration":242}}]},"Empty Suite:Empty Suite#[AfterSuite]":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":169,"unknown":0,"total":169},"items":[{"uid":"ceb6dff33d0bc1c6","status":"passed","time":{"start":1663052238000,"stop":1663052238000,"duration":0}},{"uid":"de055876a678862c","status":"passed","time":{"start":1663051980000,"stop":1663051980000,"duration":0}},{"uid":"cc9b78725b9a7b51","status":"passed","time":{"start":1663050064000,"stop":1663050064000,"duration":0}},{"uid":"999aaf4ca9a4ddd4","status":"passed","time":{"start":1663042347000,"stop":1663042347000,"duration":0}},{"uid":"b811cc713519a53e","status":"passed","time":{"start":1663041715000,"stop":1663041715000,"duration":0}},{"uid":"f5e25006e8cb6ba2","status":"passed","time":{"start":1663039756000,"stop":1663039756000,"duration":0}},{"uid":"eff2f4976fde0409","status":"passed","time":{"start":1663040168000,"stop":1663040168000,"duration":0}},{"uid":"492830c29c5c1896","status":"passed","time":{"start":1663009296000,"stop":1663009296000,"duration":0}},{"uid":"52888267c82a4db9","status":"passed","time":{"start":1663008270000,"stop":1663008270000,"duration":0}},{"uid":"27a4c5ea85a4b0eb","status":"passed","time":{"start":1662986456000,"stop":1662986456000,"duration":0}},{"uid":"8f08acb1854e0ea9","status":"passed","time":{"start":1662978394000,"stop":1662978394000,"duration":0}},{"uid":"5e1d1813d9bdcbe8","status":"passed","time":{"start":1662961730000,"stop":1662961730000,"duration":0}},{"uid":"938d0a47799e905e","status":"passed","time":{"start":1662960985000,"stop":1662960985000,"duration":0}},{"uid":"88800602fadb8f70","status":"passed","time":{"start":1662960825000,"stop":1662960825000,"duration":0}},{"uid":"a48244a2d48b8cc4","status":"passed","time":{"start":1662824760000,"stop":1662824760000,"duration":0}},{"uid":"6aa723f2bac0f9e5","status":"passed","time":{"start":1662819406000,"stop":1662819406000,"duration":0}},{"uid":"c0f61a7b8d0c15a1","status":"passed","time":{"start":1662795171000,"stop":1662795171000,"duration":0}},{"uid":"62c58b2359f2343f","status":"passed","time":{"start":1662792593000,"stop":1662792593000,"duration":0}},{"uid":"a98b10daebfe2e51","status":"passed","time":{"start":1662791140000,"stop":1662791140000,"duration":0}},{"uid":"4fb50bd3de58dd53","status":"passed","time":{"start":1662788408000,"stop":1662788408000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice Config Negative Scenario tests Slice Config Negative Scenario Update Should update while deploying sliceconfig with existing slice name":{"statistic":{"failed":13,"broken":0,"skipped":1,"passed":37,"unknown":0,"total":51},"items":[{"uid":"4745c44492f4d0f8","status":"passed","time":{"start":1663044723000,"stop":1663044723721,"duration":721}},{"uid":"bb6ca02c8e23b72f","status":"skipped","statusDetails":"skipped","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"32cddebe910da9ed","status":"passed","time":{"start":1662348908000,"stop":1662348908804,"duration":804}},{"uid":"2f5b64ea780b3837","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000ecf0>: {\n        Underlying: <*exec.ExitError | 0xc00067e6a0>{\n            ProcessState: {\n                pid: 7128,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 229406},\n                    Stime: {Sec: 0, Usec: 75078},\n                    Maxrss: 86248,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 8201,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 439,\n                    Nivcsw: 359,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"fireredslice\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\noccurred","time":{"start":1661844205000,"stop":1661844205407,"duration":407}},{"uid":"5d4ae9cd1dcaef6f","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004bef00>: {\n        Underlying: <*exec.ExitError | 0xc000659700>{\n            ProcessState: {\n                pid: 7265,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 178046},\n                    Stime: {Sec: 0, Usec: 33124},\n                    Maxrss: 86976,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3437,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 367,\n                    Nivcsw: 390,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"fireredslice\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661754870000,"stop":1661754870266,"duration":266}},{"uid":"ffed4af211a6727b","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0006f0ca8>: {\n        Underlying: <*exec.ExitError | 0xc0006a8d20>{\n            ProcessState: {\n                pid: 7083,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 214348},\n                    Stime: {Sec: 0, Usec: 65953},\n                    Maxrss: 74460,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2742,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 424,\n                    Nivcsw: 816,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"fireredslice\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661611962000,"stop":1661611962442,"duration":442}},{"uid":"db4728768904462","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00090d998>: {\n        Underlying: <*exec.ExitError | 0xc000479420>{\n            ProcessState: {\n                pid: 7291,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 188629},\n                    Stime: {Sec: 0, Usec: 26947},\n                    Maxrss: 83616,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 5007,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 374,\n                    Nivcsw: 414,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"fireredslice\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661585077000,"stop":1661585077270,"duration":270}},{"uid":"ad1ae62b1c879ca3","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00013bc38>: {\n        Underlying: <*exec.ExitError | 0xc0006ab9c0>{\n            ProcessState: {\n                pid: 6228,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 185338},\n                    Stime: {Sec: 0, Usec: 65640},\n                    Maxrss: 84236,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4067,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 515,\n                    Nivcsw: 556,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"fireredslice\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661584447000,"stop":1661584447433,"duration":433}},{"uid":"8b18cc2dd6ddd8c1","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0006d2468>: {\n        Underlying: <*exec.ExitError | 0xc000074c60>{\n            ProcessState: {\n                pid: 7277,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 159232},\n                    Stime: {Sec: 0, Usec: 51750},\n                    Maxrss: 84320,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3710,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 278,\n                    Nivcsw: 476,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"fireredslice\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661580045000,"stop":1661580045271,"duration":271}},{"uid":"c0eed808589e7219","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0007793f8>: {\n        Underlying: <*exec.ExitError | 0xc000075900>{\n            ProcessState: {\n                pid: 7268,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 236804},\n                    Stime: {Sec: 0, Usec: 42286},\n                    Maxrss: 94616,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3820,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 418,\n                    Nivcsw: 240,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"fireredslice\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661578505000,"stop":1661578505349,"duration":349}},{"uid":"d5ecffbdb9c9e181","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0008a3320>: {\n        Underlying: <*exec.ExitError | 0xc0003a6740>{\n            ProcessState: {\n                pid: 7297,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 212930},\n                    Stime: {Sec: 0, Usec: 70976},\n                    Maxrss: 90156,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3854,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 409,\n                    Nivcsw: 348,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"fireredslice\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661572479000,"stop":1661572479336,"duration":336}},{"uid":"98f600ff7bac110d","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0006eafc0>: {\n        Underlying: <*exec.ExitError | 0xc0008c9f80>{\n            ProcessState: {\n                pid: 7305,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 171516},\n                    Stime: {Sec: 0, Usec: 47440},\n                    Maxrss: 82808,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4687,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 395,\n                    Nivcsw: 346,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"fireredslice\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660832281000,"stop":1660832281316,"duration":316}},{"uid":"98e8309932e9a69d","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000989a70>: {\n        Underlying: <*exec.ExitError | 0xc000711960>{\n            ProcessState: {\n                pid: 7197,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 157560},\n                    Stime: {Sec: 0, Usec: 24240},\n                    Maxrss: 85672,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3933,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 490,\n                    Nivcsw: 221,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"fireredslice\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660808797000,"stop":1660808797263,"duration":263}},{"uid":"cc42e741da377427","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00066c150>: {\n        Underlying: <*exec.ExitError | 0xc0007e03a0>{\n            ProcessState: {\n                pid: 7100,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 148423},\n                    Stime: {Sec: 0, Usec: 42406},\n                    Maxrss: 79908,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 5968,\n                    Majflt: 1,\n                    Nswap: 0,\n                    Inblock: 256,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 463,\n                    Nivcsw: 352,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"fireredslice\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660299182000,"stop":1660299182252,"duration":252}},{"uid":"1f9f92f7f1e1c193","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00013b410>: {\n        Underlying: <*exec.ExitError | 0xc0008192c0>{\n            ProcessState: {\n                pid: 7141,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 149363},\n                    Stime: {Sec: 0, Usec: 35144},\n                    Maxrss: 80492,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 5329,\n                    Majflt: 3,\n                    Nswap: 0,\n                    Inblock: 648,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 494,\n                    Nivcsw: 204,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"fireredslice\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660295740000,"stop":1660295740251,"duration":251}},{"uid":"c26e5880f162ac68","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000eb28>: {\n        Underlying: <*exec.ExitError | 0xc00085a580>{\n            ProcessState: {\n                pid: 7204,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 169453},\n                    Stime: {Sec: 0, Usec: 63052},\n                    Maxrss: 84912,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 5388,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 422,\n                    Nivcsw: 309,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"fireredslice\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660293657000,"stop":1660293657291,"duration":291}},{"uid":"e1088d93cfb7d6c6","status":"passed","time":{"start":1660113628000,"stop":1660113628487,"duration":487}},{"uid":"3bf78a3f3ee4de48","status":"passed","time":{"start":1660104898000,"stop":1660104898510,"duration":510}},{"uid":"27407775e2e36805","status":"passed","time":{"start":1660067259000,"stop":1660067259578,"duration":578}},{"uid":"7d2b2902410b0503","status":"passed","time":{"start":1660047551000,"stop":1660047551724,"duration":724}}]},"Worker Suite:Worker Suite#[AfterSuite]":{"statistic":{"failed":7,"broken":14,"skipped":0,"passed":240,"unknown":0,"total":261},"items":[{"uid":"39d17a3d08eafb53","status":"passed","time":{"start":1663052812000,"stop":1663053115786,"duration":303786}},{"uid":"1530ba5904ab894a","status":"passed","time":{"start":1663052553000,"stop":1663052857343,"duration":304343}},{"uid":"e64ccdee7d992c10","status":"passed","time":{"start":1663050631000,"stop":1663050933804,"duration":302804}},{"uid":"dbd130b7ed9d8178","status":"broken","statusDetails":"interrupted","time":{"start":1663044723000,"stop":1663044750259,"duration":27259}},{"uid":"24e8955e5a7f386f","status":"broken","statusDetails":"interrupted","time":{"start":1663044069000,"stop":1663044089120,"duration":20120}},{"uid":"d13ea4c1cb279e13","status":"passed","time":{"start":1663041526000,"stop":1663041889148,"duration":363148}},{"uid":"bce709acf6f977d","status":"passed","time":{"start":1663011418000,"stop":1663011721241,"duration":303241}},{"uid":"243a328526f1bfd","status":"passed","time":{"start":1663008851000,"stop":1663009154543,"duration":303543}},{"uid":"b05630370bed093","status":"passed","time":{"start":1662987019000,"stop":1662987321997,"duration":302997}},{"uid":"c0f9a328a03dda31","status":"passed","time":{"start":1662980603000,"stop":1662980908436,"duration":305436}},{"uid":"f3f8bf086376fce9","status":"passed","time":{"start":1662962291000,"stop":1662962594213,"duration":303213}},{"uid":"8bad1bd7dd3540bf","status":"passed","time":{"start":1662961544000,"stop":1662961848062,"duration":304062}},{"uid":"8d05134293005519","status":"passed","time":{"start":1662961386000,"stop":1662961688894,"duration":302894}},{"uid":"cf8f8ac53bf2a0cf","status":"passed","time":{"start":1662825333000,"stop":1662825636739,"duration":303739}},{"uid":"1d1ed97877c2d39b","status":"passed","time":{"start":1662820718000,"stop":1662821080948,"duration":362948}},{"uid":"51310ad866b55bf1","status":"passed","time":{"start":1662796443000,"stop":1662796805641,"duration":362641}},{"uid":"22f3580080ef1a8d","status":"passed","time":{"start":1662793922000,"stop":1662794285059,"duration":363059}},{"uid":"ade9d352645f8fb3","status":"passed","time":{"start":1662791710000,"stop":1662792012906,"duration":302906}},{"uid":"740100b0a61846ea","status":"passed","time":{"start":1662788942000,"stop":1662789244776,"duration":302776}},{"uid":"6367da7cef22cc6b","status":"passed","time":{"start":1662784718000,"stop":1662785021601,"duration":303601}}]},"Worker Suite:Worker Suite#[It]  QosProfile tests QosProfile delete validation Delete qos profile - negative scenario":{"statistic":{"failed":1,"broken":0,"skipped":10,"passed":0,"unknown":0,"total":11},"items":[{"uid":"cb8632558c4c893d","status":"skipped","statusDetails":"skipped","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"d48befa97fbdca3a","status":"skipped","statusDetails":"skipped","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"48bf2bc96422128d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662348908000,"stop":1662348908000,"duration":0}},{"uid":"db97c82e8f95dfb","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00048d9c8>: {\n        Underlying: <*exec.ExitError | 0xc0004fcc20>{\n            ProcessState: {\n                pid: 7094,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 252308},\n                    Stime: {Sec: 0, Usec: 68811},\n                    Maxrss: 78900,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 9040,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 269,\n                    Nivcsw: 452,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"red1\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster2\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"red1\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster2\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"red1\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster2\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"red1\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster2\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"red1\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster2\noccurred","time":{"start":1661844205000,"stop":1661844205638,"duration":638}},{"uid":"df003cdeeb161066","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"cd6f8e26812d8bc6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"da70a7111c14b61e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"978e5c57c8787a60","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"3064f88deb557681","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"9c8518af53d899fc","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"b32a3fb99080c12b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Attach/Deattach Scenario Attach cluster from slice  when Deattach cluster from slice  should have gateway pod deleted from deattached cluster":{"statistic":{"failed":0,"broken":0,"skipped":36,"passed":6,"unknown":0,"total":42},"items":[{"uid":"e4e2e2f3794cd6a5","status":"passed","time":{"start":1663044723000,"stop":1663044723004,"duration":4}},{"uid":"21cd38de358a46f8","status":"passed","time":{"start":1663044069000,"stop":1663044069005,"duration":5}},{"uid":"25c858c204e2d474","status":"passed","time":{"start":1662348908000,"stop":1662348908009,"duration":9}},{"uid":"d749ff8fd2d1acf","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"740a7bcbc09d226c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"77a71bc114d36679","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"9165579ba6a09d3c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"4f93493dc6e4397d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"c077161d658296f6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"71b8cb9d3cb2c32e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"8380642870534124","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"a9753c985561045f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"542b877b460cdf3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"878085e3f2803815","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"2c6d1f100e96027a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"e30fcef856ba383b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"a07b40b4b758dab8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"5f73c8f35795ada","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"6d34a78e20f38e3d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"7803f71bb08dc67b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}}]},"Empty Suite:Empty Suite#[It] Hub Creation tests Hub Installtion Test Senarios Should fail if Hub is installed before installing Cert":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":169,"unknown":0,"total":169},"items":[{"uid":"8f7d03a9cbc8fbc0","status":"passed","time":{"start":1663052238000,"stop":1663052240235,"duration":2235}},{"uid":"e44136a57479d29f","status":"passed","time":{"start":1663051980000,"stop":1663051982890,"duration":2890}},{"uid":"e0047a68a3a3bb31","status":"passed","time":{"start":1663050064000,"stop":1663050065726,"duration":1726}},{"uid":"b340b9a9011c9b24","status":"passed","time":{"start":1663042347000,"stop":1663042348878,"duration":1878}},{"uid":"c9518f7ed4ac38b2","status":"passed","time":{"start":1663041715000,"stop":1663041717888,"duration":2888}},{"uid":"73394a469a6ee637","status":"passed","time":{"start":1663039756000,"stop":1663039757401,"duration":1401}},{"uid":"dc911192e4cc016e","status":"passed","time":{"start":1663040168000,"stop":1663040170538,"duration":2538}},{"uid":"dc310675c8f6d13b","status":"passed","time":{"start":1663009296000,"stop":1663009297578,"duration":1578}},{"uid":"228e2e55f17a25b9","status":"passed","time":{"start":1663008270000,"stop":1663008272757,"duration":2757}},{"uid":"242e709066da135d","status":"passed","time":{"start":1662986456000,"stop":1662986458081,"duration":2081}},{"uid":"8c459d7db3693aa1","status":"passed","time":{"start":1662978394000,"stop":1662978397725,"duration":3725}},{"uid":"6b04ee56f74d4db1","status":"passed","time":{"start":1662961730000,"stop":1662961731400,"duration":1400}},{"uid":"56cc1744c5b300b8","status":"passed","time":{"start":1662960985000,"stop":1662960986827,"duration":1827}},{"uid":"83983db886bfedc1","status":"passed","time":{"start":1662960825000,"stop":1662960826356,"duration":1356}},{"uid":"84c666fbe70b10e1","status":"passed","time":{"start":1662824760000,"stop":1662824761786,"duration":1786}},{"uid":"8bcd977f64eaec59","status":"passed","time":{"start":1662819406000,"stop":1662819408015,"duration":2015}},{"uid":"c38aa881837c213f","status":"passed","time":{"start":1662795171000,"stop":1662795172553,"duration":1553}},{"uid":"fca1e18fbd9c20fa","status":"passed","time":{"start":1662792593000,"stop":1662792598132,"duration":5132}},{"uid":"d9a8af8e0f96fa72","status":"passed","time":{"start":1662791140000,"stop":1662791141903,"duration":1903}},{"uid":"a705a26d89988449","status":"passed","time":{"start":1662788408000,"stop":1662788409133,"duration":1133}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests Iperf App connectivity Should install iperf-server on server cluster":{"statistic":{"failed":0,"broken":0,"skipped":44,"passed":7,"unknown":0,"total":51},"items":[{"uid":"e44f9f93340c8290","status":"skipped","statusDetails":"skipped","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"29142957967f879d","status":"skipped","statusDetails":"skipped","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"3f0a21ea5ddea009","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662348908000,"stop":1662348908000,"duration":0}},{"uid":"ffb4c9e037f8ea2d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"a15066b5f8011c70","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"a777065c1d5dab8b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"cbc3f0cd28e96bfa","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"956af9a92fe2a27","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"8bf227ff49255b1b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"df599ec35a9a24f3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"5e2801a4267faab5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"7721835a6b01244c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"6d5da9b9ff2f0faf","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"2a7fce59b0c9ebd5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"bbb3aa694ee8e679","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"e9df34f49568d398","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"50234fff618b0fdd","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"c8b848eb04b8c53f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"168abb95e985e3b5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"bb9ebf09c77d6533","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests slice with netpol should remove network policy resource from application namespace once the isolationEnabled is set to false":{"statistic":{"failed":0,"broken":3,"skipped":39,"passed":9,"unknown":0,"total":51},"items":[{"uid":"764c8ebb37a46f0c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"891661ce0562d5a","status":"passed","time":{"start":1663044069000,"stop":1663044085248,"duration":16248}},{"uid":"b654d765334f487f","status":"passed","time":{"start":1662348908000,"stop":1662348921419,"duration":13419}},{"uid":"903e8b36b7837756","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"18aac04a797e5a82","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"2e593b4d6954a0e0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"57bdae6a663bdfed","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"fb08291a59dd19a1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"dd8a67e62d0d5e71","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"28c76fe3861e742","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"a71e9ac144237f81","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"9a5f5c84baefa5e4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"fc298b6286f3394a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"49819d8bc146893","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"5f78d7a5d06f9620","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"888f79017b08484e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"c4ecdae0e74c8965","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"d666d99d343a5072","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"25f9d43772a122a1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"1d614e2ef6c93c5e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice creation scenarios: when slice red is installed workers should get attached to slice":{"statistic":{"failed":13,"broken":0,"skipped":0,"passed":38,"unknown":0,"total":51},"items":[{"uid":"6ab072db556fb822","status":"passed","time":{"start":1663044723000,"stop":1663044726422,"duration":3422}},{"uid":"b14b52ce72c28d98","status":"passed","time":{"start":1663044069000,"stop":1663044072293,"duration":3293}},{"uid":"8e10f5cea3bfd45e","status":"passed","time":{"start":1662348908000,"stop":1662348908486,"duration":486}},{"uid":"5494a2ac08496b79","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004688d0>: {\n        Underlying: <*exec.ExitError | 0xc000708d00>{\n            ProcessState: {\n                pid: 7158,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 256737},\n                    Stime: {Sec: 0, Usec: 69278},\n                    Maxrss: 86000,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 7572,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 430,\n                    Nivcsw: 521,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-red\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\noccurred","time":{"start":1661844205000,"stop":1661844205310,"duration":310}},{"uid":"4c6570c7b91e0cd1","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0007342a0>: {\n        Underlying: <*exec.ExitError | 0xc0007a5980>{\n            ProcessState: {\n                pid: 7112,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 162433},\n                    Stime: {Sec: 0, Usec: 59066},\n                    Maxrss: 76416,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 5207,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 265,\n                    Nivcsw: 518,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-red\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661754870000,"stop":1661754870225,"duration":225}},{"uid":"7b97d22a8f51cd74","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0006f1938>: {\n        Underlying: <*exec.ExitError | 0xc000861440>{\n            ProcessState: {\n                pid: 7101,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 194063},\n                    Stime: {Sec: 0, Usec: 41290},\n                    Maxrss: 87416,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3749,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 306,\n                    Nivcsw: 713,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-red\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661611962000,"stop":1661611962265,"duration":265}},{"uid":"6e452045e6877728","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00090c570>: {\n        Underlying: <*exec.ExitError | 0xc000a81200>{\n            ProcessState: {\n                pid: 7138,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 187266},\n                    Stime: {Sec: 0, Usec: 39013},\n                    Maxrss: 79272,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4614,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 356,\n                    Nivcsw: 618,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-red\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661585077000,"stop":1661585077242,"duration":242}},{"uid":"68dc6aba4cc1d9ac","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000f098>: {\n        Underlying: <*exec.ExitError | 0xc000499cc0>{\n            ProcessState: {\n                pid: 6007,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 199368},\n                    Stime: {Sec: 0, Usec: 60186},\n                    Maxrss: 79848,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3618,\n                    Majflt: 7,\n                    Nswap: 0,\n                    Inblock: 7440,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 484,\n                    Nivcsw: 668,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-red\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661584447000,"stop":1661584447339,"duration":339}},{"uid":"3ace927798f18ce6","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000810018>: {\n        Underlying: <*exec.ExitError | 0xc00013e040>{\n            ProcessState: {\n                pid: 7308,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 180411},\n                    Stime: {Sec: 0, Usec: 37585},\n                    Maxrss: 87560,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3704,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 392,\n                    Nivcsw: 311,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-red\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661580045000,"stop":1661580045202,"duration":202}},{"uid":"30a12c1d63228170","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000778bb8>: {\n        Underlying: <*exec.ExitError | 0xc0000740e0>{\n            ProcessState: {\n                pid: 7235,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 261763},\n                    Stime: {Sec: 0, Usec: 52352},\n                    Maxrss: 90508,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4766,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 362,\n                    Nivcsw: 415,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-red\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661578505000,"stop":1661578505293,"duration":293}},{"uid":"c44ef1cb1751245a","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004c9128>: {\n        Underlying: <*exec.ExitError | 0xc0008735e0>{\n            ProcessState: {\n                pid: 7243,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 221905},\n                    Stime: {Sec: 0, Usec: 60519},\n                    Maxrss: 83556,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 5479,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 271,\n                    Nivcsw: 365,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-red\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661572479000,"stop":1661572479271,"duration":271}},{"uid":"5d5acab2734fbe49","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0006ea018>: {\n        Underlying: <*exec.ExitError | 0xc0008c8040>{\n            ProcessState: {\n                pid: 7156,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 167218},\n                    Stime: {Sec: 0, Usec: 38004},\n                    Maxrss: 78816,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4139,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 362,\n                    Nivcsw: 299,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-red\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660832281000,"stop":1660832281219,"duration":219}},{"uid":"720c89f065c115e0","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0007f0fc0>: {\n        Underlying: <*exec.ExitError | 0xc00081faa0>{\n            ProcessState: {\n                pid: 7019,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 203396},\n                    Stime: {Sec: 0, Usec: 122805},\n                    Maxrss: 74888,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 7668,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 597,\n                    Nivcsw: 744,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-red\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660808797000,"stop":1660808798270,"duration":1270}},{"uid":"c6bd852ade0c5aac","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0005ccf18>: {\n        Underlying: <*exec.ExitError | 0xc000715fc0>{\n            ProcessState: {\n                pid: 7062,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 133368},\n                    Stime: {Sec: 0, Usec: 68705},\n                    Maxrss: 83528,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 10888,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 604,\n                    Nivcsw: 531,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-red\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660299182000,"stop":1660299183078,"duration":1078}},{"uid":"2a5c21ab42b102e","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004ca930>: {\n        Underlying: <*exec.ExitError | 0xc000158d40>{\n            ProcessState: {\n                pid: 7235,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 137512},\n                    Stime: {Sec: 0, Usec: 42017},\n                    Maxrss: 83500,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3187,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 342,\n                    Nivcsw: 329,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-red\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660295740000,"stop":1660295740165,"duration":165}},{"uid":"5405f180368d0a6","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00084ca68>: {\n        Underlying: <*exec.ExitError | 0xc0008a1ee0>{\n            ProcessState: {\n                pid: 7244,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 213829},\n                    Stime: {Sec: 0, Usec: 38878},\n                    Maxrss: 82620,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 5847,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 445,\n                    Nivcsw: 461,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-red\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660293657000,"stop":1660293657261,"duration":261}},{"uid":"b78277e7475b8a33","status":"passed","time":{"start":1660113628000,"stop":1660113628340,"duration":340}},{"uid":"1da6b1671cd9086d","status":"passed","time":{"start":1660104898000,"stop":1660104898386,"duration":386}},{"uid":"49a8fbddbe632ff4","status":"passed","time":{"start":1660067259000,"stop":1660067259534,"duration":534}},{"uid":"5d15b2fb7d385713","status":"passed","time":{"start":1660047551000,"stop":1660047551685,"duration":685}}]},"Worker Suite:Worker Suite#[It] Slice validation Slice with LOW QOS should have gateway pod running":{"statistic":{"failed":1,"broken":0,"skipped":2,"passed":6,"unknown":0,"total":9},"items":[{"uid":"37109e6a1fcf8e95","status":"passed","time":{"start":1659168739000,"stop":1659168769082,"duration":30082}},{"uid":"174f63e4d95b0be8","status":"passed","time":{"start":1659164084000,"stop":1659164109099,"duration":25099}},{"uid":"a62f8935b4aa5bb8","status":"passed","time":{"start":1659160188000,"stop":1659160208045,"duration":20045}},{"uid":"b0ef192b3b3a9205","status":"passed","time":{"start":1659119724000,"stop":1659119749068,"duration":25068}},{"uid":"8e90a32bd8a8113b","status":"passed","time":{"start":1659116511000,"stop":1659116511012,"duration":12}},{"uid":"f41770fc4484150","status":"passed","time":{"start":1659109470000,"stop":1659109470028,"duration":28}},{"uid":"8ad77caf63959cf7","status":"failed","statusDetails":"Timed out after 180.004s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1659106836000,"stop":1659107016285,"duration":180285}},{"uid":"17f1dcc7e00985cc","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659101522000,"stop":1659101522000,"duration":0}},{"uid":"6423839b465e949a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659082816000,"stop":1659082816000,"duration":0}}]},"Istio Suite:Istio Suite#[It] Bookinfo tests Bookinfo Scenario 2 with egress and without ingress Should install Productpage on worker cluster-1":{"statistic":{"failed":46,"broken":0,"skipped":0,"passed":11,"unknown":0,"total":57},"items":[{"uid":"c9732cd8d505af60","status":"passed","time":{"start":1663043831000,"stop":1663043849243,"duration":18243}},{"uid":"d202ed3280e81cfe","status":"passed","time":{"start":1663043075000,"stop":1663043096750,"duration":21750}},{"uid":"34a171fa49a4a49e","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1663040338000,"stop":1663040518275,"duration":180275}},{"uid":"c5b41e9ed2d31c0","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1662819574000,"stop":1662819754348,"duration":180348}},{"uid":"273a2c309c9f2a93","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1662795310000,"stop":1662795490290,"duration":180290}},{"uid":"e891fc9b5b8ccf4","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1662792773000,"stop":1662792953366,"duration":180366}},{"uid":"5036de4355684e31","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1662620110000,"stop":1662620290450,"duration":180450}},{"uid":"a0b7ce2f94bfc369","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1662617640000,"stop":1662617820296,"duration":180296}},{"uid":"ac7a518b08a3f538","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1662610377000,"stop":1662610557297,"duration":180297}},{"uid":"be5cb48dd368c9ab","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1662557911000,"stop":1662558091232,"duration":180232}},{"uid":"3003208d69156fc9","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1662553747000,"stop":1662553927332,"duration":180332}},{"uid":"2128649fe7f6bdad","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1662553327000,"stop":1662553507397,"duration":180397}},{"uid":"c09e4c62a50a5cf4","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1662381943000,"stop":1662382123364,"duration":180364}},{"uid":"dff30ae31352c35c","status":"passed","time":{"start":1662348103000,"stop":1662348132322,"duration":29322}},{"uid":"2356dd0c06987124","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1662049889000,"stop":1662050069438,"duration":180438}},{"uid":"f70f544c5e396827","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1662049074000,"stop":1662049254305,"duration":180305}},{"uid":"1ad9dd315b4efc6b","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1662028179000,"stop":1662028359434,"duration":180434}},{"uid":"c3a1ee65a9fe5e15","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1661859205000,"stop":1661859385319,"duration":180319}},{"uid":"626ae905c07f38e7","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00074f218>: {\n        Underlying: <*exec.ExitError | 0xc0007a7e40>{\n            ProcessState: {\n                pid: 6899,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 265746},\n                    Stime: {Sec: 0, Usec: 71394},\n                    Maxrss: 86324,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 11751,\n                    Majflt: 5,\n                    Nswap: 0,\n                    Inblock: 3016,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 774,\n                    Nivcsw: 486,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicewithegresswithoutingress\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicewithegresswithoutingress\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicewithegresswithoutingress\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicewithegresswithoutingress\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicewithegresswithoutingress\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\noccurred","time":{"start":1661843950000,"stop":1661843950425,"duration":425}},{"uid":"b51b6c924d4fcd61","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00060e708>: {\n        Underlying: <*exec.ExitError | 0xc0006dbd20>{\n            ProcessState: {\n                pid: 6946,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 182751},\n                    Stime: {Sec: 0, Usec: 50548},\n                    Maxrss: 78928,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 10581,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 380,\n                    Nivcsw: 503,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicewithegresswithoutingress\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicewithegresswithoutingress\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicewithegresswithoutingress\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicewithegresswithoutingress\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicewithegresswithoutingress\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661754632000,"stop":1661754632295,"duration":295}}]},"Worker Suite:Worker Suite#[It] Namespace reconcile test scenarios: Offboard slice get detached from app ns in cluster objects":{"statistic":{"failed":0,"broken":0,"skipped":10,"passed":41,"unknown":0,"total":51},"items":[{"uid":"3fb910d03713b35b","status":"skipped","statusDetails":"skipped","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"c85c458b274e19dd","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"8b71575d223770ff","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662348908000,"stop":1662348908000,"duration":0}},{"uid":"5adeb5d66e2d2e28","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"6edcbe0ac7cfd62","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"54c9304b314d8c8d","status":"passed","time":{"start":1661611962000,"stop":1661611962209,"duration":209}},{"uid":"ad034bff7992a92","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"d5ca0bf123615d96","status":"passed","time":{"start":1661584447000,"stop":1661584447200,"duration":200}},{"uid":"6029bc50c9249a37","status":"passed","time":{"start":1661580045000,"stop":1661580045169,"duration":169}},{"uid":"177a28ef64ca43e0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"1a6dfe1162d59e78","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"498f92bdf86a924a","status":"passed","time":{"start":1660832281000,"stop":1660832281163,"duration":163}},{"uid":"45c329870f108767","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"71b954fdb737555e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"8df457d3ee0f61ca","status":"passed","time":{"start":1660295740000,"stop":1660295740138,"duration":138}},{"uid":"784494d70f4ed812","status":"passed","time":{"start":1660293657000,"stop":1660293657191,"duration":191}},{"uid":"24e36ef2c7ba214e","status":"passed","time":{"start":1660113628000,"stop":1660113628144,"duration":144}},{"uid":"9d12c51634e50f5e","status":"passed","time":{"start":1660104898000,"stop":1660104898170,"duration":170}},{"uid":"928412d23ea7283e","status":"passed","time":{"start":1660067259000,"stop":1660067259292,"duration":292}},{"uid":"698a909a502fe02b","status":"passed","time":{"start":1660047551000,"stop":1660047551231,"duration":231}}]},"Hub Suite:Hub Suite#[It] Cluster negative tests Worker Cluster Registration with Wrong endpoint":{"statistic":{"failed":29,"broken":0,"skipped":0,"passed":164,"unknown":0,"total":193},"items":[{"uid":"8a5e0b8f026251b2","status":"passed","time":{"start":1663042493000,"stop":1663042808041,"duration":315041}},{"uid":"f1700cec35507ad6","status":"passed","time":{"start":1663041865000,"stop":1663042182667,"duration":317667}},{"uid":"a890b565f8e92ed5","status":"passed","time":{"start":1663039902000,"stop":1663040221413,"duration":319413}},{"uid":"56fbeb79c3da8531","status":"passed","time":{"start":1663009431000,"stop":1663009751637,"duration":320637}},{"uid":"6fba67520a899ee9","status":"passed","time":{"start":1662978562000,"stop":1662978880329,"duration":318329}},{"uid":"6d96fc60933486ba","status":"passed","time":{"start":1662702362000,"stop":1662702682143,"duration":320143}},{"uid":"b839dc8b18a0b543","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00050f290>: {\n        Underlying: <*exec.ExitError | 0xc000704cc0>{\n            ProcessState: {\n                pid: 12037,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 190721},\n                    Stime: {Sec: 0, Usec: 83917},\n                    Maxrss: 87392,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 10744,\n                    Majflt: 5,\n                    Nswap: 0,\n                    Inblock: 432,\n                    Oublock: 13344,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 481,\n                    Nivcsw: 410,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Cluster%20negative%20tests%20Worker%20Cluster%20Registration%20with%20Wrong%20endpoint2728869890\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.67.103:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Cluster%20negative%20tests%20Worker%20Cluster%20Registration%20with%20Wrong%20endpoint2728869890\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.67.103:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Cluster%20negative%20tests%20Worker%20Cluster%20Registration%20with%20Wrong%20endpoint2728869890\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.67.103:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Cluster%20negative%20tests%20Worker%20Cluster%20Registration%20with%20Wrong%20endpoint2728869890\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.67.103:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Cluster%20negative%20tests%20Worker%20Cluster%20Registration%20with%20Wrong%20endpoint2728869890\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.67.103:443: connect: connection refused\noccurred","time":{"start":1662553059000,"stop":1662553059464,"duration":464}},{"uid":"dbc852c1776da5be","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0005a5410>: {\n        Underlying: <*exec.ExitError | 0xc000426760>{\n            ProcessState: {\n                pid: 10984,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 165206},\n                    Stime: {Sec: 0, Usec: 49561},\n                    Maxrss: 78928,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4355,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 376,\n                    Nivcsw: 254,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Cluster%20negative%20tests%20Worker%20Cluster%20Registration%20with%20Wrong%20endpoint1867821006\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.123.224:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Cluster%20negative%20tests%20Worker%20Cluster%20Registration%20with%20Wrong%20endpoint1867821006\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.123.224:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Cluster%20negative%20tests%20Worker%20Cluster%20Registration%20with%20Wrong%20endpoint1867821006\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.123.224:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Cluster%20negative%20tests%20Worker%20Cluster%20Registration%20with%20Wrong%20endpoint1867821006\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.123.224:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Cluster%20negative%20tests%20Worker%20Cluster%20Registration%20with%20Wrong%20endpoint1867821006\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.123.224:443: connect: connection refused\noccurred","time":{"start":1662553042000,"stop":1662553042422,"duration":422}},{"uid":"80002980d967e88d","status":"passed","time":{"start":1662467486000,"stop":1662467625697,"duration":139697}},{"uid":"e9549eecc2fd2334","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0007012f0>: {\n        Underlying: <*exec.ExitError | 0xc000508e60>{\n            ProcessState: {\n                pid: 10951,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 323158},\n                    Stime: {Sec: 0, Usec: 52513},\n                    Maxrss: 85580,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3441,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 306,\n                    Nivcsw: 316,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Cluster%20negative%20tests%20Worker%20Cluster%20Registration%20with%20Wrong%20endpoint2187421777\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.14.145:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Cluster%20negative%20tests%20Worker%20Cluster%20Registration%20with%20Wrong%20endpoint2187421777\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.14.145:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Cluster%20negative%20tests%20Worker%20Cluster%20Registration%20with%20Wrong%20endpoint2187421777\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.14.145:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Cluster%20negative%20tests%20Worker%20Cluster%20Registration%20with%20Wrong%20endpoint2187421777\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.14.145:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Cluster%20negative%20tests%20Worker%20Cluster%20Registration%20with%20Wrong%20endpoint2187421777\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.14.145:443: connect: connection refused\noccurred","time":{"start":1662374048000,"stop":1662374048667,"duration":667}},{"uid":"eb84d5b95b713d2d","status":"passed","time":{"start":1662374250000,"stop":1662374386910,"duration":136910}},{"uid":"facb6b73710b96d1","status":"passed","time":{"start":1662361116000,"stop":1662361256615,"duration":140615}},{"uid":"62d3c6acd2aca4e","status":"passed","time":{"start":1662347286000,"stop":1662347421770,"duration":135770}},{"uid":"f2d847dca508b6f7","status":"passed","time":{"start":1662049179000,"stop":1662049321315,"duration":142315}},{"uid":"6933eb83e12cfbe5","status":"passed","time":{"start":1662048808000,"stop":1662048943767,"duration":135767}},{"uid":"cc29a9b8bb1975c0","status":"passed","time":{"start":1662036991000,"stop":1662037131306,"duration":140306}},{"uid":"1bd4fac2f4bed47a","status":"passed","time":{"start":1662031654000,"stop":1662031793470,"duration":139470}},{"uid":"2e487168faf60627","status":"passed","time":{"start":1662027354000,"stop":1662027489809,"duration":135809}},{"uid":"8ea10792bd183931","status":"passed","time":{"start":1661952242000,"stop":1661952383465,"duration":141465}},{"uid":"ddeecdbfa22a2716","status":"passed","time":{"start":1661863163000,"stop":1661863303171,"duration":140171}}]},"Worker Suite:Worker Suite#[It] Slice Creation scenarios Slice with LOW QOS should reconcile low qos slice":{"statistic":{"failed":0,"broken":0,"skipped":23,"passed":19,"unknown":0,"total":42},"items":[{"uid":"ead5e9e0bbc8b694","status":"skipped","statusDetails":"skipped","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"efabe741bf98e94c","status":"skipped","statusDetails":"skipped","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"e5efa1dbbd076cb4","status":"passed","time":{"start":1662348908000,"stop":1662348908522,"duration":522}},{"uid":"88be68a93f4d0843","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"d6f706b1db9b99ec","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"34c4708bfcb8fc6d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"9ac9b621fbf21537","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"f751f069e22dc8ab","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"47076450b378bc75","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"c31c19823058277b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"586bc686bf0e470e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"475c41c3d55257db","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"ec479d989181e3c9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"9b20e25c2a5d3171","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"82688c46bcfce6c4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"76cae015ddb4460f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"c024270115cc5970","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"cda97b10df661bed","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"3bdde8547ea9cff","status":"passed","time":{"start":1660067259000,"stop":1660067259352,"duration":352}},{"uid":"3ee95c4806144a62","status":"passed","time":{"start":1660047551000,"stop":1660047551514,"duration":514}}]},"Worker Suite:Worker Suite#[It] Hub netpol scenarios: Slice Configs Netpol scenarios Should successfully deploy for valid namespace creating clusters with * in both applicationNamespace and allowedNamespace":{"statistic":{"failed":0,"broken":0,"skipped":23,"passed":28,"unknown":0,"total":51},"items":[{"uid":"55242c1d27494a23","status":"skipped","statusDetails":"skipped","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"33ed6559abdb3aa8","status":"passed","time":{"start":1663044069000,"stop":1663044069772,"duration":772}},{"uid":"21d3a1888f7e823d","status":"passed","time":{"start":1662348908000,"stop":1662348908567,"duration":567}},{"uid":"659f799daa406a53","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"58712e30c9891d54","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"ee6e6a594d2bd85c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"69e6ed6ac5a41c4c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"b60923dd357bad16","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"e923c2b2035a0f8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"8992cf6c6e9c8b63","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"d64f481d3204b2b8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"5e6aa19f41097bc2","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"4bce15fce59422e2","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"944b083da0bff0ae","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"3e52cce1f61a860b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"cd3844150232c60c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"106a7e6a0eedaa5e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"8968ac8a980ba084","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"cde58556223c78a8","status":"passed","time":{"start":1660067259000,"stop":1660067259696,"duration":696}},{"uid":"1817a460312e12e5","status":"passed","time":{"start":1660047551000,"stop":1660047551474,"duration":474}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Creation Should update Project while applying valid manifest with existing Project name":{"statistic":{"failed":14,"broken":0,"skipped":0,"passed":179,"unknown":0,"total":193},"items":[{"uid":"cb039c168a571299","status":"passed","time":{"start":1663042493000,"stop":1663042497630,"duration":4630}},{"uid":"ab0dc788a12e13b5","status":"passed","time":{"start":1663041865000,"stop":1663041869823,"duration":4823}},{"uid":"6336c193e7aa4d5c","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00066e558>: {\n        Underlying: <*exec.ExitError | 0xc00084ca20>{\n            ProcessState: {\n                pid: 10067,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 217805},\n                    Stime: {Sec: 0, Usec: 75242},\n                    Maxrss: 76176,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3579,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13344,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 319,\n                    Nivcsw: 591,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20update%20Project%20while%20applying%20valid%20manifest%20with%20existing%20Project%20name3989923984\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.217.78:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20update%20Project%20while%20applying%20valid%20manifest%20with%20existing%20Project%20name3989923984\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.217.78:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20update%20Project%20while%20applying%20valid%20manifest%20with%20existing%20Project%20name3989923984\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.217.78:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20update%20Project%20while%20applying%20valid%20manifest%20with%20existing%20Project%20name3989923984\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.217.78:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20update%20Project%20while%20applying%20valid%20manifest%20with%20existing%20Project%20name3989923984\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.217.78:443: connect: connection refused\noccurred","time":{"start":1663039902000,"stop":1663039902324,"duration":324}},{"uid":"f7668d25102206ad","status":"passed","time":{"start":1663009431000,"stop":1663009435678,"duration":4678}},{"uid":"275a91518e1f6f3b","status":"passed","time":{"start":1662978562000,"stop":1662978566864,"duration":4864}},{"uid":"a27708931223790f","status":"passed","time":{"start":1662702362000,"stop":1662702366713,"duration":4713}},{"uid":"7f0c18e64daaedb4","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <bool>: false\nto be true","time":{"start":1662553059000,"stop":1662553125594,"duration":66594}},{"uid":"c569854e32d9e840","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0005a4438>: {\n        Underlying: <*exec.ExitError | 0xc00065d100>{\n            ProcessState: {\n                pid: 10935,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 204117},\n                    Stime: {Sec: 0, Usec: 32018},\n                    Maxrss: 72456,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 7974,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 239,\n                    Nivcsw: 277,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20update%20Project%20while%20applying%20valid%20manifest%20with%20existing%20Project%20name495982649\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.123.224:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20update%20Project%20while%20applying%20valid%20manifest%20with%20existing%20Project%20name495982649\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.123.224:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20update%20Project%20while%20applying%20valid%20manifest%20with%20existing%20Project%20name495982649\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.123.224:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20update%20Project%20while%20applying%20valid%20manifest%20with%20existing%20Project%20name495982649\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.123.224:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20update%20Project%20while%20applying%20valid%20manifest%20with%20existing%20Project%20name495982649\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.123.224:443: connect: connection refused\noccurred","time":{"start":1662553042000,"stop":1662553042275,"duration":275}},{"uid":"d4cf2266ad916618","status":"passed","time":{"start":1662467486000,"stop":1662467490631,"duration":4631}},{"uid":"5848e4aae3ee99a3","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000352de0>: {\n        Underlying: <*exec.ExitError | 0xc000887380>{\n            ProcessState: {\n                pid: 10979,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 205291},\n                    Stime: {Sec: 0, Usec: 64405},\n                    Maxrss: 85776,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4169,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 344,\n                    Nivcsw: 606,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20update%20Project%20while%20applying%20valid%20manifest%20with%20existing%20Project%20name1445506112\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.14.145:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20update%20Project%20while%20applying%20valid%20manifest%20with%20existing%20Project%20name1445506112\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.14.145:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20update%20Project%20while%20applying%20valid%20manifest%20with%20existing%20Project%20name1445506112\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.14.145:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20update%20Project%20while%20applying%20valid%20manifest%20with%20existing%20Project%20name1445506112\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.14.145:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20update%20Project%20while%20applying%20valid%20manifest%20with%20existing%20Project%20name1445506112\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.14.145:443: connect: connection refused\noccurred","time":{"start":1662374048000,"stop":1662374048437,"duration":437}},{"uid":"2ee84a97fc323aef","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000352678>: {\n        Underlying: <*exec.ExitError | 0xc00096d6a0>{\n            ProcessState: {\n                pid: 7226,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 195815},\n                    Stime: {Sec: 0, Usec: 66660},\n                    Maxrss: 90600,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4062,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13344,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 411,\n                    Nivcsw: 277,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20update%20Project%20while%20applying%20valid%20manifest%20with%20existing%20Project%20name3441437962\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.241.207:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20update%20Project%20while%20applying%20valid%20manifest%20with%20existing%20Project%20name3441437962\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.241.207:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20update%20Project%20while%20applying%20valid%20manifest%20with%20existing%20Project%20name3441437962\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.241.207:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20update%20Project%20while%20applying%20valid%20manifest%20with%20existing%20Project%20name3441437962\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.241.207:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20update%20Project%20while%20applying%20valid%20manifest%20with%20existing%20Project%20name3441437962\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.241.207:443: connect: connection refused\noccurred","time":{"start":1662374250000,"stop":1662374250369,"duration":369}},{"uid":"dec90895c45a12c8","status":"passed","time":{"start":1662361116000,"stop":1662361120691,"duration":4691}},{"uid":"e88ee616607a31b3","status":"passed","time":{"start":1662347286000,"stop":1662347290704,"duration":4704}},{"uid":"13e7c2aab62b2b70","status":"passed","time":{"start":1662049179000,"stop":1662049183953,"duration":4953}},{"uid":"3d17ba4ee8ce6664","status":"passed","time":{"start":1662048808000,"stop":1662048812760,"duration":4760}},{"uid":"4f0efff4d251d1c5","status":"passed","time":{"start":1662036991000,"stop":1662036995665,"duration":4665}},{"uid":"f5cb5362c8c11c18","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <bool>: false\nto be true","time":{"start":1662031654000,"stop":1662031720734,"duration":66734}},{"uid":"e3b4f1e93ebed26d","status":"passed","time":{"start":1662027354000,"stop":1662027358826,"duration":4826}},{"uid":"d71ace9186983b83","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <bool>: false\nto be true","time":{"start":1661952242000,"stop":1661952308678,"duration":66678}},{"uid":"a5c8633cf29677b","status":"passed","time":{"start":1661863163000,"stop":1661863167765,"duration":4765}}]},"Worker Suite:Worker Suite#[BeforeSuite]":{"statistic":{"failed":210,"broken":0,"skipped":0,"passed":51,"unknown":0,"total":261},"items":[{"uid":"5f339117ed7e0796","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000431380>: {\n        Underlying: <*exec.ExitError | 0xc00030bce0>{\n            ProcessState: {\n                pid: 6147,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 198367},\n                    Stime: {Sec: 0, Usec: 22888},\n                    Maxrss: 51240,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2680,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 24,\n                    Oublock: 360,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 857,\n                    Nivcsw: 383,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"v1.8.0\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\",\n                        \"\",\n                        \"Error: INSTALLATION FAILED: cannot re-use a name that is still in use\",\n                        \"helm.go:84: [debug] cannot re-use a name that is still in use\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).availableName\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:483\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:197\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:264\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:125\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                        \"INSTALLATION FAILED\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:127\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is inse...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"v1.8.0\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\n    \n    Error: INSTALLATION FAILED: cannot re-use a name that is still in use\n    helm.go:84: [debug] cannot re-use a name that is still in use\n    helm.sh/helm/v3/pkg/action.(*Install).availableName\n    \thelm.sh/helm/v3/pkg/action/install.go:483\n    helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\n    \thelm.sh/helm/v3/pkg/action/install.go:197\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:264\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:125\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1663052812000,"stop":1663052812632,"duration":632}},{"uid":"9196fffd421b1bd4","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000f470>: {\n        Underlying: <*exec.ExitError | 0xc0004d4840>{\n            ProcessState: {\n                pid: 6178,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 175331},\n                    Stime: {Sec: 0, Usec: 38962},\n                    Maxrss: 52256,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3262,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 16,\n                    Oublock: 360,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 652,\n                    Nivcsw: 520,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"v1.8.0\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\",\n                        \"\",\n                        \"Error: INSTALLATION FAILED: cannot re-use a name that is still in use\",\n                        \"helm.go:84: [debug] cannot re-use a name that is still in use\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).availableName\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:483\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:197\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:264\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:125\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                        \"INSTALLATION FAILED\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:127\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is inse...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"v1.8.0\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\n    \n    Error: INSTALLATION FAILED: cannot re-use a name that is still in use\n    helm.go:84: [debug] cannot re-use a name that is still in use\n    helm.sh/helm/v3/pkg/action.(*Install).availableName\n    \thelm.sh/helm/v3/pkg/action/install.go:483\n    helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\n    \thelm.sh/helm/v3/pkg/action/install.go:197\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:264\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:125\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1663052553000,"stop":1663052553575,"duration":575}},{"uid":"82eb116d1f5b2b37","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000190258>: {\n        Underlying: <*exec.ExitError | 0xc000074640>{\n            ProcessState: {\n                pid: 6175,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 120980},\n                    Stime: {Sec: 0, Usec: 28228},\n                    Maxrss: 51656,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2705,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 360,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 681,\n                    Nivcsw: 349,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"v1.8.0\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\",\n                        \"\",\n                        \"Error: INSTALLATION FAILED: cannot re-use a name that is still in use\",\n                        \"helm.go:84: [debug] cannot re-use a name that is still in use\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).availableName\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:483\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:197\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:264\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:125\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                        \"INSTALLATION FAILED\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:127\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insec...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"v1.8.0\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\n    \n    Error: INSTALLATION FAILED: cannot re-use a name that is still in use\n    helm.go:84: [debug] cannot re-use a name that is still in use\n    helm.sh/helm/v3/pkg/action.(*Install).availableName\n    \thelm.sh/helm/v3/pkg/action/install.go:483\n    helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\n    \thelm.sh/helm/v3/pkg/action/install.go:197\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:264\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:125\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1663050631000,"stop":1663050631368,"duration":368}},{"uid":"1790a82b4890d54f","status":"passed","time":{"start":1663044723000,"stop":1663044880672,"duration":157672}},{"uid":"9ff6970d9bf97ab4","status":"passed","time":{"start":1663044069000,"stop":1663044235632,"duration":166632}},{"uid":"9b2a366159ced50f","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00018c030>: {\n        Underlying: <*exec.ExitError | 0xc0003f2040>{\n            ProcessState: {\n                pid: 6246,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 157791},\n                    Stime: {Sec: 0, Usec: 19242},\n                    Maxrss: 51840,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2172,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 360,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 657,\n                    Nivcsw: 214,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"install.go:178: [debug] Original chart version: \\\"v1.8.0\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\",\n                        \"\",\n                        \"Error: INSTALLATION FAILED: cannot re-use a name that is still in use\",\n                        \"helm.go:84: [debug] cannot re-use a name that is still in use\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).availableName\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:483\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:197\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:264\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:125\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                        \"INSTALLATION FAILED\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:127\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"install.go:178: [debug] Original chart version: \\\"v1.8.0\\\"\",\n                    \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\",\n                    \"\",\n                    \"Error: INSTALLATION FAILED: cannot re-use a name that is still in use\",\n                    \"helm.go:84: [debug] cannot re-use a name that is still in ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; install.go:178: [debug] Original chart version: \"v1.8.0\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\n    \n    Error: INSTALLATION FAILED: cannot re-use a name that is still in use\n    helm.go:84: [debug] cannot re-use a name that is still in use\n    helm.sh/helm/v3/pkg/action.(*Install).availableName\n    \thelm.sh/helm/v3/pkg/action/install.go:483\n    helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\n    \thelm.sh/helm/v3/pkg/action/install.go:197\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:264\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:125\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1663041526000,"stop":1663041526345,"duration":345}},{"uid":"48c94c5e6aa385b3","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000489098>: {\n        Underlying: <*exec.ExitError | 0xc00049c2a0>{\n            ProcessState: {\n                pid: 6897,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 136928},\n                    Stime: {Sec: 0, Usec: 22085},\n                    Maxrss: 52172,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2189,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 24,\n                    Oublock: 360,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 727,\n                    Nivcsw: 475,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"v1.8.0\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\",\n                        \"\",\n                        \"Error: INSTALLATION FAILED: cannot re-use a name that is still in use\",\n                        \"helm.go:84: [debug] cannot re-use a name that is still in use\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).availableName\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:483\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:197\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:264\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:125\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                        \"INSTALLATION FAILED\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:127\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is inse...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"v1.8.0\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\n    \n    Error: INSTALLATION FAILED: cannot re-use a name that is still in use\n    helm.go:84: [debug] cannot re-use a name that is still in use\n    helm.sh/helm/v3/pkg/action.(*Install).availableName\n    \thelm.sh/helm/v3/pkg/action/install.go:483\n    helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\n    \thelm.sh/helm/v3/pkg/action/install.go:197\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:264\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:125\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1663011418000,"stop":1663011418460,"duration":460}},{"uid":"79097506bcebc05b","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000e1b0>: {\n        Underlying: <*exec.ExitError | 0xc00047a620>{\n            ProcessState: {\n                pid: 6188,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 153538},\n                    Stime: {Sec: 0, Usec: 21934},\n                    Maxrss: 52348,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2178,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 360,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 994,\n                    Nivcsw: 419,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"v1.8.0\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\",\n                        \"\",\n                        \"Error: INSTALLATION FAILED: cannot re-use a name that is still in use\",\n                        \"helm.go:84: [debug] cannot re-use a name that is still in use\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).availableName\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:483\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:197\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:264\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:125\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                        \"INSTALLATION FAILED\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:127\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insec...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"v1.8.0\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\n    \n    Error: INSTALLATION FAILED: cannot re-use a name that is still in use\n    helm.go:84: [debug] cannot re-use a name that is still in use\n    helm.sh/helm/v3/pkg/action.(*Install).availableName\n    \thelm.sh/helm/v3/pkg/action/install.go:483\n    helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\n    \thelm.sh/helm/v3/pkg/action/install.go:197\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:264\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:125\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1663008851000,"stop":1663008851532,"duration":532}},{"uid":"bb57b8b623a26e16","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004801c8>: {\n        Underlying: <*exec.ExitError | 0xc00014c000>{\n            ProcessState: {\n                pid: 6139,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 136794},\n                    Stime: {Sec: 0, Usec: 21374},\n                    Maxrss: 54280,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2290,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 360,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 756,\n                    Nivcsw: 471,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"v1.8.0\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\",\n                        \"\",\n                        \"Error: INSTALLATION FAILED: cannot re-use a name that is still in use\",\n                        \"helm.go:84: [debug] cannot re-use a name that is still in use\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).availableName\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:483\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:197\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:264\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:125\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                        \"INSTALLATION FAILED\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:127\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insec...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"v1.8.0\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\n    \n    Error: INSTALLATION FAILED: cannot re-use a name that is still in use\n    helm.go:84: [debug] cannot re-use a name that is still in use\n    helm.sh/helm/v3/pkg/action.(*Install).availableName\n    \thelm.sh/helm/v3/pkg/action/install.go:483\n    helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\n    \thelm.sh/helm/v3/pkg/action/install.go:197\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:264\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:125\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1662987019000,"stop":1662987019509,"duration":509}},{"uid":"90d0cc80c411608f","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004b0570>: {\n        Underlying: <*exec.ExitError | 0xc00047a300>{\n            ProcessState: {\n                pid: 6940,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 140229},\n                    Stime: {Sec: 0, Usec: 25496},\n                    Maxrss: 52156,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2180,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 360,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 655,\n                    Nivcsw: 222,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"v1.8.0\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\",\n                        \"\",\n                        \"Error: INSTALLATION FAILED: cannot re-use a name that is still in use\",\n                        \"helm.go:84: [debug] cannot re-use a name that is still in use\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).availableName\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:483\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:197\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:264\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:125\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                        \"INSTALLATION FAILED\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:127\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insec...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"v1.8.0\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\n    \n    Error: INSTALLATION FAILED: cannot re-use a name that is still in use\n    helm.go:84: [debug] cannot re-use a name that is still in use\n    helm.sh/helm/v3/pkg/action.(*Install).availableName\n    \thelm.sh/helm/v3/pkg/action/install.go:483\n    helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\n    \thelm.sh/helm/v3/pkg/action/install.go:197\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:264\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:125\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1662980603000,"stop":1662980603403,"duration":403}},{"uid":"53aaddb114354b21","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004b5320>: {\n        Underlying: <*exec.ExitError | 0xc00014ea40>{\n            ProcessState: {\n                pid: 6138,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 140947},\n                    Stime: {Sec: 0, Usec: 19046},\n                    Maxrss: 54148,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2158,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 360,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 647,\n                    Nivcsw: 334,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"v1.8.0\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\",\n                        \"\",\n                        \"Error: INSTALLATION FAILED: cannot re-use a name that is still in use\",\n                        \"helm.go:84: [debug] cannot re-use a name that is still in use\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).availableName\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:483\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:197\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:264\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:125\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                        \"INSTALLATION FAILED\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:127\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insec...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"v1.8.0\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\n    \n    Error: INSTALLATION FAILED: cannot re-use a name that is still in use\n    helm.go:84: [debug] cannot re-use a name that is still in use\n    helm.sh/helm/v3/pkg/action.(*Install).availableName\n    \thelm.sh/helm/v3/pkg/action/install.go:483\n    helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\n    \thelm.sh/helm/v3/pkg/action/install.go:197\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:264\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:125\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1662962291000,"stop":1662962291426,"duration":426}},{"uid":"cd13d0972746f624","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004770b0>: {\n        Underlying: <*exec.ExitError | 0xc000074240>{\n            ProcessState: {\n                pid: 6164,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 161735},\n                    Stime: {Sec: 0, Usec: 28303},\n                    Maxrss: 52204,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2155,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 360,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 630,\n                    Nivcsw: 295,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"v1.8.0\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\",\n                        \"\",\n                        \"Error: INSTALLATION FAILED: cannot re-use a name that is still in use\",\n                        \"helm.go:84: [debug] cannot re-use a name that is still in use\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).availableName\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:483\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:197\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:264\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:125\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                        \"INSTALLATION FAILED\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:127\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insec...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"v1.8.0\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\n    \n    Error: INSTALLATION FAILED: cannot re-use a name that is still in use\n    helm.go:84: [debug] cannot re-use a name that is still in use\n    helm.sh/helm/v3/pkg/action.(*Install).availableName\n    \thelm.sh/helm/v3/pkg/action/install.go:483\n    helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\n    \thelm.sh/helm/v3/pkg/action/install.go:197\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:264\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:125\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1662961544000,"stop":1662961544450,"duration":450}},{"uid":"26ca076672bebb14","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004c0048>: {\n        Underlying: <*exec.ExitError | 0xc0000d6060>{\n            ProcessState: {\n                pid: 6177,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 112635},\n                    Stime: {Sec: 0, Usec: 38839},\n                    Maxrss: 51296,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2727,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 360,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 538,\n                    Nivcsw: 286,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"v1.8.0\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\",\n                        \"\",\n                        \"Error: INSTALLATION FAILED: cannot re-use a name that is still in use\",\n                        \"helm.go:84: [debug] cannot re-use a name that is still in use\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).availableName\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:483\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:197\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:264\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:125\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                        \"INSTALLATION FAILED\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:127\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insec...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"v1.8.0\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\n    \n    Error: INSTALLATION FAILED: cannot re-use a name that is still in use\n    helm.go:84: [debug] cannot re-use a name that is still in use\n    helm.sh/helm/v3/pkg/action.(*Install).availableName\n    \thelm.sh/helm/v3/pkg/action/install.go:483\n    helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\n    \thelm.sh/helm/v3/pkg/action/install.go:197\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:264\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:125\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1662961386000,"stop":1662961386456,"duration":456}},{"uid":"3435de12ad7e0987","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000027e30>: {\n        Underlying: <*exec.ExitError | 0xc000481c20>{\n            ProcessState: {\n                pid: 6149,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 152074},\n                    Stime: {Sec: 0, Usec: 14146},\n                    Maxrss: 53400,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2805,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 24,\n                    Oublock: 360,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 651,\n                    Nivcsw: 526,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"v1.8.0\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\",\n                        \"\",\n                        \"Error: INSTALLATION FAILED: cannot re-use a name that is still in use\",\n                        \"helm.go:84: [debug] cannot re-use a name that is still in use\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).availableName\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:483\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:197\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:264\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:125\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                        \"INSTALLATION FAILED\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:127\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is inse...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"v1.8.0\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\n    \n    Error: INSTALLATION FAILED: cannot re-use a name that is still in use\n    helm.go:84: [debug] cannot re-use a name that is still in use\n    helm.sh/helm/v3/pkg/action.(*Install).availableName\n    \thelm.sh/helm/v3/pkg/action/install.go:483\n    helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\n    \thelm.sh/helm/v3/pkg/action/install.go:197\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:264\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:125\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1662825333000,"stop":1662825333514,"duration":514}},{"uid":"edccfa9bef890534","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00033ab70>: {\n        Underlying: <*exec.ExitError | 0xc00037e940>{\n            ProcessState: {\n                pid: 6257,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 149979},\n                    Stime: {Sec: 0, Usec: 26919},\n                    Maxrss: 54128,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2674,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 360,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 767,\n                    Nivcsw: 407,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"install.go:178: [debug] Original chart version: \\\"v1.8.0\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\",\n                        \"\",\n                        \"Error: INSTALLATION FAILED: cannot re-use a name that is still in use\",\n                        \"helm.go:84: [debug] cannot re-use a name that is still in use\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).availableName\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:483\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:197\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:264\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:125\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                        \"INSTALLATION FAILED\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:127\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"install.go:178: [debug] Original chart version: \\\"v1.8.0\\\"\",\n                    \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\",\n                    \"\",\n                    \"Error: INSTALLATION FAILED: cannot re-use a name that is still in use\",\n                    \"helm.go:84: [debug] cannot re-use a name that is still in ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; install.go:178: [debug] Original chart version: \"v1.8.0\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\n    \n    Error: INSTALLATION FAILED: cannot re-use a name that is still in use\n    helm.go:84: [debug] cannot re-use a name that is still in use\n    helm.sh/helm/v3/pkg/action.(*Install).availableName\n    \thelm.sh/helm/v3/pkg/action/install.go:483\n    helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\n    \thelm.sh/helm/v3/pkg/action/install.go:197\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:264\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:125\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1662820718000,"stop":1662820718482,"duration":482}},{"uid":"2fc6d3008f7a07a8","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004ae240>: {\n        Underlying: <*exec.ExitError | 0xc000456ce0>{\n            ProcessState: {\n                pid: 6226,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 116718},\n                    Stime: {Sec: 0, Usec: 28173},\n                    Maxrss: 56544,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2193,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 360,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 654,\n                    Nivcsw: 234,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"install.go:178: [debug] Original chart version: \\\"v1.8.0\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\",\n                        \"\",\n                        \"Error: INSTALLATION FAILED: cannot re-use a name that is still in use\",\n                        \"helm.go:84: [debug] cannot re-use a name that is still in use\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).availableName\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:483\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:197\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:264\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:125\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                        \"INSTALLATION FAILED\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:127\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"install.go:178: [debug] Original chart version: \\\"v1.8.0\\\"\",\n                    \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\",\n                    \"\",\n                    \"Error: INSTALLATION FAILED: cannot re-use a name that is still in use\",\n                    \"helm.go:84: [debug] cannot re-use a name that is still in ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; install.go:178: [debug] Original chart version: \"v1.8.0\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\n    \n    Error: INSTALLATION FAILED: cannot re-use a name that is still in use\n    helm.go:84: [debug] cannot re-use a name that is still in use\n    helm.sh/helm/v3/pkg/action.(*Install).availableName\n    \thelm.sh/helm/v3/pkg/action/install.go:483\n    helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\n    \thelm.sh/helm/v3/pkg/action/install.go:197\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:264\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:125\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1662796443000,"stop":1662796443361,"duration":361}},{"uid":"245129865a9a2fb0","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000e0c0>: {\n        Underlying: <*exec.ExitError | 0xc0003a88a0>{\n            ProcessState: {\n                pid: 6252,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 147303},\n                    Stime: {Sec: 0, Usec: 35830},\n                    Maxrss: 52472,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3216,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 360,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 606,\n                    Nivcsw: 320,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"install.go:178: [debug] Original chart version: \\\"v1.8.0\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\",\n                        \"\",\n                        \"Error: INSTALLATION FAILED: cannot re-use a name that is still in use\",\n                        \"helm.go:84: [debug] cannot re-use a name that is still in use\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).availableName\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:483\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:197\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:264\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:125\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                        \"INSTALLATION FAILED\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:127\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"install.go:178: [debug] Original chart version: \\\"v1.8.0\\\"\",\n                    \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\",\n                    \"\",\n                    \"Error: INSTALLATION FAILED: cannot re-use a name that is still in use\",\n                    \"helm.go:84: [debug] cannot re-use a name that is still in ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; install.go:178: [debug] Original chart version: \"v1.8.0\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\n    \n    Error: INSTALLATION FAILED: cannot re-use a name that is still in use\n    helm.go:84: [debug] cannot re-use a name that is still in use\n    helm.sh/helm/v3/pkg/action.(*Install).availableName\n    \thelm.sh/helm/v3/pkg/action/install.go:483\n    helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\n    \thelm.sh/helm/v3/pkg/action/install.go:197\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:264\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:125\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1662793922000,"stop":1662793922354,"duration":354}},{"uid":"96709bfb7092ffb2","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00018c1b0>: {\n        Underlying: <*exec.ExitError | 0xc0004e6560>{\n            ProcessState: {\n                pid: 6164,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 108903},\n                    Stime: {Sec: 0, Usec: 36301},\n                    Maxrss: 54316,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2170,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 24,\n                    Oublock: 360,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 604,\n                    Nivcsw: 159,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"v1.8.0\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\",\n                        \"\",\n                        \"Error: INSTALLATION FAILED: cannot re-use a name that is still in use\",\n                        \"helm.go:84: [debug] cannot re-use a name that is still in use\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).availableName\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:483\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:197\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:264\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:125\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                        \"INSTALLATION FAILED\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:127\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is inse...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"v1.8.0\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\n    \n    Error: INSTALLATION FAILED: cannot re-use a name that is still in use\n    helm.go:84: [debug] cannot re-use a name that is still in use\n    helm.sh/helm/v3/pkg/action.(*Install).availableName\n    \thelm.sh/helm/v3/pkg/action/install.go:483\n    helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\n    \thelm.sh/helm/v3/pkg/action/install.go:197\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:264\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:125\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1662791710000,"stop":1662791710500,"duration":500}},{"uid":"9d021e1efe141905","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0002d8ae0>: {\n        Underlying: <*exec.ExitError | 0xc0003ac7c0>{\n            ProcessState: {\n                pid: 6145,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 123341},\n                    Stime: {Sec: 0, Usec: 15417},\n                    Maxrss: 52224,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2206,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 360,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 627,\n                    Nivcsw: 373,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"v1.8.0\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\",\n                        \"\",\n                        \"Error: INSTALLATION FAILED: cannot re-use a name that is still in use\",\n                        \"helm.go:84: [debug] cannot re-use a name that is still in use\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).availableName\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:483\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:197\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:264\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:125\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                        \"INSTALLATION FAILED\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:127\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insec...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"v1.8.0\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\n    \n    Error: INSTALLATION FAILED: cannot re-use a name that is still in use\n    helm.go:84: [debug] cannot re-use a name that is still in use\n    helm.sh/helm/v3/pkg/action.(*Install).availableName\n    \thelm.sh/helm/v3/pkg/action/install.go:483\n    helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\n    \thelm.sh/helm/v3/pkg/action/install.go:197\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:264\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:125\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1662788942000,"stop":1662788942419,"duration":419}},{"uid":"8e4b80e35b80e07e","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00045c198>: {\n        Underlying: <*exec.ExitError | 0xc00046a3a0>{\n            ProcessState: {\n                pid: 6195,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 141622},\n                    Stime: {Sec: 0, Usec: 22128},\n                    Maxrss: 54324,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2182,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 360,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 790,\n                    Nivcsw: 482,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"v1.8.0\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\",\n                        \"\",\n                        \"Error: INSTALLATION FAILED: cannot re-use a name that is still in use\",\n                        \"helm.go:84: [debug] cannot re-use a name that is still in use\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).availableName\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:483\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:197\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:264\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:125\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                        \"INSTALLATION FAILED\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:127\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insec...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"v1.8.0\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\n    \n    Error: INSTALLATION FAILED: cannot re-use a name that is still in use\n    helm.go:84: [debug] cannot re-use a name that is still in use\n    helm.sh/helm/v3/pkg/action.(*Install).availableName\n    \thelm.sh/helm/v3/pkg/action/install.go:483\n    helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\n    \thelm.sh/helm/v3/pkg/action/install.go:197\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:264\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:125\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1662784718000,"stop":1662784718473,"duration":473}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting iperf-server pod Check ping between iperf-server and iperf-client after iperf-server pod restart":{"statistic":{"failed":0,"broken":0,"skipped":51,"passed":0,"unknown":0,"total":51},"items":[{"uid":"932bb9fea60aa1b0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"ca6a4cf44e46bfdd","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"96e188ed94ec6662","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662348908000,"stop":1662348908000,"duration":0}},{"uid":"6e14fa104477a13f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"e9d84da102b21840","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"5e517711eba9a066","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"50cf073fe40dd4e3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"c6a1d444f638bb5e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"c24d795e2eb5e1e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"c0a52a0ed2e2cdab","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"a7dba4b37f4ad822","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"19d3b4bc6782301e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"d7b8b124d5be2f76","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"c118fdb8c9f167cc","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"27834db9efcc7feb","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"90e61660d13f0a96","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"73958c213a534876","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"269d2763f0b7651a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"8e3191ea549b4b71","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"93d3c2492084715e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice creation scenarios: when slice red is installed workers should have gateway pod running":{"statistic":{"failed":1,"broken":0,"skipped":36,"passed":14,"unknown":0,"total":51},"items":[{"uid":"c9147c7363a92094","status":"failed","statusDetails":"Timed out after 120.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1663044723000,"stop":1663044843182,"duration":120182}},{"uid":"8fd563b7b6a740a","status":"passed","time":{"start":1663044069000,"stop":1663044111267,"duration":42267}},{"uid":"a07659fcdbf12d0d","status":"passed","time":{"start":1662348908000,"stop":1662348920055,"duration":12055}},{"uid":"2b98e4cfea151cf2","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"92a39eba28f3088c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"72afaad3fd8faab5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"acb02b8e924a8e78","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"d7ed5bfc413dafff","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"297ff7cc320f47a5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"2d20dbb0b59a19d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"1c7e63ba47a26ac9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"bb22c02fa61a64ad","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"6917e7d4f39e53eb","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"ef34e1212f4a0ae2","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"c8346067fdbf0ab1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"96cfcd29034b975a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"ce4a08a1314c8cc5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"e04f21649a41ac9b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"49fd7587906d710a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"a867e929ed7b9bcf","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Update Should update successfully when Project Applied while removing a service account name in Read users":{"statistic":{"failed":16,"broken":0,"skipped":0,"passed":174,"unknown":0,"total":190},"items":[{"uid":"56a5c3988db652f2","status":"passed","time":{"start":1663042493000,"stop":1663042498619,"duration":5619}},{"uid":"9782a8eced7a83fe","status":"passed","time":{"start":1663041865000,"stop":1663041870262,"duration":5262}},{"uid":"6986d0ad7730b07","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <bool>: false\nto be true","time":{"start":1663039902000,"stop":1663039969435,"duration":67435}},{"uid":"8b3113b98e8a9713","status":"passed","time":{"start":1663009431000,"stop":1663009436725,"duration":5725}},{"uid":"3992296ff3375d3a","status":"passed","time":{"start":1662978562000,"stop":1662978567186,"duration":5186}},{"uid":"fbeacb149d115fe6","status":"passed","time":{"start":1662702362000,"stop":1662702366776,"duration":4776}},{"uid":"77a1d51b1caa2fc5","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <bool>: false\nto be true","time":{"start":1662553059000,"stop":1662553180291,"duration":121291}},{"uid":"f3c9c7e991a0316a","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <bool>: false\nto be true","time":{"start":1662553042000,"stop":1662553108634,"duration":66634}},{"uid":"840dbfead3349167","status":"passed","time":{"start":1662467486000,"stop":1662467490676,"duration":4676}},{"uid":"5cbd2d97c6d0fbba","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <bool>: false\nto be true","time":{"start":1662374048000,"stop":1662374114617,"duration":66617}},{"uid":"d94556fa3a9c3d5a","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0003ccd68>: {\n        Underlying: <*exec.ExitError | 0xc0007065c0>{\n            ProcessState: {\n                pid: 7162,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 219728},\n                    Stime: {Sec: 0, Usec: 48414},\n                    Maxrss: 75792,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3860,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13344,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 304,\n                    Nivcsw: 488,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Read%20users1271909315\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.241.207:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Read%20users1271909315\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.241.207:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Read%20users1271909315\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.241.207:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Read%20users1271909315\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.241.207:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Read%20users1271909315\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.241.207:443: connect: connection refused\noccurred","time":{"start":1662374250000,"stop":1662374250365,"duration":365}},{"uid":"f23fb0cad3aad4c1","status":"passed","time":{"start":1662361116000,"stop":1662361120732,"duration":4732}},{"uid":"350da3d7fe563ac","status":"passed","time":{"start":1662347286000,"stop":1662347290716,"duration":4716}},{"uid":"c7e540465060ee98","status":"passed","time":{"start":1662049179000,"stop":1662049184414,"duration":5414}},{"uid":"aa5a6b6b63d2f9aa","status":"passed","time":{"start":1662048808000,"stop":1662048814001,"duration":6001}},{"uid":"b883c9d13085ec61","status":"passed","time":{"start":1662036991000,"stop":1662036995607,"duration":4607}},{"uid":"1480e1973afc3492","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004c1d88>: {\n        Underlying: <*exec.ExitError | 0xc00044a2c0>{\n            ProcessState: {\n                pid: 9108,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 191442},\n                    Stime: {Sec: 0, Usec: 45045},\n                    Maxrss: 87376,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3765,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13344,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 473,\n                    Nivcsw: 339,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when applying patch:\",\n                        \"{\\\"status\\\":{}}\",\n                        \"to:\",\n                        \"Resource: \\\"controller.kubeslice.io/v1alpha1, Resource=projects\\\", GroupVersionKind: \\\"controller.kubeslice.io/v1alpha1, Kind=Project\\\"\",\n                        \"Name: \\\"projectupdatetest\\\", Namespace: \\\"kubeslice-controller\\\"\",\n                        \"for: \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Read%20users1257624858\\\": error when patching \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Read%20users1257624858\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.231.22:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when applying patch:\",\n                    \"{\\\"status\\\":{}}\",\n                    \"to:\",\n                    \"Resource: \\\"controller.kubeslice.io/v1alpha1, Resource=projects\\\", GroupVersionKind: \\\"controller.kubeslice.io/v1alpha1, Kind=Project\\\"\",\n                    \"Name: \\\"projectupdatetest\\\", Namespace: \\\"kubeslice-controller\\\"\",\n                    \"for: \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Read%20users1257624858\\\": error when patching \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Read%20users1257624858\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.231.22:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when applying patch:\",\n                        \"{\\\"status\\\":{}}\",\n                        \"to:\",\n                        \"Resource: \\\"controller.kubeslice.io/v1alpha1, Resource=projects\\\", GroupVersionKind: \\\"controller.kubeslice.io/v1alpha1, Kind=Project\\\"\",\n                        \"Name: \\\"projectupdatetest\\\", Namespace: \\\"kubeslice-controller\\\"\",\n                        \"for: \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; Error from server (InternalError): error when applying patch:\n    {\"status\":{}}\n    to:\n    Resource: \"controller.kubeslice.io/v1alpha1, Resource=projects\", GroupVersionKind: \"controller.kubeslice.io/v1alpha1, Kind=Project\"\n    Name: \"projectupdatetest\", Namespace: \"kubeslice-controller\"\n    for: \"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Read%20users1257624858\": error when patching \"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Read%20users1257624858\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.231.22:443: connect: connection refused\noccurred","time":{"start":1662031654000,"stop":1662031714275,"duration":60275}},{"uid":"a234a8c4aef19bbe","status":"passed","time":{"start":1662027354000,"stop":1662027359303,"duration":5303}},{"uid":"77386facd2ea22ba","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000682bd0>: {\n        Underlying: <*exec.ExitError | 0xc00069bfc0>{\n            ProcessState: {\n                pid: 7700,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 194896},\n                    Stime: {Sec: 0, Usec: 35797},\n                    Maxrss: 78604,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3232,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13344,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 339,\n                    Nivcsw: 288,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Read%20users1957810418\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.242.99:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Read%20users1957810418\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.242.99:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Read%20users1957810418\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.242.99:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Read%20users1957810418\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.242.99:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Read%20users1957810418\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.242.99:443: connect: connection refused\noccurred","time":{"start":1661952242000,"stop":1661952242315,"duration":315}},{"uid":"d169f2399bf1337c","status":"passed","time":{"start":1661863163000,"stop":1661863168181,"duration":5181}}]},"Worker Suite:Worker Suite#[It] NodeRestartSlice Testing node restart should re-establish connection on node restart":{"statistic":{"failed":0,"broken":0,"skipped":42,"passed":0,"unknown":0,"total":42},"items":[{"uid":"83f6ef6187461ffd","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"be78965b6330b509","status":"skipped","statusDetails":"skipped","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"36c68a5d6124db39","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662348908000,"stop":1662348908000,"duration":0}},{"uid":"c90116881625762b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"447c7dc578a0c7ef","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"d47099c1db7be19c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"570592b1b64ec0c7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"555643decb27059f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"32ed6beff6cd7928","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"81f6c3f9cb7d56ec","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"4c35e8e068f20ee3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"32f442b946f84f8e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"91e1ff4203d606b3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"d9a11fb6de248829","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"8bb7628b8ba05538","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"3c3e5cabf7832d7b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"5c48cb060fe5b172","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"a0e7ad08cc242e71","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"e788046be4165370","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"db48046b039b4aa7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}}]},"Hub Suite:Hub Suite#[It]  QosProfile tests QosProfile delete validation Delete qosProfile positive scenario":{"statistic":{"failed":0,"broken":0,"skipped":149,"passed":0,"unknown":0,"total":149},"items":[{"uid":"71ed8524107522ad","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661501311000,"stop":1661501311000,"duration":0}},{"uid":"5d2f453a753cb5cf","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661434220000,"stop":1661434220000,"duration":0}},{"uid":"a1032a03c5fc5b59","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661356056000,"stop":1661356056000,"duration":0}},{"uid":"b8e1a3ba1adf7262","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661352461000,"stop":1661352461000,"duration":0}},{"uid":"febce3a75402692","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661352207000,"stop":1661352207000,"duration":0}},{"uid":"11e568aeae2815ae","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661352028000,"stop":1661352028000,"duration":0}},{"uid":"eb9a8e0c6b7d6a13","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661351263000,"stop":1661351263000,"duration":0}},{"uid":"75c5e6a0143b901a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661188463000,"stop":1661188463000,"duration":0}},{"uid":"c5add898cdb05dd9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661183038000,"stop":1661183038000,"duration":0}},{"uid":"131a33865450a862","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661180055000,"stop":1661180055000,"duration":0}},{"uid":"c9e3352d3a7e669a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661152474000,"stop":1661152474000,"duration":0}},{"uid":"192acab30bedbfca","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661083598000,"stop":1661083598000,"duration":0}},{"uid":"a814297a4bc804b8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661070332000,"stop":1661070332000,"duration":0}},{"uid":"f138849f9725f609","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661066603000,"stop":1661066603000,"duration":0}},{"uid":"8b0e13afaca99cdd","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661018182000,"stop":1661018182000,"duration":0}},{"uid":"4b4af5b0dd430667","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661014620000,"stop":1661014620000,"duration":0}},{"uid":"90eee801da00d03b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661011831000,"stop":1661011831000,"duration":0}},{"uid":"35925f2160cbbee6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661011780000,"stop":1661011780000,"duration":0}},{"uid":"b6751e2b7c2fa29d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661009684000,"stop":1661009684000,"duration":0}},{"uid":"e72cecce8cbfb6d1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661007831000,"stop":1661007831000,"duration":0}}]},"Worker Suite:Worker Suite#[It]  QosProfile tests QosProfile delete validation should remove gateway pod from spoke":{"statistic":{"failed":0,"broken":0,"skipped":11,"passed":0,"unknown":0,"total":11},"items":[{"uid":"af971280f72b67e6","status":"skipped","statusDetails":"skipped","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"7569e3ac3a6568aa","status":"skipped","statusDetails":"skipped","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"b0eaf9ecf121abca","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662348908000,"stop":1662348908000,"duration":0}},{"uid":"c831c5a47c4e3737","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"326758e93af1d623","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"93199e39cb8a6797","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"6400130a59c27ae2","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"7844fd06609df6ef","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"423ed20200b1b330","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"eda2449c3c0fb2cc","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"9bed7068ed65bd63","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Hub netpol scenarios: Slice Configs Netpol scenarios Should fail while Deleting Slice without removing the namespaceIsolationProfile part in slice configs":{"statistic":{"failed":0,"broken":0,"skipped":23,"passed":28,"unknown":0,"total":51},"items":[{"uid":"693d3e81bd14451e","status":"skipped","statusDetails":"skipped","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"257b2653a9989bd7","status":"passed","time":{"start":1663044069000,"stop":1663044069106,"duration":106}},{"uid":"b5974d9250d5c884","status":"passed","time":{"start":1662348908000,"stop":1662348908123,"duration":123}},{"uid":"191222d065916334","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"faf21b4d922e3f82","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"e56eec13b14cf185","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"fa2fd007e1777b41","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"bdedd5ec49fdf518","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"f0588c2e6e35520d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"c01c104adafd18ee","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"efb6c41437ff0bf0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"881e23aecd17e1bc","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"4d35a03346a35521","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"d7affabc70874cf5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"952c17cd76c850bd","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"cba509b284082409","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"afe7880f99255487","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"349aee741123efd3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"bda3aabdfcaab334","status":"passed","time":{"start":1660067259000,"stop":1660067259112,"duration":112}},{"uid":"883524a3e0e84518","status":"passed","time":{"start":1660047551000,"stop":1660047551102,"duration":102}}]},"Worker Suite:Worker Suite#[It] Slice deletion tests: Slice is installed & validated Slice object is deleted from hub should remove vl3 router from spoke":{"statistic":{"failed":40,"broken":0,"skipped":1,"passed":10,"unknown":0,"total":51},"items":[{"uid":"a93cc1cff872ee9c","status":"skipped","statusDetails":"skipped","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"72e8a7cc0bcb4080","status":"passed","time":{"start":1663044069000,"stop":1663044162253,"duration":93253}},{"uid":"a9a6e1b9ebaa070a","status":"passed","time":{"start":1662348908000,"stop":1662348945652,"duration":37652}},{"uid":"834ea0cc4c481610","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000e060>: {\n        Underlying: <*exec.ExitError | 0xc00067e020>{\n            ProcessState: {\n                pid: 7153,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 241357},\n                    Stime: {Sec: 0, Usec: 66837},\n                    Maxrss: 88584,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 7503,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 365,\n                    Nivcsw: 433,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-red\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\noccurred","time":{"start":1661844205000,"stop":1661844205313,"duration":313}},{"uid":"11494b693f885dfb","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000734018>: {\n        Underlying: <*exec.ExitError | 0xc0007ba020>{\n            ProcessState: {\n                pid: 7275,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 163208},\n                    Stime: {Sec: 0, Usec: 58288},\n                    Maxrss: 87408,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4003,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 374,\n                    Nivcsw: 530,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-red\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661754870000,"stop":1661754870232,"duration":232}},{"uid":"6ebd8d4c99dbd8cb","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0008cf578>: {\n        Underlying: <*exec.ExitError | 0xc0006d2d20>{\n            ProcessState: {\n                pid: 7261,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 190603},\n                    Stime: {Sec: 0, Usec: 55063},\n                    Maxrss: 85716,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3785,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 401,\n                    Nivcsw: 493,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-red\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661611962000,"stop":1661611962244,"duration":244}},{"uid":"1407aa4534364f80","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00084a018>: {\n        Underlying: <*exec.ExitError | 0xc0007aa000>{\n            ProcessState: {\n                pid: 7311,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 203597},\n                    Stime: {Sec: 0, Usec: 30731},\n                    Maxrss: 85320,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4145,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 410,\n                    Nivcsw: 477,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-red\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661585077000,"stop":1661585077245,"duration":245}},{"uid":"8db5ee1508d26c07","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000fa88>: {\n        Underlying: <*exec.ExitError | 0xc000716740>{\n            ProcessState: {\n                pid: 6019,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 186764},\n                    Stime: {Sec: 0, Usec: 44661},\n                    Maxrss: 84252,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3558,\n                    Majflt: 2,\n                    Nswap: 0,\n                    Inblock: 344,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 578,\n                    Nivcsw: 558,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-red\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661584447000,"stop":1661584447249,"duration":249}},{"uid":"750abfcde30564f2","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0008103d8>: {\n        Underlying: <*exec.ExitError | 0xc00013ec80>{\n            ProcessState: {\n                pid: 7314,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 189581},\n                    Stime: {Sec: 0, Usec: 30333},\n                    Maxrss: 77976,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3432,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 361,\n                    Nivcsw: 450,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-red\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661580045000,"stop":1661580045217,"duration":217}},{"uid":"51ef1178caae64aa","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000779770>: {\n        Underlying: <*exec.ExitError | 0xc0007389e0>{\n            ProcessState: {\n                pid: 7298,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 192887},\n                    Stime: {Sec: 0, Usec: 84870},\n                    Maxrss: 84924,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3810,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 312,\n                    Nivcsw: 379,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-red\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661578505000,"stop":1661578505267,"duration":267}},{"uid":"9edb35c60a964028","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00044b668>: {\n        Underlying: <*exec.ExitError | 0xc0006b7420>{\n            ProcessState: {\n                pid: 7321,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 238143},\n                    Stime: {Sec: 0, Usec: 49933},\n                    Maxrss: 87164,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3049,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 497,\n                    Nivcsw: 425,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-red\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661572479000,"stop":1661572479256,"duration":256}},{"uid":"ef00b61633f6fd14","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0006ea378>: {\n        Underlying: <*exec.ExitError | 0xc0008c85e0>{\n            ProcessState: {\n                pid: 7131,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 178699},\n                    Stime: {Sec: 0, Usec: 47653},\n                    Maxrss: 78744,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4097,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 342,\n                    Nivcsw: 374,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-red\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660832281000,"stop":1660832281215,"duration":215}},{"uid":"61e696478a276c40","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004961b0>: {\n        Underlying: <*exec.ExitError | 0xc000662060>{\n            ProcessState: {\n                pid: 7054,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 179753},\n                    Stime: {Sec: 0, Usec: 35950},\n                    Maxrss: 82292,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 6378,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 363,\n                    Nivcsw: 330,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-red\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660808797000,"stop":1660808797218,"duration":218}},{"uid":"98e093a2c5568254","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0005cc378>: {\n        Underlying: <*exec.ExitError | 0xc000720f00>{\n            ProcessState: {\n                pid: 7129,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 148879},\n                    Stime: {Sec: 0, Usec: 39178},\n                    Maxrss: 81224,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 6084,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 415,\n                    Nivcsw: 350,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-red\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660299182000,"stop":1660299182183,"duration":183}},{"uid":"a2418f3c9820c060","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0003d30e0>: {\n        Underlying: <*exec.ExitError | 0xc000709b40>{\n            ProcessState: {\n                pid: 7161,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 158288},\n                    Stime: {Sec: 0, Usec: 41456},\n                    Maxrss: 82336,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 5777,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 314,\n                    Nivcsw: 345,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-red\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660295740000,"stop":1660295740197,"duration":197}},{"uid":"cd109c43ac4ad673","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0008e8138>: {\n        Underlying: <*exec.ExitError | 0xc0004c2f00>{\n            ProcessState: {\n                pid: 7249,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 186089},\n                    Stime: {Sec: 0, Usec: 64726},\n                    Maxrss: 83836,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4991,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 596,\n                    Nivcsw: 491,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-red\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660293657000,"stop":1660293657257,"duration":257}},{"uid":"f8a6b9a3bda62f3","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 3\nto equal\n    <int>: 1","time":{"start":1660113628000,"stop":1660113688522,"duration":60522}},{"uid":"41c28d0a63c60f32","status":"failed","statusDetails":"Timed out after 120.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1660104898000,"stop":1660105018717,"duration":120717}},{"uid":"faa2b3f81db94314","status":"failed","statusDetails":"Timed out after 120.001s.\nExpected\n    <int>: 1\nto equal\n    <int>: 0","time":{"start":1660067259000,"stop":1660067394447,"duration":135447}},{"uid":"4b6f6352fa75b6fe","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 3\nto equal\n    <int>: 1","time":{"start":1660047551000,"stop":1660047612720,"duration":61720}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests slice with netpol should have gateway pod running":{"statistic":{"failed":1,"broken":0,"skipped":37,"passed":13,"unknown":0,"total":51},"items":[{"uid":"443d270eb229fc56","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"27c10b35c59a79a","status":"passed","time":{"start":1663044069000,"stop":1663044151397,"duration":82397}},{"uid":"95f59a7e8966d0c7","status":"passed","time":{"start":1662348908000,"stop":1662348948220,"duration":40220}},{"uid":"5dfe6beceb8c61ce","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"f9fa7a32cb7376af","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"52ec833057031e68","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"4e7c85dbbd80e4af","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"2b3e6ef120b11628","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"df08f591eca8e0c8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"2fad6ce21754ee9a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"fdcf15c9c974c6ef","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"bc70b806e651abf1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"9ccf4b19a13242d5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"7f25c723508252b3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"9214189e190f1652","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"7ee2fa807ec44da0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"d75e2c660ebd79d8","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1660113628000,"stop":1660113822741,"duration":194741}},{"uid":"f9f595d398047778","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"a72d51888be345ba","status":"passed","time":{"start":1660067259000,"stop":1660067259010,"duration":10}},{"uid":"539dd318245b41e1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice Config Negative Scenario tests Slice Config Negative Scenario Update Should fail while updating sliceconfig with with invalid DSCP class in qosprofiledetails":{"statistic":{"failed":1,"broken":0,"skipped":14,"passed":36,"unknown":0,"total":51},"items":[{"uid":"81fd3250a3cb6c68","status":"passed","time":{"start":1663044723000,"stop":1663044723855,"duration":855}},{"uid":"c2682a632e16eee0","status":"skipped","statusDetails":"skipped","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"cea43f74a5cf26c3","status":"passed","time":{"start":1662348908000,"stop":1662348908952,"duration":952}},{"uid":"790535c55014fbc8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"5ade1f5dbbfacfe5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"79c08f371a4303e5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"f2fde110ccef0b56","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"4dac4cc997800235","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"4a42292c63cb111c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"681c0ae15cf19c83","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"ef797daaeaa2f543","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"5ca36efcf2220f98","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"1f05c560c987e04f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"5a83bd996154abc7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"b6cbd68a04bc9afc","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"3d6ed365e8eab724","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"1123c64f65947f2f","status":"passed","time":{"start":1660113628000,"stop":1660113628359,"duration":359}},{"uid":"c2bbf1c2465db96e","status":"passed","time":{"start":1660104898000,"stop":1660104898511,"duration":511}},{"uid":"1817a5b1dca82056","status":"passed","time":{"start":1660067259000,"stop":1660067259309,"duration":309}},{"uid":"23037119b86b1c6f","status":"passed","time":{"start":1660047551000,"stop":1660047552326,"duration":1326}}]},"Worker Suite:Worker Suite#[It] Intracluster slice tests Slice with netpol should have vl3 router running":{"statistic":{"failed":22,"broken":0,"skipped":6,"passed":23,"unknown":0,"total":51},"items":[{"uid":"78b302f878f5d2fe","status":"skipped","statusDetails":"skipped","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"1a783a4f2c44b4af","status":"passed","time":{"start":1663044069000,"stop":1663044072473,"duration":3473}},{"uid":"9208b836274ad7c3","status":"passed","time":{"start":1662348908000,"stop":1662348908290,"duration":290}},{"uid":"26f807534982f600","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000469338>: {\n        Underlying: <*exec.ExitError | 0xc000709d60>{\n            ProcessState: {\n                pid: 7256,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 218947},\n                    Stime: {Sec: 0, Usec: 87578},\n                    Maxrss: 85032,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 5164,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 529,\n                    Nivcsw: 413,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\noccurred","time":{"start":1661844205000,"stop":1661844205591,"duration":591}},{"uid":"441f0d2f407baf9b","status":"skipped","statusDetails":"skipped - Controller & worker should be set up in same. Skipping!!","time":{"start":1661754870000,"stop":1661754870206,"duration":206}},{"uid":"71bf3aba59096b20","status":"skipped","statusDetails":"skipped - Controller & worker should be set up in same. Skipping!!","time":{"start":1661611962000,"stop":1661611962219,"duration":219}},{"uid":"db3e3c7e00fe7359","status":"skipped","statusDetails":"skipped - Controller & worker should be set up in same. Skipping!!","time":{"start":1661585077000,"stop":1661585077222,"duration":222}},{"uid":"4a344de0657d5ec8","status":"skipped","statusDetails":"skipped - Controller & worker should be set up in same. Skipping!!","time":{"start":1661584447000,"stop":1661584447298,"duration":298}},{"uid":"ebc45cbe2f8e73d0","status":"skipped","statusDetails":"skipped - Controller & worker should be set up in same. Skipping!!","time":{"start":1661580045000,"stop":1661580045245,"duration":245}},{"uid":"de8c2c25acefc83b","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0007789d8>: {\n        Underlying: <*exec.ExitError | 0xc000775640>{\n            ProcessState: {\n                pid: 7098,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 243772},\n                    Stime: {Sec: 0, Usec: 65006},\n                    Maxrss: 80920,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 9799,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 404,\n                    Nivcsw: 637,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661578505000,"stop":1661578505612,"duration":612}},{"uid":"2427e8ecbd83c59f","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004c8930>: {\n        Underlying: <*exec.ExitError | 0xc00071d9e0>{\n            ProcessState: {\n                pid: 7111,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 225803},\n                    Stime: {Sec: 0, Usec: 60483},\n                    Maxrss: 87144,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 7791,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 446,\n                    Nivcsw: 428,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661572479000,"stop":1661572479733,"duration":733}},{"uid":"17876f52c58a98a2","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0008bbae8>: {\n        Underlying: <*exec.ExitError | 0xc00050f6c0>{\n            ProcessState: {\n                pid: 7064,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 166399},\n                    Stime: {Sec: 0, Usec: 39618},\n                    Maxrss: 70312,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 6712,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 666,\n                    Nivcsw: 443,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660832281000,"stop":1660832282416,"duration":1416}},{"uid":"4d3469b67813ea59","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000496af8>: {\n        Underlying: <*exec.ExitError | 0xc0006ffee0>{\n            ProcessState: {\n                pid: 7034,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 168832},\n                    Stime: {Sec: 0, Usec: 64317},\n                    Maxrss: 80856,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 6535,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 375,\n                    Nivcsw: 309,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660808797000,"stop":1660808797484,"duration":484}},{"uid":"2478d1e6452ddce2","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0005cc9f0>: {\n        Underlying: <*exec.ExitError | 0xc000721de0>{\n            ProcessState: {\n                pid: 7213,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 145623},\n                    Stime: {Sec: 0, Usec: 47229},\n                    Maxrss: 85716,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 8175,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 266,\n                    Nivcsw: 403,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660299182000,"stop":1660299182380,"duration":380}},{"uid":"4a198e46afaf19c2","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00044b0c8>: {\n        Underlying: <*exec.ExitError | 0xc000806dc0>{\n            ProcessState: {\n                pid: 7190,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 117535},\n                    Stime: {Sec: 0, Usec: 39178},\n                    Maxrss: 82876,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3815,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 434,\n                    Nivcsw: 254,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660295740000,"stop":1660295740316,"duration":316}},{"uid":"35024475e4e76e70","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00084c780>: {\n        Underlying: <*exec.ExitError | 0xc0008a14a0>{\n            ProcessState: {\n                pid: 7223,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 177523},\n                    Stime: {Sec: 0, Usec: 33814},\n                    Maxrss: 80248,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4303,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 410,\n                    Nivcsw: 356,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660293657000,"stop":1660293657433,"duration":433}},{"uid":"8695f0e807b3dc0a","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 3\nto equal\n    <int>: 1","time":{"start":1660113628000,"stop":1660113697033,"duration":69033}},{"uid":"2a6c5000e4ecf209","status":"passed","time":{"start":1660104898000,"stop":1660104911531,"duration":13531}},{"uid":"134545b34a325aa4","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 2\nto equal\n    <int>: 1","time":{"start":1660067259000,"stop":1660067327837,"duration":68837}},{"uid":"1c906c1908f5806c","status":"passed","time":{"start":1660047551000,"stop":1660047564042,"duration":13042}}]},"Empty Suite:Empty Suite#[It] Hub Deletion tests Hub Uninstalltion Test Scenarios Should pass if project is uninstalled first and then hub":{"statistic":{"failed":32,"broken":0,"skipped":0,"passed":137,"unknown":0,"total":169},"items":[{"uid":"8f15bc6a2f6b61c9","status":"passed","time":{"start":1663052238000,"stop":1663052297342,"duration":59342}},{"uid":"5202cb9e2403f62f","status":"passed","time":{"start":1663051980000,"stop":1663052088951,"duration":108951}},{"uid":"540ccf1bef8f72","status":"passed","time":{"start":1663050064000,"stop":1663050176960,"duration":112960}},{"uid":"85742f6873e93e4a","status":"passed","time":{"start":1663042347000,"stop":1663042450179,"duration":103179}},{"uid":"45a4be4f7de4e964","status":"passed","time":{"start":1663041715000,"stop":1663041817699,"duration":102699}},{"uid":"bc70281f6597093a","status":"passed","time":{"start":1663039756000,"stop":1663039804669,"duration":48669}},{"uid":"3767e889e02f3ab7","status":"passed","time":{"start":1663040168000,"stop":1663040286225,"duration":118225}},{"uid":"4ed1a8ace0629b9a","status":"passed","time":{"start":1663009296000,"stop":1663009344294,"duration":48294}},{"uid":"7695bc0935bdad5f","status":"passed","time":{"start":1663008270000,"stop":1663008373065,"duration":103065}},{"uid":"bc2144c3f81fd252","status":"passed","time":{"start":1662986456000,"stop":1662986553376,"duration":97376}},{"uid":"ae178203e7f73cf2","status":"passed","time":{"start":1662978394000,"stop":1662978458800,"duration":64800}},{"uid":"3c5086f343ec4a64","status":"passed","time":{"start":1662961730000,"stop":1662961773874,"duration":43874}},{"uid":"f7ba9eca73a0594e","status":"passed","time":{"start":1662960985000,"stop":1662961052052,"duration":67052}},{"uid":"88196727cbba6433","status":"passed","time":{"start":1662960825000,"stop":1662960877369,"duration":52369}},{"uid":"e39d576026fd0158","status":"passed","time":{"start":1662824760000,"stop":1662824810071,"duration":50071}},{"uid":"8723a27ee2828133","status":"passed","time":{"start":1662819406000,"stop":1662819474331,"duration":68331}},{"uid":"33eabae8e9e9e7c5","status":"passed","time":{"start":1662795171000,"stop":1662795274715,"duration":103715}},{"uid":"c95862dee7a0e257","status":"passed","time":{"start":1662792593000,"stop":1662792716176,"duration":123176}},{"uid":"c7dfecbfaf013392","status":"passed","time":{"start":1662791140000,"stop":1662791254124,"duration":114124}},{"uid":"698378ab73721242","status":"passed","time":{"start":1662788408000,"stop":1662788455501,"duration":47501}}]},"Worker Suite:Worker Suite#[It] Hub netpol scenarios: Slice Configs Netpol scenarios Should fail when deploying with invalid clusters in both applicationNamespace and allowedNamespace":{"statistic":{"failed":0,"broken":0,"skipped":23,"passed":28,"unknown":0,"total":51},"items":[{"uid":"a111cdc01a6ce7c4","status":"skipped","statusDetails":"skipped","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"42ae9634874ad18c","status":"passed","time":{"start":1663044069000,"stop":1663044070106,"duration":1106}},{"uid":"1864f49b9386a6b5","status":"passed","time":{"start":1662348908000,"stop":1662348909367,"duration":1367}},{"uid":"be5aa2fa212e2489","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"9fdd5da9a9c0ea1c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"234c6e0f704150c2","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"17efd4b0f80b2a0c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"d98cb2fe6bceb4f9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"79a7c7306f2bb01b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"87124cd76bd0cffd","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"2b23f86de93c6f1d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"22bfe8e7b9952476","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"24dfc86c653369c0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"1398910f30c53639","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"fe009663d3aa37aa","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"e45fe61d57c6ebff","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"97a9135b25dc1db1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"cc77ad471859959d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"4e512b6ecc5ca831","status":"passed","time":{"start":1660067259000,"stop":1660067259951,"duration":951}},{"uid":"d04536ee29a113d2","status":"passed","time":{"start":1660047551000,"stop":1660047551847,"duration":847}}]},"Worker Suite:Worker Suite#[It]  QosProfile tests QosProfile Create validation Create qos profile- validation of BandwidthCeilingKbps > BandwidthGuaranteedKbps":{"statistic":{"failed":0,"broken":0,"skipped":10,"passed":1,"unknown":0,"total":11},"items":[{"uid":"26cf6a81352b3811","status":"skipped","statusDetails":"skipped","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"45ebfa1fd4dbe336","status":"skipped","statusDetails":"skipped","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"60c502c8247c16e1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662348908000,"stop":1662348908000,"duration":0}},{"uid":"2b06c9e7afb06aa9","status":"passed","time":{"start":1661844205000,"stop":1661844205433,"duration":433}},{"uid":"af3a6233c3ba205b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"57bd0b6779980827","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"a4aff9d305ef80e6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"f4fbed2a64c6fb2d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"2b5d0127fb0e4906","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"6ca8f28e3e9412eb","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"5285e9c69b2911f7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Intracluster slice tests Iperf connectivity Verify Connectivity between iperf-server and iperf-client":{"statistic":{"failed":1,"broken":0,"skipped":45,"passed":5,"unknown":0,"total":51},"items":[{"uid":"9ca5b48ec8878874","status":"skipped","statusDetails":"skipped","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"8d7648cd09d7ad1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"6053ef2c01cc8684","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662348908000,"stop":1662348908000,"duration":0}},{"uid":"f67efaaae396144a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"8cf9e67e9badeaec","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"1f30fb83398c4f92","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"9e11f540217918c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"805c5b76c832a715","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"2ef787ee050781d4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"c3273467e80db79b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"75434e9864c23f36","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"d6e56b7b30d1a516","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"e8fa2bd80b7fb2f1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"2562db67c3bdcf3f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"3b5c0947583c57f8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"7fc120c049725ee4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"77dbe44598075993","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"6d4e04110e2a88ec","status":"passed","time":{"start":1660104898000,"stop":1660104904754,"duration":6754}},{"uid":"c8a1aaa7af80547a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"6722ccd10c859182","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Hub netpol scenarios: Slice Configs Netpol scenarios Should fail when deploying with invalid namespace in both applicationNamespace and allowedNamespace":{"statistic":{"failed":0,"broken":0,"skipped":23,"passed":28,"unknown":0,"total":51},"items":[{"uid":"b0cb9180a00d1221","status":"skipped","statusDetails":"skipped","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"5c20229f846ef78f","status":"passed","time":{"start":1663044069000,"stop":1663044069789,"duration":789}},{"uid":"8d388cdeb7687c0a","status":"passed","time":{"start":1662348908000,"stop":1662348908880,"duration":880}},{"uid":"7e4d991adad19255","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"e7835573633cd2ab","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"87da5c8ca5050dcc","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"9d5c8e407b5a6d62","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"6aaa73e2d4ba7cb6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"c4015dc7b703ce4c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"bd7ece63012204bc","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"c4e68c926e6e44ac","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"259d9609ee150f90","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"400f724cc595a88a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"c00eec3ebea0edae","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"f2285aa25e713989","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"e39401ceb5e72756","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"eb939a8da12694af","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"9c399eb1f6d9191c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"cd09dd02ce7594c8","status":"passed","time":{"start":1660067259000,"stop":1660067259765,"duration":765}},{"uid":"27c52a30ecc3a82d","status":"passed","time":{"start":1660047551000,"stop":1660047551497,"duration":497}}]},"Hub Suite:Hub Suite#[AfterSuite]":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":253,"unknown":0,"total":253},"items":[{"uid":"81a6041c06aee2bd","status":"passed","time":{"start":1663042493000,"stop":1663042495031,"duration":2031}},{"uid":"47768ba464dee625","status":"passed","time":{"start":1663041865000,"stop":1663041867973,"duration":2973}},{"uid":"874e0aaaf92acd2e","status":"passed","time":{"start":1663039902000,"stop":1663040203905,"duration":301905}},{"uid":"b9e3213483a80884","status":"passed","time":{"start":1663009431000,"stop":1663009433553,"duration":2553}},{"uid":"3e37cd6d110c9db9","status":"passed","time":{"start":1662978562000,"stop":1662978566475,"duration":4475}},{"uid":"cfd231a3045b96b4","status":"passed","time":{"start":1662702362000,"stop":1662702364408,"duration":2408}},{"uid":"63f8e69a474556c","status":"passed","time":{"start":1662699422000,"stop":1662699424254,"duration":2254}},{"uid":"6ab6f5f78b2a4d97","status":"passed","time":{"start":1662553059000,"stop":1662553361247,"duration":302247}},{"uid":"5bb00110e6087e","status":"passed","time":{"start":1662553042000,"stop":1662553344222,"duration":302222}},{"uid":"e6aede2da6f4607c","status":"passed","time":{"start":1662538997000,"stop":1662538998864,"duration":1864}},{"uid":"552516120bf4b817","status":"passed","time":{"start":1662487441000,"stop":1662487443047,"duration":2047}},{"uid":"17f18f086cd51647","status":"passed","time":{"start":1662484875000,"stop":1662484876817,"duration":1817}},{"uid":"5554ef56af2ccda7","status":"passed","time":{"start":1662467486000,"stop":1662467488122,"duration":2122}},{"uid":"15adfd213fcf7660","status":"passed","time":{"start":1662374048000,"stop":1662374350838,"duration":302838}},{"uid":"6acbeed420a30743","status":"passed","time":{"start":1662374250000,"stop":1662374553077,"duration":303077}},{"uid":"93e94c24d137729d","status":"passed","time":{"start":1662361116000,"stop":1662361118351,"duration":2351}},{"uid":"170297605cb8e190","status":"passed","time":{"start":1662347286000,"stop":1662347288385,"duration":2385}},{"uid":"3022d54aef9eb742","status":"passed","time":{"start":1662049179000,"stop":1662049182364,"duration":3364}},{"uid":"8931085323cdbe68","status":"passed","time":{"start":1662048808000,"stop":1662048810945,"duration":2945}},{"uid":"95ad91ea8d0dc7f6","status":"passed","time":{"start":1662036991000,"stop":1662036993040,"duration":2040}}]},"Hub Suite:Hub Suite#[It] Cluster CR tests Cluster CR validation Creates cluster secrets":{"statistic":{"failed":3,"broken":0,"skipped":0,"passed":190,"unknown":0,"total":193},"items":[{"uid":"2c649a010cff1d6d","status":"passed","time":{"start":1663042493000,"stop":1663042496664,"duration":3664}},{"uid":"20984e734fb2d6f7","status":"passed","time":{"start":1663041865000,"stop":1663041868787,"duration":3787}},{"uid":"1c7f9453556ddc9f","status":"passed","time":{"start":1663039902000,"stop":1663039914544,"duration":12544}},{"uid":"65ab775a711f80ab","status":"passed","time":{"start":1663009431000,"stop":1663009434597,"duration":3597}},{"uid":"272cda53e2474435","status":"passed","time":{"start":1662978562000,"stop":1662978565738,"duration":3738}},{"uid":"d3627c6db199aad2","status":"passed","time":{"start":1662702362000,"stop":1662702365676,"duration":3676}},{"uid":"136bd67e428c8050","status":"passed","time":{"start":1662553059000,"stop":1662553070546,"duration":11546}},{"uid":"2bef307e70df0732","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0005a5890>: {\n        Underlying: <*exec.ExitError | 0xc000427020>{\n            ProcessState: {\n                pid: 11002,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 179735},\n                    Stime: {Sec: 0, Usec: 35947},\n                    Maxrss: 81976,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 5764,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 332,\n                    Nivcsw: 265,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Cluster%20CR%20tests%20Cluster%20CR%20validation%20Creates%20cluster%20secrets391636753\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.123.224:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Cluster%20CR%20tests%20Cluster%20CR%20validation%20Creates%20cluster%20secrets391636753\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.123.224:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Cluster%20CR%20tests%20Cluster%20CR%20validation%20Creates%20cluster%20secrets391636753\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.123.224:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Cluster%20CR%20tests%20Cluster%20CR%20validation%20Creates%20cluster%20secrets391636753\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.123.224:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Cluster%20CR%20tests%20Cluster%20CR%20validation%20Creates%20cluster%20secrets391636753\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.123.224:443: connect: connection refused\noccurred","time":{"start":1662553042000,"stop":1662553042265,"duration":265}},{"uid":"996448c68a1123ec","status":"passed","time":{"start":1662467486000,"stop":1662467490334,"duration":4334}},{"uid":"9ecd6197bc276ac7","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0000cb068>: {\n        Underlying: <*exec.ExitError | 0xc0008c9e80>{\n            ProcessState: {\n                pid: 10942,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 317862},\n                    Stime: {Sec: 0, Usec: 67545},\n                    Maxrss: 80124,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3415,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 360,\n                    Nivcsw: 427,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Cluster%20CR%20tests%20Cluster%20CR%20validation%20Creates%20cluster%20secrets4283529870\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.14.145:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Cluster%20CR%20tests%20Cluster%20CR%20validation%20Creates%20cluster%20secrets4283529870\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.14.145:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Cluster%20CR%20tests%20Cluster%20CR%20validation%20Creates%20cluster%20secrets4283529870\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.14.145:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Cluster%20CR%20tests%20Cluster%20CR%20validation%20Creates%20cluster%20secrets4283529870\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.14.145:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Cluster%20CR%20tests%20Cluster%20CR%20validation%20Creates%20cluster%20secrets4283529870\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.14.145:443: connect: connection refused\noccurred","time":{"start":1662374048000,"stop":1662374048500,"duration":500}},{"uid":"e5d29cc33d939cb2","status":"passed","time":{"start":1662374250000,"stop":1662374261894,"duration":11894}},{"uid":"6818fb42a41a8fa5","status":"passed","time":{"start":1662361116000,"stop":1662361120409,"duration":4409}},{"uid":"39c3145b8c2916de","status":"passed","time":{"start":1662347286000,"stop":1662347289598,"duration":3598}},{"uid":"44b2e639e720d388","status":"passed","time":{"start":1662049179000,"stop":1662049182908,"duration":3908}},{"uid":"177c5ed58e1e28d9","status":"passed","time":{"start":1662048808000,"stop":1662048811709,"duration":3709}},{"uid":"6fdeef1f0832ea55","status":"passed","time":{"start":1662036991000,"stop":1662036994638,"duration":3638}},{"uid":"843d9b396503d447","status":"passed","time":{"start":1662031654000,"stop":1662031666538,"duration":12538}},{"uid":"3abc831f19082ce1","status":"passed","time":{"start":1662027354000,"stop":1662027364919,"duration":10919}},{"uid":"13627663a3afc727","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000f0c8>: {\n        Underlying: <*exec.ExitError | 0xc000904b60>{\n            ProcessState: {\n                pid: 7655,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 182828},\n                    Stime: {Sec: 0, Usec: 56879},\n                    Maxrss: 86936,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 8358,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13344,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 419,\n                    Nivcsw: 341,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Cluster%20CR%20tests%20Cluster%20CR%20validation%20Creates%20cluster%20secrets126278272\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.242.99:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Cluster%20CR%20tests%20Cluster%20CR%20validation%20Creates%20cluster%20secrets126278272\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.242.99:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Cluster%20CR%20tests%20Cluster%20CR%20validation%20Creates%20cluster%20secrets126278272\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.242.99:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Cluster%20CR%20tests%20Cluster%20CR%20validation%20Creates%20cluster%20secrets126278272\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.242.99:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Cluster%20CR%20tests%20Cluster%20CR%20validation%20Creates%20cluster%20secrets126278272\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.242.99:443: connect: connection refused\noccurred","time":{"start":1661952242000,"stop":1661952242303,"duration":303}},{"uid":"e066576938d52e54","status":"passed","time":{"start":1661863163000,"stop":1661863166722,"duration":3722}}]},"Istio Suite:Istio Suite#[It] Bookinfo tests Bookinfo Scenario 2 with egress and without ingress Should install Details, Reviews, Ratings and Serviceexport on worker cluster-2":{"statistic":{"failed":1,"broken":0,"skipped":46,"passed":10,"unknown":0,"total":57},"items":[{"uid":"660fdd582045707","status":"passed","time":{"start":1663043831000,"stop":1663043864232,"duration":33232}},{"uid":"ba8c0cd119a7b689","status":"passed","time":{"start":1663043075000,"stop":1663043112388,"duration":37388}},{"uid":"ddcf56711d66851","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663040338000,"stop":1663040338000,"duration":0}},{"uid":"9b25d032a03709f0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662819574000,"stop":1662819574000,"duration":0}},{"uid":"24fb560452b749c3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662795310000,"stop":1662795310000,"duration":0}},{"uid":"2c7cfb26519680f9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662792773000,"stop":1662792773000,"duration":0}},{"uid":"e3ff0de845583d71","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662620110000,"stop":1662620110000,"duration":0}},{"uid":"cefd4e07a68dda44","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662617640000,"stop":1662617640000,"duration":0}},{"uid":"696ca689b91b8ec9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662610377000,"stop":1662610377000,"duration":0}},{"uid":"674f00dc2fa4b206","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662557911000,"stop":1662557911000,"duration":0}},{"uid":"f990048d66edde6e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662553747000,"stop":1662553747000,"duration":0}},{"uid":"ac353af123ed65a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662553327000,"stop":1662553327000,"duration":0}},{"uid":"dc89d232c79f101c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662381943000,"stop":1662381943000,"duration":0}},{"uid":"ce099b822e31f017","status":"passed","time":{"start":1662348103000,"stop":1662348136361,"duration":33361}},{"uid":"66fb3d0e3d857866","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662049889000,"stop":1662049889000,"duration":0}},{"uid":"ddc91af671a50fde","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662049074000,"stop":1662049074000,"duration":0}},{"uid":"2f81e5bd70000366","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662028179000,"stop":1662028179000,"duration":0}},{"uid":"8ab4e4db0e357c3e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661859205000,"stop":1661859205000,"duration":0}},{"uid":"5c3bff032e4cc4e0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661843950000,"stop":1661843950000,"duration":0}},{"uid":"82f01af05bfa7a81","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754632000,"stop":1661754632000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Hub netpol scenarios: Slice Configs Netpol scenarios Should remove successfully while Deleting Slice after removing the applicationNamespace in namespaceIsolationProfile part in slice configs":{"statistic":{"failed":0,"broken":0,"skipped":23,"passed":28,"unknown":0,"total":51},"items":[{"uid":"610e7f2adb2159c9","status":"skipped","statusDetails":"skipped","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"53daed9f505670bb","status":"passed","time":{"start":1663044069000,"stop":1663044079977,"duration":10977}},{"uid":"6598c174ab8b47a","status":"passed","time":{"start":1662348908000,"stop":1662348918996,"duration":10996}},{"uid":"eae7e4d94802ab20","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"f18f29d2858c0d41","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"36cd36ebe3eaeb80","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"22264f7672151cae","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"c43b6d5a7ef52bf9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"b403a03f520457f2","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"b5c9d21a15a38ae5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"a5b83e406fd93c25","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"d3f90d50467d513a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"53c3b1560abc31f5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"2ecb17c64020439a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"c52a50f99fbbb084","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"c1d173ab677fb8dd","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"8dede603bdf60bf","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"ee48266bbafa92a5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"91c3685558aaa853","status":"passed","time":{"start":1660067259000,"stop":1660067269623,"duration":10623}},{"uid":"c034f5d350da7db5","status":"passed","time":{"start":1660047551000,"stop":1660047561652,"duration":10652}}]},"Worker Suite:Worker Suite#[It] Intracluster slice tests Slice with netpol should label app ns with kubeslice label":{"statistic":{"failed":0,"broken":0,"skipped":35,"passed":16,"unknown":0,"total":51},"items":[{"uid":"5f8f919c9aa299d7","status":"skipped","statusDetails":"skipped","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"1583e1400f0d5e22","status":"passed","time":{"start":1663044069000,"stop":1663044069089,"duration":89}},{"uid":"a89fa6fee04905d5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662348908000,"stop":1662348908000,"duration":0}},{"uid":"1838bc4ef72e07b0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"d5f10d323dd924ea","status":"skipped","statusDetails":"skipped - Spec skipped because Skip() was called in BeforeAll","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"27d94491c2fccd09","status":"skipped","statusDetails":"skipped - Spec skipped because Skip() was called in BeforeAll","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"bb64b3d542664fd6","status":"skipped","statusDetails":"skipped - Spec skipped because Skip() was called in BeforeAll","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"9d99d083b562a135","status":"skipped","statusDetails":"skipped - Spec skipped because Skip() was called in BeforeAll","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"3f476438a702f6e1","status":"skipped","statusDetails":"skipped - Spec skipped because Skip() was called in BeforeAll","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"7b54da786f23381a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"261bf5b138165450","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"c8952cee7aa22be3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"3fc42fab330d955b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"1345357952dd5706","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"4cb071edab3c4f18","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"7eb33bac35287641","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"bfe5d6d1deb00c32","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"5edf25e216623b2f","status":"passed","time":{"start":1660104898000,"stop":1660104898090,"duration":90}},{"uid":"e3057d1f988f5870","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"46644bab25a7659b","status":"passed","time":{"start":1660047551000,"stop":1660047551064,"duration":64}}]},"Istio Suite:Istio Suite#[It] Bookinfo tests Bookinfo Scenario 3 without egress and with ingress Should install Productpage on worker cluster-1":{"statistic":{"failed":48,"broken":0,"skipped":0,"passed":9,"unknown":0,"total":57},"items":[{"uid":"853bb1188e2fc585","status":"passed","time":{"start":1663043831000,"stop":1663043856057,"duration":25057}},{"uid":"206e2eb4365a4288","status":"passed","time":{"start":1663043075000,"stop":1663043106640,"duration":31640}},{"uid":"7e99934f1a8adb1f","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1663040338000,"stop":1663040518660,"duration":180660}},{"uid":"89a3559cbc089770","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1662819574000,"stop":1662819754390,"duration":180390}},{"uid":"530cff9d698681fc","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1662795310000,"stop":1662795490260,"duration":180260}},{"uid":"21dc164033e827f8","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1662792773000,"stop":1662792953314,"duration":180314}},{"uid":"6411d347f86937dc","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1662620110000,"stop":1662620290269,"duration":180269}},{"uid":"bf8a63c12861385b","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1662617640000,"stop":1662617820380,"duration":180380}},{"uid":"c2f0b9741c5b7cb7","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1662610377000,"stop":1662610557241,"duration":180241}},{"uid":"7788df0b9b018a58","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1662557911000,"stop":1662558091242,"duration":180242}},{"uid":"3a88290c16b68a19","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1662553747000,"stop":1662553927267,"duration":180267}},{"uid":"81aa9a06bfed4d11","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1662553327000,"stop":1662553507367,"duration":180367}},{"uid":"2fe2731639a83839","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1662381943000,"stop":1662382123614,"duration":180614}},{"uid":"a0629a55c8a7c7fe","status":"passed","time":{"start":1662348103000,"stop":1662348129008,"duration":26008}},{"uid":"299d0616da45209","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1662049889000,"stop":1662050069557,"duration":180557}},{"uid":"3121386ef01898ff","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1662049074000,"stop":1662049254349,"duration":180349}},{"uid":"1c7a15c8ca470251","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1662028179000,"stop":1662028359346,"duration":180346}},{"uid":"bc719db6c7d8cb1f","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1661859205000,"stop":1661859385327,"duration":180327}},{"uid":"a5a23ff0002327ce","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00074e018>: {\n        Underlying: <*exec.ExitError | 0xc0006d4000>{\n            ProcessState: {\n                pid: 6908,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 256167},\n                    Stime: {Sec: 0, Usec: 98526},\n                    Maxrss: 91896,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 13149,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 496,\n                    Nivcsw: 421,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicewithoutegresswithingress\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicewithoutegresswithingress\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicewithoutegresswithingress\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicewithoutegresswithingress\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicewithoutegresswithingress\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\noccurred","time":{"start":1661843950000,"stop":1661843950401,"duration":401}},{"uid":"8b903b42d7078c65","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00060e3d8>: {\n        Underlying: <*exec.ExitError | 0xc0006db460>{\n            ProcessState: {\n                pid: 6936,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 161033},\n                    Stime: {Sec: 0, Usec: 82480},\n                    Maxrss: 85712,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 13306,\n                    Majflt: 1,\n                    Nswap: 0,\n                    Inblock: 4616,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 457,\n                    Nivcsw: 498,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicewithoutegresswithingress\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicewithoutegresswithingress\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicewithoutegresswithingress\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicewithoutegresswithingress\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicewithoutegresswithingress\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661754632000,"stop":1661754632244,"duration":244}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Iperf connectivity should onboard deployment in application Namespaces without slice annotations":{"statistic":{"failed":21,"broken":0,"skipped":29,"passed":1,"unknown":0,"total":51},"items":[{"uid":"7d20d0ca0ca0d4d1","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 1\nto equal\n    <int>: 2","time":{"start":1663044723000,"stop":1663044915461,"duration":192461}},{"uid":"3e9bfa113bca179e","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 1\nto equal\n    <int>: 2","time":{"start":1663044069000,"stop":1663044261779,"duration":192779}},{"uid":"eb87d4a35aed7ac4","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 1\nto equal\n    <int>: 2","time":{"start":1662348908000,"stop":1662349089516,"duration":181516}},{"uid":"483bca29438922cd","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"df2f7aa3c51583a9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"937763fb855eac10","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"679933505d2598e0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"45daf61ad3444d48","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"d4ac226b3e5b63e3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"2e6deea408a38baa","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"d7604b2a3ef79583","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"b1cffe7f741752f3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"dd40c43b37cfcc22","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"3fde2ff935cb2ac3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"c46cb01428e4c7ee","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"b18d26fd781c5b52","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"2fd7096c25e04333","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"1209d2fb5dd692a2","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"d6667b5009dc2e81","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 1\nto equal\n    <int>: 2","time":{"start":1660067259000,"stop":1660067440053,"duration":181053}},{"uid":"3e82acd47805ef73","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 1\nto equal\n    <int>: 2","time":{"start":1660047551000,"stop":1660047732291,"duration":181291}}]},"Worker Suite:Worker Suite#[It] Namespace reconcile test scenarios: Offboard remove label from app ns":{"statistic":{"failed":0,"broken":0,"skipped":10,"passed":41,"unknown":0,"total":51},"items":[{"uid":"bfc4f9a2e2e5723a","status":"skipped","statusDetails":"skipped","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"b2c4c71d143d0beb","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"4643a5166bbd7099","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662348908000,"stop":1662348908000,"duration":0}},{"uid":"150074768115e8b8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"7147981e97376569","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"382d3c4444646e35","status":"passed","time":{"start":1661611962000,"stop":1661611962248,"duration":248}},{"uid":"2c3b977708f7d23a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"51bf8601e2832aff","status":"passed","time":{"start":1661584447000,"stop":1661584447245,"duration":245}},{"uid":"60f922ffac3fa280","status":"passed","time":{"start":1661580045000,"stop":1661580045181,"duration":181}},{"uid":"94bc58ef10cbf8aa","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"7cf8f62b9e4ca910","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"8dd2881a312ad599","status":"passed","time":{"start":1660832281000,"stop":1660832281176,"duration":176}},{"uid":"bd98bd3c5fa6fa56","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"f32a5516494d50f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"13d279c71956fc1b","status":"passed","time":{"start":1660295740000,"stop":1660295740167,"duration":167}},{"uid":"bc48a1037f396a73","status":"passed","time":{"start":1660293657000,"stop":1660293657204,"duration":204}},{"uid":"c592ef78edda75fd","status":"passed","time":{"start":1660113628000,"stop":1660113628146,"duration":146}},{"uid":"8d7e85c04a75cc84","status":"passed","time":{"start":1660104898000,"stop":1660104898168,"duration":168}},{"uid":"5a057c865089ab3f","status":"passed","time":{"start":1660067259000,"stop":1660067259293,"duration":293}},{"uid":"f84cf2131e852cf7","status":"passed","time":{"start":1660047551000,"stop":1660047551172,"duration":172}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting nsm-kernel-forwarder pod Check ping between iperf-server and iperf-client after nsm-kernel-forwarder pod restart":{"statistic":{"failed":0,"broken":0,"skipped":51,"passed":0,"unknown":0,"total":51},"items":[{"uid":"d530f0e879d2ba5b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"eb3b657637037cfc","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"98e76a7f4775e9da","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662348908000,"stop":1662348908000,"duration":0}},{"uid":"6b5feebc218feb53","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"2e36a1a5764448a1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"b40cf7f38609c545","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"c51f1c3761566a68","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"d28889b49e75aa8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"ba31d19c76a2ef14","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"d4b3f0603ab2bba","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"c76c597d4218a1b4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"ba5d82b19eddff91","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"aebc1bd80316ba7e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"85487c84e8ca86f8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"ad8edf6be7023e17","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"3e7806c0562b5ac0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"cb93d54740688c87","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"1f9053edbbdc4cec","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"d5adb1757d825c89","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"ff2adf10077a1759","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}}]},"Istio Suite:Istio Suite#[AfterSuite]":{"statistic":{"failed":126,"broken":23,"skipped":0,"passed":16,"unknown":0,"total":165},"items":[{"uid":"e725001b207e48d9","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000638018>: {\n        Underlying: <*exec.ExitError | 0xc000408040>{\n            ProcessState: {\n                pid: 6132,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 52593},\n                    Stime: {Sec: 0, Usec: 18783},\n                    Maxrss: 44856,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 1930,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 224,\n                    Nivcsw: 153,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    Error: uninstall: Release not loaded: istiod: release: not found\noccurred","time":{"start":1663052393000,"stop":1663052394592,"duration":1592}},{"uid":"647c33fb2cdef772","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000e408>: {\n        Underlying: <*exec.ExitError | 0xc000b12d40>{\n            ProcessState: {\n                pid: 6161,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 46884},\n                    Stime: {Sec: 0, Usec: 23442},\n                    Maxrss: 42356,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2344,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 172,\n                    Nivcsw: 182,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    Error: uninstall: Release not loaded: istiod: release: not found\noccurred","time":{"start":1663052147000,"stop":1663052148521,"duration":1521}},{"uid":"3906ad40dbc0f3ab","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004dc888>: {\n        Underlying: <*exec.ExitError | 0xc0006fd320>{\n            ProcessState: {\n                pid: 6159,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 42423},\n                    Stime: {Sec: 0, Usec: 12726},\n                    Maxrss: 40532,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 1904,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 148,\n                    Nivcsw: 174,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    Error: uninstall: Release not loaded: istiod: release: not found\noccurred","time":{"start":1663050219000,"stop":1663050220280,"duration":1280}},{"uid":"b22d8403c38f18a9","status":"passed","time":{"start":1663043831000,"stop":1663043863029,"duration":32029}},{"uid":"16a025714f0ee3d","status":"passed","time":{"start":1663043075000,"stop":1663043110516,"duration":35516}},{"uid":"38411b5474f78509","status":"failed","statusDetails":"Timed out after 210.008s.\nExpected an error to have occurred.  Got:\n    <nil>: nil","time":{"start":1663040338000,"stop":1663040556776,"duration":218776}},{"uid":"9535921aa1a03d25","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0006020c0>: {\n        Underlying: <*exec.ExitError | 0xc000770200>{\n            ProcessState: {\n                pid: 6880,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 46984},\n                    Stime: {Sec: 0, Usec: 4698},\n                    Maxrss: 44372,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 1912,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 211,\n                    Nivcsw: 219,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    Error: uninstall: Release not loaded: istiod: release: not found\noccurred","time":{"start":1663011012000,"stop":1663011013245,"duration":1245}},{"uid":"fb91313e860244cc","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000622fa8>: {\n        Underlying: <*exec.ExitError | 0xc0000745c0>{\n            ProcessState: {\n                pid: 6171,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 55276},\n                    Stime: {Sec: 0, Usec: 7896},\n                    Maxrss: 43156,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 1882,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 311,\n                    Nivcsw: 174,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    Error: uninstall: Release not loaded: istiod: release: not found\noccurred","time":{"start":1663008428000,"stop":1663008429481,"duration":1481}},{"uid":"f4c504c80960d5be","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0005dcdf8>: {\n        Underlying: <*exec.ExitError | 0xc000662f00>{\n            ProcessState: {\n                pid: 6122,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 34092},\n                    Stime: {Sec: 0, Usec: 20455},\n                    Maxrss: 43752,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 1903,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 359,\n                    Nivcsw: 202,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    Error: uninstall: Release not loaded: istiod: release: not found\noccurred","time":{"start":1662986600000,"stop":1662986601306,"duration":1306}},{"uid":"80827d2abde84db8","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000110390>: {\n        Underlying: <*exec.ExitError | 0xc00010b080>{\n            ProcessState: {\n                pid: 6922,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 45019},\n                    Stime: {Sec: 0, Usec: 12278},\n                    Maxrss: 43208,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 1865,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 150,\n                    Nivcsw: 126,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    Error: uninstall: Release not loaded: istiod: release: not found\noccurred","time":{"start":1662980199000,"stop":1662980200371,"duration":1371}},{"uid":"9388bcc172abe068","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000132bd0>: {\n        Underlying: <*exec.ExitError | 0xc000423cc0>{\n            ProcessState: {\n                pid: 6123,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 49845},\n                    Stime: {Sec: 0, Usec: 9969},\n                    Maxrss: 42200,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2371,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 233,\n                    Nivcsw: 224,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    Error: uninstall: Release not loaded: istiod: release: not found\noccurred","time":{"start":1662961880000,"stop":1662961881214,"duration":1214}},{"uid":"7570f11c6ffa0ef2","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000130708>: {\n        Underlying: <*exec.ExitError | 0xc000884b80>{\n            ProcessState: {\n                pid: 6148,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 34861},\n                    Stime: {Sec: 0, Usec: 23241},\n                    Maxrss: 42296,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 1875,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 232,\n                    Nivcsw: 87,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    Error: uninstall: Release not loaded: istiod: release: not found\noccurred","time":{"start":1662961140000,"stop":1662961141766,"duration":1766}},{"uid":"6d4af1db093d8660","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004afdb8>: {\n        Underlying: <*exec.ExitError | 0xc00038aae0>{\n            ProcessState: {\n                pid: 6161,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 41388},\n                    Stime: {Sec: 0, Usec: 7525},\n                    Maxrss: 43784,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 1920,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 132,\n                    Nivcsw: 119,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    Error: uninstall: Release not loaded: istiod: release: not found\noccurred","time":{"start":1662960982000,"stop":1662960983254,"duration":1254}},{"uid":"851603e0ef994379","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000516ac8>: {\n        Underlying: <*exec.ExitError | 0xc0004a7ce0>{\n            ProcessState: {\n                pid: 6129,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 44849},\n                    Stime: {Sec: 0, Usec: 8154},\n                    Maxrss: 41432,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2696,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 221,\n                    Nivcsw: 157,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    Error: uninstall: Release not loaded: istiod: release: not found\noccurred","time":{"start":1662824919000,"stop":1662824920541,"duration":1541}},{"uid":"3a9ebb9946928bfd","status":"failed","statusDetails":"Timed out after 210.009s.\nExpected an error to have occurred.  Got:\n    <nil>: nil","time":{"start":1662819574000,"stop":1662819793350,"duration":219350}},{"uid":"db03d92eb11459a","status":"failed","statusDetails":"Timed out after 210.010s.\nExpected an error to have occurred.  Got:\n    <nil>: nil","time":{"start":1662795310000,"stop":1662795527305,"duration":217305}},{"uid":"47d0d9a36b0c7c5f","status":"failed","statusDetails":"Timed out after 210.009s.\nExpected an error to have occurred.  Got:\n    <nil>: nil","time":{"start":1662792773000,"stop":1662792990986,"duration":217986}},{"uid":"36793bdcb1253da2","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0006c66c0>: {\n        Underlying: <*exec.ExitError | 0xc00082a5a0>{\n            ProcessState: {\n                pid: 6146,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 28803},\n                    Stime: {Sec: 0, Usec: 14401},\n                    Maxrss: 42840,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 1922,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 153,\n                    Nivcsw: 53,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    Error: uninstall: Release not loaded: istiod: release: not found\noccurred","time":{"start":1662791306000,"stop":1662791307085,"duration":1085}},{"uid":"6d7013323cb7adb8","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00037c5a0>: {\n        Underlying: <*exec.ExitError | 0xc0008e9c20>{\n            ProcessState: {\n                pid: 6128,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 40380},\n                    Stime: {Sec: 0, Usec: 6730},\n                    Maxrss: 43764,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 1868,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 176,\n                    Nivcsw: 260,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    Error: uninstall: Release not loaded: istiod: release: not found\noccurred","time":{"start":1662788545000,"stop":1662788546144,"duration":1144}},{"uid":"bb3a769afdb5cd7d","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000e888>: {\n        Underlying: <*exec.ExitError | 0xc0006e4320>{\n            ProcessState: {\n                pid: 6178,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 56352},\n                    Stime: {Sec: 0, Usec: 4025},\n                    Maxrss: 43668,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 1871,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 173,\n                    Nivcsw: 156,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    Error: uninstall: Release not loaded: istiod: release: not found\noccurred","time":{"start":1662784297000,"stop":1662784298578,"duration":1578}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Creation Should fail while creating Project with  project name as blank":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":193,"unknown":0,"total":193},"items":[{"uid":"9f12001c84608ba4","status":"passed","time":{"start":1663042493000,"stop":1663042493228,"duration":228}},{"uid":"54e7e0f9844326f0","status":"passed","time":{"start":1663041865000,"stop":1663041865250,"duration":250}},{"uid":"e83554a9d1480e8d","status":"passed","time":{"start":1663039902000,"stop":1663039902258,"duration":258}},{"uid":"294cf177f43e433b","status":"passed","time":{"start":1663009431000,"stop":1663009431244,"duration":244}},{"uid":"7cb2dbb664c68c6c","status":"passed","time":{"start":1662978562000,"stop":1662978562293,"duration":293}},{"uid":"6ef158adecb6caa3","status":"passed","time":{"start":1662702362000,"stop":1662702362248,"duration":248}},{"uid":"72d978e5de90c10c","status":"passed","time":{"start":1662553059000,"stop":1662553059265,"duration":265}},{"uid":"4ff95f2953c2bf43","status":"passed","time":{"start":1662553042000,"stop":1662553042250,"duration":250}},{"uid":"22080cfa0a67f678","status":"passed","time":{"start":1662467486000,"stop":1662467486225,"duration":225}},{"uid":"6e55b9fba26d952","status":"passed","time":{"start":1662374048000,"stop":1662374048427,"duration":427}},{"uid":"d68a7e874843630e","status":"passed","time":{"start":1662374250000,"stop":1662374250289,"duration":289}},{"uid":"7634f985e866c082","status":"passed","time":{"start":1662361116000,"stop":1662361116254,"duration":254}},{"uid":"8e331c8be515d7a4","status":"passed","time":{"start":1662347286000,"stop":1662347286263,"duration":263}},{"uid":"ba790fe3ce9c1c24","status":"passed","time":{"start":1662049179000,"stop":1662049179375,"duration":375}},{"uid":"b261413284e66aaa","status":"passed","time":{"start":1662048808000,"stop":1662048808278,"duration":278}},{"uid":"1f5088c802d8165d","status":"passed","time":{"start":1662036991000,"stop":1662036991254,"duration":254}},{"uid":"2867083ed409c6df","status":"passed","time":{"start":1662031654000,"stop":1662031654246,"duration":246}},{"uid":"5d1be1a8c51f097a","status":"passed","time":{"start":1662027354000,"stop":1662027354297,"duration":297}},{"uid":"373aa7429674c97c","status":"passed","time":{"start":1661952242000,"stop":1661952242297,"duration":297}},{"uid":"55b53e208b359203","status":"passed","time":{"start":1661863163000,"stop":1661863163324,"duration":324}}]},"Worker Suite:Worker Suite#[It] Slice Creation scenarios Slice with MEDIUM QOS should have gateway pod running":{"statistic":{"failed":4,"broken":0,"skipped":19,"passed":19,"unknown":0,"total":42},"items":[{"uid":"f86d579c3225a54e","status":"skipped","statusDetails":"skipped","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"3191f4d19c55fca8","status":"skipped","statusDetails":"skipped","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"aca1eb167cb45338","status":"passed","time":{"start":1662348908000,"stop":1662348933088,"duration":25088}},{"uid":"fe8b84be6cb53771","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"9004f78de0591fe0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"a163181453316c56","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"1573d265a1f921eb","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"71ce76ef25872769","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"c8c2c1f3cbd28cca","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"a47f31af68b9fb57","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"703e488c44c4dbb8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"4b48970583d0baf9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"6b1267010d9a1c54","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"672d3c8786f95f79","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"be8ca8ee7c8b0fe7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"9032582f8f13f746","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"bd69a0674582e107","status":"failed","statusDetails":"Timed out after 180.004s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1660113628000,"stop":1660113808378,"duration":180378}},{"uid":"741098da46901686","status":"failed","statusDetails":"Timed out after 180.004s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1660104898000,"stop":1660105078192,"duration":180192}},{"uid":"b9eec9322c615330","status":"passed","time":{"start":1660067259000,"stop":1660067314130,"duration":55130}},{"uid":"af70513e880982f5","status":"passed","time":{"start":1660047551000,"stop":1660047571086,"duration":20086}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests Iperf App connectivity Verify Iperf traffic":{"statistic":{"failed":0,"broken":0,"skipped":44,"passed":7,"unknown":0,"total":51},"items":[{"uid":"fd32469b1f452c0d","status":"skipped","statusDetails":"skipped","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"b76c7de624509035","status":"skipped","statusDetails":"skipped","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"d64a508b4c515d9b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662348908000,"stop":1662348908000,"duration":0}},{"uid":"bbea5e6d6e7b4675","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"b7a3b84ce7cf26c3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"a7708c616edd8226","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"bb1b35d191c2acb9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"3f245019dffdec0e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"2a5331ecc78098db","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"5947ec98cf9bcd2a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"bbc8cd25d94a43e3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"36688e4fe4c16aef","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"528127249e998e0a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"a0bc99e2d05b09c8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"ffc67f6897dc058f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"a194cb0c4891fb5d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"641fe44b3bdaeb15","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"d19d5ebb840dfb4f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"90100b52a9767d0b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"f5feb2f611247554","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}}]},"Worker Suite:Worker Suite#[It] NodeRestartSlice Testing node restart should have vl3 router running":{"statistic":{"failed":32,"broken":0,"skipped":1,"passed":9,"unknown":0,"total":42},"items":[{"uid":"45c1adb0e5444df8","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1663044723000,"stop":1663044783699,"duration":60699}},{"uid":"4efb66f9ff203e8b","status":"skipped","statusDetails":"skipped","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"afff9409c86ce699","status":"passed","time":{"start":1662348908000,"stop":1662348911266,"duration":3266}},{"uid":"22d48265324d9cdb","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1661844205000,"stop":1661844265429,"duration":60429}},{"uid":"d938fcfc2fe7cb6a","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1661754870000,"stop":1661754930344,"duration":60344}},{"uid":"28602454d682aa45","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1661611962000,"stop":1661612022414,"duration":60414}},{"uid":"a78bd7494a2a7eb1","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1661585077000,"stop":1661585137346,"duration":60346}},{"uid":"b9e5cd9ce35047fa","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1661584447000,"stop":1661584507341,"duration":60341}},{"uid":"d1f0304a69722a7","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1661580045000,"stop":1661580105393,"duration":60393}},{"uid":"1d91c9a48cdd7bad","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1661578505000,"stop":1661578565417,"duration":60417}},{"uid":"2fda9ef594df6649","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1661572479000,"stop":1661572539412,"duration":60412}},{"uid":"74753db8d9b87460","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1660832281000,"stop":1660832341295,"duration":60295}},{"uid":"e6f7ca5e15c401fc","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1660808797000,"stop":1660808857312,"duration":60312}},{"uid":"9707232065015d73","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1660299182000,"stop":1660299242232,"duration":60232}},{"uid":"ef52de3f0fa8e531","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1660295740000,"stop":1660295800296,"duration":60296}},{"uid":"4e31aeeec9603f23","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1660293657000,"stop":1660293718457,"duration":61457}},{"uid":"6c1e22e56abddc59","status":"passed","time":{"start":1660113628000,"stop":1660113629169,"duration":1169}},{"uid":"b640df5e971a0db2","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 2\nto equal\n    <int>: 1","time":{"start":1660104898000,"stop":1660104960822,"duration":62822}},{"uid":"d00e68b993ab4788","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1660067259000,"stop":1660067321581,"duration":62581}},{"uid":"c33b3f5965b650ec","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1660047551000,"stop":1660047611568,"duration":60568}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests slice with netpol Should label all application namespaces with kubeslice namespace":{"statistic":{"failed":0,"broken":0,"skipped":39,"passed":12,"unknown":0,"total":51},"items":[{"uid":"b20f4da2175b45","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"67f04061dedad363","status":"passed","time":{"start":1663044069000,"stop":1663044069228,"duration":228}},{"uid":"80ead9258e0dde9c","status":"passed","time":{"start":1662348908000,"stop":1662348908239,"duration":239}},{"uid":"74170158a138b13","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"75692a6a333f26b8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"3f8af537cdf37827","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"75206208c57f5586","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"50588fadb9371bb5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"c1f3f80895673175","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"855229b51cf6146c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"6ead3fd861447c65","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"4b96b562f450a3e3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"c5a6f9b1e5589237","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"ea0f571e1703d74c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"6add127231c55114","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"7837df298b71b402","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"569ccbb531b9abc3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"22e718ca030d26df","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"3857180907e76007","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"67ee87bb3198af92","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice Creation scenarios Slice with MEDIUM QOS should reconcile medium qos slice":{"statistic":{"failed":0,"broken":0,"skipped":23,"passed":19,"unknown":0,"total":42},"items":[{"uid":"5b77c38782bf4ef4","status":"skipped","statusDetails":"skipped","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"e10c1c309cfa7e97","status":"skipped","statusDetails":"skipped","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"b28471853df0626e","status":"passed","time":{"start":1662348908000,"stop":1662348908495,"duration":495}},{"uid":"a342fdeb47ae71f8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"af3ff75705cd6c29","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"21fe891e3bf3854d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"2710389000432fc3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"d710ebe527520350","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"61d574d7c95449aa","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"51b730c4de75916c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"3667e4005b74aa6a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"a88cd6ff713322df","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"2f1612e3bfd1d243","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"3746b2ba88c57bd5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"91900d866d8d3c2b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"57daac34bf55c1af","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"5ba4f84babc0480a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"9ef9632d99cd5c01","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"e716b45d51b81686","status":"passed","time":{"start":1660067259000,"stop":1660067259509,"duration":509}},{"uid":"150e6c7673077fbd","status":"passed","time":{"start":1660047551000,"stop":1660047551461,"duration":461}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Iperf connectivity Verify Connectivity between iperf-server and iperf-client":{"statistic":{"failed":1,"broken":0,"skipped":50,"passed":0,"unknown":0,"total":51},"items":[{"uid":"4e26c9f599aebc63","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"1a51936f010b6639","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"715f01b458fb8a3b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662348908000,"stop":1662348908000,"duration":0}},{"uid":"58956537c26e4720","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"ad17f0b975fdde94","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"561198aa8b5d8fb","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"27c5715c8e0e140","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"c1f288fd03623b1e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"43c8d8d03e6cf987","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"3f97f4c0d1044bc9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"57dd4203843d73cf","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"5ea5e08694755c4c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"7c90237538704011","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"9c9283b2f17a3ebc","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"ed0ba7f9b7b79cff","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"5be50ad18496c366","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"b04bcacbaa34becd","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"1fbdf937900a2a80","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"63f31b30f54bd7e4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"79a6b75a1e593857","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}}]},"Worker Suite:Worker Suite#[It]  QosProfile tests QosProfile delete validation should remove vl3 router from spoke":{"statistic":{"failed":0,"broken":0,"skipped":11,"passed":0,"unknown":0,"total":11},"items":[{"uid":"d2b0a17f2aed919f","status":"skipped","statusDetails":"skipped","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"6da0d933e7c5eac0","status":"skipped","statusDetails":"skipped","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"e13dc9d47891f78c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662348908000,"stop":1662348908000,"duration":0}},{"uid":"928d06893d2bebf","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"6e9ed6f26ded6729","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"cd34f3e751b7eeca","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"47c3e4db18675120","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"141694a06b33f5ec","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"da46f12d4199ed81","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"a445775450d4e3a3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"e493a900973d166a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}}]},"Worker Suite:Worker Suite#[It] NodeRestartSlice Testing node restart iperf connectivity across multi cluster":{"statistic":{"failed":9,"broken":0,"skipped":33,"passed":0,"unknown":0,"total":42},"items":[{"uid":"953bc8875b91d1f1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"11641fdd40682965","status":"skipped","statusDetails":"skipped","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"63136416bc75516e","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <bool>: false\nto be true","time":{"start":1662348908000,"stop":1662348970783,"duration":62783}},{"uid":"b252ab46f7824628","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"31e65391a34de01f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"bab1369012fd84d0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"a3fb2d1abdb0721","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"c25f04ef7ff023c7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"d81dfd73aeb8e4af","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"4ea9883bc65fc442","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"73e56cd0dddbd4fd","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"f4eea7f5e6a5acb9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"27799419333af5b1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"322ffeb50fc37847","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"c123415bfa37b331","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"37637f232c472e3f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"c40ac25977c86333","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <bool>: false\nto be true","time":{"start":1660113628000,"stop":1660113688454,"duration":60454}},{"uid":"cc87a733b2c3b626","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"8b10849df7b7784c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"92a4cbe8f2ecc3e3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}}]},"Hub Suite:Hub Suite#[It] Cluster negative tests Worker Cluster Registration with Wrong token":{"statistic":{"failed":0,"broken":0,"skipped":29,"passed":164,"unknown":0,"total":193},"items":[{"uid":"b61758192c25cdd5","status":"passed","time":{"start":1663042493000,"stop":1663042802416,"duration":309416}},{"uid":"e569601f6b0a3e6b","status":"passed","time":{"start":1663041865000,"stop":1663042176715,"duration":311715}},{"uid":"36a7205a09a8a105","status":"passed","time":{"start":1663039902000,"stop":1663040211868,"duration":309868}},{"uid":"5cfe97d4ff6ea924","status":"passed","time":{"start":1663009431000,"stop":1663009740611,"duration":309611}},{"uid":"fc67d2ea510a1e68","status":"passed","time":{"start":1662978562000,"stop":1662978874268,"duration":312268}},{"uid":"a5d6600e69f350f6","status":"passed","time":{"start":1662702362000,"stop":1662702676908,"duration":314908}},{"uid":"cc030fbc91a59a3f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662553059000,"stop":1662553059000,"duration":0}},{"uid":"9bbd1acb4889c235","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662553042000,"stop":1662553042000,"duration":0}},{"uid":"7acaa3e98f8f8ab4","status":"passed","time":{"start":1662467486000,"stop":1662467619939,"duration":133939}},{"uid":"55425e5541243328","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662374048000,"stop":1662374048000,"duration":0}},{"uid":"d3e8764690e41554","status":"passed","time":{"start":1662374250000,"stop":1662374380733,"duration":130733}},{"uid":"277809881f6383e8","status":"passed","time":{"start":1662361116000,"stop":1662361250608,"duration":134608}},{"uid":"910e8a801020e9e2","status":"passed","time":{"start":1662347286000,"stop":1662347420378,"duration":134378}},{"uid":"7208a65539b8dcac","status":"passed","time":{"start":1662049179000,"stop":1662049316422,"duration":137422}},{"uid":"1bae305c2773f4ca","status":"passed","time":{"start":1662048808000,"stop":1662048938545,"duration":130545}},{"uid":"d91ca9037e3fb881","status":"passed","time":{"start":1662036991000,"stop":1662037125483,"duration":134483}},{"uid":"c4198e8116e9cb24","status":"passed","time":{"start":1662031654000,"stop":1662031784144,"duration":130144}},{"uid":"2c8580ee57256628","status":"passed","time":{"start":1662027354000,"stop":1662027485333,"duration":131333}},{"uid":"76a8eb652292a120","status":"passed","time":{"start":1661952242000,"stop":1661952371330,"duration":129330}},{"uid":"285d677462c3b4b6","status":"passed","time":{"start":1661863163000,"stop":1661863293197,"duration":130197}}]},"Worker Suite:Worker Suite#[It] Slice Creation scenarios Slice with MEDIUM QOS should have vl3 router running":{"statistic":{"failed":17,"broken":0,"skipped":2,"passed":23,"unknown":0,"total":42},"items":[{"uid":"ce0749b8ed26af92","status":"skipped","statusDetails":"skipped","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"564ab726673cac7d","status":"skipped","statusDetails":"skipped","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"e8a238e122bc4c6","status":"passed","time":{"start":1662348908000,"stop":1662348912857,"duration":4857}},{"uid":"da87875a46f2e64d","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0002ba510>: {\n        Underlying: <*exec.ExitError | 0xc000490b20>{\n            ProcessState: {\n                pid: 7287,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 246248},\n                    Stime: {Sec: 0, Usec: 49249},\n                    Maxrss: 89296,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4481,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 428,\n                    Nivcsw: 682,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicemediumqos\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\noccurred","time":{"start":1661844205000,"stop":1661844205686,"duration":686}},{"uid":"4accd64d5510a2f1","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000734b70>: {\n        Underlying: <*exec.ExitError | 0xc00017efc0>{\n            ProcessState: {\n                pid: 7222,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 158146},\n                    Stime: {Sec: 0, Usec: 48660},\n                    Maxrss: 82676,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2742,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 436,\n                    Nivcsw: 450,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicemediumqos\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661754870000,"stop":1661754870285,"duration":285}},{"uid":"c44610eb5ac8323d","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0008cfae8>: {\n        Underlying: <*exec.ExitError | 0xc0006d3be0>{\n            ProcessState: {\n                pid: 7301,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 165625},\n                    Stime: {Sec: 0, Usec: 46715},\n                    Maxrss: 72040,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3838,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 263,\n                    Nivcsw: 385,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicemediumqos\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661611962000,"stop":1661611962277,"duration":277}},{"uid":"7a70c0fedbe84237","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0009d9c68>: {\n        Underlying: <*exec.ExitError | 0xc0007ab3c0>{\n            ProcessState: {\n                pid: 7104,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 177356},\n                    Stime: {Sec: 0, Usec: 61689},\n                    Maxrss: 85328,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3772,\n                    Majflt: 1,\n                    Nswap: 0,\n                    Inblock: 264,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 381,\n                    Nivcsw: 392,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicemediumqos\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661585077000,"stop":1661585077316,"duration":316}},{"uid":"b6a1219c68bedef","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000e0d8>: {\n        Underlying: <*exec.ExitError | 0xc0003b1c40>{\n            ProcessState: {\n                pid: 6247,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 188764},\n                    Stime: {Sec: 0, Usec: 48195},\n                    Maxrss: 87308,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4174,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 311,\n                    Nivcsw: 294,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicemediumqos\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661584447000,"stop":1661584447298,"duration":298}},{"uid":"ef7cb4766ac2f668","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0006d2780>: {\n        Underlying: <*exec.ExitError | 0xc000075b20>{\n            ProcessState: {\n                pid: 7294,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 186662},\n                    Stime: {Sec: 0, Usec: 33599},\n                    Maxrss: 75548,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3565,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 417,\n                    Nivcsw: 441,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicemediumqos\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661580045000,"stop":1661580045307,"duration":307}},{"uid":"7359d02804ffa348","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000779650>: {\n        Underlying: <*exec.ExitError | 0xc0007384e0>{\n            ProcessState: {\n                pid: 7289,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 230185},\n                    Stime: {Sec: 0, Usec: 53709},\n                    Maxrss: 89996,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4300,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 354,\n                    Nivcsw: 414,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicemediumqos\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661578505000,"stop":1661578505364,"duration":364}},{"uid":"e6e7c5b073ce0b9c","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0008a2198>: {\n        Underlying: <*exec.ExitError | 0xc00074c640>{\n            ProcessState: {\n                pid: 7234,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 232731},\n                    Stime: {Sec: 0, Usec: 77577},\n                    Maxrss: 90280,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 8943,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 499,\n                    Nivcsw: 394,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicemediumqos\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661572479000,"stop":1661572479379,"duration":379}},{"uid":"2f33dafb51f6da0b","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004c2a50>: {\n        Underlying: <*exec.ExitError | 0xc00076dec0>{\n            ProcessState: {\n                pid: 7146,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 148251},\n                    Stime: {Sec: 0, Usec: 53535},\n                    Maxrss: 89516,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 5155,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 314,\n                    Nivcsw: 360,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicemediumqos\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660832281000,"stop":1660832281276,"duration":276}},{"uid":"e96433d83a0cae1","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004dd020>: {\n        Underlying: <*exec.ExitError | 0xc0007d47e0>{\n            ProcessState: {\n                pid: 7217,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 130784},\n                    Stime: {Sec: 0, Usec: 54493},\n                    Maxrss: 89336,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3769,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 261,\n                    Nivcsw: 265,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicemediumqos\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660808797000,"stop":1660808797260,"duration":260}},{"uid":"d5c79c81ac0886da","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0005cc258>: {\n        Underlying: <*exec.ExitError | 0xc0007209c0>{\n            ProcessState: {\n                pid: 7120,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 133532},\n                    Stime: {Sec: 0, Usec: 34460},\n                    Maxrss: 85304,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 6881,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 377,\n                    Nivcsw: 265,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicemediumqos\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660299182000,"stop":1660299182227,"duration":227}},{"uid":"6454ddafda446f0c","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0000cf878>: {\n        Underlying: <*exec.ExitError | 0xc00078ba20>{\n            ProcessState: {\n                pid: 7175,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 117176},\n                    Stime: {Sec: 0, Usec: 56418},\n                    Maxrss: 86588,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 6539,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 340,\n                    Nivcsw: 285,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicemediumqos\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660295740000,"stop":1660295740257,"duration":257}},{"uid":"964570ca7b2786e5","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00084d1b8>: {\n        Underlying: <*exec.ExitError | 0xc00047a5a0>{\n            ProcessState: {\n                pid: 7263,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 164428},\n                    Stime: {Sec: 0, Usec: 50593},\n                    Maxrss: 83572,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4337,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 404,\n                    Nivcsw: 363,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicemediumqos\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660293657000,"stop":1660293657302,"duration":302}},{"uid":"e38253dca24054bf","status":"passed","time":{"start":1660113628000,"stop":1660113632531,"duration":4531}},{"uid":"343fb057e308248e","status":"passed","time":{"start":1660104898000,"stop":1660104901714,"duration":3714}},{"uid":"7c86720bcb17df48","status":"passed","time":{"start":1660067259000,"stop":1660067263669,"duration":4669}},{"uid":"8d6b9f872be941b2","status":"passed","time":{"start":1660047551000,"stop":1660047556593,"duration":5593}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Iperf connectivity Verify Iperf traffic":{"statistic":{"failed":0,"broken":0,"skipped":51,"passed":0,"unknown":0,"total":51},"items":[{"uid":"a3164befa5e7c6cb","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"855ee1ab5bba115","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"50e55987c01817b4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662348908000,"stop":1662348908000,"duration":0}},{"uid":"cba352a377063e9c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"272f925f49e31de9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"3f2a3245676109d8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"76437315e1a4425c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"8dc20bcb30943a1d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"53597780f33090d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"1a9eed50c2c2c191","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"b03292f05649a379","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"a97e4cd4fe3823e2","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"72e95de472e49e4e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"a1b002457298af83","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"bd9d6c5987d793f4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"1cc07f65e0a26f0a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"74b0c4a68123bb79","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"4bd7a17867fd253d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"df9833c409829e1a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"3c3b6674a8ab1e1a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice validation Slice with LOW QOS should have vl3 router running":{"statistic":{"failed":2,"broken":0,"skipped":0,"passed":7,"unknown":0,"total":9},"items":[{"uid":"46aca11723b30ab4","status":"passed","time":{"start":1659168739000,"stop":1659168753412,"duration":14412}},{"uid":"23c755beac37ab66","status":"passed","time":{"start":1659164084000,"stop":1659164087272,"duration":3272}},{"uid":"88103c88c0b75f1c","status":"passed","time":{"start":1659160188000,"stop":1659160191197,"duration":3197}},{"uid":"3d2e40ff3ef5d0e5","status":"passed","time":{"start":1659119724000,"stop":1659119725567,"duration":1567}},{"uid":"502e7078fbd0aa84","status":"passed","time":{"start":1659116511000,"stop":1659116511230,"duration":230}},{"uid":"b2daf468d54fc42b","status":"passed","time":{"start":1659109470000,"stop":1659109470577,"duration":577}},{"uid":"e5cba288b1a3ee92","status":"passed","time":{"start":1659106836000,"stop":1659106864747,"duration":28747}},{"uid":"9676d1b60703e180","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1659101522000,"stop":1659101582708,"duration":60708}},{"uid":"3834bb8efced8f8","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 4\nto equal\n    <int>: 1","time":{"start":1659082816000,"stop":1659082876703,"duration":60703}}]},"Hub Suite:Hub Suite#[It]  QosProfile tests QosProfile Create validation Create qos profile- validation of BandwidthCeilingKbps > BandwidthGuaranteedKbps":{"statistic":{"failed":0,"broken":0,"skipped":128,"passed":21,"unknown":0,"total":149},"items":[{"uid":"d767fc83717b0b59","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661501311000,"stop":1661501311000,"duration":0}},{"uid":"77a9d1c91ffe9d4b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661434220000,"stop":1661434220000,"duration":0}},{"uid":"19faeb6c534a07c9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661356056000,"stop":1661356056000,"duration":0}},{"uid":"cf9313be5d59208c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661352461000,"stop":1661352461000,"duration":0}},{"uid":"e6d5a1f3f8c4e663","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661352207000,"stop":1661352207000,"duration":0}},{"uid":"93b327d8c9e08018","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661352028000,"stop":1661352028000,"duration":0}},{"uid":"b2dc580598fa6992","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661351263000,"stop":1661351263000,"duration":0}},{"uid":"ab3b35e695d44f82","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661188463000,"stop":1661188463000,"duration":0}},{"uid":"1de041c4d3f9b9d7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661183038000,"stop":1661183038000,"duration":0}},{"uid":"8f243e486b7e0b1f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661180055000,"stop":1661180055000,"duration":0}},{"uid":"a420b35e1405998f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661152474000,"stop":1661152474000,"duration":0}},{"uid":"6f987cd3f468c846","status":"passed","time":{"start":1661083598000,"stop":1661083604343,"duration":6343}},{"uid":"6c71ca8f0013171e","status":"passed","time":{"start":1661070332000,"stop":1661070338286,"duration":6286}},{"uid":"93a98cae41798b3e","status":"passed","time":{"start":1661066603000,"stop":1661066609334,"duration":6334}},{"uid":"b1d40cbaa6bf14b0","status":"passed","time":{"start":1661018182000,"stop":1661018188253,"duration":6253}},{"uid":"24c3e8b0f4b7b990","status":"passed","time":{"start":1661014620000,"stop":1661014626329,"duration":6329}},{"uid":"40b0168102d2ebfc","status":"passed","time":{"start":1661011831000,"stop":1661011837288,"duration":6288}},{"uid":"357086ff3676e0d3","status":"passed","time":{"start":1661011780000,"stop":1661011786332,"duration":6332}},{"uid":"f3093b0dbc1ea7a4","status":"passed","time":{"start":1661009684000,"stop":1661009690285,"duration":6285}},{"uid":"ab262e3a5aef9b65","status":"passed","time":{"start":1661007831000,"stop":1661007837266,"duration":6266}}]},"Worker Suite:Worker Suite#[It] Attach/Deattach Scenario Attach cluster from slice  when Deattach cluster from slice  should have vl3 router deleted from deattach cluster":{"statistic":{"failed":34,"broken":1,"skipped":0,"passed":7,"unknown":0,"total":42},"items":[{"uid":"c3c130517f1c9959","status":"passed","time":{"start":1663044723000,"stop":1663044814435,"duration":91435}},{"uid":"502531b8d44b3640","status":"passed","time":{"start":1663044069000,"stop":1663044162747,"duration":93747}},{"uid":"b14da86b24ade3de","status":"passed","time":{"start":1662348908000,"stop":1662348940407,"duration":32407}},{"uid":"9748c32287d0c2c1","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000f4b8>: {\n        Underlying: <*exec.ExitError | 0xc00067f2e0>{\n            ProcessState: {\n                pid: 7143,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 250197},\n                    Stime: {Sec: 0, Usec: 81961},\n                    Maxrss: 88952,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 8108,\n                    Majflt: 2,\n                    Nswap: 0,\n                    Inblock: 264,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 575,\n                    Nivcsw: 730,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicetwocluster\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\noccurred","time":{"start":1661844205000,"stop":1661844205383,"duration":383}},{"uid":"f454cd4dc00f3085","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004be480>: {\n        Underlying: <*exec.ExitError | 0xc000658120>{\n            ProcessState: {\n                pid: 7280,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 163878},\n                    Stime: {Sec: 0, Usec: 51961},\n                    Maxrss: 74664,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4218,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 408,\n                    Nivcsw: 514,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicetwocluster\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661754870000,"stop":1661754870202,"duration":202}},{"uid":"164ecf7249dbd691","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0008cf7b8>: {\n        Underlying: <*exec.ExitError | 0xc0006d3360>{\n            ProcessState: {\n                pid: 7271,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 188173},\n                    Stime: {Sec: 0, Usec: 34562},\n                    Maxrss: 73440,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3764,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 331,\n                    Nivcsw: 517,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicetwocluster\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661611962000,"stop":1661611962228,"duration":228}},{"uid":"772d7e0693e2cb4","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00090c270>: {\n        Underlying: <*exec.ExitError | 0xc000a80ac0>{\n            ProcessState: {\n                pid: 7133,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 166559},\n                    Stime: {Sec: 0, Usec: 66623},\n                    Maxrss: 76488,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3259,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 376,\n                    Nivcsw: 507,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicetwocluster\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661585077000,"stop":1661585077214,"duration":214}},{"uid":"115dded2744bbe29","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00013a9f0>: {\n        Underlying: <*exec.ExitError | 0xc0006aa2a0>{\n            ProcessState: {\n                pid: 6193,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 221008},\n                    Stime: {Sec: 0, Usec: 29189},\n                    Maxrss: 69624,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3562,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 404,\n                    Nivcsw: 566,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicetwocluster\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661584447000,"stop":1661584447267,"duration":267}},{"uid":"d64bb4fb2f5188a6","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0006d2378>: {\n        Underlying: <*exec.ExitError | 0xc00013e620>{\n            ProcessState: {\n                pid: 7065,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 175824},\n                    Stime: {Sec: 0, Usec: 69511},\n                    Maxrss: 84816,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 10031,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 461,\n                    Nivcsw: 577,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicetwocluster\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661580045000,"stop":1661580045275,"duration":275}},{"uid":"13d09ae5114db91d","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00016e348>: {\n        Underlying: <*exec.ExitError | 0xc0000dae20>{\n            ProcessState: {\n                pid: 7068,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 296073},\n                    Stime: {Sec: 0, Usec: 143682},\n                    Maxrss: 79340,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 11281,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 644,\n                    Nivcsw: 643,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicetwocluster\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661578505000,"stop":1661578506579,"duration":1579}},{"uid":"6d94ff61d86e5d84","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00083aff0>: {\n        Underlying: <*exec.ExitError | 0xc000680040>{\n            ProcessState: {\n                pid: 7316,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 267549},\n                    Stime: {Sec: 0, Usec: 39932},\n                    Maxrss: 89432,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3507,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 317,\n                    Nivcsw: 479,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicetwocluster\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661572479000,"stop":1661572479317,"duration":317}},{"uid":"ce8bf07087d48db","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004c3668>: {\n        Underlying: <*exec.ExitError | 0xc00086e180>{\n            ProcessState: {\n                pid: 7106,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 150399},\n                    Stime: {Sec: 0, Usec: 57505},\n                    Maxrss: 83564,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 5957,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 366,\n                    Nivcsw: 376,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicetwocluster\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660832281000,"stop":1660832281200,"duration":200}},{"uid":"dd97ab75259c6dfc","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000f938>: {\n        Underlying: <*exec.ExitError | 0xc0006ff640>{\n            ProcessState: {\n                pid: 7024,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 191010},\n                    Stime: {Sec: 0, Usec: 62370},\n                    Maxrss: 83420,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 7217,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 485,\n                    Nivcsw: 457,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicetwocluster\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660808797000,"stop":1660808797245,"duration":245}},{"uid":"3b25b12501699b75","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000442ed0>: {\n        Underlying: <*exec.ExitError | 0xc0006ef680>{\n            ProcessState: {\n                pid: 7247,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 132812},\n                    Stime: {Sec: 0, Usec: 30989},\n                    Maxrss: 93756,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3766,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 372,\n                    Nivcsw: 261,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicetwocluster\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660299182000,"stop":1660299182166,"duration":166}},{"uid":"7b396665c2d82388","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004cadb0>: {\n        Underlying: <*exec.ExitError | 0xc0007937c0>{\n            ProcessState: {\n                pid: 7105,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 196941},\n                    Stime: {Sec: 0, Usec: 74317},\n                    Maxrss: 77184,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4905,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 421,\n                    Nivcsw: 556,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicetwocluster\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660295740000,"stop":1660295741128,"duration":1128}},{"uid":"835871cd8b9f80b","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000f770>: {\n        Underlying: <*exec.ExitError | 0xc00085b300>{\n            ProcessState: {\n                pid: 7218,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 171448},\n                    Stime: {Sec: 0, Usec: 55907},\n                    Maxrss: 91464,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 7067,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 484,\n                    Nivcsw: 452,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicetwocluster\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660293657000,"stop":1660293657268,"duration":268}},{"uid":"75aedaf35c3f7f72","status":"failed","statusDetails":"Timed out after 180.004s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1660113628000,"stop":1660113808241,"duration":180241}},{"uid":"b9f986577f69cc6c","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1660104898000,"stop":1660105081644,"duration":183644}},{"uid":"f9394230d84aa75f","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 3\nto equal\n    <int>: 1","time":{"start":1660067259000,"stop":1660067319210,"duration":60210}},{"uid":"b5795c2e99782bba","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <int>: 4\nto equal\n    <int>: 1","time":{"start":1660047551000,"stop":1660047611447,"duration":60447}}]},"Hub Suite:Hub Suite#[BeforeSuite]":{"statistic":{"failed":60,"broken":0,"skipped":0,"passed":193,"unknown":0,"total":253},"items":[{"uid":"f0037896307bfd27","status":"passed","time":{"start":1663042493000,"stop":1663042579701,"duration":86701}},{"uid":"129292b32755bfb7","status":"passed","time":{"start":1663041865000,"stop":1663041968551,"duration":103551}},{"uid":"5527af389a7b1b78","status":"passed","time":{"start":1663039902000,"stop":1663039988799,"duration":86799}},{"uid":"3f9e6dc5d85ad084","status":"passed","time":{"start":1663009431000,"stop":1663009529039,"duration":98039}},{"uid":"2df2c11ddf132f17","status":"passed","time":{"start":1662978562000,"stop":1662978656903,"duration":94903}},{"uid":"a23b134f3d5eefb3","status":"passed","time":{"start":1662702362000,"stop":1662702454160,"duration":92160}},{"uid":"ae83d19f4acbe5ef","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004ee048>: {\n        Underlying: <*exec.ExitError | 0xc0004fe000>{\n            ProcessState: {\n                pid: 6040,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 2, Usec: 572879},\n                    Stime: {Sec: 0, Usec: 477574},\n                    Maxrss: 94256,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 9002,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 16200,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 38298,\n                    Nivcsw: 9310,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-0.2.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 26 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 26 resources with timeout of 5m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-0.2.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 26 resource(s)\n    wait.go:48: [debug] beginning wait for 26 resources with timeout of 5m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-controller\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-manager-metrics-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-controller-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-editor-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"workerslicegateways.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"clusters.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"projects.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"serviceexportconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"sliceconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workerserviceimports.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workersliceconfigs.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-serving-cert\" Certificate\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-selfsigned-issuer\" Issuer\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-validating-webhook-configuration\" ValidatingWebhookConfiguration\n    uninstall.go:144: [debug] purge requested for kubeslice-controller\n    Error: INSTALLATION FAILED: release kubeslice-controller failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-controller failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1662699422000,"stop":1662699820418,"duration":398418}},{"uid":"e731e295329ce31b","status":"passed","time":{"start":1662553059000,"stop":1662553146241,"duration":87241}},{"uid":"9b32f79cdc68ec36","status":"passed","time":{"start":1662553042000,"stop":1662553139389,"duration":97389}},{"uid":"7a3406b1fd17c5cb","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004c6180>: {\n        Underlying: <*exec.ExitError | 0xc000074000>{\n            ProcessState: {\n                pid: 6018,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 1, Usec: 712818},\n                    Stime: {Sec: 0, Usec: 239878},\n                    Maxrss: 90032,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 5182,\n                    Majflt: 2,\n                    Nswap: 0,\n                    Inblock: 384,\n                    Oublock: 12840,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 18234,\n                    Nivcsw: 5118,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-0.2.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 26 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 26 resources with timeout of 2m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                      ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-0.2.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 26 resource(s)\n    wait.go:48: [debug] beginning wait for 26 resources with timeout of 2m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-controller\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-manager-metrics-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-controller-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-editor-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"workerslicegateways.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"clusters.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"projects.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"serviceexportconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"sliceconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workerserviceimports.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workersliceconfigs.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-serving-cert\" Certificate\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-selfsigned-issuer\" Issuer\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-validating-webhook-configuration\" ValidatingWebhookConfiguration\n    uninstall.go:144: [debug] purge requested for kubeslice-controller\n    Error: INSTALLATION FAILED: release kubeslice-controller failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-controller failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1662538997000,"stop":1662539206225,"duration":209225}},{"uid":"115186eeb811f2e1","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00047e210>: {\n        Underlying: <*exec.ExitError | 0xc00048a000>{\n            ProcessState: {\n                pid: 6017,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 1, Usec: 527412},\n                    Stime: {Sec: 0, Usec: 257478},\n                    Maxrss: 94268,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 11772,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 16200,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 16928,\n                    Nivcsw: 4975,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-0.2.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 26 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 26 resources with timeout of 2m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                       ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-0.2.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 26 resource(s)\n    wait.go:48: [debug] beginning wait for 26 resources with timeout of 2m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-controller\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-manager-metrics-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-controller-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-editor-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"serviceexportconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"clusters.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"projects.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workerslicegateways.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"sliceconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workerserviceimports.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workersliceconfigs.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-serving-cert\" Certificate\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-selfsigned-issuer\" Issuer\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-validating-webhook-configuration\" ValidatingWebhookConfiguration\n    uninstall.go:144: [debug] purge requested for kubeslice-controller\n    Error: INSTALLATION FAILED: release kubeslice-controller failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-controller failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1662487441000,"stop":1662487650213,"duration":209213}},{"uid":"346bac1d84bf5fdd","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000354060>: {\n        Underlying: <*exec.ExitError | 0xc00014a000>{\n            ProcessState: {\n                pid: 6022,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 1, Usec: 383389},\n                    Stime: {Sec: 0, Usec: 260124},\n                    Maxrss: 94476,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 12800,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 16200,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 16833,\n                    Nivcsw: 4216,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-0.2.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 26 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 26 resources with timeout of 2m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                       ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-0.2.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 26 resource(s)\n    wait.go:48: [debug] beginning wait for 26 resources with timeout of 2m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-controller\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-manager-metrics-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-controller-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-editor-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"workerslicegateways.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"clusters.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"projects.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"serviceexportconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"sliceconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workerserviceimports.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workersliceconfigs.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-serving-cert\" Certificate\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-selfsigned-issuer\" Issuer\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-validating-webhook-configuration\" ValidatingWebhookConfiguration\n    uninstall.go:144: [debug] purge requested for kubeslice-controller\n    Error: INSTALLATION FAILED: release kubeslice-controller failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-controller failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1662484875000,"stop":1662485083653,"duration":208653}},{"uid":"2cb3b193d3a6a0d0","status":"passed","time":{"start":1662467486000,"stop":1662467582315,"duration":96315}},{"uid":"a739ce7797f38cb1","status":"passed","time":{"start":1662374048000,"stop":1662374139854,"duration":91854}},{"uid":"22aac05df0bad03e","status":"passed","time":{"start":1662374250000,"stop":1662374348841,"duration":98841}},{"uid":"fb3ffdfb42745ba1","status":"passed","time":{"start":1662361116000,"stop":1662361202611,"duration":86611}},{"uid":"5d5e5be9428b70c1","status":"passed","time":{"start":1662347286000,"stop":1662347380201,"duration":94201}},{"uid":"9ecc5681fa9b1040","status":"passed","time":{"start":1662049179000,"stop":1662049278768,"duration":99768}},{"uid":"10fa0f1f67143bb5","status":"passed","time":{"start":1662048808000,"stop":1662048906135,"duration":98135}},{"uid":"9048bfbfa6c3d375","status":"passed","time":{"start":1662036991000,"stop":1662037092746,"duration":101746}}]},"Worker Suite:Worker Suite#[It] Slice creation scenarios: when slice red is installed workers should have vl3 router pods running":{"statistic":{"failed":23,"broken":0,"skipped":13,"passed":15,"unknown":0,"total":51},"items":[{"uid":"aaf8d84553250793","status":"passed","time":{"start":1663044723000,"stop":1663044729032,"duration":6032}},{"uid":"3c1875d347274211","status":"passed","time":{"start":1663044069000,"stop":1663044071021,"duration":2021}},{"uid":"f22e65ae4ae3a09d","status":"passed","time":{"start":1662348908000,"stop":1662348910025,"duration":2025}},{"uid":"476f7dc314b3bce0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"98c3a37734d63a3f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"3b7ccc813960397c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"4c006875a54ffd0f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"8753af89e1455bd9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"37bb0eff2eea1b2f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"bcbe41593efc2599","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"10487af7ba0b9060","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"c1444d947f57f320","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"bba39c270532456","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"a721720e8cf1c1e1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"43f281e297cc3e49","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"e1a9928d14cd30e3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"a00e71172c2c972f","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 3\nto equal\n    <int>: 1","time":{"start":1660113628000,"stop":1660113688206,"duration":60206}},{"uid":"b00389e53c48f1bc","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 4\nto equal\n    <int>: 1","time":{"start":1660104898000,"stop":1660104958219,"duration":60219}},{"uid":"f86ec96bb86b5ca1","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 4\nto equal\n    <int>: 1","time":{"start":1660067259000,"stop":1660067319170,"duration":60170}},{"uid":"f42a9323b9da382f","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 4\nto equal\n    <int>: 1","time":{"start":1660047551000,"stop":1660047611173,"duration":60173}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests Iperf App connectivity Application namespaces should be isolated":{"statistic":{"failed":7,"broken":0,"skipped":44,"passed":0,"unknown":0,"total":51},"items":[{"uid":"f3cfad3f43784680","status":"skipped","statusDetails":"skipped","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"e60ddc5af962d85b","status":"skipped","statusDetails":"skipped","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"9c5aca12a707b5c7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662348908000,"stop":1662348908000,"duration":0}},{"uid":"ce8bfceeb4ba4f89","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"122e120bcd6fb0a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"7600059cd7c06648","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"92fd8f5d5e1fa968","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"29cb0009afab34e1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"4416d79eee90bcfc","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"5b484616f871c2a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"bdcfed8721ed3c00","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"b44be581377fe210","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"d97c93c657a33794","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"9725a16fe4e8c7af","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"1c93cf49c8f0cdca","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"eb9932417827ecc4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"4fc9286750e399a5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"6d3f1759fe352220","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"2aa3f287d14b3323","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"e500e1e7697014b0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}}]},"Istio Suite:Istio Suite#[It] Bookinfo tests Bookinfo Scenario 3 without egress and with ingress Should install Details, Reviews, Ratings and Serviceexport on worker cluster-2":{"statistic":{"failed":0,"broken":0,"skipped":48,"passed":9,"unknown":0,"total":57},"items":[{"uid":"492b0e1bda1ecc64","status":"passed","time":{"start":1663043831000,"stop":1663043861188,"duration":30188}},{"uid":"f3852b2f23f2705","status":"passed","time":{"start":1663043075000,"stop":1663043111216,"duration":36216}},{"uid":"1feb437e0ab97441","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663040338000,"stop":1663040338000,"duration":0}},{"uid":"e11b7150a1c61c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662819574000,"stop":1662819574000,"duration":0}},{"uid":"5d08acbe5af5717b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662795310000,"stop":1662795310000,"duration":0}},{"uid":"f42bb6d506ec4711","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662792773000,"stop":1662792773000,"duration":0}},{"uid":"9d83ab467006d8c8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662620110000,"stop":1662620110000,"duration":0}},{"uid":"14390845be7c02d1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662617640000,"stop":1662617640000,"duration":0}},{"uid":"ff2978a002d319f2","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662610377000,"stop":1662610377000,"duration":0}},{"uid":"1671c22c3da4887a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662557911000,"stop":1662557911000,"duration":0}},{"uid":"3a1e910d82358e79","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662553747000,"stop":1662553747000,"duration":0}},{"uid":"7d1bf53b6d2f9737","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662553327000,"stop":1662553327000,"duration":0}},{"uid":"a8aea99164528144","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662381943000,"stop":1662381943000,"duration":0}},{"uid":"ce20601a0374d68","status":"passed","time":{"start":1662348103000,"stop":1662348134198,"duration":31198}},{"uid":"18109251c2c4c2be","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662049889000,"stop":1662049889000,"duration":0}},{"uid":"d932a72719648332","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662049074000,"stop":1662049074000,"duration":0}},{"uid":"f2354bd96f4213ea","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662028179000,"stop":1662028179000,"duration":0}},{"uid":"2fdf363b67723759","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661859205000,"stop":1661859205000,"duration":0}},{"uid":"58dfecb5d41a2438","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661843950000,"stop":1661843950000,"duration":0}},{"uid":"94137779412da5d3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754632000,"stop":1661754632000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice deletion tests: Slice is installed & validated Slice object is deleted from hub should remove gateway pod from spoke":{"statistic":{"failed":0,"broken":0,"skipped":41,"passed":10,"unknown":0,"total":51},"items":[{"uid":"92dfa97c7c306f6c","status":"skipped","statusDetails":"skipped","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"ad6b4ecc8c4ee565","status":"passed","time":{"start":1663044069000,"stop":1663044069011,"duration":11}},{"uid":"78bbfe3551b2b3f0","status":"passed","time":{"start":1662348908000,"stop":1662348910037,"duration":2037}},{"uid":"bd061ee4c55ded7a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"ef2fd20991551d83","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"b31b5567fa765444","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"cc6c4907abc995f3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"62325b4cb464bbd7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"819e6bd0e15c744e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"e2607ff6685b72d2","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"b150432b4c14fba2","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"7c3e21a2d4fd850d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"5c3f709f3f5fe60e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"929fcfa78387f8af","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"2a9ef5d74ac33d05","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"fe7b4d71a5e23fcd","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"5327462f8ff4e436","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"7d7598ffdfd14785","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"e41c10057cdaa0b4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"ea78d1c0af1b8979","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}}]},"Hub Suite:Hub Suite#[It] Cluster negative tests Worker Cluster Registration with Wrong ca.cert":{"statistic":{"failed":0,"broken":0,"skipped":29,"passed":164,"unknown":0,"total":193},"items":[{"uid":"ac0592091ea6e0f5","status":"passed","time":{"start":1663042493000,"stop":1663042802567,"duration":309567}},{"uid":"ad66c433b62e27e7","status":"passed","time":{"start":1663041865000,"stop":1663042182057,"duration":317057}},{"uid":"460a4a37f8d07143","status":"passed","time":{"start":1663039902000,"stop":1663040211556,"duration":309556}},{"uid":"ad0c678d938e10ae","status":"passed","time":{"start":1663009431000,"stop":1663009740871,"duration":309871}},{"uid":"208835100854be75","status":"passed","time":{"start":1662978562000,"stop":1662978873396,"duration":311396}},{"uid":"1ce2396e25d97c4e","status":"passed","time":{"start":1662702362000,"stop":1662702677087,"duration":315087}},{"uid":"40007cb25ac10b88","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662553059000,"stop":1662553059000,"duration":0}},{"uid":"f80a1dc4e95232db","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662553042000,"stop":1662553042000,"duration":0}},{"uid":"72881409e79168f3","status":"passed","time":{"start":1662467486000,"stop":1662467614984,"duration":128984}},{"uid":"4b14b8080cbc8b8f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662374048000,"stop":1662374048000,"duration":0}},{"uid":"711446184a0f49","status":"passed","time":{"start":1662374250000,"stop":1662374387274,"duration":137274}},{"uid":"5941467219026c0e","status":"passed","time":{"start":1662361116000,"stop":1662361251817,"duration":135817}},{"uid":"cad8fdc359ebd393","status":"passed","time":{"start":1662347286000,"stop":1662347421565,"duration":135565}},{"uid":"f6184572a79ba7af","status":"passed","time":{"start":1662049179000,"stop":1662049310341,"duration":131341}},{"uid":"86189eaf3645518f","status":"passed","time":{"start":1662048808000,"stop":1662048937946,"duration":129946}},{"uid":"5b1bf8c81b1bfd17","status":"passed","time":{"start":1662036991000,"stop":1662037126968,"duration":135968}},{"uid":"a06995a90a9a4ca4","status":"passed","time":{"start":1662031654000,"stop":1662031784123,"duration":130123}},{"uid":"1eea1597cfae80c9","status":"passed","time":{"start":1662027354000,"stop":1662027485672,"duration":131672}},{"uid":"b30aada796df273b","status":"passed","time":{"start":1661952242000,"stop":1661952377175,"duration":135175}},{"uid":"fbd237086429c7df","status":"passed","time":{"start":1661863163000,"stop":1661863299097,"duration":136097}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Creation Should fail while creating service accounts as combination of special characters":{"statistic":{"failed":6,"broken":0,"skipped":0,"passed":187,"unknown":0,"total":193},"items":[{"uid":"38c617e7db459ef3","status":"passed","time":{"start":1663042493000,"stop":1663042493268,"duration":268}},{"uid":"94213c5f28da4fa5","status":"passed","time":{"start":1663041865000,"stop":1663041865357,"duration":357}},{"uid":"5a85410dec120194","status":"passed","time":{"start":1663039902000,"stop":1663039902240,"duration":240}},{"uid":"4dff7c217bee0257","status":"passed","time":{"start":1663009431000,"stop":1663009431256,"duration":256}},{"uid":"8ca7de1969ddd08c","status":"passed","time":{"start":1662978562000,"stop":1662978562359,"duration":359}},{"uid":"f0789eab8e8b0256","status":"passed","time":{"start":1662702362000,"stop":1662702362265,"duration":265}},{"uid":"78555d988bde4d71","status":"passed","time":{"start":1662553059000,"stop":1662553059279,"duration":279}},{"uid":"7d672d9bd09050f","status":"passed","time":{"start":1662553042000,"stop":1662553042273,"duration":273}},{"uid":"e8f2814f50f539a3","status":"passed","time":{"start":1662467486000,"stop":1662467486266,"duration":266}},{"uid":"a76120fb6d4b89d8","status":"passed","time":{"start":1662374048000,"stop":1662374048399,"duration":399}},{"uid":"bdc1957712126b1d","status":"passed","time":{"start":1662374250000,"stop":1662374250368,"duration":368}},{"uid":"e2d62bfc3e27766","status":"passed","time":{"start":1662361116000,"stop":1662361116275,"duration":275}},{"uid":"2eaeffac1fe276f6","status":"passed","time":{"start":1662347286000,"stop":1662347286247,"duration":247}},{"uid":"83caf0d0fbdc76f5","status":"passed","time":{"start":1662049179000,"stop":1662049179438,"duration":438}},{"uid":"2c828c8e750a5930","status":"passed","time":{"start":1662048808000,"stop":1662048808311,"duration":311}},{"uid":"d9ea0ae506b751c2","status":"passed","time":{"start":1662036991000,"stop":1662036991246,"duration":246}},{"uid":"b7c1cda65f6514e8","status":"passed","time":{"start":1662031654000,"stop":1662031654309,"duration":309}},{"uid":"2d32410c4751b02d","status":"passed","time":{"start":1662027354000,"stop":1662027354314,"duration":314}},{"uid":"b28bbaad1d27d35b","status":"passed","time":{"start":1661952242000,"stop":1661952242285,"duration":285}},{"uid":"5b87b5ac13d0675e","status":"passed","time":{"start":1661863163000,"stop":1661863163310,"duration":310}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting nsm-kernel-forwarder pod Should restart nsm-kernel-forwarder pod":{"statistic":{"failed":0,"broken":0,"skipped":51,"passed":0,"unknown":0,"total":51},"items":[{"uid":"bab1604755dc4f8f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"f54c44ba325a682e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"c7df81c0d0a6faec","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662348908000,"stop":1662348908000,"duration":0}},{"uid":"3b26182dcdc41865","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"2f03587f0d886acb","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"140f31bc64e92cd9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"d3746a0a44be434","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"d73c6c777539b8c6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"bf81a7bb2f799acc","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"1e60db2b48bcf957","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"728bbf6484958452","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"e0e3cba859ce36b4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"84e51ba34eca9079","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"3fe899c8a904648b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"24c1d41b52cd0959","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"a9c8f0c2915b4ccf","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"88de0417b8c590c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"8c4d338133296a87","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"1a9fba1a18cfb37b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"be3b4ae9d6ba3d61","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting nsm-manager pod Check ping between iperf-server and iperf-client after nsm-manager pod restart":{"statistic":{"failed":0,"broken":0,"skipped":51,"passed":0,"unknown":0,"total":51},"items":[{"uid":"dab16a7695d78d76","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"79e0cec45a9dbf6d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"9356e4aa13eb0ddf","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662348908000,"stop":1662348908000,"duration":0}},{"uid":"2963636467deefac","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"b667a924a5a68b73","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"5f3053d2fea5293b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"f70945cabffa5e34","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"f094bcabe7d18f25","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"8c4226a47f1f2e4d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"d1642c78040ad3a8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"fa7040522711ce5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"48141d0fcaa46345","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"4f7746b37e6c2587","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"1e22290a9c5deab2","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"4040b4d0da3ab12d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"2b1b10be263ccda4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"5c30549487d0ea09","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"6ea7cf1fd8397dbe","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"9c041fb0e17fd02c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"78cd510da75148c6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Creation Should fail while creating Project with project name as combination of special characters":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":193,"unknown":0,"total":193},"items":[{"uid":"a392c11bbcbd30b1","status":"passed","time":{"start":1663042493000,"stop":1663042493242,"duration":242}},{"uid":"1931c97603fda317","status":"passed","time":{"start":1663041865000,"stop":1663041865273,"duration":273}},{"uid":"d1f0461eb74c4d29","status":"passed","time":{"start":1663039902000,"stop":1663039902227,"duration":227}},{"uid":"ed9b4faa7346f2fe","status":"passed","time":{"start":1663009431000,"stop":1663009431280,"duration":280}},{"uid":"322b8c1195994077","status":"passed","time":{"start":1662978562000,"stop":1662978562352,"duration":352}},{"uid":"4adc96248f4060c1","status":"passed","time":{"start":1662702362000,"stop":1662702362253,"duration":253}},{"uid":"fe324a63556f89ea","status":"passed","time":{"start":1662553059000,"stop":1662553059287,"duration":287}},{"uid":"a1ec0d68eccf0bd8","status":"passed","time":{"start":1662553042000,"stop":1662553042249,"duration":249}},{"uid":"4db0c0b0d21898b6","status":"passed","time":{"start":1662467486000,"stop":1662467486222,"duration":222}},{"uid":"ccc07f20ad9b3fb1","status":"passed","time":{"start":1662374048000,"stop":1662374048434,"duration":434}},{"uid":"fc916e5a8c674226","status":"passed","time":{"start":1662374250000,"stop":1662374250352,"duration":352}},{"uid":"f1f4b56924ffe184","status":"passed","time":{"start":1662361116000,"stop":1662361116258,"duration":258}},{"uid":"4dca6ac71e49107","status":"passed","time":{"start":1662347286000,"stop":1662347286248,"duration":248}},{"uid":"379ece6a60192766","status":"passed","time":{"start":1662049179000,"stop":1662049179358,"duration":358}},{"uid":"696f9efef060026f","status":"passed","time":{"start":1662048808000,"stop":1662048808303,"duration":303}},{"uid":"af91d25aa53fb1a4","status":"passed","time":{"start":1662036991000,"stop":1662036991253,"duration":253}},{"uid":"3ca691c0a77cefa","status":"passed","time":{"start":1662031654000,"stop":1662031654255,"duration":255}},{"uid":"50030621e2991daf","status":"passed","time":{"start":1662027354000,"stop":1662027354291,"duration":291}},{"uid":"7aa71ef6fac7ca9e","status":"passed","time":{"start":1661952242000,"stop":1661952242279,"duration":279}},{"uid":"f4491ef19babe001","status":"passed","time":{"start":1661863163000,"stop":1661863163284,"duration":284}}]},"Istio Suite:Istio Suite#[It] Bookinfo tests Bookinfo Scenario 2 with egress and without ingress Should verify if Curl Traffic works on Productpage test":{"statistic":{"failed":0,"broken":0,"skipped":47,"passed":10,"unknown":0,"total":57},"items":[{"uid":"8af5a32132cf69c0","status":"passed","time":{"start":1663043831000,"stop":1663043834532,"duration":3532}},{"uid":"177700ac9a9d453b","status":"passed","time":{"start":1663043075000,"stop":1663043078638,"duration":3638}},{"uid":"61ba29170d529d9f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663040338000,"stop":1663040338000,"duration":0}},{"uid":"91d056611138139f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662819574000,"stop":1662819574000,"duration":0}},{"uid":"9e46f414730e7af1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662795310000,"stop":1662795310000,"duration":0}},{"uid":"94f21693848fa693","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662792773000,"stop":1662792773000,"duration":0}},{"uid":"3884683cfd049a14","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662620110000,"stop":1662620110000,"duration":0}},{"uid":"3d97d3b54c93fa78","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662617640000,"stop":1662617640000,"duration":0}},{"uid":"1f9744ddbff25d18","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662610377000,"stop":1662610377000,"duration":0}},{"uid":"c2081fadc824754c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662557911000,"stop":1662557911000,"duration":0}},{"uid":"db5bc96d54f80d57","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662553747000,"stop":1662553747000,"duration":0}},{"uid":"3afd81a2e6f5312","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662553327000,"stop":1662553327000,"duration":0}},{"uid":"cc5ab983c6469400","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662381943000,"stop":1662381943000,"duration":0}},{"uid":"77176eef0b007eb3","status":"passed","time":{"start":1662348103000,"stop":1662348112454,"duration":9454}},{"uid":"544f3c5eb3aab7bb","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662049889000,"stop":1662049889000,"duration":0}},{"uid":"e3b666d8d96db08d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662049074000,"stop":1662049074000,"duration":0}},{"uid":"230b39ae99e61723","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662028179000,"stop":1662028179000,"duration":0}},{"uid":"1611055c42654d77","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661859205000,"stop":1661859205000,"duration":0}},{"uid":"1f48cfaf3eb586e8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661843950000,"stop":1661843950000,"duration":0}},{"uid":"739564e9270c9a05","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754632000,"stop":1661754632000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Hub netpol scenarios: Slice Configs Netpol scenarios Should fail while creating namespaces with same name in both allowedNamespace and applicationNamespace":{"statistic":{"failed":0,"broken":0,"skipped":23,"passed":28,"unknown":0,"total":51},"items":[{"uid":"be64c54d88cfb966","status":"skipped","statusDetails":"skipped","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"976cc37ea47f59a8","status":"passed","time":{"start":1663044069000,"stop":1663044069370,"duration":370}},{"uid":"d3698fd735697f7b","status":"passed","time":{"start":1662348908000,"stop":1662348908445,"duration":445}},{"uid":"8a4c51547048f39d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"11808bec17b61f1a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"5d1f3f52129e7611","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"49e0f00780778633","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"d4f1dbf5c8b4aec6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"d45ad3b4e794d652","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"8c4986df27fa89fd","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"8aa4c90b54a1b266","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"bac4c1d7400b2452","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"94198843ab048a4b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"f73da2f7d60ef140","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"ca06e6e756849081","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"86c8b7e171227643","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"b295c3bc52481d14","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"cacb1bf0c65dc3b6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"b18e8be3034e07a0","status":"passed","time":{"start":1660067259000,"stop":1660067259327,"duration":327}},{"uid":"a599479aa6ba95f3","status":"passed","time":{"start":1660047551000,"stop":1660047551345,"duration":345}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting iperf-server pod Should restart iperf-server pod":{"statistic":{"failed":0,"broken":0,"skipped":51,"passed":0,"unknown":0,"total":51},"items":[{"uid":"8081250bbe48fb2","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"f65b4144331c01ac","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"5bcc8940d841feb7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662348908000,"stop":1662348908000,"duration":0}},{"uid":"8965ffd1cd973fea","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"2f201c39f2637de4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"f4a8568c77581b01","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"aeba418482a7bd02","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"3bbc65071655101f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"ca7d31b55f4212d2","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"d87857ed33ac6263","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"909b275241ff49cb","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"3296b687f015ec6c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"414d751d0312a41f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"b0b94777d2cb702e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"caba256e1cc2bbef","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"431365f80d15803","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"34ba068544ff09c7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"475d48b671990726","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"fd93fcd5dcf92398","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"e81f0f8c906863c6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting worker-operator pod Should restart worker-operator pod":{"statistic":{"failed":0,"broken":0,"skipped":51,"passed":0,"unknown":0,"total":51},"items":[{"uid":"1e6dc2879d32836f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"61b87cbd94381832","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"8819e7cadc626814","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662348908000,"stop":1662348908000,"duration":0}},{"uid":"c5c051f448e935e0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"9a9b7d209bb06c01","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"dac0496380837d0e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"a408481c08a0d0a7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"ce8318c5093f6d35","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"a5f9abb86d9705b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"1dd4ed06f138f58","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"aadd51aacbaef52e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"408d61094ed1283d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"f01d1a92ba60e192","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"d5729991e9b3cb90","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"e46d1ce3a1d9ef6c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"f196b038b33e8198","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"bbd2eacb60b58a3f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"bf242a58aac28486","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"cb543711d9edb312","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"be4c88760885ea1b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice Creation scenarios Slice with LOW QOS should have vl3 router running":{"statistic":{"failed":17,"broken":0,"skipped":2,"passed":23,"unknown":0,"total":42},"items":[{"uid":"a4fe29846cbe71a2","status":"skipped","statusDetails":"skipped","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"87f3f337b8546ba3","status":"skipped","statusDetails":"skipped","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"230640294a3a5444","status":"passed","time":{"start":1662348908000,"stop":1662348912686,"duration":4686}},{"uid":"1882dfa2665a9c54","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004695c0>: {\n        Underlying: <*exec.ExitError | 0xc00065c880>{\n            ProcessState: {\n                pid: 7276,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 275895},\n                    Stime: {Sec: 0, Usec: 85202},\n                    Maxrss: 96456,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4595,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 312,\n                    Nivcsw: 644,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicelowqos\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\noccurred","time":{"start":1661844205000,"stop":1661844205437,"duration":437}},{"uid":"d8f312c38f1d96f7","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000734a38>: {\n        Underlying: <*exec.ExitError | 0xc00017eaa0>{\n            ProcessState: {\n                pid: 7213,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 155613},\n                    Stime: {Sec: 0, Usec: 41750},\n                    Maxrss: 81264,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2782,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 291,\n                    Nivcsw: 718,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicelowqos\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661754870000,"stop":1661754870278,"duration":278}},{"uid":"8b9cfbf181d7c35e","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0006f05d0>: {\n        Underlying: <*exec.ExitError | 0xc0006a85c0>{\n            ProcessState: {\n                pid: 7291,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 153745},\n                    Stime: {Sec: 0, Usec: 43927},\n                    Maxrss: 88652,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4153,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 429,\n                    Nivcsw: 325,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicelowqos\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661611962000,"stop":1661611962299,"duration":299}},{"uid":"533af60f240e19d","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0009d9b48>: {\n        Underlying: <*exec.ExitError | 0xc0007aaea0>{\n            ProcessState: {\n                pid: 7094,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 189610},\n                    Stime: {Sec: 0, Usec: 52914},\n                    Maxrss: 85092,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3515,\n                    Majflt: 1,\n                    Nswap: 0,\n                    Inblock: 136,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 437,\n                    Nivcsw: 526,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicelowqos\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661585077000,"stop":1661585077370,"duration":370}},{"uid":"1ee9400b251f9da1","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00013bea8>: {\n        Underlying: <*exec.ExitError | 0xc0003b0d40>{\n            ProcessState: {\n                pid: 6238,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 213236},\n                    Stime: {Sec: 0, Usec: 43437},\n                    Maxrss: 82840,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4045,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 389,\n                    Nivcsw: 498,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicelowqos\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661584447000,"stop":1661584447409,"duration":409}},{"uid":"cb349d98869bf1a0","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0006d2660>: {\n        Underlying: <*exec.ExitError | 0xc000075500>{\n            ProcessState: {\n                pid: 7285,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 177183},\n                    Stime: {Sec: 0, Usec: 30814},\n                    Maxrss: 79996,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3955,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 449,\n                    Nivcsw: 412,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicelowqos\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661580045000,"stop":1661580045265,"duration":265}},{"uid":"c898dacd0afb595e","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000779518>: {\n        Underlying: <*exec.ExitError | 0xc000075fa0>{\n            ProcessState: {\n                pid: 7278,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 255062},\n                    Stime: {Sec: 0, Usec: 50239},\n                    Maxrss: 90280,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3608,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 401,\n                    Nivcsw: 404,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicelowqos\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661578505000,"stop":1661578505388,"duration":388}},{"uid":"b6d1552f84a18ff","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0008a2078>: {\n        Underlying: <*exec.ExitError | 0xc00074c120>{\n            ProcessState: {\n                pid: 7224,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 249331},\n                    Stime: {Sec: 0, Usec: 76408},\n                    Maxrss: 96576,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 8216,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 497,\n                    Nivcsw: 467,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicelowqos\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661572479000,"stop":1661572479442,"duration":442}},{"uid":"e66cc0fe579d7aeb","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0006ea5d0>: {\n        Underlying: <*exec.ExitError | 0xc0008c8c00>{\n            ProcessState: {\n                pid: 7136,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 140565},\n                    Stime: {Sec: 0, Usec: 53745},\n                    Maxrss: 82532,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4199,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 282,\n                    Nivcsw: 342,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicelowqos\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660832281000,"stop":1660832281267,"duration":267}},{"uid":"90c863483faf9005","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004dc828>: {\n        Underlying: <*exec.ExitError | 0xc0007d42c0>{\n            ProcessState: {\n                pid: 7207,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 155805},\n                    Stime: {Sec: 0, Usec: 67375},\n                    Maxrss: 80652,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4200,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 487,\n                    Nivcsw: 354,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicelowqos\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660808797000,"stop":1660808797280,"duration":280}},{"uid":"70e33a27619371c","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0005cc138>: {\n        Underlying: <*exec.ExitError | 0xc000720480>{\n            ProcessState: {\n                pid: 7110,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 130640},\n                    Stime: {Sec: 0, Usec: 39917},\n                    Maxrss: 85196,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 6182,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 293,\n                    Nivcsw: 295,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicelowqos\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660299182000,"stop":1660299182230,"duration":230}},{"uid":"890837a5f9a4b54b","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00044a018>: {\n        Underlying: <*exec.ExitError | 0xc000806160>{\n            ProcessState: {\n                pid: 7166,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 142515},\n                    Stime: {Sec: 0, Usec: 43546},\n                    Maxrss: 79220,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4888,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 376,\n                    Nivcsw: 286,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicelowqos\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660295740000,"stop":1660295740243,"duration":243}},{"uid":"7b6af30713ee0624","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00084ce28>: {\n        Underlying: <*exec.ExitError | 0xc000074920>{\n            ProcessState: {\n                pid: 7254,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 200962},\n                    Stime: {Sec: 0, Usec: 64459},\n                    Maxrss: 80860,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 7546,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 442,\n                    Nivcsw: 427,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicelowqos\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660293657000,"stop":1660293657334,"duration":334}},{"uid":"94c4e83ac5902d5","status":"passed","time":{"start":1660113628000,"stop":1660113631562,"duration":3562}},{"uid":"2f74166b31149e41","status":"passed","time":{"start":1660104898000,"stop":1660104902701,"duration":4701}},{"uid":"90ab2f86812d46bd","status":"passed","time":{"start":1660067259000,"stop":1660067262335,"duration":3335}},{"uid":"83ca3d46dbaac9da","status":"passed","time":{"start":1660047551000,"stop":1660047554342,"duration":3342}}]},"Worker Suite:Worker Suite#[It]  QosProfile tests QosProfile delete validation Delete qosProfile positive scenario":{"statistic":{"failed":0,"broken":0,"skipped":11,"passed":0,"unknown":0,"total":11},"items":[{"uid":"6371d1296d488ade","status":"skipped","statusDetails":"skipped","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"d9dbc3aba3f08d90","status":"skipped","statusDetails":"skipped","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"899da889ae2fa6e9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662348908000,"stop":1662348908000,"duration":0}},{"uid":"60cdbe906792bd5a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"2cf368eb6ed830ed","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"4f48b9a048a5c8bf","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"a29f4c5c7f29e8b0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"7894400e1f9e6da4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"4a93daf1db12328","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"3af3d49caf80d8c4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"1e1304e464873599","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests slice with netpol should install slice on each worker cluster with correct namespaceisolationprofile":{"statistic":{"failed":0,"broken":1,"skipped":38,"passed":12,"unknown":0,"total":51},"items":[{"uid":"472bef7be3e3a819","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"7d402d65c69e84ec","status":"passed","time":{"start":1663044069000,"stop":1663044069224,"duration":224}},{"uid":"c4b23ef83d49f9a2","status":"passed","time":{"start":1662348908000,"stop":1662348908133,"duration":133}},{"uid":"7de98636769354ef","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"66a073eed7a9a266","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"8c0243915cadc01","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"44d38c14bf542d54","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"f72499492e91415f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"3ed3d6cd2fa7c7fc","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"8d1d1d844fb7f35b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"550d9281874ed37f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"8ab30acc0ae11912","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"7f42d02ee97e57ba","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"77bb8a277c2f216","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"5b93153683a52b0f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"e7dcda63f7952683","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"ec51288878fc5a11","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"76ba8125e909631a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"90307464dcdff2ea","status":"broken","statusDetails":"runtime error: invalid memory address or nil pointer dereference","time":{"start":1660067259000,"stop":1660067272200,"duration":13200}},{"uid":"6cbfa9832ad737b5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Namespace reconcile test scenarios: Delete deboarded app ns gets deleted":{"statistic":{"failed":0,"broken":0,"skipped":10,"passed":41,"unknown":0,"total":51},"items":[{"uid":"ad05e6e09e197475","status":"skipped","statusDetails":"skipped","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"2a42ad0870c9d4bd","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"74afe4d4bf31bf47","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662348908000,"stop":1662348908000,"duration":0}},{"uid":"28d39ab43cfcba4b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"7b674e701816d2f0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"2e25d4834f475498","status":"passed","time":{"start":1661611962000,"stop":1661611973192,"duration":11192}},{"uid":"4de6f18dd3ae2d2d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"eab9ef4116cc011f","status":"passed","time":{"start":1661584447000,"stop":1661584458008,"duration":11008}},{"uid":"2251b32027c5ac26","status":"passed","time":{"start":1661580045000,"stop":1661580055741,"duration":10741}},{"uid":"213e8652dc3fe0ac","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"aa5c80f34b538831","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"85f85706f8186b36","status":"passed","time":{"start":1660832281000,"stop":1660832291583,"duration":10583}},{"uid":"8baaf6138f0968e6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"8667aa84b2de2312","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"b75ab78ef085c5e5","status":"passed","time":{"start":1660295740000,"stop":1660295750533,"duration":10533}},{"uid":"eff33a8e00776e17","status":"passed","time":{"start":1660293657000,"stop":1660293667716,"duration":10716}},{"uid":"2831a67a404a7357","status":"passed","time":{"start":1660113628000,"stop":1660113638487,"duration":10487}},{"uid":"8f676df91669f63","status":"passed","time":{"start":1660104898000,"stop":1660104908574,"duration":10574}},{"uid":"cd93f51a1ab83ad3","status":"passed","time":{"start":1660067259000,"stop":1660067269534,"duration":10534}},{"uid":"13fcd3859cc66725","status":"passed","time":{"start":1660047551000,"stop":1660047562318,"duration":11318}}]},"Istio Suite:Istio Suite#[It] Bookinfo tests Bookinfo Scenario 1 without egress and ingress Should install Productpage on worker cluster-1":{"statistic":{"failed":43,"broken":0,"skipped":0,"passed":14,"unknown":0,"total":57},"items":[{"uid":"399817e2c7c9e047","status":"passed","time":{"start":1663043831000,"stop":1663043849030,"duration":18030}},{"uid":"1cf30db0ce168c0b","status":"passed","time":{"start":1663043075000,"stop":1663043180300,"duration":105300}},{"uid":"fa20c6794d5ac6f1","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1663040338000,"stop":1663040518376,"duration":180376}},{"uid":"16bde7f003ead557","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1662819574000,"stop":1662819754327,"duration":180327}},{"uid":"2a3673c35cc220ef","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1662795310000,"stop":1662795490298,"duration":180298}},{"uid":"a7a5a4b5327d3e19","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1662792773000,"stop":1662792953304,"duration":180304}},{"uid":"5ee2bb3e809d8a72","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1662620110000,"stop":1662620290364,"duration":180364}},{"uid":"ce8c40649f38e02e","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1662617640000,"stop":1662617820366,"duration":180366}},{"uid":"b2d62149ec37ee8","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1662610377000,"stop":1662610557267,"duration":180267}},{"uid":"9d96be34a1bf037e","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1662557911000,"stop":1662558091331,"duration":180331}},{"uid":"35bf00d87a4a831a","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1662553747000,"stop":1662553927258,"duration":180258}},{"uid":"b79a49cb8e76dabe","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1662553327000,"stop":1662553507388,"duration":180388}},{"uid":"7959f2d6e75db8f7","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1662381943000,"stop":1662382123344,"duration":180344}},{"uid":"a9483c00a8cf4988","status":"passed","time":{"start":1662348103000,"stop":1662348121307,"duration":18307}},{"uid":"a9b3c78bc4a1e701","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1662049889000,"stop":1662050069354,"duration":180354}},{"uid":"657040deb2075286","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1662049074000,"stop":1662049254358,"duration":180358}},{"uid":"b24cd555529af20","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1662028179000,"stop":1662028359335,"duration":180335}},{"uid":"c01807558b1a3125","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1661859205000,"stop":1661859385685,"duration":180685}},{"uid":"e9137fbffb1d612","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00074e348>: {\n        Underlying: <*exec.ExitError | 0xc0006d49a0>{\n            ProcessState: {\n                pid: 6913,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 269580},\n                    Stime: {Sec: 0, Usec: 78139},\n                    Maxrss: 90664,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 12849,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 338,\n                    Nivcsw: 393,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicewithoutegressingress\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicewithoutegressingress\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicewithoutegressingress\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicewithoutegressingress\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicewithoutegressingress\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\noccurred","time":{"start":1661843950000,"stop":1661843950319,"duration":319}},{"uid":"615a86c5439c3dd","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0003b4948>: {\n        Underlying: <*exec.ExitError | 0xc00030bf40>{\n            ProcessState: {\n                pid: 6951,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 199650},\n                    Stime: {Sec: 0, Usec: 50891},\n                    Maxrss: 77924,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 7397,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 560,\n                    Nivcsw: 708,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicewithoutegressingress\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicewithoutegressingress\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicewithoutegressingress\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicewithoutegressingress\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicewithoutegressingress\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661754632000,"stop":1661754632500,"duration":500}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting mesh-dns pod Check ping between iperf-server and iperf-client after mesh-dns pod restart":{"statistic":{"failed":0,"broken":0,"skipped":51,"passed":0,"unknown":0,"total":51},"items":[{"uid":"3446ea721c8b7d89","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"538a11e006559e1e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"9c515adb02e4bf3c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662348908000,"stop":1662348908000,"duration":0}},{"uid":"ed886fe832ae3bbf","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"b48ed8135872c5e3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"2afcbab9cd5408ef","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"2086ea31fb96902c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"c52ca08667842ed","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"53f44c4fe4695f56","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"f73a0bbff4f59a76","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"909f72f81f0800a4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"4a0805df51341ecb","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"4d75542c9f4f09a1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"4662c5300922074d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"5a44f16fcfd50051","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"3bee2a48eb5c6e6a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"9a90dd3bcc877f76","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"d67a0bffcb44f2a0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"5ebbcb010527076","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"2664ad03adc8ecca","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice creation scenarios: when slice red is installed install slice blue workers should get attached to slice blue":{"statistic":{"failed":0,"broken":0,"skipped":37,"passed":14,"unknown":0,"total":51},"items":[{"uid":"528e14e2def29c4c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"c237ca3c9658523c","status":"passed","time":{"start":1663044069000,"stop":1663044069993,"duration":993}},{"uid":"e4fffa9b72ef5f2d","status":"passed","time":{"start":1662348908000,"stop":1662348908696,"duration":696}},{"uid":"e5f620beee22a294","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"72d42f07cede60fd","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"29f5547b6a272cb8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"8aa1935873442ce7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"43cbf96270ad252a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"fb47e1e943fc637b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"a344cd57af9818bc","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"9254048968aabaca","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"84f16902ea966f47","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"a1c7b24a9868efce","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"b0b113a3f84633da","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"9292148a6305abe5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"e9b50a3c93bd90dd","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"915e33568f1b60f0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"6318749ab84150f0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"20ab880307c7c7a8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"ca501e21f13e3dbf","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests slice with netpol should have vl3 router running":{"statistic":{"failed":37,"broken":0,"skipped":0,"passed":14,"unknown":0,"total":51},"items":[{"uid":"b1f2f4209b765506","status":"failed","statusDetails":"Timed out after 280.000s.\nExpected\n    <int>: 2\nto equal\n    <int>: 1","time":{"start":1663044723000,"stop":1663045018428,"duration":295428}},{"uid":"9732ee24e4b5b9ea","status":"passed","time":{"start":1663044069000,"stop":1663044075870,"duration":6870}},{"uid":"f6e3fb8cff53a060","status":"passed","time":{"start":1662348908000,"stop":1662348914888,"duration":6888}},{"uid":"3690bf568643aa65","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0003f9470>: {\n        Underlying: <*exec.ExitError | 0xc000403f40>{\n            ProcessState: {\n                pid: 7297,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 232557},\n                    Stime: {Sec: 0, Usec: 70597},\n                    Maxrss: 92404,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4094,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 420,\n                    Nivcsw: 629,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\noccurred","time":{"start":1661844205000,"stop":1661844205633,"duration":633}},{"uid":"b4acc7dde5577690","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000734060>: {\n        Underlying: <*exec.ExitError | 0xc0007a4f40>{\n            ProcessState: {\n                pid: 7092,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 176987},\n                    Stime: {Sec: 0, Usec: 58995},\n                    Maxrss: 74624,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3261,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 381,\n                    Nivcsw: 568,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661754870000,"stop":1661754870513,"duration":513}},{"uid":"a67c86b4f48e988b","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0008ce6c0>: {\n        Underlying: <*exec.ExitError | 0xc000074440>{\n            ProcessState: {\n                pid: 7106,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 190827},\n                    Stime: {Sec: 0, Usec: 40601},\n                    Maxrss: 86464,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4559,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 450,\n                    Nivcsw: 915,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661611962000,"stop":1661611962704,"duration":704}},{"uid":"b782f4bad8d76d6","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00090c030>: {\n        Underlying: <*exec.ExitError | 0xc000a800a0>{\n            ProcessState: {\n                pid: 7112,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 197039},\n                    Stime: {Sec: 0, Usec: 49259},\n                    Maxrss: 85024,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3700,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 417,\n                    Nivcsw: 715,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661585077000,"stop":1661585077590,"duration":590}},{"uid":"4ab91ddcf5e1d3eb","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000363470>: {\n        Underlying: <*exec.ExitError | 0xc000825200>{\n            ProcessState: {\n                pid: 6172,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 209965},\n                    Stime: {Sec: 0, Usec: 44415},\n                    Maxrss: 75824,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3204,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 397,\n                    Nivcsw: 688,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661584447000,"stop":1661584447586,"duration":586}},{"uid":"954f5abfeae757ce","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000810570>: {\n        Underlying: <*exec.ExitError | 0xc000626ae0>{\n            ProcessState: {\n                pid: 7228,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 199943},\n                    Stime: {Sec: 0, Usec: 39988},\n                    Maxrss: 82476,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4558,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 320,\n                    Nivcsw: 809,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661580045000,"stop":1661580045511,"duration":511}},{"uid":"6c551f040bddfcdc","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0007782a0>: {\n        Underlying: <*exec.ExitError | 0xc000774420>{\n            ProcessState: {\n                pid: 7074,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 239443},\n                    Stime: {Sec: 0, Usec: 97400},\n                    Maxrss: 86832,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 11012,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 512,\n                    Nivcsw: 638,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661578505000,"stop":1661578505714,"duration":714}},{"uid":"8e71fa55c6b99f29","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004c9fb0>: {\n        Underlying: <*exec.ExitError | 0xc000886880>{\n            ProcessState: {\n                pid: 7248,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 267937},\n                    Stime: {Sec: 0, Usec: 46597},\n                    Maxrss: 90744,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 6607,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 563,\n                    Nivcsw: 469,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661572479000,"stop":1661572479726,"duration":726}},{"uid":"16ab229423139acd","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0006ead20>: {\n        Underlying: <*exec.ExitError | 0xc0008cb180>{\n            ProcessState: {\n                pid: 7085,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 181211},\n                    Stime: {Sec: 0, Usec: 53539},\n                    Maxrss: 72000,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 6414,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 444,\n                    Nivcsw: 515,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660832281000,"stop":1660832281553,"duration":553}},{"uid":"3271f69ef5bca0e5","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0001266f0>: {\n        Underlying: <*exec.ExitError | 0xc000663480>{\n            ProcessState: {\n                pid: 7142,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 166188},\n                    Stime: {Sec: 0, Usec: 47482},\n                    Maxrss: 86340,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4052,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 391,\n                    Nivcsw: 257,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660808797000,"stop":1660808797552,"duration":552}},{"uid":"974d621c049fb879","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000350948>: {\n        Underlying: <*exec.ExitError | 0xc000828e60>{\n            ProcessState: {\n                pid: 7252,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 133880},\n                    Stime: {Sec: 0, Usec: 44626},\n                    Maxrss: 82964,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2723,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 590,\n                    Nivcsw: 512,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660299182000,"stop":1660299182459,"duration":459}},{"uid":"c662752d8114479a","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004ca618>: {\n        Underlying: <*exec.ExitError | 0xc00065fc60>{\n            ProcessState: {\n                pid: 7215,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 141036},\n                    Stime: {Sec: 0, Usec: 43094},\n                    Maxrss: 74312,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3629,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 312,\n                    Nivcsw: 567,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660295740000,"stop":1660295740433,"duration":433}},{"uid":"f16388458dc45ba2","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000649860>: {\n        Underlying: <*exec.ExitError | 0xc000828ea0>{\n            ProcessState: {\n                pid: 7148,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 192477},\n                    Stime: {Sec: 0, Usec: 55604},\n                    Maxrss: 73856,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 5400,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 430,\n                    Nivcsw: 736,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660293657000,"stop":1660293657539,"duration":539}},{"uid":"9006817b9c332608","status":"passed","time":{"start":1660113628000,"stop":1660113649243,"duration":21243}},{"uid":"e827510464e03727","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 4\nto equal\n    <int>: 1","time":{"start":1660104898000,"stop":1660105093414,"duration":195414}},{"uid":"d064732320ea6e1e","status":"passed","time":{"start":1660067259000,"stop":1660067259349,"duration":349}},{"uid":"7e2022ee47a9ac83","status":"failed","statusDetails":"Unexpected error:\n    <*errors.StatusError | 0xc00069a280>: {\n        ErrStatus: {\n            TypeMeta: {Kind: \"\", APIVersion: \"\"},\n            ListMeta: {\n                SelfLink: \"\",\n                ResourceVersion: \"\",\n                Continue: \"\",\n                RemainingItemCount: nil,\n            },\n            Status: \"Failure\",\n            Message: \"namespaces \\\"iperf\\\" already exists\",\n            Reason: \"AlreadyExists\",\n            Details: {Name: \"iperf\", Group: \"\", Kind: \"namespaces\", UID: \"\", Causes: nil, RetryAfterSeconds: 0},\n            Code: 409,\n        },\n    }\n    namespaces \"iperf\" already exists\noccurred","time":{"start":1660047551000,"stop":1660047559604,"duration":8604}}]},"Empty Suite:Empty Suite#[BeforeSuite]":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":169,"unknown":0,"total":169},"items":[{"uid":"e225997f9fef3d42","status":"passed","time":{"start":1663052238000,"stop":1663052238000,"duration":0}},{"uid":"e091f94e1c1a70a9","status":"passed","time":{"start":1663051980000,"stop":1663051980000,"duration":0}},{"uid":"5714e483c1bce4b9","status":"passed","time":{"start":1663050064000,"stop":1663050064000,"duration":0}},{"uid":"8d2a8fcc8f2ed89f","status":"passed","time":{"start":1663042347000,"stop":1663042347000,"duration":0}},{"uid":"f16e1c9dcbfd581f","status":"passed","time":{"start":1663041715000,"stop":1663041715000,"duration":0}},{"uid":"770b710ca2d6f6f5","status":"passed","time":{"start":1663039756000,"stop":1663039756000,"duration":0}},{"uid":"4b7b2212905e289c","status":"passed","time":{"start":1663040168000,"stop":1663040168002,"duration":2}},{"uid":"31f7af7a0280df37","status":"passed","time":{"start":1663009296000,"stop":1663009296000,"duration":0}},{"uid":"aaac49524c663157","status":"passed","time":{"start":1663008270000,"stop":1663008270000,"duration":0}},{"uid":"652b44140e189399","status":"passed","time":{"start":1662986456000,"stop":1662986456000,"duration":0}},{"uid":"2be7f20ccae96d51","status":"passed","time":{"start":1662978394000,"stop":1662978394000,"duration":0}},{"uid":"a3456d23c1117a15","status":"passed","time":{"start":1662961730000,"stop":1662961730000,"duration":0}},{"uid":"3205035855d740ca","status":"passed","time":{"start":1662960985000,"stop":1662960985000,"duration":0}},{"uid":"c9a04460ea150ab2","status":"passed","time":{"start":1662960825000,"stop":1662960825000,"duration":0}},{"uid":"7afc6cbe13277881","status":"passed","time":{"start":1662824760000,"stop":1662824760000,"duration":0}},{"uid":"6035451d11823bf4","status":"passed","time":{"start":1662819406000,"stop":1662819406000,"duration":0}},{"uid":"e972b65fed42029e","status":"passed","time":{"start":1662795171000,"stop":1662795171000,"duration":0}},{"uid":"ba1281abe59e7ed2","status":"passed","time":{"start":1662792593000,"stop":1662792593000,"duration":0}},{"uid":"ba1e6677075f153b","status":"passed","time":{"start":1662791140000,"stop":1662791140000,"duration":0}},{"uid":"daf3e2b5dcb96a75","status":"passed","time":{"start":1662788408000,"stop":1662788408000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice validation Slice with MEDIUM QOS should have vl3 router running":{"statistic":{"failed":3,"broken":0,"skipped":0,"passed":6,"unknown":0,"total":9},"items":[{"uid":"24dbffb43308dcfe","status":"passed","time":{"start":1659168739000,"stop":1659168739689,"duration":689}},{"uid":"32d8b5c697e6c847","status":"passed","time":{"start":1659164084000,"stop":1659164084631,"duration":631}},{"uid":"49c7996737d1d261","status":"passed","time":{"start":1659160188000,"stop":1659160188690,"duration":690}},{"uid":"dad5cb3f70e541bb","status":"passed","time":{"start":1659119724000,"stop":1659119724538,"duration":538}},{"uid":"3696a0b392e3f2eb","status":"passed","time":{"start":1659116511000,"stop":1659116511419,"duration":419}},{"uid":"df9e0e068ede27d4","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 3\nto equal\n    <int>: 1","time":{"start":1659109470000,"stop":1659109530750,"duration":60750}},{"uid":"b166459c6ae18a80","status":"passed","time":{"start":1659106836000,"stop":1659106836774,"duration":774}},{"uid":"55bf1b7112ff65f6","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1659101522000,"stop":1659101582765,"duration":60765}},{"uid":"6b9bc19dfb598b06","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <int>: 4\nto equal\n    <int>: 1","time":{"start":1659082816000,"stop":1659082876914,"duration":60914}}]},"Worker Suite:Worker Suite#[It] Slice validation Slice with MEDIUM QOS should reconcile medium qos slice":{"statistic":{"failed":5,"broken":0,"skipped":4,"passed":0,"unknown":0,"total":9},"items":[{"uid":"531abd47defaec11","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004a9788>: {\n        Underlying: <*exec.ExitError | 0xc00040d860>{\n            ProcessState: {\n                pid: 7260,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 72127},\n                    Stime: {Sec: 0, Usec: 18031},\n                    Maxrss: 43584,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4297,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 230,\n                    Nivcsw: 359,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicemediumqos\\\" not found\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicemediumqos\\\" not found\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicemediumqos\\\" not found\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicemediumqos\\\" not found\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (NotFound): slice.networking.kubeslice.io \"slicemediumqos\" not found\noccurred","time":{"start":1659168739000,"stop":1659168740414,"duration":1414}},{"uid":"46130730b1f11a1a","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00087c120>: {\n        Underlying: <*exec.ExitError | 0xc00069c5e0>{\n            ProcessState: {\n                pid: 7406,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 74896},\n                    Stime: {Sec: 0, Usec: 12482},\n                    Maxrss: 43348,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3354,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 199,\n                    Nivcsw: 297,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicemediumqos\\\" not found\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicemediumqos\\\" not found\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicemediumqos\\\" not found\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicemediumqos\\\" not found\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (NotFound): slice.networking.kubeslice.io \"slicemediumqos\" not found\noccurred","time":{"start":1659164084000,"stop":1659164085098,"duration":1098}},{"uid":"cdf275d41a8481c7","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00049ad20>: {\n        Underlying: <*exec.ExitError | 0xc000843c20>{\n            ProcessState: {\n                pid: 7452,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 44988},\n                    Stime: {Sec: 0, Usec: 22494},\n                    Maxrss: 41588,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2497,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 210,\n                    Nivcsw: 113,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicemediumqos\\\" not found\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicemediumqos\\\" not found\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicemediumqos\\\" not found\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicemediumqos\\\" not found\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (NotFound): slice.networking.kubeslice.io \"slicemediumqos\" not found\noccurred","time":{"start":1659160188000,"stop":1659160188732,"duration":732}},{"uid":"7f1c6ad507808e9d","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000347c38>: {\n        Underlying: <*exec.ExitError | 0xc000778740>{\n            ProcessState: {\n                pid: 8030,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 59177},\n                    Stime: {Sec: 0, Usec: 12680},\n                    Maxrss: 42032,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2537,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 257,\n                    Nivcsw: 140,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicemediumqos\\\" not found\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicemediumqos\\\" not found\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicemediumqos\\\" not found\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicemediumqos\\\" not found\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (NotFound): slice.networking.kubeslice.io \"slicemediumqos\" not found\noccurred","time":{"start":1659119724000,"stop":1659119725126,"duration":1126}},{"uid":"88a740814282788f","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004e9560>: {\n        Underlying: <*exec.ExitError | 0xc0004510c0>{\n            ProcessState: {\n                pid: 7542,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 66414},\n                    Stime: {Sec: 0, Usec: 15626},\n                    Maxrss: 45792,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2977,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 225,\n                    Nivcsw: 318,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicemediumqos\\\" not found\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicemediumqos\\\" not found\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicemediumqos\\\" not found\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicemediumqos\\\" not found\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (NotFound): slice.networking.kubeslice.io \"slicemediumqos\" not found\noccurred","time":{"start":1659116511000,"stop":1659116511874,"duration":874}},{"uid":"f1ae2dd55b9e07f8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659109470000,"stop":1659109470000,"duration":0}},{"uid":"afe8c318d5877363","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659106836000,"stop":1659106836000,"duration":0}},{"uid":"cffa1fbdfce28b40","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659101522000,"stop":1659101522000,"duration":0}},{"uid":"ce0be5d753f333ab","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659082816000,"stop":1659082816000,"duration":0}}]},"Hub Suite:Hub Suite#[It] Cluster negative tests Worker Cluster Registration with Wrong clustername":{"statistic":{"failed":2,"broken":0,"skipped":29,"passed":162,"unknown":0,"total":193},"items":[{"uid":"f296fc8335609181","status":"passed","time":{"start":1663042493000,"stop":1663042680017,"duration":187017}},{"uid":"f32fbe91e38d2eb4","status":"passed","time":{"start":1663041865000,"stop":1663041889299,"duration":24299}},{"uid":"8539a54348938b1c","status":"failed","statusDetails":"Timed out after 210.000s.\nExpected an error to have occurred.  Got:\n    <nil>: nil","time":{"start":1663039902000,"stop":1663040278655,"duration":376655}},{"uid":"a5ee5b666a842e55","status":"passed","time":{"start":1663009431000,"stop":1663009570492,"duration":139492}},{"uid":"d4901cd3a283c824","status":"passed","time":{"start":1662978562000,"stop":1662978751571,"duration":189571}},{"uid":"5ecfe7bcfc9d6c21","status":"passed","time":{"start":1662702362000,"stop":1662702510544,"duration":148544}},{"uid":"49502b2b6f345a92","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662553059000,"stop":1662553059000,"duration":0}},{"uid":"121b25128e4efdb2","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662553042000,"stop":1662553042000,"duration":0}},{"uid":"2062a7be7fc4ba97","status":"passed","time":{"start":1662467486000,"stop":1662467668703,"duration":182703}},{"uid":"f48dcb4412e4ee3a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662374048000,"stop":1662374048000,"duration":0}},{"uid":"8e3200e378265b6f","status":"passed","time":{"start":1662374250000,"stop":1662374445054,"duration":195054}},{"uid":"e25ca315cf74f506","status":"passed","time":{"start":1662361116000,"stop":1662361294946,"duration":178946}},{"uid":"142c32600f36170b","status":"passed","time":{"start":1662347286000,"stop":1662347473745,"duration":187745}},{"uid":"b909ecf59a818b0d","status":"passed","time":{"start":1662049179000,"stop":1662049224280,"duration":45280}},{"uid":"211f2b347969ed90","status":"passed","time":{"start":1662048808000,"stop":1662049002355,"duration":194355}},{"uid":"7b4ff2a50804e4b1","status":"passed","time":{"start":1662036991000,"stop":1662037178635,"duration":187635}},{"uid":"bd0d9655463dea46","status":"passed","time":{"start":1662031654000,"stop":1662031842280,"duration":188280}},{"uid":"22107549ec619ee9","status":"passed","time":{"start":1662027354000,"stop":1662027543659,"duration":189659}},{"uid":"d34a81e8890e3777","status":"passed","time":{"start":1661952242000,"stop":1661952378962,"duration":136962}},{"uid":"fcad05de9f6e727a","status":"passed","time":{"start":1661863163000,"stop":1661863318396,"duration":155396}}]},"Istio Suite:Istio Suite#[It] Bookinfo tests Bookinfo Scenario 1 without egress and ingress Should install Details, Reviews, Ratings and Serviceexport on worker cluster-2":{"statistic":{"failed":1,"broken":0,"skipped":43,"passed":13,"unknown":0,"total":57},"items":[{"uid":"9af1d393e90fac71","status":"passed","time":{"start":1663043831000,"stop":1663043863761,"duration":32761}},{"uid":"e9c6b14aedde3701","status":"passed","time":{"start":1663043075000,"stop":1663043105087,"duration":30087}},{"uid":"24ef71e6ea371ba6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663040338000,"stop":1663040338000,"duration":0}},{"uid":"3bfb839a24e2894b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662819574000,"stop":1662819574000,"duration":0}},{"uid":"daedffeecc448079","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662795310000,"stop":1662795310000,"duration":0}},{"uid":"aaad5237a088b491","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662792773000,"stop":1662792773000,"duration":0}},{"uid":"70031b712a90955f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662620110000,"stop":1662620110000,"duration":0}},{"uid":"787b078c2b1833dc","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662617640000,"stop":1662617640000,"duration":0}},{"uid":"85bcf3dbdad44a70","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662610377000,"stop":1662610377000,"duration":0}},{"uid":"68e719d3a78b541d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662557911000,"stop":1662557911000,"duration":0}},{"uid":"e1a6d2ec3db5a6b9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662553747000,"stop":1662553747000,"duration":0}},{"uid":"263065796e369f1a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662553327000,"stop":1662553327000,"duration":0}},{"uid":"41b27e0c8a92bdb8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662381943000,"stop":1662381943000,"duration":0}},{"uid":"e462475aec1f6c1f","status":"passed","time":{"start":1662348103000,"stop":1662348135148,"duration":32148}},{"uid":"b07032c32f1ad9ab","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662049889000,"stop":1662049889000,"duration":0}},{"uid":"3ccbb5a8325a128","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662049074000,"stop":1662049074000,"duration":0}},{"uid":"fb91f995f810462a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662028179000,"stop":1662028179000,"duration":0}},{"uid":"17ff148c5736db96","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661859205000,"stop":1661859205000,"duration":0}},{"uid":"a692f7ccbca78c3e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661843950000,"stop":1661843950000,"duration":0}},{"uid":"8f326b87367b6f31","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754632000,"stop":1661754632000,"duration":0}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Update Should fail when Project is Applied with service account name as blank in Read users":{"statistic":{"failed":16,"broken":0,"skipped":0,"passed":174,"unknown":0,"total":190},"items":[{"uid":"41b4f8cea515f65","status":"passed","time":{"start":1663042493000,"stop":1663042497858,"duration":4858}},{"uid":"2786428446999a","status":"passed","time":{"start":1663041865000,"stop":1663041871035,"duration":6035}},{"uid":"313823e53c9cb63f","status":"failed","statusDetails":"Timed out after 60.127s.\nExpected\n    <bool>: false\nto be true","time":{"start":1663039902000,"stop":1663040023402,"duration":121402}},{"uid":"6d3669563ebda4e6","status":"passed","time":{"start":1663009431000,"stop":1663009436916,"duration":5916}},{"uid":"a91e29471af8e4a8","status":"passed","time":{"start":1662978562000,"stop":1662978567076,"duration":5076}},{"uid":"70fc8d6bd018947","status":"passed","time":{"start":1662702362000,"stop":1662702366868,"duration":4868}},{"uid":"38037ada10d9c534","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <bool>: false\nto be true","time":{"start":1662553059000,"stop":1662553125651,"duration":66651}},{"uid":"d501b2f81ac641b2","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <bool>: false\nto be true","time":{"start":1662553042000,"stop":1662553163321,"duration":121321}},{"uid":"1d7827ab78d39b90","status":"passed","time":{"start":1662467486000,"stop":1662467490907,"duration":4907}},{"uid":"dd7c6311c6920571","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <bool>: false\nto be true","time":{"start":1662374048000,"stop":1662374117391,"duration":69391}},{"uid":"f505afa6304d3b35","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0001310b0>: {\n        Underlying: <*exec.ExitError | 0xc00074a4a0>{\n            ProcessState: {\n                pid: 7190,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 231000},\n                    Stime: {Sec: 0, Usec: 24750},\n                    Maxrss: 84988,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2941,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13344,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 419,\n                    Nivcsw: 295,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20blank%20in%20Read%20users329793668\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.241.207:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20blank%20in%20Read%20users329793668\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.241.207:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20blank%20in%20Read%20users329793668\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.241.207:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20blank%20in%20Read%20users329793668\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.241.207:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20blank%20in%20Read%20users329793668\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.241.207:443: connect: connection refused\noccurred","time":{"start":1662374250000,"stop":1662374250348,"duration":348}},{"uid":"5044ccc152125a10","status":"passed","time":{"start":1662361116000,"stop":1662361120942,"duration":4942}},{"uid":"794878a3c4ddce1","status":"passed","time":{"start":1662347286000,"stop":1662347290852,"duration":4852}},{"uid":"84044c7b55c51019","status":"passed","time":{"start":1662049179000,"stop":1662049184269,"duration":5269}},{"uid":"7846997140975390","status":"passed","time":{"start":1662048808000,"stop":1662048812937,"duration":4937}},{"uid":"2c37b2d92c4bda94","status":"passed","time":{"start":1662036991000,"stop":1662036995867,"duration":4867}},{"uid":"fb24cc4b455a8fe6","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0006b5428>: {\n        Underlying: <*exec.ExitError | 0xc00091c640>{\n            ProcessState: {\n                pid: 9607,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 169037},\n                    Stime: {Sec: 0, Usec: 47677},\n                    Maxrss: 83312,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4155,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13344,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 426,\n                    Nivcsw: 466,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20blank%20in%20Read%20users3545676354\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.231.22:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20blank%20in%20Read%20users3545676354\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.231.22:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20blank%20in%20Read%20users3545676354\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.231.22:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20blank%20in%20Read%20users3545676354\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.231.22:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20blank%20in%20Read%20users3545676354\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.231.22:443: connect: connection refused\noccurred","time":{"start":1662031654000,"stop":1662031654333,"duration":333}},{"uid":"6975e11c309e80d2","status":"passed","time":{"start":1662027354000,"stop":1662027359094,"duration":5094}},{"uid":"53b2eb9cf9a5a369","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000e5a0>: {\n        Underlying: <*exec.ExitError | 0xc0006d85a0>{\n            ProcessState: {\n                pid: 7728,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 187087},\n                    Stime: {Sec: 0, Usec: 47767},\n                    Maxrss: 74140,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 5513,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13344,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 303,\n                    Nivcsw: 292,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20blank%20in%20Read%20users10992993\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.242.99:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20blank%20in%20Read%20users10992993\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.242.99:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20blank%20in%20Read%20users10992993\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.242.99:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20blank%20in%20Read%20users10992993\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.242.99:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20blank%20in%20Read%20users10992993\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.242.99:443: connect: connection refused\noccurred","time":{"start":1661952242000,"stop":1661952242269,"duration":269}},{"uid":"980dc2146ef1022b","status":"passed","time":{"start":1661863163000,"stop":1661863168047,"duration":5047}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting mesh-dns pod Should restart mesh-dns pod":{"statistic":{"failed":0,"broken":0,"skipped":51,"passed":0,"unknown":0,"total":51},"items":[{"uid":"6a898adc23c054e9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"65feedfdd8cd68ee","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"8c55c67b068f5469","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662348908000,"stop":1662348908000,"duration":0}},{"uid":"fba51997b321a4ae","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"2853d4fbc0b639d2","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"ac3dfc0cbd4ba59c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"7e1cf7084ca6af67","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"a6dc1922abea6d8f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"afa904ee254b7dc6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"72acdc5f7e2b1d28","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"bfa3c728cc766a73","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"eed80751e61e42b1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"492cded192505f05","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"cd3931edeb60e724","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"e69aed4f153b962","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"ce5b72621fa27679","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"be37a4982c5d2ea4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"22b3e07e8562cb","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"6087ee1466eafde6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"24b8df24665b65fc","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}}]},"Empty Suite:Empty Suite#[It] Hub Creation tests Hub Installtion Test Senarios Should fail if Cert is installed again in the same cluster":{"statistic":{"failed":0,"broken":0,"skipped":32,"passed":137,"unknown":0,"total":169},"items":[{"uid":"9ef39369a3a30d66","status":"passed","time":{"start":1663052238000,"stop":1663052238350,"duration":350}},{"uid":"f4c97cf6a5791302","status":"passed","time":{"start":1663051980000,"stop":1663051980302,"duration":302}},{"uid":"82764214b6831b52","status":"passed","time":{"start":1663050064000,"stop":1663050064303,"duration":303}},{"uid":"a91ff26cb13d115d","status":"passed","time":{"start":1663042347000,"stop":1663042347305,"duration":305}},{"uid":"379147ffbcd94c81","status":"passed","time":{"start":1663041715000,"stop":1663041715332,"duration":332}},{"uid":"2fa194610f943787","status":"passed","time":{"start":1663039756000,"stop":1663039756274,"duration":274}},{"uid":"dc12eac23a64befd","status":"passed","time":{"start":1663040168000,"stop":1663040168300,"duration":300}},{"uid":"a7072a64a84cecc0","status":"passed","time":{"start":1663009296000,"stop":1663009296361,"duration":361}},{"uid":"c072c685441fdc88","status":"passed","time":{"start":1663008270000,"stop":1663008270307,"duration":307}},{"uid":"bea1ac10b139299b","status":"passed","time":{"start":1662986456000,"stop":1662986456307,"duration":307}},{"uid":"7b5be218deeba63c","status":"passed","time":{"start":1662978394000,"stop":1662978394285,"duration":285}},{"uid":"4402053f2e62cbb9","status":"passed","time":{"start":1662961730000,"stop":1662961730208,"duration":208}},{"uid":"dcd5cc800372f518","status":"passed","time":{"start":1662960985000,"stop":1662960985314,"duration":314}},{"uid":"dca9b14615cc87c5","status":"passed","time":{"start":1662960825000,"stop":1662960825307,"duration":307}},{"uid":"afb1b89ddf327d48","status":"passed","time":{"start":1662824760000,"stop":1662824760321,"duration":321}},{"uid":"540d01ff217df779","status":"passed","time":{"start":1662819406000,"stop":1662819406303,"duration":303}},{"uid":"d50dfecc896ee9d3","status":"passed","time":{"start":1662795171000,"stop":1662795171252,"duration":252}},{"uid":"2f1b070d35b9883d","status":"passed","time":{"start":1662792593000,"stop":1662792593244,"duration":244}},{"uid":"8919c09559d64436","status":"passed","time":{"start":1662791140000,"stop":1662791140298,"duration":298}},{"uid":"652580d7d89ce353","status":"passed","time":{"start":1662788408000,"stop":1662788408293,"duration":293}}]},"Worker Suite:Worker Suite#[It] Namespace reconcile test scenarios: Delete deleted app ns entry should get removed from cluster objs":{"statistic":{"failed":0,"broken":0,"skipped":10,"passed":41,"unknown":0,"total":51},"items":[{"uid":"d611339f4e27b852","status":"skipped","statusDetails":"skipped","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"64f09a6b65355e39","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"425fcc33a9dd9661","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662348908000,"stop":1662348908000,"duration":0}},{"uid":"530a47d53fe0b896","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"a2b4de87864eaec9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"52e7c3d1f478bf28","status":"passed","time":{"start":1661611962000,"stop":1661611973458,"duration":11458}},{"uid":"9886f800d6c53ba6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"25dde8bfc1427be7","status":"passed","time":{"start":1661584447000,"stop":1661584458199,"duration":11199}},{"uid":"670f58925eb16e8","status":"passed","time":{"start":1661580045000,"stop":1661580056050,"duration":11050}},{"uid":"e1ae8ec305ef0de","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"592371c755efcb68","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"15cb93a996aefe4f","status":"passed","time":{"start":1660832281000,"stop":1660832292230,"duration":11230}},{"uid":"a68c16167562932","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"1784a268ba9df126","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"fd817c32325dd5ba","status":"passed","time":{"start":1660295740000,"stop":1660295750929,"duration":10929}},{"uid":"1ccf3fe80a7095e9","status":"passed","time":{"start":1660293657000,"stop":1660293668363,"duration":11363}},{"uid":"5f0118be17c85c16","status":"passed","time":{"start":1660113628000,"stop":1660113638889,"duration":10889}},{"uid":"8df3fb4cbb9def6c","status":"passed","time":{"start":1660104898000,"stop":1660104909074,"duration":11074}},{"uid":"65a0fe8d141d0c32","status":"passed","time":{"start":1660067259000,"stop":1660067269991,"duration":10991}},{"uid":"42d748857c27a54b","status":"passed","time":{"start":1660047551000,"stop":1660047562383,"duration":11383}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting nsm-manager pod Should restart nsm-manager pod":{"statistic":{"failed":0,"broken":0,"skipped":51,"passed":0,"unknown":0,"total":51},"items":[{"uid":"b4dd0e4fe34ab260","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"ffe5466a5425c6f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"eb372a77d5f5e98e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662348908000,"stop":1662348908000,"duration":0}},{"uid":"fc6c7b3f34d4314e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"3c49417b2256e7c9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"934202b07de4bf9e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"f8d856db33589a8d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"8b416a6975106a7b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"dcec2f776db6af1e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"c5fda82d5684f152","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"ecab6d18993bbe23","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"110f980690607381","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"5981e303f107ed9c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"d74cfd4282b121d6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"fc366e0a0c3434b1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"b649882982554a75","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"db55039887229fe","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"1c60e4002c5b7046","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"5cce28e3a0c00b9f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"ea20699f2b725d82","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}}]},"Hub Suite:Hub Suite#[It]  QosProfile tests QosProfile Create validation Creates qosprofile":{"statistic":{"failed":128,"broken":0,"skipped":0,"passed":21,"unknown":0,"total":149},"items":[{"uid":"4a4860c4cf29447a","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0000b1338>: {\n        Underlying: <*exec.ExitError | 0xc0006bd360>{\n            ProcessState: {\n                pid: 6635,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 135543},\n                    Stime: {Sec: 0, Usec: 58735},\n                    Maxrss: 77980,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4428,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14104,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 868,\n                    Nivcsw: 487,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile3742660238\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile3742660238\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile3742660238\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile3742660238\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile3742660238\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1661501311000,"stop":1661501319252,"duration":8252}},{"uid":"fa39efcc9fc328fc","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0006b1278>: {\n        Underlying: <*exec.ExitError | 0xc0006d1340>{\n            ProcessState: {\n                pid: 6738,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 114422},\n                    Stime: {Sec: 0, Usec: 96818},\n                    Maxrss: 83208,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3984,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14104,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 1048,\n                    Nivcsw: 538,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile3848206832\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile3848206832\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile3848206832\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile3848206832\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile3848206832\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1661434220000,"stop":1661434227539,"duration":7539}},{"uid":"158fbc830ddcac8b","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0001784f8>: {\n        Underlying: <*exec.ExitError | 0xc00070a040>{\n            ProcessState: {\n                pid: 6781,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 137136},\n                    Stime: {Sec: 0, Usec: 56468},\n                    Maxrss: 67968,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3964,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14104,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 733,\n                    Nivcsw: 469,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1089868911\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1089868911\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1089868911\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1089868911\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1089868911\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1661356056000,"stop":1661356062517,"duration":6517}},{"uid":"401b2d0452faf79e","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0006ce7b0>: {\n        Underlying: <*exec.ExitError | 0xc000691960>{\n            ProcessState: {\n                pid: 6161,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 159178},\n                    Stime: {Sec: 0, Usec: 83778},\n                    Maxrss: 81324,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3612,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14104,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 820,\n                    Nivcsw: 531,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile2022566549\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile2022566549\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile2022566549\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile2022566549\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile2022566549\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1661352461000,"stop":1661352469576,"duration":8576}},{"uid":"4f86bc50546ea33e","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000e090>: {\n        Underlying: <*exec.ExitError | 0xc0009800c0>{\n            ProcessState: {\n                pid: 6712,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 127511},\n                    Stime: {Sec: 0, Usec: 42503},\n                    Maxrss: 77436,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3998,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14104,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 800,\n                    Nivcsw: 464,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile2106556113\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile2106556113\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile2106556113\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile2106556113\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile2106556113\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1661352207000,"stop":1661352214464,"duration":7464}},{"uid":"fe9452fb0b1680b0","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00081b7d0>: {\n        Underlying: <*exec.ExitError | 0xc000429f40>{\n            ProcessState: {\n                pid: 6768,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 152247},\n                    Stime: {Sec: 0, Usec: 65836},\n                    Maxrss: 70040,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3780,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14104,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 873,\n                    Nivcsw: 521,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile2022265779\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile2022265779\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile2022265779\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile2022265779\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile2022265779\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1661352028000,"stop":1661352035512,"duration":7512}},{"uid":"33c9e68f40c4f5de","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004cb968>: {\n        Underlying: <*exec.ExitError | 0xc0008c65e0>{\n            ProcessState: {\n                pid: 6082,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 137313},\n                    Stime: {Sec: 0, Usec: 74898},\n                    Maxrss: 78320,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 8874,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13616,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 844,\n                    Nivcsw: 1089,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile3930494543\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile3930494543\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile3930494543\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile3930494543\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile3930494543\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1661351263000,"stop":1661351277469,"duration":14469}},{"uid":"1b0b7a7ff76eff97","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004761c8>: {\n        Underlying: <*exec.ExitError | 0xc000483960>{\n            ProcessState: {\n                pid: 6605,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 219241},\n                    Stime: {Sec: 0, Usec: 99279},\n                    Maxrss: 76252,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3740,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14104,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 1184,\n                    Nivcsw: 725,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1359327084\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1359327084\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1359327084\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1359327084\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1359327084\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1661188463000,"stop":1661188471770,"duration":8770}},{"uid":"59acfcd17290b207","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0000d6558>: {\n        Underlying: <*exec.ExitError | 0xc0005c7180>{\n            ProcessState: {\n                pid: 6323,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 136153},\n                    Stime: {Sec: 0, Usec: 38293},\n                    Maxrss: 82884,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3930,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14104,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 880,\n                    Nivcsw: 390,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile3251245932\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile3251245932\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile3251245932\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile3251245932\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile3251245932\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1661183038000,"stop":1661183045440,"duration":7440}},{"uid":"e98ffdfcf23e7113","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000666b58>: {\n        Underlying: <*exec.ExitError | 0xc000621240>{\n            ProcessState: {\n                pid: 6121,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 135655},\n                    Stime: {Sec: 0, Usec: 71595},\n                    Maxrss: 76692,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4993,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14104,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 878,\n                    Nivcsw: 438,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1560004569\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1560004569\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1560004569\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1560004569\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1560004569\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1661180055000,"stop":1661180063215,"duration":8215}},{"uid":"e63325d4e7aa28a5","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0001eb458>: {\n        Underlying: <*exec.ExitError | 0xc000638f00>{\n            ProcessState: {\n                pid: 6139,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 170535},\n                    Stime: {Sec: 0, Usec: 68939},\n                    Maxrss: 75804,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4258,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14104,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 884,\n                    Nivcsw: 770,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1593006879\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1593006879\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1593006879\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1593006879\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1593006879\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1661152474000,"stop":1661152482518,"duration":8518}},{"uid":"a1d1dc9a090ff3cf","status":"passed","time":{"start":1661083598000,"stop":1661083599486,"duration":1486}},{"uid":"8198e552b3245c55","status":"passed","time":{"start":1661070332000,"stop":1661070340454,"duration":8454}},{"uid":"962d4da1aabd8f74","status":"passed","time":{"start":1661066603000,"stop":1661066605377,"duration":2377}},{"uid":"c80144c41a63a81c","status":"passed","time":{"start":1661018182000,"stop":1661018184081,"duration":2081}},{"uid":"4b784b4655361be1","status":"passed","time":{"start":1661014620000,"stop":1661014621489,"duration":1489}},{"uid":"62cecc1dfd6f786a","status":"passed","time":{"start":1661011831000,"stop":1661011840540,"duration":9540}},{"uid":"a837313995e0c06c","status":"passed","time":{"start":1661011780000,"stop":1661011788505,"duration":8505}},{"uid":"a67145573841a6ea","status":"passed","time":{"start":1661009684000,"stop":1661009685434,"duration":1434}},{"uid":"60dfc81fc105b6d6","status":"passed","time":{"start":1661007831000,"stop":1661007833089,"duration":2089}}]},"Hub Suite:Hub Suite#[It]  QosProfile tests QosProfile delete validation Delete qos profile - negative scenario":{"statistic":{"failed":149,"broken":0,"skipped":0,"passed":0,"unknown":0,"total":149},"items":[{"uid":"1ce3d6e6d5e59033","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000451890>: {\n        Underlying: <*exec.ExitError | 0xc0007a6540>{\n            ProcessState: {\n                pid: 6650,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 154838},\n                    Stime: {Sec: 0, Usec: 50322},\n                    Maxrss: 82024,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 6185,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14104,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 989,\n                    Nivcsw: 461,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario1814993184\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario1814993184\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario1814993184\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario1814993184\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario1814993184\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1661501311000,"stop":1661501315538,"duration":4538}},{"uid":"29726927de4ca0ca","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0006b13b0>: {\n        Underlying: <*exec.ExitError | 0xc0001bc560>{\n            ProcessState: {\n                pid: 6753,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 162528},\n                    Stime: {Sec: 0, Usec: 56885},\n                    Maxrss: 77348,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4147,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14104,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 835,\n                    Nivcsw: 509,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario1673518560\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario1673518560\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario1673518560\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario1673518560\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario1673518560\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1661434220000,"stop":1661434224539,"duration":4539}},{"uid":"82188d0f3759eb19","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0001797a0>: {\n        Underlying: <*exec.ExitError | 0xc0007b7460>{\n            ProcessState: {\n                pid: 6797,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 128944},\n                    Stime: {Sec: 0, Usec: 54703},\n                    Maxrss: 81364,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3771,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14104,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 826,\n                    Nivcsw: 490,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario3977468634\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario3977468634\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario3977468634\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario3977468634\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario3977468634\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1661356056000,"stop":1661356060476,"duration":4476}},{"uid":"edaa492eae13d072","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000e480>: {\n        Underlying: <*exec.ExitError | 0xc0007d4060>{\n            ProcessState: {\n                pid: 6175,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 211788},\n                    Stime: {Sec: 0, Usec: 52947},\n                    Maxrss: 78136,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3790,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14104,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 1046,\n                    Nivcsw: 671,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario2329930294\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario2329930294\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario2329930294\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario2329930294\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario2329930294\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1661352461000,"stop":1661352465706,"duration":4706}},{"uid":"5cdd75934524f846","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0006f1f08>: {\n        Underlying: <*exec.ExitError | 0xc0005fa9a0>{\n            ProcessState: {\n                pid: 6726,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 150132},\n                    Stime: {Sec: 0, Usec: 40945},\n                    Maxrss: 70184,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4196,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14104,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 948,\n                    Nivcsw: 436,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario3536546150\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario3536546150\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario3536546150\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario3536546150\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario3536546150\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1661352207000,"stop":1661352211436,"duration":4436}},{"uid":"9e87d3b562b9459f","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00034e168>: {\n        Underlying: <*exec.ExitError | 0xc0005520e0>{\n            ProcessState: {\n                pid: 6782,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 150025},\n                    Stime: {Sec: 0, Usec: 65865},\n                    Maxrss: 71872,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3842,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14104,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 831,\n                    Nivcsw: 556,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario859807691\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario859807691\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario859807691\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario859807691\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario859807691\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1661352028000,"stop":1661352032518,"duration":4518}},{"uid":"15ea942155e56498","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004f7b30>: {\n        Underlying: <*exec.ExitError | 0xc000718040>{\n            ProcessState: {\n                pid: 6098,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 146324},\n                    Stime: {Sec: 0, Usec: 67534},\n                    Maxrss: 76772,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 8499,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13616,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 836,\n                    Nivcsw: 434,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario4031362300\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario4031362300\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario4031362300\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario4031362300\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario4031362300\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1661351263000,"stop":1661351267481,"duration":4481}},{"uid":"d03048e2b39638a1","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004eb608>: {\n        Underlying: <*exec.ExitError | 0xc00014cd20>{\n            ProcessState: {\n                pid: 6620,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 187175},\n                    Stime: {Sec: 0, Usec: 80825},\n                    Maxrss: 78264,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3574,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14104,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 1285,\n                    Nivcsw: 874,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario4161556296\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario4161556296\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario4161556296\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario4161556296\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario4161556296\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1661188463000,"stop":1661188467706,"duration":4706}},{"uid":"75d3f346c8284831","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0000d7560>: {\n        Underlying: <*exec.ExitError | 0xc00014ba80>{\n            ProcessState: {\n                pid: 6337,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 147238},\n                    Stime: {Sec: 0, Usec: 36809},\n                    Maxrss: 82224,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3544,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14104,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 846,\n                    Nivcsw: 481,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario2610685715\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario2610685715\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario2610685715\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario2610685715\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario2610685715\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1661183038000,"stop":1661183042462,"duration":4462}},{"uid":"316f303230c3ea55","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000fc80>: {\n        Underlying: <*exec.ExitError | 0xc00039d0e0>{\n            ProcessState: {\n                pid: 6135,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 154629},\n                    Stime: {Sec: 0, Usec: 63437},\n                    Maxrss: 77152,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3973,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14104,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 953,\n                    Nivcsw: 594,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario713382670\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario713382670\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario713382670\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario713382670\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario713382670\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1661180055000,"stop":1661180059521,"duration":4521}},{"uid":"67fee4d1f09fe05c","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004e8858>: {\n        Underlying: <*exec.ExitError | 0xc0004a9460>{\n            ProcessState: {\n                pid: 6154,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 164267},\n                    Stime: {Sec: 0, Usec: 73920},\n                    Maxrss: 75720,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4692,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14104,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 920,\n                    Nivcsw: 513,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario2317615998\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario2317615998\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario2317615998\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario2317615998\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario2317615998\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1661152474000,"stop":1661152478586,"duration":4586}},{"uid":"c25184a295f3251e","status":"failed","statusDetails":"Expected an error to have occurred.  Got:\n    <nil>: nil","time":{"start":1661083598000,"stop":1661083638227,"duration":40227}},{"uid":"4cab2c9ee2ce180b","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00068eee8>: {\n        Underlying: <*exec.ExitError | 0xc00060da60>{\n            ProcessState: {\n                pid: 6152,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 61689},\n                    Stime: {Sec: 0, Usec: 12337},\n                    Maxrss: 42388,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2015,\n                    Majflt: 1,\n                    Nswap: 0,\n                    Inblock: 288,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 204,\n                    Nivcsw: 180,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Cluster \\\"cluster2\\\" is invalid: metadata.name: Invalid value: \\\"cluster2\\\": can't delete cluster which is participating in any slice\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The Cluster \\\"cluster2\\\" is invalid: metadata.name: Invalid value: \\\"cluster2\\\": can't delete cluster which is participating in any slice\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Cluster \\\"cluster2\\\" is invalid: metadata.name: Invalid value: \\\"cluster2\\\": can't delete cluster which is participating in any slice\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The Cluster \\\"cluster2\\\" is invalid: metadata.name: Invalid value: \\\"cluster2\\\": can't delete cluster which is participating in any slice\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The Cluster \"cluster2\" is invalid: metadata.name: Invalid value: \"cluster2\": can't delete cluster which is participating in any slice\noccurred","time":{"start":1661070332000,"stop":1661070704227,"duration":372227}},{"uid":"a0a0ce17e682a18b","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0007c0a80>: {\n        Underlying: <*exec.ExitError | 0xc000691ce0>{\n            ProcessState: {\n                pid: 6341,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 66709},\n                    Stime: {Sec: 0, Usec: 15696},\n                    Maxrss: 42924,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2491,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 224,\n                    Nivcsw: 324,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Cluster \\\"cluster2\\\" is invalid: metadata.name: Invalid value: \\\"cluster2\\\": can't delete cluster which is participating in any slice\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The Cluster \\\"cluster2\\\" is invalid: metadata.name: Invalid value: \\\"cluster2\\\": can't delete cluster which is participating in any slice\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Cluster \\\"cluster2\\\" is invalid: metadata.name: Invalid value: \\\"cluster2\\\": can't delete cluster which is participating in any slice\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The Cluster \\\"cluster2\\\" is invalid: metadata.name: Invalid value: \\\"cluster2\\\": can't delete cluster which is participating in any slice\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The Cluster \"cluster2\" is invalid: metadata.name: Invalid value: \"cluster2\": can't delete cluster which is participating in any slice\noccurred","time":{"start":1661066603000,"stop":1661066974882,"duration":371882}},{"uid":"2122a9659720938f","status":"failed","statusDetails":"Expected an error to have occurred.  Got:\n    <nil>: nil","time":{"start":1661018182000,"stop":1661018211885,"duration":29885}},{"uid":"fdde6f631bd2d936","status":"failed","statusDetails":"Expected an error to have occurred.  Got:\n    <nil>: nil","time":{"start":1661014620000,"stop":1661014651411,"duration":31411}},{"uid":"12730e0e0eb276ea","status":"failed","statusDetails":"Expected an error to have occurred.  Got:\n    <nil>: nil","time":{"start":1661011831000,"stop":1661011872519,"duration":41519}},{"uid":"365bbec59132abc0","status":"failed","statusDetails":"Expected an error to have occurred.  Got:\n    <nil>: nil","time":{"start":1661011780000,"stop":1661011831980,"duration":51980}},{"uid":"d1a914a0b7c5ab8","status":"failed","statusDetails":"Expected an error to have occurred.  Got:\n    <nil>: nil","time":{"start":1661009684000,"stop":1661009716043,"duration":32043}},{"uid":"7f38d0d56c16799c","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00064f218>: {\n        Underlying: <*exec.ExitError | 0xc000670240>{\n            ProcessState: {\n                pid: 6203,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 48153},\n                    Stime: {Sec: 0, Usec: 10318},\n                    Maxrss: 43040,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 1536,\n                    Majflt: 1,\n                    Nswap: 0,\n                    Inblock: 136,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 146,\n                    Nivcsw: 428,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Cluster \\\"cluster2\\\" is invalid: metadata.name: Invalid value: \\\"cluster2\\\": can't delete cluster which is participating in any slice\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The Cluster \\\"cluster2\\\" is invalid: metadata.name: Invalid value: \\\"cluster2\\\": can't delete cluster which is participating in any slice\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Cluster \\\"cluster2\\\" is invalid: metadata.name: Invalid value: \\\"cluster2\\\": can't delete cluster which is participating in any slice\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The Cluster \\\"cluster2\\\" is invalid: metadata.name: Invalid value: \\\"cluster2\\\": can't delete cluster which is participating in any slice\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The Cluster \"cluster2\" is invalid: metadata.name: Invalid value: \"cluster2\": can't delete cluster which is participating in any slice\noccurred","time":{"start":1661007831000,"stop":1661008198810,"duration":367810}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting iperf-client pod Check ping between iperf-server and iperf-client after iperf-client pod restart":{"statistic":{"failed":0,"broken":0,"skipped":51,"passed":0,"unknown":0,"total":51},"items":[{"uid":"2c308ea47f229aa0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"8d07a22cde1337a7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"1fca5d6461662ddf","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662348908000,"stop":1662348908000,"duration":0}},{"uid":"5c241307aa6296db","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"fde747318530b907","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"4c998bde6a802b2a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"884b18b00f66167a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"74cc8da19e9cec66","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"70dc208251a2ec37","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"7222796b4d2aa11d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"af14cbdb9b9a69b6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"3067af1dd0e0e0ed","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"30285a81205946e7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"614d5b7d2c1a71ac","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"17355c47a78ffa8a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"81eb1f9ea8106452","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"13bccfdb1f55ccf","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"5ecdc7763ea8b5fb","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"1c4d34acdc179a9c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"e229c74d458f3317","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice Creation scenarios Slice with LOW QOS should have gateway pod running":{"statistic":{"failed":4,"broken":0,"skipped":19,"passed":19,"unknown":0,"total":42},"items":[{"uid":"c930c2a744d5c379","status":"skipped","statusDetails":"skipped","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"8ff45f21c9baad58","status":"skipped","statusDetails":"skipped","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"974b81851b9e9583","status":"passed","time":{"start":1662348908000,"stop":1662348923065,"duration":15065}},{"uid":"fd91aee50d7caf8b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"f3d10113362257f8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"bdda39e964ef6ba0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"87db5fb32f48588d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"24286b78495bd28","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"f36c145611e6087b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"828f9da68e0aace","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"73f3a818e0d16fa6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"7c8643c6ab286ef3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"f91d469747efd535","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"8f2696314b982364","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"2bd1b4515291389b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"56239780991e202d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"6d97e2f3ddbf091a","status":"failed","statusDetails":"Timed out after 180.003s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1660113628000,"stop":1660113808230,"duration":180230}},{"uid":"db984b14975dc419","status":"failed","statusDetails":"Timed out after 180.003s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1660104898000,"stop":1660105078329,"duration":180329}},{"uid":"52d0b1af77db83e0","status":"passed","time":{"start":1660067259000,"stop":1660067304107,"duration":45107}},{"uid":"10963f3710a2b8f1","status":"passed","time":{"start":1660047551000,"stop":1660047576084,"duration":25084}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests slice with netpol Should install networkpolicies in all application namespaces":{"statistic":{"failed":0,"broken":0,"skipped":39,"passed":12,"unknown":0,"total":51},"items":[{"uid":"b709948847085595","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"70a3f134a94a722b","status":"passed","time":{"start":1663044069000,"stop":1663044069440,"duration":440}},{"uid":"b113cfea0aee742c","status":"passed","time":{"start":1662348908000,"stop":1662348908296,"duration":296}},{"uid":"b69f2e6b55cc9b45","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"b95084324ffcd2ac","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"16f4d74866a3e47b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"87a0370108a63528","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"33e6afd657a582b1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"725873a99af47eae","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"ba40c43acbeb7564","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"65b594b9485302dc","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"c4634056cf7f64e2","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"b754077ad098a7c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"f8b23fccc62594e5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"5f51308b0d424df0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"2bced604e45755f0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"f9790e32a3152ebf","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"abe1a1dc4ce71d5a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"c829bdfb31dc1fa1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"d7a74d3deea8d4ca","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}}]},"Istio Suite:Istio Suite#[It] Bookinfo tests Bookinfo Scenario 3 without egress and with ingress Should verify if Curl Traffic works on Productpage test":{"statistic":{"failed":0,"broken":0,"skipped":48,"passed":9,"unknown":0,"total":57},"items":[{"uid":"92c904a7ff85e66","status":"passed","time":{"start":1663043831000,"stop":1663043840302,"duration":9302}},{"uid":"715650d99643413","status":"passed","time":{"start":1663043075000,"stop":1663043084370,"duration":9370}},{"uid":"53d11a8d8367c24f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663040338000,"stop":1663040338000,"duration":0}},{"uid":"da90a3707433f08d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662819574000,"stop":1662819574000,"duration":0}},{"uid":"b4b8c0ebd849c7d3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662795310000,"stop":1662795310000,"duration":0}},{"uid":"42bd6c200a76ae99","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662792773000,"stop":1662792773000,"duration":0}},{"uid":"c0eec6874db4c2fa","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662620110000,"stop":1662620110000,"duration":0}},{"uid":"5cde4a47483534b2","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662617640000,"stop":1662617640000,"duration":0}},{"uid":"9590f255c143d869","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662610377000,"stop":1662610377000,"duration":0}},{"uid":"cc6099b548e67b1c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662557911000,"stop":1662557911000,"duration":0}},{"uid":"eca49435e73a0c0d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662553747000,"stop":1662553747000,"duration":0}},{"uid":"ca102f38d3b747fa","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662553327000,"stop":1662553327000,"duration":0}},{"uid":"cb7d79fdd7cfaf54","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662381943000,"stop":1662381943000,"duration":0}},{"uid":"3ef49b9ea9370415","status":"passed","time":{"start":1662348103000,"stop":1662348103542,"duration":542}},{"uid":"77ee0228857418c8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662049889000,"stop":1662049889000,"duration":0}},{"uid":"6b07e3985b8c2022","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662049074000,"stop":1662049074000,"duration":0}},{"uid":"a03247fc1dcc622e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662028179000,"stop":1662028179000,"duration":0}},{"uid":"1d6b9c87503a7fb","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661859205000,"stop":1661859205000,"duration":0}},{"uid":"a52a2e0352236318","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661843950000,"stop":1661843950000,"duration":0}},{"uid":"c4bd1565583b0380","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754632000,"stop":1661754632000,"duration":0}}]},"Istio Suite:Istio Suite#[It] Bookinfo tests Bookinfo Scenario 4 with egress and ingress Should verify if Ping works successfully from Productpage to details endpoint":{"statistic":{"failed":2,"broken":0,"skipped":50,"passed":5,"unknown":0,"total":57},"items":[{"uid":"7f7009252383f8fb","status":"passed","time":{"start":1663043831000,"stop":1663043919299,"duration":88299}},{"uid":"9c759007999e32e9","status":"passed","time":{"start":1663043075000,"stop":1663043135800,"duration":60800}},{"uid":"372980324663ea33","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663040338000,"stop":1663040338000,"duration":0}},{"uid":"2f22f11c93f06ba8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662819574000,"stop":1662819574000,"duration":0}},{"uid":"fc94fe2cbe0d2fd8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662795310000,"stop":1662795310000,"duration":0}},{"uid":"1044171832dac565","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662792773000,"stop":1662792773000,"duration":0}},{"uid":"ba48915df2f680b7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662620110000,"stop":1662620110000,"duration":0}},{"uid":"4921b7f5b2dd6cc3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662617640000,"stop":1662617640000,"duration":0}},{"uid":"6292561e4d4c64ce","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662610377000,"stop":1662610377000,"duration":0}},{"uid":"ddd64d906ec87e17","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662557911000,"stop":1662557911000,"duration":0}},{"uid":"63bd3a9a1eb090cc","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662553747000,"stop":1662553747000,"duration":0}},{"uid":"b1385d26dd6ab2e1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662553327000,"stop":1662553327000,"duration":0}},{"uid":"6f0edc9d84fc5f5d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662381943000,"stop":1662381943000,"duration":0}},{"uid":"46d1d4c2d95a0d74","status":"passed","time":{"start":1662348103000,"stop":1662348161469,"duration":58469}},{"uid":"d1026f276f422e97","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662049889000,"stop":1662049889000,"duration":0}},{"uid":"954f34b729754b84","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662049074000,"stop":1662049074000,"duration":0}},{"uid":"a2f052109f7f1e24","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662028179000,"stop":1662028179000,"duration":0}},{"uid":"3d5c956b99c3ec42","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661859205000,"stop":1661859205000,"duration":0}},{"uid":"a63ee062f191c088","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661843950000,"stop":1661843950000,"duration":0}},{"uid":"cae6440ce0d3636","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754632000,"stop":1661754632000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Hub netpol scenarios: Slice Configs Netpol scenarios Should fail when deploying clusters in namespaceisolationprofile with * and a cluster name in both applicationNamespace and allowedNamespace":{"statistic":{"failed":0,"broken":0,"skipped":23,"passed":28,"unknown":0,"total":51},"items":[{"uid":"ac1b283582f4407b","status":"skipped","statusDetails":"skipped","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"d26bf7646f1c10b6","status":"passed","time":{"start":1663044069000,"stop":1663044069484,"duration":484}},{"uid":"6049a55a659da158","status":"passed","time":{"start":1662348908000,"stop":1662348908527,"duration":527}},{"uid":"d2f7ced8524d507f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"811c3fe238560661","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"d472be9bbb2847af","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"d48b5d3625e00c77","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"a99d197257089012","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"c76209c7b49bdcf2","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"17451cce686a7b8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"628b1b82efaf6691","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"e59f6b7c3d0297d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"122dffff7cf89791","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"20992d36dc97c400","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"75f8e1c62cb14df9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"b7bc0a8f88bcbc15","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"b03d36df7f44a05d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"600d71a75a9351de","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"d07887859a71b553","status":"passed","time":{"start":1660067259000,"stop":1660067259358,"duration":358}},{"uid":"4ed7671a8dd4de62","status":"passed","time":{"start":1660047551000,"stop":1660047551304,"duration":304}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Update Should update successfully when Project applied with valid service account name in Write users":{"statistic":{"failed":16,"broken":0,"skipped":0,"passed":174,"unknown":0,"total":190},"items":[{"uid":"7aa9caa47a6aa6e3","status":"passed","time":{"start":1663042493000,"stop":1663042497225,"duration":4225}},{"uid":"b9b2bc8a047e8604","status":"passed","time":{"start":1663041865000,"stop":1663041869495,"duration":4495}},{"uid":"e0e0c5457daf9c48","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <bool>: false\nto be true","time":{"start":1663039902000,"stop":1663039968530,"duration":66530}},{"uid":"df30b7445b8acad4","status":"passed","time":{"start":1663009431000,"stop":1663009435254,"duration":4254}},{"uid":"e4174face24546fe","status":"passed","time":{"start":1662978562000,"stop":1662978566522,"duration":4522}},{"uid":"c2513ee65636d512","status":"passed","time":{"start":1662702362000,"stop":1662702367150,"duration":5150}},{"uid":"1ae1088a53113f9e","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <bool>: false\nto be true","time":{"start":1662553059000,"stop":1662553126501,"duration":67501}},{"uid":"9be54b046d86d320","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <bool>: false\nto be true","time":{"start":1662553042000,"stop":1662553163299,"duration":121299}},{"uid":"2d716a1a2f20beff","status":"passed","time":{"start":1662467486000,"stop":1662467491127,"duration":5127}},{"uid":"49f5cafe38f26cf2","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <bool>: false\nto be true","time":{"start":1662374048000,"stop":1662374169349,"duration":121349}},{"uid":"245281f4b9ca89bf","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0003cc138>: {\n        Underlying: <*exec.ExitError | 0xc000706040>{\n            ProcessState: {\n                pid: 7153,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 234562},\n                    Stime: {Sec: 0, Usec: 47781},\n                    Maxrss: 84968,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3032,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13344,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 350,\n                    Nivcsw: 295,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20applied%20with%20valid%20service%20account%20name%20in%20Write%20users1354068921\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.241.207:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20applied%20with%20valid%20service%20account%20name%20in%20Write%20users1354068921\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.241.207:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20applied%20with%20valid%20service%20account%20name%20in%20Write%20users1354068921\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.241.207:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20applied%20with%20valid%20service%20account%20name%20in%20Write%20users1354068921\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.241.207:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20applied%20with%20valid%20service%20account%20name%20in%20Write%20users1354068921\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.241.207:443: connect: connection refused\noccurred","time":{"start":1662374250000,"stop":1662374250333,"duration":333}},{"uid":"c178cb61eb1e5f2b","status":"passed","time":{"start":1662361116000,"stop":1662361121163,"duration":5163}},{"uid":"5daface006d6f9ad","status":"passed","time":{"start":1662347286000,"stop":1662347291164,"duration":5164}},{"uid":"5d571f8de3845747","status":"passed","time":{"start":1662049179000,"stop":1662049183659,"duration":4659}},{"uid":"6d0da5f0006f1ef","status":"passed","time":{"start":1662048808000,"stop":1662048812453,"duration":4453}},{"uid":"fc990ea1b07ba8f4","status":"passed","time":{"start":1662036991000,"stop":1662036996125,"duration":5125}},{"uid":"7f112da4ef629a28","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0007bb128>: {\n        Underlying: <*exec.ExitError | 0xc00091dca0>{\n            ProcessState: {\n                pid: 9099,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 141659},\n                    Stime: {Sec: 0, Usec: 60711},\n                    Maxrss: 84436,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4369,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13344,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 363,\n                    Nivcsw: 279,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when applying patch:\",\n                        \"{\\\"status\\\":{}}\",\n                        \"to:\",\n                        \"Resource: \\\"controller.kubeslice.io/v1alpha1, Resource=projects\\\", GroupVersionKind: \\\"controller.kubeslice.io/v1alpha1, Kind=Project\\\"\",\n                        \"Name: \\\"projectupdatetest\\\", Namespace: \\\"kubeslice-controller\\\"\",\n                        \"for: \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20applied%20with%20valid%20service%20account%20name%20in%20Write%20users1081897832\\\": error when patching \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20applied%20with%20valid%20service%20account%20name%20in%20Write%20users1081897832\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.231.22:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when applying patch:\",\n                    \"{\\\"status\\\":{}}\",\n                    \"to:\",\n                    \"Resource: \\\"controller.kubeslice.io/v1alpha1, Resource=projects\\\", GroupVersionKind: \\\"controller.kubeslice.io/v1alpha1, Kind=Project\\\"\",\n                    \"Name: \\\"projectupdatetest\\\", Namespace: \\\"kubeslice-controller\\\"\",\n                    \"for: \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20applied%20with%20valid%20service%20account%20name%20in%20Write%20users1081897832\\\": error when patching \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20applied%20with%20valid%20service%20account%20name%20in%20Write%20users1081897832\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.231.22:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when applying patch:\",\n                        \"{\\\"status\\\":{}}\",\n                        \"to:\",\n                        \"Resource: \\\"controller.kubeslice.io/v1alpha1, Resource=projects\\\", GroupVersionKind: \\\"controller.kubeslice.io/v1alpha1, Kind=Project\\\"\",\n                        \"Name: \\\"projectupdatetest\\\", Namespace: \\\"kubeslice-controller\\\"\",\n                        \"for: \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20applied%20with%20valid%20service%20account%20n...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; Error from server (InternalError): error when applying patch:\n    {\"status\":{}}\n    to:\n    Resource: \"controller.kubeslice.io/v1alpha1, Resource=projects\", GroupVersionKind: \"controller.kubeslice.io/v1alpha1, Kind=Project\"\n    Name: \"projectupdatetest\", Namespace: \"kubeslice-controller\"\n    for: \"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20applied%20with%20valid%20service%20account%20name%20in%20Write%20users1081897832\": error when patching \"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20applied%20with%20valid%20service%20account%20name%20in%20Write%20users1081897832\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.231.22:443: connect: connection refused\noccurred","time":{"start":1662031654000,"stop":1662031714303,"duration":60303}},{"uid":"14e48bb19f9e4060","status":"passed","time":{"start":1662027354000,"stop":1662027358460,"duration":4460}},{"uid":"25a4d45eceb78583","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000f8d8>: {\n        Underlying: <*exec.ExitError | 0xc0006d8060>{\n            ProcessState: {\n                pid: 7692,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 162291},\n                    Stime: {Sec: 0, Usec: 57961},\n                    Maxrss: 77032,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 5472,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13344,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 396,\n                    Nivcsw: 242,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20applied%20with%20valid%20service%20account%20name%20in%20Write%20users3471566378\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.242.99:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20applied%20with%20valid%20service%20account%20name%20in%20Write%20users3471566378\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.242.99:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20applied%20with%20valid%20service%20account%20name%20in%20Write%20users3471566378\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.242.99:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20applied%20with%20valid%20service%20account%20name%20in%20Write%20users3471566378\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.242.99:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20applied%20with%20valid%20service%20account%20name%20in%20Write%20users3471566378\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.242.99:443: connect: connection refused\noccurred","time":{"start":1661952242000,"stop":1661952242273,"duration":273}},{"uid":"dd330fc1d4e345a8","status":"passed","time":{"start":1661863163000,"stop":1661863167439,"duration":4439}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Update Should fail when Project is Applied with service account name as combination of special characters in Read users":{"statistic":{"failed":16,"broken":0,"skipped":0,"passed":174,"unknown":0,"total":190},"items":[{"uid":"6ebe867d20730406","status":"passed","time":{"start":1663042493000,"stop":1663042497851,"duration":4851}},{"uid":"3c985031bab05ed8","status":"passed","time":{"start":1663041865000,"stop":1663041870162,"duration":5162}},{"uid":"368180edbfd00cee","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <bool>: false\nto be true","time":{"start":1663039902000,"stop":1663039969432,"duration":67432}},{"uid":"2b2070d0ba9f44c4","status":"passed","time":{"start":1663009431000,"stop":1663009435930,"duration":4930}},{"uid":"f5d9cf778d596517","status":"passed","time":{"start":1662978562000,"stop":1662978568136,"duration":6136}},{"uid":"1bb5fc6dd5a42676","status":"passed","time":{"start":1662702362000,"stop":1662702367903,"duration":5903}},{"uid":"354783cb8fe6093e","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <bool>: false\nto be true","time":{"start":1662553059000,"stop":1662553180292,"duration":121292}},{"uid":"a3a481df1cc69129","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <bool>: false\nto be true","time":{"start":1662553042000,"stop":1662553109464,"duration":67464}},{"uid":"49bbbbcbaf030a49","status":"passed","time":{"start":1662467486000,"stop":1662467490916,"duration":4916}},{"uid":"b8bcd26d2c3bd74b","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <bool>: false\nto be true","time":{"start":1662374048000,"stop":1662374115415,"duration":67415}},{"uid":"c894a63a87aeabc4","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0003cdf80>: {\n        Underlying: <*exec.ExitError | 0xc0007070a0>{\n            ProcessState: {\n                pid: 7181,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 224603},\n                    Stime: {Sec: 0, Usec: 42597},\n                    Maxrss: 88972,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2879,\n                    Majflt: 1,\n                    Nswap: 0,\n                    Inblock: 128,\n                    Oublock: 13344,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 449,\n                    Nivcsw: 374,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Read%20users2968677060\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.241.207:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Read%20users2968677060\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.241.207:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Read%20users2968677060\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.241.207:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Read%20users2968677060\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.241.207:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Read%20users2968677060\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.241.207:443: connect: connection refused\noccurred","time":{"start":1662374250000,"stop":1662374250354,"duration":354}},{"uid":"41216688f0c1db5f","status":"passed","time":{"start":1662361116000,"stop":1662361122081,"duration":6081}},{"uid":"2fe5c3ffb2d78848","status":"passed","time":{"start":1662347286000,"stop":1662347291882,"duration":5882}},{"uid":"95792dd21982742f","status":"passed","time":{"start":1662049179000,"stop":1662049184232,"duration":5232}},{"uid":"2350d47a2665f2a2","status":"passed","time":{"start":1662048808000,"stop":1662048813040,"duration":5040}},{"uid":"54c8e793a384c057","status":"passed","time":{"start":1662036991000,"stop":1662036995816,"duration":4816}},{"uid":"7fc1d0d004f5ec37","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0006b5308>: {\n        Underlying: <*exec.ExitError | 0xc00091c100>{\n            ProcessState: {\n                pid: 9598,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 172065},\n                    Stime: {Sec: 0, Usec: 42060},\n                    Maxrss: 79768,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4055,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13344,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 323,\n                    Nivcsw: 208,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Read%20users1535329239\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.231.22:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Read%20users1535329239\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.231.22:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Read%20users1535329239\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.231.22:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Read%20users1535329239\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.231.22:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Read%20users1535329239\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.231.22:443: connect: connection refused\noccurred","time":{"start":1662031654000,"stop":1662031654260,"duration":260}},{"uid":"91e4723d6332e77b","status":"passed","time":{"start":1662027354000,"stop":1662027359012,"duration":5012}},{"uid":"66cc2cb99de4f078","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000e030>: {\n        Underlying: <*exec.ExitError | 0xc0006d8040>{\n            ProcessState: {\n                pid: 7719,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 194948},\n                    Stime: {Sec: 0, Usec: 26757},\n                    Maxrss: 80404,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4312,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13344,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 416,\n                    Nivcsw: 409,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Read%20users3297848746\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.242.99:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Read%20users3297848746\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.242.99:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Read%20users3297848746\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.242.99:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Read%20users3297848746\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.242.99:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Read%20users3297848746\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.242.99:443: connect: connection refused\noccurred","time":{"start":1661952242000,"stop":1661952242275,"duration":275}},{"uid":"23104be4cc204e2b","status":"passed","time":{"start":1661863163000,"stop":1661863169084,"duration":6084}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Creation Should create Project while using valid manifest":{"statistic":{"failed":14,"broken":0,"skipped":0,"passed":179,"unknown":0,"total":193},"items":[{"uid":"44f4066563362da1","status":"passed","time":{"start":1663042493000,"stop":1663042501328,"duration":8328}},{"uid":"e041b4a95cfd7cdf","status":"passed","time":{"start":1663041865000,"stop":1663041872837,"duration":7837}},{"uid":"6d68451afcad9f2b","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000824ab0>: {\n        Underlying: <*exec.ExitError | 0xc00030d620>{\n            ProcessState: {\n                pid: 10058,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 166690},\n                    Stime: {Sec: 0, Usec: 48787},\n                    Maxrss: 85960,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2812,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13344,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 438,\n                    Nivcsw: 315,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20create%20Project%20while%20using%20valid%20manifest1638295105\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.217.78:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20create%20Project%20while%20using%20valid%20manifest1638295105\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.217.78:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20create%20Project%20while%20using%20valid%20manifest1638295105\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.217.78:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20create%20Project%20while%20using%20valid%20manifest1638295105\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.217.78:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20create%20Project%20while%20using%20valid%20manifest1638295105\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.217.78:443: connect: connection refused\noccurred","time":{"start":1663039902000,"stop":1663039902260,"duration":260}},{"uid":"f51ed767517809a5","status":"passed","time":{"start":1663009431000,"stop":1663009439567,"duration":8567}},{"uid":"c78d07339a353618","status":"passed","time":{"start":1662978562000,"stop":1662978577831,"duration":15831}},{"uid":"896fd573d55d4b39","status":"passed","time":{"start":1662702362000,"stop":1662702369675,"duration":7675}},{"uid":"37d90046d980a40c","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <bool>: false\nto be true","time":{"start":1662553059000,"stop":1662553127191,"duration":68191}},{"uid":"e19a84139670f819","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0005a4300>: {\n        Underlying: <*exec.ExitError | 0xc00065cbc0>{\n            ProcessState: {\n                pid: 10926,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 154985},\n                    Stime: {Sec: 0, Usec: 79479},\n                    Maxrss: 76416,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 6262,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 364,\n                    Nivcsw: 338,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20create%20Project%20while%20using%20valid%20manifest2166957845\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.123.224:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20create%20Project%20while%20using%20valid%20manifest2166957845\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.123.224:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20create%20Project%20while%20using%20valid%20manifest2166957845\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.123.224:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20create%20Project%20while%20using%20valid%20manifest2166957845\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.123.224:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20create%20Project%20while%20using%20valid%20manifest2166957845\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.123.224:443: connect: connection refused\noccurred","time":{"start":1662553042000,"stop":1662553042276,"duration":276}},{"uid":"46e237393ec59e5f","status":"passed","time":{"start":1662467486000,"stop":1662467493677,"duration":7677}},{"uid":"de575b312ae9144","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000700018>: {\n        Underlying: <*exec.ExitError | 0xc000508040>{\n            ProcessState: {\n                pid: 10970,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 290174},\n                    Stime: {Sec: 0, Usec: 115274},\n                    Maxrss: 86112,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4327,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 306,\n                    Nivcsw: 854,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20create%20Project%20while%20using%20valid%20manifest331697125\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.14.145:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20create%20Project%20while%20using%20valid%20manifest331697125\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.14.145:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20create%20Project%20while%20using%20valid%20manifest331697125\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.14.145:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20create%20Project%20while%20using%20valid%20manifest331697125\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.14.145:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20create%20Project%20while%20using%20valid%20manifest331697125\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.14.145:443: connect: connection refused\noccurred","time":{"start":1662374048000,"stop":1662374048501,"duration":501}},{"uid":"befa866c0079ddf6","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0003529d8>: {\n        Underlying: <*exec.ExitError | 0xc0008901a0>{\n            ProcessState: {\n                pid: 7217,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 216463},\n                    Stime: {Sec: 0, Usec: 38199},\n                    Maxrss: 76876,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2959,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13344,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 369,\n                    Nivcsw: 354,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20create%20Project%20while%20using%20valid%20manifest2596022009\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.241.207:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20create%20Project%20while%20using%20valid%20manifest2596022009\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.241.207:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20create%20Project%20while%20using%20valid%20manifest2596022009\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.241.207:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20create%20Project%20while%20using%20valid%20manifest2596022009\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.241.207:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20create%20Project%20while%20using%20valid%20manifest2596022009\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.241.207:443: connect: connection refused\noccurred","time":{"start":1662374250000,"stop":1662374250334,"duration":334}},{"uid":"da9599a4127e0cd9","status":"passed","time":{"start":1662361116000,"stop":1662361131554,"duration":15554}},{"uid":"55cf1be788598d44","status":"passed","time":{"start":1662347286000,"stop":1662347294479,"duration":8479}},{"uid":"46207211ccdb14fd","status":"passed","time":{"start":1662049179000,"stop":1662049187925,"duration":8925}},{"uid":"765df7d40e64a53c","status":"passed","time":{"start":1662048808000,"stop":1662048816804,"duration":8804}},{"uid":"ef388b4c83e8ae8d","status":"passed","time":{"start":1662036991000,"stop":1662036998656,"duration":7656}},{"uid":"c419ebf81f1a48e3","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <bool>: false\nto be true","time":{"start":1662031654000,"stop":1662031721412,"duration":67412}},{"uid":"e2e28880addbaa95","status":"passed","time":{"start":1662027354000,"stop":1662027362948,"duration":8948}},{"uid":"558d259b61e9ebf1","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <bool>: false\nto be true","time":{"start":1661952242000,"stop":1661952364094,"duration":122094}},{"uid":"1e31f42a9d6ce420","status":"passed","time":{"start":1661863163000,"stop":1661863171753,"duration":8753}}]},"Istio Suite:Istio Suite#[It] Bookinfo tests Bookinfo Scenario 1 without egress and ingress Should verify if Curl Traffic works on Productpage test":{"statistic":{"failed":0,"broken":0,"skipped":44,"passed":13,"unknown":0,"total":57},"items":[{"uid":"d11a6f3a5dfb0e7d","status":"passed","time":{"start":1663043831000,"stop":1663043834278,"duration":3278}},{"uid":"8117bc2d28568dd2","status":"passed","time":{"start":1663043075000,"stop":1663043075551,"duration":551}},{"uid":"f9fa7af15dea66e4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663040338000,"stop":1663040338000,"duration":0}},{"uid":"80ab7dbde32f2cfe","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662819574000,"stop":1662819574000,"duration":0}},{"uid":"d7f3fe3936598d21","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662795310000,"stop":1662795310000,"duration":0}},{"uid":"d7d09a59832ab00e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662792773000,"stop":1662792773000,"duration":0}},{"uid":"52aa1f9f4c4c2010","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662620110000,"stop":1662620110000,"duration":0}},{"uid":"a2b637fd4063439a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662617640000,"stop":1662617640000,"duration":0}},{"uid":"9a88e958dde7e098","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662610377000,"stop":1662610377000,"duration":0}},{"uid":"998e345aec242ff1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662557911000,"stop":1662557911000,"duration":0}},{"uid":"7b8f3ce56e8fed53","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662553747000,"stop":1662553747000,"duration":0}},{"uid":"c49d6348f17245b1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662553327000,"stop":1662553327000,"duration":0}},{"uid":"23101d762ac56195","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662381943000,"stop":1662381943000,"duration":0}},{"uid":"bdd3f54dfba2f8ce","status":"passed","time":{"start":1662348103000,"stop":1662348103629,"duration":629}},{"uid":"8e54ebfd8f2a1b6a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662049889000,"stop":1662049889000,"duration":0}},{"uid":"14b9c0a17a4eb9db","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662049074000,"stop":1662049074000,"duration":0}},{"uid":"7cd2d504b69c9222","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662028179000,"stop":1662028179000,"duration":0}},{"uid":"821468a8b1b1c830","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661859205000,"stop":1661859205000,"duration":0}},{"uid":"850cd25bf36fa157","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661843950000,"stop":1661843950000,"duration":0}},{"uid":"bc67b01d22178af2","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754632000,"stop":1661754632000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting iperf-client pod Should restart iperf-client pod":{"statistic":{"failed":0,"broken":0,"skipped":51,"passed":0,"unknown":0,"total":51},"items":[{"uid":"926c0e10413cadea","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"254ce42c2479c0a3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"bf18af7d9dbb5b81","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662348908000,"stop":1662348908000,"duration":0}},{"uid":"e754f608ee414e0e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"1b773cf23f270ed8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"282a0f31b143fea","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"dfd1e8ddf42f469f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"332c38b62b4bc970","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"1244a264093dd370","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"b82c09aad3b05740","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"a2fe4bd92c780911","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"b673f154df1323e8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"65b51b0637fbc5cb","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"22cbbcb5807ceea7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"3f1bdf2d3340251c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"97eccf445f16f957","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"3771a5d1186d5a76","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"9f15fa0a8bdf138c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"bd27a581f053c534","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"74c77ba825a795ea","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Namespace reconcile test scenarios: Onboard cluster objs should have app ns & attached slice entry":{"statistic":{"failed":9,"broken":0,"skipped":1,"passed":41,"unknown":0,"total":51},"items":[{"uid":"a10e62794104967c","status":"skipped","statusDetails":"skipped","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"6d1296a18410294d","status":"failed","statusDetails":"Expected\n    <int>: 0\nto equal\n    <int>: 2","time":{"start":1663044069000,"stop":1663044091249,"duration":22249}},{"uid":"ec96360cad334a84","status":"failed","statusDetails":"Expected\n    <int>: 0\nto equal\n    <int>: 2","time":{"start":1662348908000,"stop":1662348929673,"duration":21673}},{"uid":"3ab56ca33c2b180e","status":"failed","statusDetails":"Expected\n    <int>: 0\nto equal\n    <int>: 2","time":{"start":1661844205000,"stop":1661844227038,"duration":22038}},{"uid":"7d68d6ca7c5773f9","status":"failed","statusDetails":"Expected\n    <int>: 0\nto equal\n    <int>: 2","time":{"start":1661754870000,"stop":1661754891762,"duration":21762}},{"uid":"f684b94ff9cbaa6b","status":"passed","time":{"start":1661611962000,"stop":1661611962178,"duration":178}},{"uid":"9a1a7b83d50f4e9d","status":"failed","statusDetails":"Expected\n    <int>: 0\nto equal\n    <int>: 2","time":{"start":1661585077000,"stop":1661585098905,"duration":21905}},{"uid":"efff75aab0b286a6","status":"passed","time":{"start":1661584447000,"stop":1661584447213,"duration":213}},{"uid":"b3f1f2598ec7b96c","status":"passed","time":{"start":1661580045000,"stop":1661580045197,"duration":197}},{"uid":"5a0c2ae2acb1c21a","status":"failed","statusDetails":"Expected\n    <int>: 0\nto equal\n    <int>: 2","time":{"start":1661578505000,"stop":1661578527023,"duration":22023}},{"uid":"88e078d8587aeafa","status":"failed","statusDetails":"Expected\n    <int>: 0\nto equal\n    <int>: 2","time":{"start":1661572479000,"stop":1661572501148,"duration":22148}},{"uid":"669450aa59ab134b","status":"passed","time":{"start":1660832281000,"stop":1660832281144,"duration":144}},{"uid":"9135abeb8e2cca39","status":"failed","statusDetails":"Expected\n    <int>: 0\nto equal\n    <int>: 2","time":{"start":1660808797000,"stop":1660808818719,"duration":21719}},{"uid":"10cfc04b8b336a0e","status":"failed","statusDetails":"Expected\n    <int>: 0\nto equal\n    <int>: 2","time":{"start":1660299182000,"stop":1660299203653,"duration":21653}},{"uid":"3774be67279f862b","status":"passed","time":{"start":1660295740000,"stop":1660295740137,"duration":137}},{"uid":"841c993985726627","status":"passed","time":{"start":1660293657000,"stop":1660293657168,"duration":168}},{"uid":"fd20553455f463a","status":"passed","time":{"start":1660113628000,"stop":1660113628155,"duration":155}},{"uid":"a122f0fcd157933f","status":"passed","time":{"start":1660104898000,"stop":1660104898158,"duration":158}},{"uid":"5bf00521790334c7","status":"passed","time":{"start":1660067259000,"stop":1660067259351,"duration":351}},{"uid":"ea7a254be89923dc","status":"passed","time":{"start":1660047551000,"stop":1660047551258,"duration":258}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Iperf connectivity Should install slice":{"statistic":{"failed":29,"broken":0,"skipped":0,"passed":20,"unknown":0,"total":49},"items":[{"uid":"3c921c498b7b45cc","status":"passed","time":{"start":1662348908000,"stop":1662348908260,"duration":260}},{"uid":"c40635d0d2aaa996","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004688e8>: {\n        Underlying: <*exec.ExitError | 0xc000708100>{\n            ProcessState: {\n                pid: 7138,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 249795},\n                    Stime: {Sec: 0, Usec: 62448},\n                    Maxrss: 90740,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 7322,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 413,\n                    Nivcsw: 397,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\noccurred","time":{"start":1661844205000,"stop":1661844205286,"duration":286}},{"uid":"237a215a03366311","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0008f0018>: {\n        Underlying: <*exec.ExitError | 0xc00095e960>{\n            ProcessState: {\n                pid: 7088,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 169526},\n                    Stime: {Sec: 0, Usec: 65498},\n                    Maxrss: 84788,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4448,\n                    Majflt: 1,\n                    Nswap: 0,\n                    Inblock: 48,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 358,\n                    Nivcsw: 561,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661754870000,"stop":1661754870289,"duration":289}},{"uid":"a658e32c1cd40c18","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004425e8>: {\n        Underlying: <*exec.ExitError | 0xc0007b1d80>{\n            ProcessState: {\n                pid: 7266,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 227669},\n                    Stime: {Sec: 0, Usec: 62805},\n                    Maxrss: 81584,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3523,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 782,\n                    Nivcsw: 436,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661611962000,"stop":1661611962291,"duration":291}},{"uid":"8b53401a459dff6a","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00084bd40>: {\n        Underlying: <*exec.ExitError | 0xc0006fb6e0>{\n            ProcessState: {\n                pid: 7258,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 176391},\n                    Stime: {Sec: 0, Usec: 53684},\n                    Maxrss: 84804,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3717,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 320,\n                    Nivcsw: 410,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661585077000,"stop":1661585077246,"duration":246}},{"uid":"b4592669ce917cbf","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00048e210>: {\n        Underlying: <*exec.ExitError | 0xc000717d00>{\n            ProcessState: {\n                pid: 6167,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 156261},\n                    Stime: {Sec: 0, Usec: 58598},\n                    Maxrss: 91564,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4112,\n                    Majflt: 1,\n                    Nswap: 0,\n                    Inblock: 128,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 321,\n                    Nivcsw: 488,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661584447000,"stop":1661584447254,"duration":254}},{"uid":"5e2f5c89299ff842","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0001917a0>: {\n        Underlying: <*exec.ExitError | 0xc00083f0a0>{\n            ProcessState: {\n                pid: 7223,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 176362},\n                    Stime: {Sec: 0, Usec: 34412},\n                    Maxrss: 85092,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4031,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 392,\n                    Nivcsw: 977,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661580045000,"stop":1661580045233,"duration":233}},{"uid":"a7736798c199909b","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0007780f0>: {\n        Underlying: <*exec.ExitError | 0xc000774500>{\n            ProcessState: {\n                pid: 7093,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 244570},\n                    Stime: {Sec: 0, Usec: 65995},\n                    Maxrss: 87740,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 12288,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 450,\n                    Nivcsw: 494,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661578505000,"stop":1661578505298,"duration":298}},{"uid":"e0c1fe4260693465","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000895350>: {\n        Underlying: <*exec.ExitError | 0xc0007f9760>{\n            ProcessState: {\n                pid: 7106,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 274360},\n                    Stime: {Sec: 0, Usec: 77803},\n                    Maxrss: 88844,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 9306,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 868,\n                    Nivcsw: 791,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661572479000,"stop":1661572480334,"duration":1334}},{"uid":"d336dad48b1ea343","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004c2ae0>: {\n        Underlying: <*exec.ExitError | 0xc00086e5c0>{\n            ProcessState: {\n                pid: 7116,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 190582},\n                    Stime: {Sec: 0, Usec: 34651},\n                    Maxrss: 84548,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4907,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 362,\n                    Nivcsw: 279,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660832281000,"stop":1660832281207,"duration":207}},{"uid":"df1a04414ab30732","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000127110>: {\n        Underlying: <*exec.ExitError | 0xc0006ba4e0>{\n            ProcessState: {\n                pid: 7162,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 158077},\n                    Stime: {Sec: 0, Usec: 56745},\n                    Maxrss: 92508,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4012,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 333,\n                    Nivcsw: 533,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660808797000,"stop":1660808797247,"duration":247}},{"uid":"ea42b83a1b9092cf","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00066c3a8>: {\n        Underlying: <*exec.ExitError | 0xc0006471c0>{\n            ProcessState: {\n                pid: 7067,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 128316},\n                    Stime: {Sec: 0, Usec: 44247},\n                    Maxrss: 80264,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 9521,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 412,\n                    Nivcsw: 300,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660299182000,"stop":1660299182197,"duration":197}},{"uid":"eacf557b92ba96b8","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0000ce5b8>: {\n        Underlying: <*exec.ExitError | 0xc0008b8b00>{\n            ProcessState: {\n                pid: 7210,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 132413},\n                    Stime: {Sec: 0, Usec: 46985},\n                    Maxrss: 80556,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3993,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 377,\n                    Nivcsw: 203,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660295740000,"stop":1660295740186,"duration":186}},{"uid":"9dcf6e391f45ea53","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000649ad0>: {\n        Underlying: <*exec.ExitError | 0xc000829900>{\n            ProcessState: {\n                pid: 7169,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 169532},\n                    Stime: {Sec: 0, Usec: 59139},\n                    Maxrss: 82468,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 5967,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 299,\n                    Nivcsw: 316,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660293657000,"stop":1660293657257,"duration":257}},{"uid":"82afcbf8277b6889","status":"failed","statusDetails":"Unexpected error:\n    <*errors.StatusError | 0xc0000f6a00>: {\n        ErrStatus: {\n            TypeMeta: {Kind: \"\", APIVersion: \"\"},\n            ListMeta: {\n                SelfLink: \"\",\n                ResourceVersion: \"\",\n                Continue: \"\",\n                RemainingItemCount: nil,\n            },\n            Status: \"Failure\",\n            Message: \"namespaces \\\"iperf\\\" already exists\",\n            Reason: \"AlreadyExists\",\n            Details: {Name: \"iperf\", Group: \"\", Kind: \"namespaces\", UID: \"\", Causes: nil, RetryAfterSeconds: 0},\n            Code: 409,\n        },\n    }\n    namespaces \"iperf\" already exists\noccurred","time":{"start":1660113628000,"stop":1660113628360,"duration":360}},{"uid":"8898fd59cdc037cd","status":"failed","statusDetails":"Unexpected error:\n    <*errors.StatusError | 0xc00098bf40>: {\n        ErrStatus: {\n            TypeMeta: {Kind: \"\", APIVersion: \"\"},\n            ListMeta: {\n                SelfLink: \"\",\n                ResourceVersion: \"\",\n                Continue: \"\",\n                RemainingItemCount: nil,\n            },\n            Status: \"Failure\",\n            Message: \"namespaces \\\"iperf\\\" already exists\",\n            Reason: \"AlreadyExists\",\n            Details: {Name: \"iperf\", Group: \"\", Kind: \"namespaces\", UID: \"\", Causes: nil, RetryAfterSeconds: 0},\n            Code: 409,\n        },\n    }\n    namespaces \"iperf\" already exists\noccurred","time":{"start":1660104898000,"stop":1660104898528,"duration":528}},{"uid":"f7f482a66199998b","status":"passed","time":{"start":1660067259000,"stop":1660067259240,"duration":240}},{"uid":"16cb0f9377c13402","status":"passed","time":{"start":1660047551000,"stop":1660047551904,"duration":904}},{"uid":"81397e1af8087c56","status":"passed","time":{"start":1659982549000,"stop":1659982550984,"duration":1984}},{"uid":"55f4eddab014d14","status":"passed","time":{"start":1659970182000,"stop":1659970182838,"duration":838}}]},"Worker Suite:Worker Suite#[It] Intracluster slice tests Iperf connectivity Verify Iperf traffic":{"statistic":{"failed":0,"broken":0,"skipped":46,"passed":5,"unknown":0,"total":51},"items":[{"uid":"6005108bdb98a8c5","status":"skipped","statusDetails":"skipped","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"63a004836a11a663","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"d932ba3959d64d78","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662348908000,"stop":1662348908000,"duration":0}},{"uid":"f6055a860c82c191","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"a060fd9c7ffa3609","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"1e95eeeb30062bcb","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"d8c9c0880e401d6a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"73395a9d55d5d05b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"2a74c8ba061ca6d5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"3cd0ba31c4f0bbb2","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"37c564a63eb77e21","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"bb95be07e74753d5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"f628ee148388006b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"ae99bacad219ebc1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"93c952e4bb2b6a94","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"d44bc2219e08aebe","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"93e4a76dfc51a352","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"5279f931074749bd","status":"passed","time":{"start":1660104898000,"stop":1660104967723,"duration":69723}},{"uid":"a60ed165405bfb9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"ac8fbb740fe2a6d8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}}]},"Istio Suite:Istio Suite#[It] Bookinfo tests Bookinfo Scenario 1 without egress and ingress Should verify if Ping works successfully from Productpage to details endpoint":{"statistic":{"failed":5,"broken":0,"skipped":44,"passed":8,"unknown":0,"total":57},"items":[{"uid":"88b0f28cf8bc871d","status":"failed","statusDetails":"Unexpected non-nil/non-zero argument at index 1:\n\t<*shell.ErrWithCmdOutput>: &shell.ErrWithCmdOutput{Underlying:(*exec.ExitError)(0xc0005f2c00), Output:(*shell.output)(0xc000490e28)}","time":{"start":1663043831000,"stop":1663043897886,"duration":66886}},{"uid":"9aa7f6dbd9bed433","status":"passed","time":{"start":1663043075000,"stop":1663043135208,"duration":60208}},{"uid":"eb94510176218015","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663040338000,"stop":1663040338000,"duration":0}},{"uid":"4a487778518b128f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662819574000,"stop":1662819574000,"duration":0}},{"uid":"86411691f3276eff","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662795310000,"stop":1662795310000,"duration":0}},{"uid":"8c94a11f1dbf5520","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662792773000,"stop":1662792773000,"duration":0}},{"uid":"ebdb190ab7d7a948","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662620110000,"stop":1662620110000,"duration":0}},{"uid":"6f255fa1866a42e3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662617640000,"stop":1662617640000,"duration":0}},{"uid":"3444ee98dfe3749c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662610377000,"stop":1662610377000,"duration":0}},{"uid":"835bfa7b5c384c09","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662557911000,"stop":1662557911000,"duration":0}},{"uid":"d5d9f45f016b1d94","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662553747000,"stop":1662553747000,"duration":0}},{"uid":"7a49591121ed60ba","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662553327000,"stop":1662553327000,"duration":0}},{"uid":"e66556f7fd833f3a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662381943000,"stop":1662381943000,"duration":0}},{"uid":"b1497bbee9a6cdd3","status":"passed","time":{"start":1662348103000,"stop":1662348160544,"duration":57544}},{"uid":"35a71a3eac9575d6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662049889000,"stop":1662049889000,"duration":0}},{"uid":"55dbc73e96c36d0c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662049074000,"stop":1662049074000,"duration":0}},{"uid":"5a8f2926e097e497","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662028179000,"stop":1662028179000,"duration":0}},{"uid":"bdd87f18feff512a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661859205000,"stop":1661859205000,"duration":0}},{"uid":"e31c5c713158f204","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661843950000,"stop":1661843950000,"duration":0}},{"uid":"abafd098e08d1696","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754632000,"stop":1661754632000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Intracluster slice tests Slice with netpol should reconcile netpol config":{"statistic":{"failed":7,"broken":0,"skipped":28,"passed":16,"unknown":0,"total":51},"items":[{"uid":"82fc235d39fd0e76","status":"skipped","statusDetails":"skipped","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"3c83827c5535a59f","status":"passed","time":{"start":1663044069000,"stop":1663044071285,"duration":2285}},{"uid":"4dc51b57091f9dd2","status":"failed","statusDetails":"\n\tError Trace:\tnamespace.go:15\n\t            \t\t\t\tt_intra_slice_test.go:98\n\t            \t\t\t\tsuite.go:596\n\t            \t\t\t\tasm_amd64.s:1581\n\tError:      \tReceived unexpected error:\n\t            \tnamespaces \"iperf\" already exists\n\tTest:       \tIntracluster slice tests Slice with netpol should reconcile netpol config\n","time":{"start":1662348908000,"stop":1662348916781,"duration":8781}},{"uid":"5213fb1fa9cf7106","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"d72c7c11f9b556d7","status":"skipped","statusDetails":"skipped - Spec skipped because Skip() was called in BeforeAll","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"f7e8db10b0c7f2d8","status":"skipped","statusDetails":"skipped - Spec skipped because Skip() was called in BeforeAll","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"541d546f465a1a1","status":"skipped","statusDetails":"skipped - Spec skipped because Skip() was called in BeforeAll","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"c5887bae2dfdfa38","status":"skipped","statusDetails":"skipped - Spec skipped because Skip() was called in BeforeAll","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"4a439b1c31f91b41","status":"skipped","statusDetails":"skipped - Spec skipped because Skip() was called in BeforeAll","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"e281ccec06d5fb39","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"c30d86a7c721e642","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"fa9258c646703d4d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"d9f832b7af9640d8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"a0d675ae50a80275","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"9d5f92a1e32ba1e8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"d616e72d6e91c868","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"ad455162458977d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"ddbbf6a16b8f369a","status":"passed","time":{"start":1660104898000,"stop":1660104900319,"duration":2319}},{"uid":"964d7e47f9ad0048","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"dda066b66ff4e4e","status":"passed","time":{"start":1660047551000,"stop":1660047553273,"duration":2273}}]},"Worker Suite:Worker Suite#[It] Slice creation scenarios: when slice red is installed install slice blue workers should have gateway pods from both slices":{"statistic":{"failed":1,"broken":0,"skipped":38,"passed":12,"unknown":0,"total":51},"items":[{"uid":"9af88758bd342c0c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"bd23e6b94f50d704","status":"failed","statusDetails":"Timed out after 120.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1663044069000,"stop":1663044190399,"duration":121399}},{"uid":"5cbd5667d4281063","status":"passed","time":{"start":1662348908000,"stop":1662348921347,"duration":13347}},{"uid":"79ad51329d267ce6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"7568107bd2de62c1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"4c133f8cba87c353","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"2416ef114877b86a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"4e7918b223e340f6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"5e72a78f2df8b574","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"dbdbe7563a8dfe39","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"7ab63ae684c9fb2d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"66e2a217e11ca447","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"7ac2f98b7a731107","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"e93f7d98170f432","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"50896baa337f0314","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"b5c613f710dc5ce3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"2b39c17db0aba308","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"34eb0385ebe89ef8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"f9fe66694047a9a7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"edc7efe3b07c09dc","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Namespace reconcile test scenarios: Onboard label app ns on workers":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":50,"unknown":0,"total":51},"items":[{"uid":"b325d5bdacd9cee1","status":"skipped","statusDetails":"skipped","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"e4c9e2a1d416d824","status":"passed","time":{"start":1663044069000,"stop":1663044069817,"duration":817}},{"uid":"fc8cc8b1a96679e9","status":"passed","time":{"start":1662348908000,"stop":1662348908682,"duration":682}},{"uid":"4c3089a464a192bb","status":"passed","time":{"start":1661844205000,"stop":1661844205804,"duration":804}},{"uid":"4b061e94c6403789","status":"passed","time":{"start":1661754870000,"stop":1661754870711,"duration":711}},{"uid":"73f51aeb264087f3","status":"passed","time":{"start":1661611962000,"stop":1661611963174,"duration":1174}},{"uid":"a818dbb9868c7995","status":"passed","time":{"start":1661585077000,"stop":1661585077801,"duration":801}},{"uid":"42aadae216c3d87a","status":"passed","time":{"start":1661584447000,"stop":1661584448119,"duration":1119}},{"uid":"22733e4b76b19d07","status":"passed","time":{"start":1661580045000,"stop":1661580045797,"duration":797}},{"uid":"56d15e35e9598ca1","status":"passed","time":{"start":1661578505000,"stop":1661578505782,"duration":782}},{"uid":"45b4bfb31a3c6c81","status":"passed","time":{"start":1661572479000,"stop":1661572479826,"duration":826}},{"uid":"84cd19c53efd3ab4","status":"passed","time":{"start":1660832281000,"stop":1660832281719,"duration":719}},{"uid":"ed236ef7ddc8b5b","status":"passed","time":{"start":1660808797000,"stop":1660808797641,"duration":641}},{"uid":"d727d2d177b96a1c","status":"passed","time":{"start":1660299182000,"stop":1660299182576,"duration":576}},{"uid":"639a60d7406e1ed1","status":"passed","time":{"start":1660295740000,"stop":1660295740624,"duration":624}},{"uid":"e9d26784f4775377","status":"passed","time":{"start":1660293657000,"stop":1660293657867,"duration":867}},{"uid":"92e75ee914ea46c9","status":"passed","time":{"start":1660113628000,"stop":1660113628644,"duration":644}},{"uid":"2a0eef263107f5d8","status":"passed","time":{"start":1660104898000,"stop":1660104898751,"duration":751}},{"uid":"c1326a6b9188664e","status":"passed","time":{"start":1660067259000,"stop":1660067260343,"duration":1343}},{"uid":"7cfb401f88ceeb0e","status":"passed","time":{"start":1660047551000,"stop":1660047552076,"duration":1076}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests Iperf App connectivity should have vl3 router running":{"statistic":{"failed":33,"broken":1,"skipped":0,"passed":17,"unknown":0,"total":51},"items":[{"uid":"c0f160d2bca1e23b","status":"broken","statusDetails":"interrupted","time":{"start":1663044723000,"stop":1663044914375,"duration":191375}},{"uid":"9c4f4a2f0f6b8890","status":"passed","time":{"start":1663044069000,"stop":1663044070242,"duration":1242}},{"uid":"360026ac951ee1d4","status":"passed","time":{"start":1662348908000,"stop":1662348908863,"duration":863}},{"uid":"5e7fda9ca9653053","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0002bb428>: {\n        Underlying: <*exec.ExitError | 0xc0004b3940>{\n            ProcessState: {\n                pid: 7306,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 234338},\n                    Stime: {Sec: 0, Usec: 80806},\n                    Maxrss: 82452,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4300,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 452,\n                    Nivcsw: 554,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\noccurred","time":{"start":1661844205000,"stop":1661844205595,"duration":595}},{"uid":"b900c3b8da2cb89d","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000734180>: {\n        Underlying: <*exec.ExitError | 0xc0007a5460>{\n            ProcessState: {\n                pid: 7102,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 169157},\n                    Stime: {Sec: 0, Usec: 63903},\n                    Maxrss: 83076,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 5297,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 376,\n                    Nivcsw: 406,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661754870000,"stop":1661754870441,"duration":441}},{"uid":"88a7541829bc854a","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0008ce798>: {\n        Underlying: <*exec.ExitError | 0xc000074c40>{\n            ProcessState: {\n                pid: 7116,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 243558},\n                    Stime: {Sec: 0, Usec: 44652},\n                    Maxrss: 82708,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3450,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 355,\n                    Nivcsw: 609,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661611962000,"stop":1661611962631,"duration":631}},{"uid":"3ddaadb86181e209","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00090c150>: {\n        Underlying: <*exec.ExitError | 0xc000a805c0>{\n            ProcessState: {\n                pid: 7122,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 178465},\n                    Stime: {Sec: 0, Usec: 51556},\n                    Maxrss: 81444,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3395,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 276,\n                    Nivcsw: 417,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661585077000,"stop":1661585077527,"duration":527}},{"uid":"47434529d1bee15a","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00013a768>: {\n        Underlying: <*exec.ExitError | 0xc000825da0>{\n            ProcessState: {\n                pid: 6182,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 228202},\n                    Stime: {Sec: 0, Usec: 29928},\n                    Maxrss: 86092,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3412,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 290,\n                    Nivcsw: 494,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661584447000,"stop":1661584447508,"duration":508}},{"uid":"89f059c467136a8a","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0008106c0>: {\n        Underlying: <*exec.ExitError | 0xc000627000>{\n            ProcessState: {\n                pid: 7238,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 185452},\n                    Stime: {Sec: 0, Usec: 33718},\n                    Maxrss: 78140,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3623,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 356,\n                    Nivcsw: 373,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661580045000,"stop":1661580045452,"duration":452}},{"uid":"35759955519f403b","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000778018>: {\n        Underlying: <*exec.ExitError | 0xc000774040>{\n            ProcessState: {\n                pid: 7084,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 249551},\n                    Stime: {Sec: 0, Usec: 72450},\n                    Maxrss: 87784,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 11790,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 288,\n                    Nivcsw: 446,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661578505000,"stop":1661578505621,"duration":621}},{"uid":"908146a04ed760af","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0008a2db0>: {\n        Underlying: <*exec.ExitError | 0xc0004d8e40>{\n            ProcessState: {\n                pid: 7258,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 262784},\n                    Stime: {Sec: 0, Usec: 63705},\n                    Maxrss: 91384,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 5494,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 295,\n                    Nivcsw: 551,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661572479000,"stop":1661572479618,"duration":618}},{"uid":"b0eb87b8d5c5491d","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0006eae40>: {\n        Underlying: <*exec.ExitError | 0xc0008cb6a0>{\n            ProcessState: {\n                pid: 7096,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 133537},\n                    Stime: {Sec: 0, Usec: 86406},\n                    Maxrss: 78624,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 6779,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 376,\n                    Nivcsw: 348,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660832281000,"stop":1660832281433,"duration":433}},{"uid":"4f3f14917e294823","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000989518>: {\n        Underlying: <*exec.ExitError | 0xc000710900>{\n            ProcessState: {\n                pid: 7152,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 159208},\n                    Stime: {Sec: 0, Usec: 55537},\n                    Maxrss: 83908,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4107,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 377,\n                    Nivcsw: 344,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660808797000,"stop":1660808797413,"duration":413}},{"uid":"c6dbabf0998b8d95","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000350ba0>: {\n        Underlying: <*exec.ExitError | 0xc000829380>{\n            ProcessState: {\n                pid: 7262,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 137557},\n                    Stime: {Sec: 0, Usec: 33459},\n                    Maxrss: 86792,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4348,\n                    Majflt: 1,\n                    Nswap: 0,\n                    Inblock: 160,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 505,\n                    Nivcsw: 381,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660299182000,"stop":1660299182351,"duration":351}},{"uid":"ad89085f103f7bb8","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004ca780>: {\n        Underlying: <*exec.ExitError | 0xc000158400>{\n            ProcessState: {\n                pid: 7225,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 140590},\n                    Stime: {Sec: 0, Usec: 37215},\n                    Maxrss: 86032,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3706,\n                    Majflt: 1,\n                    Nswap: 0,\n                    Inblock: 104,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 367,\n                    Nivcsw: 290,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660295740000,"stop":1660295740325,"duration":325}},{"uid":"4a870b9dec063265","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000649998>: {\n        Underlying: <*exec.ExitError | 0xc0008293e0>{\n            ProcessState: {\n                pid: 7158,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 153874},\n                    Stime: {Sec: 0, Usec: 48360},\n                    Maxrss: 82512,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 5046,\n                    Majflt: 2,\n                    Nswap: 0,\n                    Inblock: 392,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 359,\n                    Nivcsw: 315,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660293657000,"stop":1660293657456,"duration":456}},{"uid":"9c1ac4c625de9212","status":"passed","time":{"start":1660113628000,"stop":1660113628487,"duration":487}},{"uid":"8b394d22a22410ed","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 4\nto equal\n    <int>: 1","time":{"start":1660104898000,"stop":1660105081464,"duration":183464}},{"uid":"45c02c19e20732cd","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 2\nto equal\n    <int>: 1","time":{"start":1660067259000,"stop":1660067442133,"duration":183133}},{"uid":"397ec039985cdb94","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 2\nto equal\n    <int>: 1","time":{"start":1660047551000,"stop":1660047731889,"duration":180889}}]},"Istio Suite:Istio Suite#[It] Bookinfo tests Bookinfo Scenario 4 with egress and ingress Should verify if Curl Traffic works on Productpage test":{"statistic":{"failed":0,"broken":0,"skipped":50,"passed":7,"unknown":0,"total":57},"items":[{"uid":"3ca7d9b2afa6acb4","status":"passed","time":{"start":1663043831000,"stop":1663043831357,"duration":357}},{"uid":"28d47663ba7beb54","status":"passed","time":{"start":1663043075000,"stop":1663043075428,"duration":428}},{"uid":"e5e6101f37f9f19b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663040338000,"stop":1663040338000,"duration":0}},{"uid":"6a1064b99c2898cf","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662819574000,"stop":1662819574000,"duration":0}},{"uid":"5743bc50e65dd928","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662795310000,"stop":1662795310000,"duration":0}},{"uid":"b31993a5d84fc845","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662792773000,"stop":1662792773000,"duration":0}},{"uid":"979a402ca5332035","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662620110000,"stop":1662620110000,"duration":0}},{"uid":"5c721a030b85514c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662617640000,"stop":1662617640000,"duration":0}},{"uid":"66dba30fdc12a329","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662610377000,"stop":1662610377000,"duration":0}},{"uid":"a93d69467ca11473","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662557911000,"stop":1662557911000,"duration":0}},{"uid":"baec517688356879","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662553747000,"stop":1662553747000,"duration":0}},{"uid":"2b338a0e94723e63","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662553327000,"stop":1662553327000,"duration":0}},{"uid":"aebe6305207ffdbe","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662381943000,"stop":1662381943000,"duration":0}},{"uid":"d86ba8f84a4abb86","status":"passed","time":{"start":1662348103000,"stop":1662348103453,"duration":453}},{"uid":"2d79c984ae70e93d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662049889000,"stop":1662049889000,"duration":0}},{"uid":"60efa2aa3ad4bc5e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662049074000,"stop":1662049074000,"duration":0}},{"uid":"78bab1f98340b01","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662028179000,"stop":1662028179000,"duration":0}},{"uid":"8352cae7c360149","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661859205000,"stop":1661859205000,"duration":0}},{"uid":"a94d1a4e0fa0ba78","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661843950000,"stop":1661843950000,"duration":0}},{"uid":"953496a717ea6b7d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754632000,"stop":1661754632000,"duration":0}}]},"Istio Suite:Istio Suite#[BeforeSuite]":{"statistic":{"failed":108,"broken":0,"skipped":0,"passed":57,"unknown":0,"total":165},"items":[{"uid":"38b8f743f88e3065","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000638018>: {\n        Underlying: <*exec.ExitError | 0xc000074020>{\n            ProcessState: {\n                pid: 6089,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 5, Usec: 75730},\n                    Stime: {Sec: 0, Usec: 802503},\n                    Maxrss: 83732,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 10027,\n                    Majflt: 4,\n                    Nswap: 0,\n                    Inblock: 832,\n                    Oublock: 13888,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 54197,\n                    Nivcsw: 11313,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.3.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:165: [debug] Clearing discovery cache\",\n                        \"wait.go:48: [debug] beginning wait for 4 resources with timeout of 1m0s\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 44 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 44 resources with timeout of 5m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.3.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:165: [debug] Clearing discovery cache\n    wait.go:48: [debug] beginning wait for 4 resources with timeout of 1m0s\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 44 resource(s)\n    wait.go:48: [debug] beginning wait for 44 resources with timeout of 5m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:310: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"jaeger\" Service\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-svc\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:310: [debug] Starting delete for \"jaeger\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook\" Deployment\n    client.go:310: [debug] Starting delete for \"prefix-service\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsm-kernel-forwarder\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:310: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"networkservices.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkserviceendpoints.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservicemanagers.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:310: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-cfg\" MutatingWebhookConfiguration\n    uninstall.go:144: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: client rate limiter Wait returned an error: context deadline exceeded\n    helm.go:84: [debug] client rate limiter Wait returned an error: context deadline exceeded\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1663052393000,"stop":1663052810308,"duration":417308}},{"uid":"c320bb3654be5e1b","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000e030>: {\n        Underlying: <*exec.ExitError | 0xc000074020>{\n            ProcessState: {\n                pid: 6119,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 4, Usec: 723006},\n                    Stime: {Sec: 0, Usec: 772126},\n                    Maxrss: 83192,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 9342,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13888,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 53263,\n                    Nivcsw: 10472,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.3.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:165: [debug] Clearing discovery cache\",\n                        \"wait.go:48: [debug] beginning wait for 4 resources with timeout of 1m0s\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 44 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 44 resources with timeout of 5m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.3.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:165: [debug] Clearing discovery cache\n    wait.go:48: [debug] beginning wait for 4 resources with timeout of 1m0s\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 44 resource(s)\n    wait.go:48: [debug] beginning wait for 44 resources with timeout of 5m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:310: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"jaeger\" Service\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-svc\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook\" Deployment\n    client.go:310: [debug] Starting delete for \"jaeger\" Deployment\n    client.go:310: [debug] Starting delete for \"prefix-service\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsm-kernel-forwarder\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:310: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"networkserviceendpoints.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservicemanagers.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservices.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-cfg\" MutatingWebhookConfiguration\n    uninstall.go:144: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: client rate limiter Wait returned an error: context deadline exceeded\n    helm.go:84: [debug] client rate limiter Wait returned an error: context deadline exceeded\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1663052147000,"stop":1663052552215,"duration":405215}},{"uid":"f34e40c023a6bf31","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000480168>: {\n        Underlying: <*exec.ExitError | 0xc00052e000>{\n            ProcessState: {\n                pid: 6117,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 3, Usec: 391488},\n                    Stime: {Sec: 0, Usec: 658373},\n                    Maxrss: 80868,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 7459,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13888,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 48795,\n                    Nivcsw: 9012,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.3.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:165: [debug] Clearing discovery cache\",\n                        \"wait.go:48: [debug] beginning wait for 4 resources with timeout of 1m0s\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 44 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 44 resources with timeout of 5m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.3.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:165: [debug] Clearing discovery cache\n    wait.go:48: [debug] beginning wait for 4 resources with timeout of 1m0s\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 44 resource(s)\n    wait.go:48: [debug] beginning wait for 44 resources with timeout of 5m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:310: [debug] Starting delete for \"jaeger\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-svc\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:310: [debug] Starting delete for \"jaeger\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook\" Deployment\n    client.go:310: [debug] Starting delete for \"prefix-service\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsm-kernel-forwarder\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"networkservices.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkserviceendpoints.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservicemanagers.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-cfg\" MutatingWebhookConfiguration\n    uninstall.go:144: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1663050219000,"stop":1663050630267,"duration":411267}},{"uid":"39596a2184be3c97","status":"passed","time":{"start":1663043831000,"stop":1663044034490,"duration":203490}},{"uid":"713082c125fbfe11","status":"passed","time":{"start":1663043075000,"stop":1663043288593,"duration":213593}},{"uid":"2e0113bc165b86b6","status":"passed","time":{"start":1663040338000,"stop":1663040585114,"duration":247114}},{"uid":"ff4911669861ddf0","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000602018>: {\n        Underlying: <*exec.ExitError | 0xc000770000>{\n            ProcessState: {\n                pid: 6839,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 3, Usec: 853145},\n                    Stime: {Sec: 0, Usec: 545105},\n                    Maxrss: 84980,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3974,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14288,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 50122,\n                    Nivcsw: 9052,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.3.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD serviceexports.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD serviceimports.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD slice.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD slicegateways.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 44 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 44 resources with timeout of 5m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.3.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD serviceexports.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD serviceimports.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD slice.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD slicegateways.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 44 resource(s)\n    wait.go:48: [debug] beginning wait for 44 resources with timeout of 5m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:310: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"jaeger\" Service\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-svc\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:310: [debug] Starting delete for \"jaeger\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook\" Deployment\n    client.go:310: [debug] Starting delete for \"prefix-service\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsm-kernel-forwarder\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"networkservices.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkserviceendpoints.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservicemanagers.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-cfg\" MutatingWebhookConfiguration\n    uninstall.go:144: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1663011012000,"stop":1663011416731,"duration":404731}},{"uid":"6404704f1a613747","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000e510>: {\n        Underlying: <*exec.ExitError | 0xc000640000>{\n            ProcessState: {\n                pid: 6127,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 4, Usec: 14891},\n                    Stime: {Sec: 0, Usec: 751450},\n                    Maxrss: 81984,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 9480,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14352,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 51927,\n                    Nivcsw: 9811,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.3.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:165: [debug] Clearing discovery cache\",\n                        \"wait.go:48: [debug] beginning wait for 4 resources with timeout of 1m0s\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 44 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 44 resources with timeout of 5m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out o...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.3.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:165: [debug] Clearing discovery cache\n    wait.go:48: [debug] beginning wait for 4 resources with timeout of 1m0s\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 44 resource(s)\n    wait.go:48: [debug] beginning wait for 44 resources with timeout of 5m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-svc\" Service\n    client.go:310: [debug] Starting delete for \"jaeger\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:310: [debug] Starting delete for \"jaeger\" Deployment\n    client.go:310: [debug] Starting delete for \"prefix-service\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-kernel-forwarder\" DaemonSet\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:310: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"networkservices.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkserviceendpoints.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservicemanagers.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-cfg\" MutatingWebhookConfiguration\n    uninstall.go:144: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: client rate limiter Wait returned an error: rate: Wait(n=1) would exceed context deadline\n    helm.go:84: [debug] client rate limiter Wait returned an error: rate: Wait(n=1) would exceed context deadline\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1663008428000,"stop":1663008849100,"duration":421100}},{"uid":"850393a119bfd2d0","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0001325a0>: {\n        Underlying: <*exec.ExitError | 0xc00069e000>{\n            ProcessState: {\n                pid: 6078,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 4, Usec: 4606},\n                    Stime: {Sec: 0, Usec: 503414},\n                    Maxrss: 82420,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 14151,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14232,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 51692,\n                    Nivcsw: 9598,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.2.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:165: [debug] Clearing discovery cache\",\n                        \"wait.go:48: [debug] beginning wait for 4 resources with timeout of 1m0s\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 44 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 44 resources with timeout of 5m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.2.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:165: [debug] Clearing discovery cache\n    wait.go:48: [debug] beginning wait for 4 resources with timeout of 1m0s\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 44 resource(s)\n    wait.go:48: [debug] beginning wait for 44 resources with timeout of 5m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-svc\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:310: [debug] Starting delete for \"jaeger\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook\" Deployment\n    client.go:310: [debug] Starting delete for \"jaeger\" Deployment\n    client.go:310: [debug] Starting delete for \"prefix-service\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsm-kernel-forwarder\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:310: [debug] Starting delete for \"networkserviceendpoints.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservicemanagers.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservices.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:310: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-cfg\" MutatingWebhookConfiguration\n    uninstall.go:144: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1662986600000,"stop":1662987017750,"duration":417750}},{"uid":"a8dfcaf059a62297","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00070e018>: {\n        Underlying: <*exec.ExitError | 0xc000674000>{\n            ProcessState: {\n                pid: 6880,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 4, Usec: 352445},\n                    Stime: {Sec: 0, Usec: 654695},\n                    Maxrss: 81080,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 10366,\n                    Majflt: 2,\n                    Nswap: 0,\n                    Inblock: 280,\n                    Oublock: 14168,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 53878,\n                    Nivcsw: 10115,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.2.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD serviceexports.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD serviceimports.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD slice.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD slicegateways.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 44 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 44 resources with timeout of 5m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deploymen...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.2.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD serviceexports.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD serviceimports.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD slice.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD slicegateways.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 44 resource(s)\n    wait.go:48: [debug] beginning wait for 44 resources with timeout of 5m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:310: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-svc\" Service\n    client.go:310: [debug] Starting delete for \"jaeger\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:310: [debug] Starting delete for \"jaeger\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook\" Deployment\n    client.go:310: [debug] Starting delete for \"prefix-service\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsm-kernel-forwarder\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:310: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"networkservices.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkserviceendpoints.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservicemanagers.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-cfg\" MutatingWebhookConfiguration\n    uninstall.go:144: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1662980199000,"stop":1662980602483,"duration":403483}},{"uid":"c8eb901920403462","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000352030>: {\n        Underlying: <*exec.ExitError | 0xc00048a000>{\n            ProcessState: {\n                pid: 6080,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 4, Usec: 484862},\n                    Stime: {Sec: 0, Usec: 738361},\n                    Maxrss: 80972,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 9121,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13768,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 52114,\n                    Nivcsw: 11603,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.2.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:165: [debug] Clearing discovery cache\",\n                        \"wait.go:48: [debug] beginning wait for 4 resources with timeout of 1m0s\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 44 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 44 resources with timeout of 5m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kub...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.2.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:165: [debug] Clearing discovery cache\n    wait.go:48: [debug] beginning wait for 4 resources with timeout of 1m0s\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 44 resource(s)\n    wait.go:48: [debug] beginning wait for 44 resources with timeout of 5m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:310: [debug] Starting delete for \"jaeger\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-svc\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook\" Deployment\n    client.go:310: [debug] Starting delete for \"jaeger\" Deployment\n    client.go:310: [debug] Starting delete for \"prefix-service\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsm-kernel-forwarder\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"networkservices.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkserviceendpoints.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservicemanagers.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-cfg\" MutatingWebhookConfiguration\n    uninstall.go:144: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1662961880000,"stop":1662962289702,"duration":409702}},{"uid":"64bfd2d6d9c7bc5","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00035c048>: {\n        Underlying: <*exec.ExitError | 0xc000074000>{\n            ProcessState: {\n                pid: 6106,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 4, Usec: 645273},\n                    Stime: {Sec: 0, Usec: 687639},\n                    Maxrss: 82256,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 12703,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14232,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 53176,\n                    Nivcsw: 10423,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.2.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:165: [debug] Clearing discovery cache\",\n                        \"wait.go:48: [debug] beginning wait for 4 resources with timeout of 1m0s\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 44 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 44 resources with timeout of 5m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-syst...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.2.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:165: [debug] Clearing discovery cache\n    wait.go:48: [debug] beginning wait for 4 resources with timeout of 1m0s\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 44 resource(s)\n    wait.go:48: [debug] beginning wait for 44 resources with timeout of 5m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-svc\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:310: [debug] Starting delete for \"jaeger\" Service\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:310: [debug] Starting delete for \"prefix-service\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:310: [debug] Starting delete for \"jaeger\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsm-kernel-forwarder\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"networkservices.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkserviceendpoints.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservicemanagers.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-cfg\" MutatingWebhookConfiguration\n    uninstall.go:144: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: client rate limiter Wait returned an error: rate: Wait(n=1) would exceed context deadline\n    helm.go:84: [debug] client rate limiter Wait returned an error: rate: Wait(n=1) would exceed context deadline\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1662961140000,"stop":1662961541574,"duration":401574}},{"uid":"331f6429d0e2ef4c","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00046e1b0>: {\n        Underlying: <*exec.ExitError | 0xc000438000>{\n            ProcessState: {\n                pid: 6124,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 3, Usec: 573063},\n                    Stime: {Sec: 0, Usec: 553225},\n                    Maxrss: 80076,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 11686,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13768,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 50178,\n                    Nivcsw: 10143,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.2.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:165: [debug] Clearing discovery cache\",\n                        \"wait.go:48: [debug] beginning wait for 4 resources with timeout of 1m0s\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 44 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 44 resources with timeout of 5m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.2.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:165: [debug] Clearing discovery cache\n    wait.go:48: [debug] beginning wait for 4 resources with timeout of 1m0s\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 44 resource(s)\n    wait.go:48: [debug] beginning wait for 44 resources with timeout of 5m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:310: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:310: [debug] Starting delete for \"jaeger\" Service\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-svc\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:310: [debug] Starting delete for \"jaeger\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook\" Deployment\n    client.go:310: [debug] Starting delete for \"prefix-service\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsm-kernel-forwarder\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"networkservices.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkserviceendpoints.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservicemanagers.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-cfg\" MutatingWebhookConfiguration\n    uninstall.go:144: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1662960982000,"stop":1662961385016,"duration":403016}},{"uid":"fff26bd008e724d1","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000e240>: {\n        Underlying: <*exec.ExitError | 0xc0004e0000>{\n            ProcessState: {\n                pid: 6088,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 3, Usec: 944154},\n                    Stime: {Sec: 0, Usec: 531375},\n                    Maxrss: 90908,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 12569,\n                    Majflt: 4,\n                    Nswap: 0,\n                    Inblock: 720,\n                    Oublock: 17128,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 50940,\n                    Nivcsw: 9533,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.2.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:165: [debug] Clearing discovery cache\",\n                        \"wait.go:48: [debug] beginning wait for 4 resources with timeout of 1m0s\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 44 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 44 resources with timeout of 5m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 o...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.2.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:165: [debug] Clearing discovery cache\n    wait.go:48: [debug] beginning wait for 4 resources with timeout of 1m0s\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 44 resource(s)\n    wait.go:48: [debug] beginning wait for 44 resources with timeout of 5m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:310: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-svc\" Service\n    client.go:310: [debug] Starting delete for \"jaeger\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:310: [debug] Starting delete for \"jaeger\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook\" Deployment\n    client.go:310: [debug] Starting delete for \"prefix-service\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsm-kernel-forwarder\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"networkservices.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkserviceendpoints.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservicemanagers.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-cfg\" MutatingWebhookConfiguration\n    uninstall.go:144: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: client rate limiter Wait returned an error: rate: Wait(n=1) would exceed context deadline\n    helm.go:84: [debug] client rate limiter Wait returned an error: rate: Wait(n=1) would exceed context deadline\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1662824919000,"stop":1662825331835,"duration":412835}},{"uid":"8ad975a9226eae65","status":"passed","time":{"start":1662819574000,"stop":1662819777078,"duration":203078}},{"uid":"d0740b6e2062aa","status":"passed","time":{"start":1662795310000,"stop":1662795504708,"duration":194708}},{"uid":"ebf46b7dcdc13bd4","status":"passed","time":{"start":1662792773000,"stop":1662792981995,"duration":208995}},{"uid":"cc90bfeb3b5b9363","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0005f6018>: {\n        Underlying: <*exec.ExitError | 0xc0006c2020>{\n            ProcessState: {\n                pid: 6103,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 3, Usec: 907304},\n                    Stime: {Sec: 0, Usec: 594589},\n                    Maxrss: 84632,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 12303,\n                    Majflt: 1,\n                    Nswap: 0,\n                    Inblock: 152,\n                    Oublock: 14232,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 50437,\n                    Nivcsw: 10162,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.2.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:165: [debug] Clearing discovery cache\",\n                        \"wait.go:48: [debug] beginning wait for 4 resources with timeout of 1m0s\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 44 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 44 resources with timeout of 5m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not read...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.2.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:165: [debug] Clearing discovery cache\n    wait.go:48: [debug] beginning wait for 4 resources with timeout of 1m0s\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 44 resource(s)\n    wait.go:48: [debug] beginning wait for 44 resources with timeout of 5m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:310: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"jaeger\" Service\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-svc\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:310: [debug] Starting delete for \"jaeger\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook\" Deployment\n    client.go:310: [debug] Starting delete for \"prefix-service\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsm-kernel-forwarder\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:310: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"networkservices.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkserviceendpoints.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservicemanagers.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-cfg\" MutatingWebhookConfiguration\n    uninstall.go:144: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: client rate limiter Wait returned an error: rate: Wait(n=1) would exceed context deadline\n    helm.go:84: [debug] client rate limiter Wait returned an error: rate: Wait(n=1) would exceed context deadline\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1662791306000,"stop":1662791708524,"duration":402524}},{"uid":"372aab72f976e4a6","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0002d2048>: {\n        Underlying: <*exec.ExitError | 0xc0004aa000>{\n            ProcessState: {\n                pid: 6087,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 3, Usec: 373228},\n                    Stime: {Sec: 0, Usec: 502020},\n                    Maxrss: 80368,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 9716,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13768,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 47953,\n                    Nivcsw: 8747,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.2.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:165: [debug] Clearing discovery cache\",\n                        \"wait.go:48: [debug] beginning wait for 4 resources with timeout of 1m0s\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 44 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 44 resources with timeout of 5m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: k...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.2.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:165: [debug] Clearing discovery cache\n    wait.go:48: [debug] beginning wait for 4 resources with timeout of 1m0s\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 44 resource(s)\n    wait.go:48: [debug] beginning wait for 44 resources with timeout of 5m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:310: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-svc\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:310: [debug] Starting delete for \"jaeger\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:310: [debug] Starting delete for \"jaeger\" Deployment\n    client.go:310: [debug] Starting delete for \"prefix-service\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsm-kernel-forwarder\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"networkservices.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkserviceendpoints.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservicemanagers.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-cfg\" MutatingWebhookConfiguration\n    uninstall.go:144: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: client rate limiter Wait returned an error: context deadline exceeded\n    helm.go:84: [debug] client rate limiter Wait returned an error: context deadline exceeded\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1662788545000,"stop":1662788941438,"duration":396438}},{"uid":"a453d7f567ec8779","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00046e138>: {\n        Underlying: <*exec.ExitError | 0xc0005d8000>{\n            ProcessState: {\n                pid: 6136,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 4, Usec: 657579},\n                    Stime: {Sec: 0, Usec: 679754},\n                    Maxrss: 82468,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 11304,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14232,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 52603,\n                    Nivcsw: 10219,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.2.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:165: [debug] Clearing discovery cache\",\n                        \"wait.go:48: [debug] beginning wait for 4 resources with timeout of 1m0s\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 44 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 44 resources with timeout of 5m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment i...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.2.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:165: [debug] Clearing discovery cache\n    wait.go:48: [debug] beginning wait for 4 resources with timeout of 1m0s\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 44 resource(s)\n    wait.go:48: [debug] beginning wait for 44 resources with timeout of 5m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:310: [debug] Starting delete for \"jaeger\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-svc\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:310: [debug] Starting delete for \"jaeger\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook\" Deployment\n    client.go:310: [debug] Starting delete for \"prefix-service\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsm-kernel-forwarder\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"networkservices.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkserviceendpoints.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservicemanagers.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:310: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-cfg\" MutatingWebhookConfiguration\n    uninstall.go:144: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1662784297000,"stop":1662784716159,"duration":419159}}]},"Worker Suite:Worker Suite#[It] Slice Config Negative Scenario tests Slice Config Negative Scenario Update Should fail while updating sliceconfig with bandwithguaranteedkbps more than bandwidthceilingkbps in qosprofiledetailsnvalid ":{"statistic":{"failed":0,"broken":0,"skipped":14,"passed":37,"unknown":0,"total":51},"items":[{"uid":"5738291c87b108ad","status":"passed","time":{"start":1663044723000,"stop":1663044723470,"duration":470}},{"uid":"6d5ed4840beda05e","status":"skipped","statusDetails":"skipped","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"3833ac77775def48","status":"passed","time":{"start":1662348908000,"stop":1662348908643,"duration":643}},{"uid":"ab09f0054c2e7f64","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"378d2afbddc41a40","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"36520b0e20999d4c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"3f7f23d8c95e1dab","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"23881546fc7d7041","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"e1d0ec1bdb70c6de","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"2e8b3f8a6b1a08d9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"510058c78723a37c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"37ecdcf4f925aa67","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"12168e5c90449c47","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"508565ea32d74e03","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"b4e9dc4b51d6d55e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"a95a975cb1069401","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"fdb8860c9119d6c6","status":"passed","time":{"start":1660113628000,"stop":1660113628216,"duration":216}},{"uid":"7f217d9fae26e48d","status":"passed","time":{"start":1660104898000,"stop":1660104898230,"duration":230}},{"uid":"baddd58a1b48f1e3","status":"passed","time":{"start":1660067259000,"stop":1660067259302,"duration":302}},{"uid":"db127599586e5ed9","status":"passed","time":{"start":1660047551000,"stop":1660047551612,"duration":612}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Update Should update successfully when Project Applied while removing a service account name in Write users":{"statistic":{"failed":16,"broken":0,"skipped":0,"passed":174,"unknown":0,"total":190},"items":[{"uid":"992fbdc2fc175f5","status":"passed","time":{"start":1663042493000,"stop":1663042498638,"duration":5638}},{"uid":"dcac82e4b743604e","status":"passed","time":{"start":1663041865000,"stop":1663041870422,"duration":5422}},{"uid":"fe391b3b1ea87f47","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <bool>: false\nto be true","time":{"start":1663039902000,"stop":1663039968526,"duration":66526}},{"uid":"dc67dfb482ecc883","status":"passed","time":{"start":1663009431000,"stop":1663009435765,"duration":4765}},{"uid":"76ec61eb3caf4879","status":"passed","time":{"start":1662978562000,"stop":1662978567250,"duration":5250}},{"uid":"ed3368f132af7a68","status":"passed","time":{"start":1662702362000,"stop":1662702366683,"duration":4683}},{"uid":"3beb19a01100c046","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <bool>: false\nto be true","time":{"start":1662553059000,"stop":1662553125638,"duration":66638}},{"uid":"85e35f98b01e49ed","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <bool>: false\nto be true","time":{"start":1662553042000,"stop":1662553108762,"duration":66762}},{"uid":"99611a28cafd315f","status":"passed","time":{"start":1662467486000,"stop":1662467491599,"duration":5599}},{"uid":"955488c30d8c3b6c","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <bool>: false\nto be true","time":{"start":1662374048000,"stop":1662374114537,"duration":66537}},{"uid":"a1327037f9cf478a","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0003cd6c8>: {\n        Underlying: <*exec.ExitError | 0xc000706b00>{\n            ProcessState: {\n                pid: 7172,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 193764},\n                    Stime: {Sec: 0, Usec: 68387},\n                    Maxrss: 78856,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3774,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13344,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 447,\n                    Nivcsw: 267,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Write%20users2875629612\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.241.207:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Write%20users2875629612\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.241.207:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Write%20users2875629612\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.241.207:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Write%20users2875629612\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.241.207:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Write%20users2875629612\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.241.207:443: connect: connection refused\noccurred","time":{"start":1662374250000,"stop":1662374250329,"duration":329}},{"uid":"1e9ed98776c281fd","status":"passed","time":{"start":1662361116000,"stop":1662361120750,"duration":4750}},{"uid":"a532917055cb6fca","status":"passed","time":{"start":1662347286000,"stop":1662347290737,"duration":4737}},{"uid":"fe7a1847c4d103b0","status":"passed","time":{"start":1662049179000,"stop":1662049184563,"duration":5563}},{"uid":"4aa5afe4663b0867","status":"passed","time":{"start":1662048808000,"stop":1662048813100,"duration":5100}},{"uid":"66c40783364c3fa","status":"passed","time":{"start":1662036991000,"stop":1662036996570,"duration":5570}},{"uid":"1c5188352d46d895","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <bool>: false\nto be true","time":{"start":1662031654000,"stop":1662031720637,"duration":66637}},{"uid":"1bb6c8dd389490f","status":"passed","time":{"start":1662027354000,"stop":1662027360138,"duration":6138}},{"uid":"15d02fa8677c5e05","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0006833b0>: {\n        Underlying: <*exec.ExitError | 0xc0007495c0>{\n            ProcessState: {\n                pid: 7709,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 178886},\n                    Stime: {Sec: 0, Usec: 54082},\n                    Maxrss: 82212,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4133,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13344,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 361,\n                    Nivcsw: 442,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Write%20users3340290568\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.242.99:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Write%20users3340290568\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.242.99:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Write%20users3340290568\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.242.99:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Write%20users3340290568\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.242.99:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Write%20users3340290568\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.242.99:443: connect: connection refused\noccurred","time":{"start":1661952242000,"stop":1661952242285,"duration":285}},{"uid":"79a1bcff87e50f9c","status":"passed","time":{"start":1661863163000,"stop":1661863168250,"duration":5250}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Update Should Update successfullyy when Project is Applied with valid service account name in Read users":{"statistic":{"failed":16,"broken":0,"skipped":0,"passed":174,"unknown":0,"total":190},"items":[{"uid":"a97eca569d326128","status":"passed","time":{"start":1663042493000,"stop":1663042501184,"duration":8184}},{"uid":"cfccb302e07eca10","status":"passed","time":{"start":1663041865000,"stop":1663041874727,"duration":9727}},{"uid":"126996829726f0df","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <bool>: false\nto be true","time":{"start":1663039902000,"stop":1663039970218,"duration":68218}},{"uid":"dd8792fe79513116","status":"passed","time":{"start":1663009431000,"stop":1663009447194,"duration":16194}},{"uid":"baedf4596dcdc63a","status":"passed","time":{"start":1662978562000,"stop":1662978570551,"duration":8551}},{"uid":"6e07b0f76f86116b","status":"passed","time":{"start":1662702362000,"stop":1662702370965,"duration":8965}},{"uid":"dac3080eaaf88b98","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <bool>: false\nto be true","time":{"start":1662553059000,"stop":1662553125545,"duration":66545}},{"uid":"2e1d7db428a74739","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <bool>: false\nto be true","time":{"start":1662553042000,"stop":1662553109430,"duration":67430}},{"uid":"f8d19233ab056a5a","status":"passed","time":{"start":1662467486000,"stop":1662467494225,"duration":8225}},{"uid":"5079addd209896f2","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <bool>: false\nto be true","time":{"start":1662374048000,"stop":1662374123303,"duration":75303}},{"uid":"2da508a5e54c14d","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00087c1e0>: {\n        Underlying: <*exec.ExitError | 0xc0006f2140>{\n            ProcessState: {\n                pid: 7143,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 193520},\n                    Stime: {Sec: 0, Usec: 59544},\n                    Maxrss: 87244,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3745,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13344,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 379,\n                    Nivcsw: 326,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20Update%20successfullyy%20when%20Project%20is%20Applied%20with%20valid%20service%20account%20name%20in%20Read%20users4294027726\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.241.207:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20Update%20successfullyy%20when%20Project%20is%20Applied%20with%20valid%20service%20account%20name%20in%20Read%20users4294027726\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.241.207:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20Update%20successfullyy%20when%20Project%20is%20Applied%20with%20valid%20service%20account%20name%20in%20Read%20users4294027726\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.241.207:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20Update%20successfullyy%20when%20Project%20is%20Applied%20with%20valid%20service%20account%20name%20in%20Read%20users4294027726\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.241.207:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Update%20Should%20Update%20successfullyy%20when%20Project%20is%20Applied%20with%20valid%20service%20account%20name%20in%20Read%20users4294027726\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.241.207:443: connect: connection refused\noccurred","time":{"start":1662374250000,"stop":1662374250357,"duration":357}},{"uid":"8a0468aab5aa854","status":"passed","time":{"start":1662361116000,"stop":1662361124917,"duration":8917}},{"uid":"a49bb13c5916acd4","status":"passed","time":{"start":1662347286000,"stop":1662347295006,"duration":9006}},{"uid":"21570df3d0630aac","status":"passed","time":{"start":1662049179000,"stop":1662049194027,"duration":15027}},{"uid":"5277db6a1530c5ed","status":"passed","time":{"start":1662048808000,"stop":1662048816451,"duration":8451}},{"uid":"9967d1e424172d4d","status":"passed","time":{"start":1662036991000,"stop":1662036999877,"duration":8877}},{"uid":"57921a1c04862dbd","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <bool>: false\nto be true","time":{"start":1662031654000,"stop":1662031775989,"duration":121989}},{"uid":"598a928f1f6d8a3b","status":"passed","time":{"start":1662027354000,"stop":1662027363263,"duration":9263}},{"uid":"f4d8d91e10503eb2","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000f788>: {\n        Underlying: <*exec.ExitError | 0xc000905b20>{\n            ProcessState: {\n                pid: 7683,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 198823},\n                    Stime: {Sec: 0, Usec: 24852},\n                    Maxrss: 83960,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 5302,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13344,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 290,\n                    Nivcsw: 285,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20Update%20successfullyy%20when%20Project%20is%20Applied%20with%20valid%20service%20account%20name%20in%20Read%20users1553601793\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.242.99:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20Update%20successfullyy%20when%20Project%20is%20Applied%20with%20valid%20service%20account%20name%20in%20Read%20users1553601793\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.242.99:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20Update%20successfullyy%20when%20Project%20is%20Applied%20with%20valid%20service%20account%20name%20in%20Read%20users1553601793\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.242.99:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20Update%20successfullyy%20when%20Project%20is%20Applied%20with%20valid%20service%20account%20name%20in%20Read%20users1553601793\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.242.99:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Update%20Should%20Update%20successfullyy%20when%20Project%20is%20Applied%20with%20valid%20service%20account%20name%20in%20Read%20users1553601793\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.242.99:443: connect: connection refused\noccurred","time":{"start":1661952242000,"stop":1661952242276,"duration":276}},{"uid":"c284032f8b279c61","status":"passed","time":{"start":1661863163000,"stop":1661863172506,"duration":9506}}]},"Worker Suite:Worker Suite#[It] Attach/Deattach Scenario Attach cluster from slice  when Deattach cluster from slice  should have gateway pod running on attached cluster":{"statistic":{"failed":0,"broken":0,"skipped":36,"passed":6,"unknown":0,"total":42},"items":[{"uid":"971d4a596823479d","status":"passed","time":{"start":1663044723000,"stop":1663044723189,"duration":189}},{"uid":"ee7e88f722bd4632","status":"passed","time":{"start":1663044069000,"stop":1663044069234,"duration":234}},{"uid":"7bc0f4d96c4fbae9","status":"passed","time":{"start":1662348908000,"stop":1662348908195,"duration":195}},{"uid":"6b9243d8d3936367","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"ec1dfa5271eb13ba","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"ca05c4cbf936e973","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"bd63a0b728626443","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"b6ef3d008bfc49da","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"dfe31f6d5d983f65","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"6883dcb4a110964c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"5d96117f97af06ad","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"4982514423ed9821","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"f88d39f93ca0fa8f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"d3425fc9e6ffe8d5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"1e84f89bc917f3cf","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"b81c8c769888e8ec","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"555e31240e34c50e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"790d9b3d5f899d50","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"e235180fb4a2ffa7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"363616fddfb55827","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Intracluster slice tests Iperf connectivity should onboard deployment in application Namespaces without slice annotations":{"statistic":{"failed":44,"broken":0,"skipped":1,"passed":6,"unknown":0,"total":51},"items":[{"uid":"c3a215ddf902787d","status":"skipped","statusDetails":"skipped","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"53a61ff2f72fe091","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 1\nto equal\n    <int>: 2","time":{"start":1663044069000,"stop":1663044259563,"duration":190563}},{"uid":"bb1d280d25722e47","status":"failed","statusDetails":"Unexpected error:\n    <*errors.StatusError | 0xc000929a40>: {\n        ErrStatus: {\n            TypeMeta: {Kind: \"\", APIVersion: \"\"},\n            ListMeta: {\n                SelfLink: \"\",\n                ResourceVersion: \"\",\n                Continue: \"\",\n                RemainingItemCount: nil,\n            },\n            Status: \"Failure\",\n            Message: \"namespaces \\\"appns1\\\" already exists\",\n            Reason: \"AlreadyExists\",\n            Details: {Name: \"appns1\", Group: \"\", Kind: \"namespaces\", UID: \"\", Causes: nil, RetryAfterSeconds: 0},\n            Code: 409,\n        },\n    }\n    namespaces \"appns1\" already exists\noccurred","time":{"start":1662348908000,"stop":1662348912393,"duration":4393}},{"uid":"62187bf11a4b9605","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000469488>: {\n        Underlying: <*exec.ExitError | 0xc00065c2e0>{\n            ProcessState: {\n                pid: 7266,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 254055},\n                    Stime: {Sec: 0, Usec: 30794},\n                    Maxrss: 87476,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3104,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 560,\n                    Nivcsw: 450,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\noccurred","time":{"start":1661844205000,"stop":1661844205540,"duration":540}},{"uid":"fc2742c06943e0b5","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004bf818>: {\n        Underlying: <*exec.ExitError | 0xc0006589e0>{\n            ProcessState: {\n                pid: 7288,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 177560},\n                    Stime: {Sec: 0, Usec: 36991},\n                    Maxrss: 81984,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3949,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 425,\n                    Nivcsw: 375,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661754870000,"stop":1661754870436,"duration":436}},{"uid":"feb8f9d1f9d8f6b7","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0006f0228>: {\n        Underlying: <*exec.ExitError | 0xc0006c5760>{\n            ProcessState: {\n                pid: 7281,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 204410},\n                    Stime: {Sec: 0, Usec: 42585},\n                    Maxrss: 74204,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4302,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 396,\n                    Nivcsw: 373,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661611962000,"stop":1661611962490,"duration":490}},{"uid":"b34b29b681f555a4","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00090d1a0>: {\n        Underlying: <*exec.ExitError | 0xc00096b2c0>{\n            ProcessState: {\n                pid: 7242,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 199888},\n                    Stime: {Sec: 0, Usec: 32634},\n                    Maxrss: 82008,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 5590,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 373,\n                    Nivcsw: 424,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661585077000,"stop":1661585077457,"duration":457}},{"uid":"5cc79a559af9c8dd","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000362030>: {\n        Underlying: <*exec.ExitError | 0xc000824040>{\n            ProcessState: {\n                pid: 6030,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 183740},\n                    Stime: {Sec: 0, Usec: 43937},\n                    Maxrss: 75996,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3781,\n                    Majflt: 1,\n                    Nswap: 0,\n                    Inblock: 240,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 235,\n                    Nivcsw: 438,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661584447000,"stop":1661584447536,"duration":536}},{"uid":"4f8b63b63beaafc9","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0008946c0>: {\n        Underlying: <*exec.ExitError | 0xc00066efa0>{\n            ProcessState: {\n                pid: 7208,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 186074},\n                    Stime: {Sec: 0, Usec: 47508},\n                    Maxrss: 83684,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3822,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 381,\n                    Nivcsw: 458,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661580045000,"stop":1661580045439,"duration":439}},{"uid":"55390aebaa3e14f3","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000866570>: {\n        Underlying: <*exec.ExitError | 0xc000858e40>{\n            ProcessState: {\n                pid: 7108,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 236135},\n                    Stime: {Sec: 0, Usec: 66283},\n                    Maxrss: 88660,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 8113,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 280,\n                    Nivcsw: 380,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661578505000,"stop":1661578505593,"duration":593}},{"uid":"7b751b00662ac37d","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004c8ab0>: {\n        Underlying: <*exec.ExitError | 0xc00071df00>{\n            ProcessState: {\n                pid: 7121,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 260887},\n                    Stime: {Sec: 0, Usec: 66195},\n                    Maxrss: 94132,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 9700,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 595,\n                    Nivcsw: 636,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661572479000,"stop":1661572479862,"duration":862}},{"uid":"1f6dc4f746746f60","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0008bbc08>: {\n        Underlying: <*exec.ExitError | 0xc00050fbe0>{\n            ProcessState: {\n                pid: 7074,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 179506},\n                    Stime: {Sec: 0, Usec: 58614},\n                    Maxrss: 80152,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 7595,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 405,\n                    Nivcsw: 411,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660832281000,"stop":1660832281462,"duration":462}},{"uid":"35807fefa231a726","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000496d20>: {\n        Underlying: <*exec.ExitError | 0xc000930400>{\n            ProcessState: {\n                pid: 7044,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 184573},\n                    Stime: {Sec: 0, Usec: 53833},\n                    Maxrss: 82224,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 7897,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 289,\n                    Nivcsw: 449,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660808797000,"stop":1660808797491,"duration":491}},{"uid":"107e5f38bf8d71f5","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00066d4b8>: {\n        Underlying: <*exec.ExitError | 0xc000917600>{\n            ProcessState: {\n                pid: 7222,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 136603},\n                    Stime: {Sec: 0, Usec: 54641},\n                    Maxrss: 93836,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 10623,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 327,\n                    Nivcsw: 527,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660299182000,"stop":1660299182381,"duration":381}},{"uid":"3ea76e28d6be1481","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0000ce3c0>: {\n        Underlying: <*exec.ExitError | 0xc0008b8540>{\n            ProcessState: {\n                pid: 7200,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 155972},\n                    Stime: {Sec: 0, Usec: 14179},\n                    Maxrss: 79248,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3736,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 279,\n                    Nivcsw: 219,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660295740000,"stop":1660295740341,"duration":341}},{"uid":"a4a174dadea5e72e","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00084c948>: {\n        Underlying: <*exec.ExitError | 0xc0008a19c0>{\n            ProcessState: {\n                pid: 7233,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 178679},\n                    Stime: {Sec: 0, Usec: 45708},\n                    Maxrss: 86816,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 5422,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 527,\n                    Nivcsw: 554,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660293657000,"stop":1660293657468,"duration":468}},{"uid":"eab693cd63959094","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 1\nto equal\n    <int>: 2","time":{"start":1660113628000,"stop":1660113817401,"duration":189401}},{"uid":"60d82e8838da507","status":"passed","time":{"start":1660104898000,"stop":1660104931357,"duration":33357}},{"uid":"a9fe513d1ac9c176","status":"failed","statusDetails":"Unexpected error:\n    <*errors.StatusError | 0xc0008fdcc0>: {\n        ErrStatus: {\n            TypeMeta: {Kind: \"\", APIVersion: \"\"},\n            ListMeta: {\n                SelfLink: \"\",\n                ResourceVersion: \"\",\n                Continue: \"\",\n                RemainingItemCount: nil,\n            },\n            Status: \"Failure\",\n            Message: \"namespaces \\\"appns1\\\" already exists\",\n            Reason: \"AlreadyExists\",\n            Details: {Name: \"appns1\", Group: \"\", Kind: \"namespaces\", UID: \"\", Causes: nil, RetryAfterSeconds: 0},\n            Code: 409,\n        },\n    }\n    namespaces \"appns1\" already exists\noccurred","time":{"start":1660067259000,"stop":1660067265261,"duration":6261}},{"uid":"6510f61f87f6472e","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 1\nto equal\n    <int>: 2","time":{"start":1660047551000,"stop":1660047741110,"duration":190110}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting vl3 pod Should restart vl3 pod":{"statistic":{"failed":0,"broken":0,"skipped":51,"passed":0,"unknown":0,"total":51},"items":[{"uid":"4274a85f0b96831b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"7856d6774cb0d44e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"604296451d27452f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662348908000,"stop":1662348908000,"duration":0}},{"uid":"5684070c6dedb89a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"b553f673ed18de4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"6cbfefa9d958b61b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"ad4b2ec22202de52","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"4799abbfb9b59932","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"58aa1e3209bed2c9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"9ac11b20acfb89df","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"d77de24e490d6e58","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"bb7f048e68880f49","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"12f4fd71e8305024","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"6e9da996a03df39f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"4704c9aba44cacc0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"a0f6d0c2dddea2bb","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"6d8449b7c8a56722","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"6e56ad5d7fd6f3fe","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"ae0e2b6adc6d0521","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"bec01fdc3a6c357a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice Config Negative Scenario tests Slice Config Negative Scenario Creation Should fail while creating sliceconfig with bandwithguaranteedkbps more than bandwidthceilingkbps in qosprofiledetailsnvalid ":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":50,"unknown":0,"total":51},"items":[{"uid":"449b954471ba5e61","status":"passed","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"c67b87348eda6554","status":"skipped","statusDetails":"skipped","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"281176c9ef5bef16","status":"passed","time":{"start":1662348908000,"stop":1662348908250,"duration":250}},{"uid":"5346c58dccc993e2","status":"passed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"7180303146c9df90","status":"passed","time":{"start":1661754870000,"stop":1661754870001,"duration":1}},{"uid":"735860aba6a7704f","status":"passed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"27a3bd2828605a13","status":"passed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"bec373edda85cc56","status":"passed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"e53f2f1f54b87894","status":"passed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"a51311e79247276e","status":"passed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"5227ae2b1df15b1d","status":"passed","time":{"start":1661572479000,"stop":1661572479001,"duration":1}},{"uid":"55a39973d9ccaa63","status":"passed","time":{"start":1660832281000,"stop":1660832281005,"duration":5}},{"uid":"1e0fafdd28899c4a","status":"passed","time":{"start":1660808797000,"stop":1660808797001,"duration":1}},{"uid":"8d2bd0b889d12b51","status":"passed","time":{"start":1660299182000,"stop":1660299182001,"duration":1}},{"uid":"4d77327659a45351","status":"passed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"a90b9b894ea2363b","status":"passed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"90a0e6b8d695258b","status":"passed","time":{"start":1660113628000,"stop":1660113628205,"duration":205}},{"uid":"55c390b09b40186d","status":"passed","time":{"start":1660104898000,"stop":1660104898200,"duration":200}},{"uid":"679987ed71045ea4","status":"passed","time":{"start":1660067259000,"stop":1660067259155,"duration":155}},{"uid":"7ed2d94e39a6cea1","status":"passed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}}]},"Hub Suite:Hub Suite#[It] Cluster CR tests Cluster CR validation Register worker cluster":{"statistic":{"failed":57,"broken":0,"skipped":3,"passed":133,"unknown":0,"total":193},"items":[{"uid":"a020659b23d95677","status":"passed","time":{"start":1663042493000,"stop":1663042539106,"duration":46106}},{"uid":"735fef008cd6864a","status":"passed","time":{"start":1663041865000,"stop":1663041912193,"duration":47193}},{"uid":"17ecfe19a05bd137","status":"passed","time":{"start":1663039902000,"stop":1663039938259,"duration":36259}},{"uid":"feb6336ba716b833","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0006d2018>: {\n        Underlying: <*exec.ExitError | 0xc0003ba000>{\n            ProcessState: {\n                pid: 6766,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 3, Usec: 540504},\n                    Stime: {Sec: 0, Usec: 830818},\n                    Maxrss: 86340,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4995,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14288,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 51445,\n                    Nivcsw: 10421,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.3.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD serviceexports.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD serviceimports.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD slice.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD slicegateways.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 44 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 44 resources with timeout of 5m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment i...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.3.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD serviceexports.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD serviceimports.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD slice.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD slicegateways.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 44 resource(s)\n    wait.go:48: [debug] beginning wait for 44 resources with timeout of 5m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:310: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-svc\" Service\n    client.go:310: [debug] Starting delete for \"jaeger\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:310: [debug] Starting delete for \"jaeger\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook\" Deployment\n    client.go:310: [debug] Starting delete for \"prefix-service\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsm-kernel-forwarder\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"networkservices.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkserviceendpoints.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservicemanagers.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-cfg\" MutatingWebhookConfiguration\n    uninstall.go:144: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1663009431000,"stop":1663009742160,"duration":311160}},{"uid":"a08478715d46183c","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00064c018>: {\n        Underlying: <*exec.ExitError | 0xc00064a000>{\n            ProcessState: {\n                pid: 6734,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 4, Usec: 640060},\n                    Stime: {Sec: 0, Usec: 944612},\n                    Maxrss: 82692,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 5929,\n                    Majflt: 2,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14232,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 54200,\n                    Nivcsw: 10983,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.2.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:165: [debug] Clearing discovery cache\",\n                        \"wait.go:48: [debug] beginning wait for 4 resources with timeout of 1m0s\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 44 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 44 resources with timeout of 5m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/kube...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.2.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:165: [debug] Clearing discovery cache\n    wait.go:48: [debug] beginning wait for 4 resources with timeout of 1m0s\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 44 resource(s)\n    wait.go:48: [debug] beginning wait for 44 resources with timeout of 5m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:310: [debug] Starting delete for \"jaeger\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-svc\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:310: [debug] Starting delete for \"jaeger\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook\" Deployment\n    client.go:310: [debug] Starting delete for \"prefix-service\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsm-kernel-forwarder\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"networkservices.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkserviceendpoints.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservicemanagers.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-cfg\" MutatingWebhookConfiguration\n    uninstall.go:144: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1662978562000,"stop":1662978876742,"duration":314742}},{"uid":"15ab2e21e12f86f6","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004f4060>: {\n        Underlying: <*exec.ExitError | 0xc000720000>{\n            ProcessState: {\n                pid: 6767,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 3, Usec: 561999},\n                    Stime: {Sec: 0, Usec: 499929},\n                    Maxrss: 88336,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4440,\n                    Majflt: 2,\n                    Nswap: 0,\n                    Inblock: 512,\n                    Oublock: 14168,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 48859,\n                    Nivcsw: 9377,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.2.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD serviceexports.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD serviceimports.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD slice.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD slicegateways.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 44 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 44 resources with timeout of 5m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.2.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD serviceexports.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD serviceimports.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD slice.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD slicegateways.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 44 resource(s)\n    wait.go:48: [debug] beginning wait for 44 resources with timeout of 5m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:310: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-svc\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:310: [debug] Starting delete for \"jaeger\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:310: [debug] Starting delete for \"jaeger\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook\" Deployment\n    client.go:310: [debug] Starting delete for \"prefix-service\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsm-kernel-forwarder\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"networkservices.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkserviceendpoints.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservicemanagers.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-cfg\" MutatingWebhookConfiguration\n    uninstall.go:144: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1662702362000,"stop":1662702673218,"duration":311218}},{"uid":"1e6a84601440d8e4","status":"passed","time":{"start":1662553059000,"stop":1662553105330,"duration":46330}},{"uid":"eb8a853df8728076","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662553042000,"stop":1662553042000,"duration":0}},{"uid":"2510bda9a4b120d6","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000520018>: {\n        Underlying: <*exec.ExitError | 0xc000718000>{\n            ProcessState: {\n                pid: 6786,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 2, Usec: 88757},\n                    Stime: {Sec: 0, Usec: 269945},\n                    Maxrss: 90060,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4822,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14168,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 19998,\n                    Nivcsw: 4727,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.2.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD serviceexports.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD serviceimports.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD slice.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD slicegateways.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 44 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 44 resources with timeout of 2m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kube...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.2.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD serviceexports.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD serviceimports.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD slice.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD slicegateways.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 44 resource(s)\n    wait.go:48: [debug] beginning wait for 44 resources with timeout of 2m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:310: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"jaeger\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-svc\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:310: [debug] Starting delete for \"jaeger\" Deployment\n    client.go:310: [debug] Starting delete for \"prefix-service\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsm-kernel-forwarder\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"networkservices.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkserviceendpoints.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservicemanagers.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-cfg\" MutatingWebhookConfiguration\n    uninstall.go:144: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1662467486000,"stop":1662467616926,"duration":130926}},{"uid":"43f20f4d19fc8932","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662374048000,"stop":1662374048000,"duration":0}},{"uid":"a2526881526eba","status":"passed","time":{"start":1662374250000,"stop":1662374297722,"duration":47722}},{"uid":"71d8daffd3850f6f","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00082a018>: {\n        Underlying: <*exec.ExitError | 0xc000896000>{\n            ProcessState: {\n                pid: 6313,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 2, Usec: 225523},\n                    Stime: {Sec: 0, Usec: 272806},\n                    Maxrss: 89848,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4386,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14168,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 21230,\n                    Nivcsw: 4441,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.2.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD serviceexports.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD serviceimports.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD slice.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD slicegateways.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 44 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 44 resources with timeout of 2m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.2.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD serviceexports.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD serviceimports.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD slice.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD slicegateways.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 44 resource(s)\n    wait.go:48: [debug] beginning wait for 44 resources with timeout of 2m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:310: [debug] Starting delete for \"jaeger\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-svc\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:310: [debug] Starting delete for \"jaeger\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook\" Deployment\n    client.go:310: [debug] Starting delete for \"prefix-service\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsm-kernel-forwarder\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"networkservices.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkserviceendpoints.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservicemanagers.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-cfg\" MutatingWebhookConfiguration\n    uninstall.go:144: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1662361116000,"stop":1662361247507,"duration":131507}},{"uid":"4dd5356c76dcb283","status":"passed","time":{"start":1662347286000,"stop":1662347320919,"duration":34919}},{"uid":"ca55f9a14f80898","status":"passed","time":{"start":1662049179000,"stop":1662049237594,"duration":58594}},{"uid":"a9655513316b2c96","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00071a018>: {\n        Underlying: <*exec.ExitError | 0xc00088a000>{\n            ProcessState: {\n                pid: 6222,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 2, Usec: 903139},\n                    Stime: {Sec: 0, Usec: 421492},\n                    Maxrss: 81360,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 7789,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13768,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 24296,\n                    Nivcsw: 6468,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.2.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:165: [debug] Clearing discovery cache\",\n                        \"wait.go:48: [debug] beginning wait for 4 resources with timeout of 1m0s\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 44 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 44 resources with timeout of 2m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.2.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:165: [debug] Clearing discovery cache\n    wait.go:48: [debug] beginning wait for 4 resources with timeout of 1m0s\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 44 resource(s)\n    wait.go:48: [debug] beginning wait for 44 resources with timeout of 2m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:310: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-svc\" Service\n    client.go:310: [debug] Starting delete for \"jaeger\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:310: [debug] Starting delete for \"jaeger\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook\" Deployment\n    client.go:310: [debug] Starting delete for \"prefix-service\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsm-kernel-forwarder\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"networkservices.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkserviceendpoints.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservicemanagers.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-cfg\" MutatingWebhookConfiguration\n    uninstall.go:144: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1662048808000,"stop":1662048940182,"duration":132182}},{"uid":"e70a9aab723cd69b","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004963c0>: {\n        Underlying: <*exec.ExitError | 0xc00071c000>{\n            ProcessState: {\n                pid: 6156,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 2, Usec: 688202},\n                    Stime: {Sec: 0, Usec: 171395},\n                    Maxrss: 80756,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 6948,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14232,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 20489,\n                    Nivcsw: 5453,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.2.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:165: [debug] Clearing discovery cache\",\n                        \"wait.go:48: [debug] beginning wait for 4 resources with timeout of 1m0s\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 44 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 44 resources with timeout of 2m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment i...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.2.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:165: [debug] Clearing discovery cache\n    wait.go:48: [debug] beginning wait for 4 resources with timeout of 1m0s\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 44 resource(s)\n    wait.go:48: [debug] beginning wait for 44 resources with timeout of 2m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:310: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"jaeger\" Service\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-svc\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:310: [debug] Starting delete for \"jaeger\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook\" Deployment\n    client.go:310: [debug] Starting delete for \"prefix-service\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsm-kernel-forwarder\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"networkservices.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkserviceendpoints.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservicemanagers.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-cfg\" MutatingWebhookConfiguration\n    uninstall.go:144: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1662036991000,"stop":1662037123413,"duration":132413}},{"uid":"53554db81d853fcb","status":"passed","time":{"start":1662031654000,"stop":1662031697750,"duration":43750}},{"uid":"165671f51c8e0c4e","status":"passed","time":{"start":1662027354000,"stop":1662027412406,"duration":58406}},{"uid":"c1d3d0a6004f7aea","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661952242000,"stop":1661952242000,"duration":0}},{"uid":"f4e4b7409bc87be5","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000536018>: {\n        Underlying: <*exec.ExitError | 0xc0005c4000>{\n            ProcessState: {\n                pid: 6771,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 3, Usec: 157684},\n                    Stime: {Sec: 0, Usec: 324302},\n                    Maxrss: 86584,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4099,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14168,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 23416,\n                    Nivcsw: 5339,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.2.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD serviceexports.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD serviceimports.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD slice.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD slicegateways.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 44 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 44 resources with timeout of 2m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.2.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD serviceexports.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD serviceimports.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD slice.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD slicegateways.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 44 resource(s)\n    wait.go:48: [debug] beginning wait for 44 resources with timeout of 2m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:310: [debug] Starting delete for \"jaeger\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-svc\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:310: [debug] Starting delete for \"jaeger\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook\" Deployment\n    client.go:310: [debug] Starting delete for \"prefix-service\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsm-kernel-forwarder\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"networkservices.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkserviceendpoints.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservicemanagers.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-cfg\" MutatingWebhookConfiguration\n    uninstall.go:144: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1661863163000,"stop":1661863295143,"duration":132143}}]},"Worker Suite:Worker Suite#[It] Slice Config Negative Scenario tests Slice Config Negative Scenario Creation Should fail while creating sliceconfig with invalid values on slice name":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":50,"unknown":0,"total":51},"items":[{"uid":"291742a93f910f17","status":"passed","time":{"start":1663044723000,"stop":1663044723729,"duration":729}},{"uid":"c6cb34457197f93","status":"skipped","statusDetails":"skipped","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"1e95f653b65c2fd2","status":"passed","time":{"start":1662348908000,"stop":1662348908504,"duration":504}},{"uid":"6166e3bb4e56b615","status":"passed","time":{"start":1661844205000,"stop":1661844205594,"duration":594}},{"uid":"cacdd1f222a09788","status":"passed","time":{"start":1661754870000,"stop":1661754870384,"duration":384}},{"uid":"e1fe5a0d443981db","status":"passed","time":{"start":1661611962000,"stop":1661611962554,"duration":554}},{"uid":"fa56fdff5b3fb08b","status":"passed","time":{"start":1661585077000,"stop":1661585077506,"duration":506}},{"uid":"ead6cb3c94a54b53","status":"passed","time":{"start":1661584447000,"stop":1661584447438,"duration":438}},{"uid":"a15349353408f82b","status":"passed","time":{"start":1661580045000,"stop":1661580045431,"duration":431}},{"uid":"7e7760fdabf87cc0","status":"passed","time":{"start":1661578505000,"stop":1661578505519,"duration":519}},{"uid":"8176e736452b6199","status":"passed","time":{"start":1661572479000,"stop":1661572479544,"duration":544}},{"uid":"2cac2431f32beae0","status":"passed","time":{"start":1660832281000,"stop":1660832281440,"duration":440}},{"uid":"6003aa082d9a15e2","status":"passed","time":{"start":1660808797000,"stop":1660808797416,"duration":416}},{"uid":"a9c35014c0d5286a","status":"passed","time":{"start":1660299182000,"stop":1660299182377,"duration":377}},{"uid":"52382223a86a0c0","status":"passed","time":{"start":1660295740000,"stop":1660295740342,"duration":342}},{"uid":"7b683e9b2dddfecb","status":"passed","time":{"start":1660293657000,"stop":1660293657456,"duration":456}},{"uid":"264aa8647beb9e93","status":"passed","time":{"start":1660113628000,"stop":1660113628773,"duration":773}},{"uid":"6e4fe1fbc6893916","status":"passed","time":{"start":1660104898000,"stop":1660104898923,"duration":923}},{"uid":"2a08273bcd85497f","status":"passed","time":{"start":1660067259000,"stop":1660067260202,"duration":1202}},{"uid":"f85ca57d0077a3c9","status":"passed","time":{"start":1660047551000,"stop":1660047551518,"duration":518}}]}}