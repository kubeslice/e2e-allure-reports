{"Worker Suite:Worker Suite#[It] Slice Config Negative Scenario tests Slice Config Negative Scenario Update Should fail while updating sliceconfig with invalid clusters":{"statistic":{"failed":0,"broken":0,"skipped":4,"passed":35,"unknown":0,"total":39},"items":[{"uid":"46651dd944544c23","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"a1fcb0e25a7117c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"657df4a50e7fe701","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"23817cfa9d4d7d62","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"91f8c07fc6364c3f","status":"passed","time":{"start":1660113628000,"stop":1660113628215,"duration":215}},{"uid":"5db4ff86031bf856","status":"passed","time":{"start":1660104898000,"stop":1660104898207,"duration":207}},{"uid":"89d6070f92bcb80d","status":"passed","time":{"start":1660067259000,"stop":1660067259386,"duration":386}},{"uid":"cd0bbf108d6b54a2","status":"passed","time":{"start":1660047551000,"stop":1660047551502,"duration":502}},{"uid":"86e577fd9d856ab3","status":"passed","time":{"start":1659982549000,"stop":1659982549722,"duration":722}},{"uid":"84eea051792f19bc","status":"passed","time":{"start":1659970182000,"stop":1659970182344,"duration":344}},{"uid":"c99f4a151bc5c1a9","status":"passed","time":{"start":1659944495000,"stop":1659944495576,"duration":576}},{"uid":"f91e421b7b66d091","status":"passed","time":{"start":1659874977000,"stop":1659874977414,"duration":414}},{"uid":"cb821361306247ab","status":"passed","time":{"start":1659496050000,"stop":1659496050360,"duration":360}},{"uid":"b929d7f5ac5f1dba","status":"passed","time":{"start":1659496083000,"stop":1659496083592,"duration":592}},{"uid":"9a59f2a3aea9a0e0","status":"passed","time":{"start":1659451501000,"stop":1659451502130,"duration":1130}},{"uid":"c3c00816c21f2da7","status":"passed","time":{"start":1659450916000,"stop":1659450916470,"duration":470}},{"uid":"ad6dc42bb92e8932","status":"passed","time":{"start":1659451524000,"stop":1659451524460,"duration":460}},{"uid":"d474ce3b8d7c59da","status":"passed","time":{"start":1659447298000,"stop":1659447298367,"duration":367}},{"uid":"d4ab08c777ea2c11","status":"passed","time":{"start":1659447116000,"stop":1659447116375,"duration":375}},{"uid":"b72b5053e1b375ac","status":"passed","time":{"start":1659448205000,"stop":1659448205458,"duration":458}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting worker-operator pod Check ping between iperf-server and iperf-client after worker-operator pod restart":{"statistic":{"failed":0,"broken":0,"skipped":39,"passed":0,"unknown":0,"total":39},"items":[{"uid":"83c8f056812fe9f2","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"a4b18b22b1de744d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"dd95e720072bb5ba","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"96cf94ab4b822d32","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"287b484dbf2fa5ea","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"9c287adab578aaf5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"f2226ffe7a4c9f89","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"26f9a0d42688abd0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}},{"uid":"411b12010d759cff","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659982549000,"stop":1659982549000,"duration":0}},{"uid":"aab84b2848251745","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659970182000,"stop":1659970182000,"duration":0}},{"uid":"65ced3bb74049344","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659944495000,"stop":1659944495000,"duration":0}},{"uid":"be9f361998284ef2","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659874977000,"stop":1659874977000,"duration":0}},{"uid":"392cb59d501efc9a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496050000,"stop":1659496050000,"duration":0}},{"uid":"7e864c504cd22411","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496083000,"stop":1659496083000,"duration":0}},{"uid":"603953b95f02e5c7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"f547876d408079e4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659450916000,"stop":1659450916000,"duration":0}},{"uid":"15820a2aa9e5b224","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"c2e485325f290d3b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447298000,"stop":1659447298000,"duration":0}},{"uid":"e2b4431c4a93196e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447116000,"stop":1659447116000,"duration":0}},{"uid":"28c84f68459f7dc9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448205000,"stop":1659448205000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice validation Slice with LOW QOS should reconcile low qos slice":{"statistic":{"failed":2,"broken":0,"skipped":3,"passed":4,"unknown":0,"total":9},"items":[{"uid":"163950f3c0e6d910","status":"passed","time":{"start":1659168739000,"stop":1659168739395,"duration":395}},{"uid":"de409261aa8cd036","status":"passed","time":{"start":1659164084000,"stop":1659164084315,"duration":315}},{"uid":"e4c1a18074ae344e","status":"passed","time":{"start":1659160188000,"stop":1659160188292,"duration":292}},{"uid":"34602a2dfa4121a2","status":"passed","time":{"start":1659119724000,"stop":1659119724310,"duration":310}},{"uid":"1e46ed45be6838ba","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000130ac8>: {\n        Underlying: <*exec.ExitError | 0xc00079e000>{\n            ProcessState: {\n                pid: 7527,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 50928},\n                    Stime: {Sec: 0, Usec: 18188},\n                    Maxrss: 45792,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2561,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 184,\n                    Nivcsw: 163,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicelowqos\\\" not found\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicelowqos\\\" not found\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicelowqos\\\" not found\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicelowqos\\\" not found\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (NotFound): slice.networking.kubeslice.io \"slicelowqos\" not found\noccurred","time":{"start":1659116511000,"stop":1659116511891,"duration":891}},{"uid":"4d3617c2bce05dfb","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00088d680>: {\n        Underlying: <*exec.ExitError | 0xc0007e2400>{\n            ProcessState: {\n                pid: 7608,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 61643},\n                    Stime: {Sec: 0, Usec: 14225},\n                    Maxrss: 41880,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2500,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 187,\n                    Nivcsw: 280,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicelowqos\\\" not found\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicelowqos\\\" not found\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicelowqos\\\" not found\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicelowqos\\\" not found\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (NotFound): slice.networking.kubeslice.io \"slicelowqos\" not found\noccurred","time":{"start":1659109470000,"stop":1659109470829,"duration":829}},{"uid":"70e2f9f641c12986","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659106836000,"stop":1659106836000,"duration":0}},{"uid":"5d48e97edd82140c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659101522000,"stop":1659101522000,"duration":0}},{"uid":"758a38f347a57677","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659082816000,"stop":1659082816000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Intracluster slice tests Slice with netpol should reconcile netpol in app NS":{"statistic":{"failed":0,"broken":0,"skipped":24,"passed":15,"unknown":0,"total":39},"items":[{"uid":"31b11f2d72313b84","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"73e92772cb87ca71","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"21edcf4724cf6993","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"3ebbdaf318e2157","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"625cff30c35f4ecf","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"516b5a95f9a8362b","status":"passed","time":{"start":1660104898000,"stop":1660104898190,"duration":190}},{"uid":"9519a31c10379b4e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"d318c1fe9592a06d","status":"passed","time":{"start":1660047551000,"stop":1660047551147,"duration":147}},{"uid":"a57a9aa7fa31ce9e","status":"passed","time":{"start":1659982549000,"stop":1659982549292,"duration":292}},{"uid":"671680ea2c147f4e","status":"passed","time":{"start":1659970182000,"stop":1659970182191,"duration":191}},{"uid":"7a401cf17587a736","status":"passed","time":{"start":1659944495000,"stop":1659944495339,"duration":339}},{"uid":"738cff4f7117659f","status":"passed","time":{"start":1659874977000,"stop":1659874977135,"duration":135}},{"uid":"d3da3d440157aec9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496050000,"stop":1659496050000,"duration":0}},{"uid":"3281d07c26395e16","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496083000,"stop":1659496083000,"duration":0}},{"uid":"425caf17792147f7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"8da1ab0e5413633e","status":"passed","time":{"start":1659450916000,"stop":1659450916166,"duration":166}},{"uid":"fe41b30072a9c422","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"62cfb6ca6a3ef0c","status":"passed","time":{"start":1659447298000,"stop":1659447298134,"duration":134}},{"uid":"58ab373f90d48379","status":"passed","time":{"start":1659447116000,"stop":1659447116229,"duration":229}},{"uid":"ea20309d7318db8","status":"passed","time":{"start":1659448205000,"stop":1659448205139,"duration":139}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting vl3 pod Check ping between iperf-server and iperf-client after vl3 pod restart":{"statistic":{"failed":0,"broken":0,"skipped":39,"passed":0,"unknown":0,"total":39},"items":[{"uid":"ace3bab5c7d41de5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"8e342b171cb4e98f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"65957c53148a92f2","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"2ac206fc9fbd64f8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"2c49fbb5f5ddb3c0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"e3852ee003573a86","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"b11735feb6c8e0fe","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"9c06e997af9ed3e4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}},{"uid":"945d7d094f0b3280","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659982549000,"stop":1659982549000,"duration":0}},{"uid":"238bbe129130275a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659970182000,"stop":1659970182000,"duration":0}},{"uid":"329a149cd0bfaed6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659944495000,"stop":1659944495000,"duration":0}},{"uid":"5cc26af20c3799fe","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659874977000,"stop":1659874977000,"duration":0}},{"uid":"bb3370e2bb312359","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496050000,"stop":1659496050000,"duration":0}},{"uid":"12ed68d51b213ed9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496083000,"stop":1659496083000,"duration":0}},{"uid":"90c7add9006dbf55","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"ab0d0c15e1d7c3bd","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659450916000,"stop":1659450916000,"duration":0}},{"uid":"8dbe4ba521abb97b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"f095cf381e3cea45","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447298000,"stop":1659447298000,"duration":0}},{"uid":"7ddfb23b194fbb7d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447116000,"stop":1659447116000,"duration":0}},{"uid":"60203d46de37efce","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448205000,"stop":1659448205000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Hub netpol scenarios: Slice Configs Netpol scenarios Should create slice for valid namespace and valid clusters in applicationNamespaces of sliceconfigs manifest":{"statistic":{"failed":4,"broken":0,"skipped":0,"passed":35,"unknown":0,"total":39},"items":[{"uid":"3637107585add953","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0007f13b0>: {\n        Underlying: <*exec.ExitError | 0xc0006623a0>{\n            ProcessState: {\n                pid: 7029,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 163457},\n                    Stime: {Sec: 0, Usec: 66161},\n                    Maxrss: 85640,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 7899,\n                    Majflt: 1,\n                    Nswap: 0,\n                    Inblock: 128,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 441,\n                    Nivcsw: 329,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"fireredslice\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660808797000,"stop":1660808797220,"duration":220}},{"uid":"ed59468a101d0f76","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00066d5d8>: {\n        Underlying: <*exec.ExitError | 0xc000917b20>{\n            ProcessState: {\n                pid: 7232,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 136938},\n                    Stime: {Sec: 0, Usec: 41842},\n                    Maxrss: 88508,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 5791,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 322,\n                    Nivcsw: 296,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"fireredslice\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660299182000,"stop":1660299182176,"duration":176}},{"uid":"2fd8ed7ea4ab8a5e","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0000cf9e0>: {\n        Underlying: <*exec.ExitError | 0xc00078bf40>{\n            ProcessState: {\n                pid: 7185,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 110522},\n                    Stime: {Sec: 0, Usec: 55261},\n                    Maxrss: 84484,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3732,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 271,\n                    Nivcsw: 231,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"fireredslice\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660295740000,"stop":1660295740156,"duration":156}},{"uid":"8b1c3530302b945a","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00084c228>: {\n        Underlying: <*exec.ExitError | 0xc0008a05e0>{\n            ProcessState: {\n                pid: 7213,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 158752},\n                    Stime: {Sec: 0, Usec: 48846},\n                    Maxrss: 74180,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4579,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 395,\n                    Nivcsw: 412,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"fireredslice\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660293657000,"stop":1660293657205,"duration":205}},{"uid":"b0cd5625a8c65951","status":"passed","time":{"start":1660113628000,"stop":1660113638508,"duration":10508}},{"uid":"6fb85775730300cf","status":"passed","time":{"start":1660104898000,"stop":1660104898482,"duration":482}},{"uid":"347b84b841b98233","status":"passed","time":{"start":1660067259000,"stop":1660067269513,"duration":10513}},{"uid":"c7a92ac4d03e8b95","status":"passed","time":{"start":1660047551000,"stop":1660047561644,"duration":10644}},{"uid":"609f221d2e51426d","status":"passed","time":{"start":1659982549000,"stop":1659982559875,"duration":10875}},{"uid":"82b8a889eb3761af","status":"passed","time":{"start":1659970182000,"stop":1659970192699,"duration":10699}},{"uid":"3cc2871a535536a3","status":"passed","time":{"start":1659944495000,"stop":1659944505752,"duration":10752}},{"uid":"86dd66947bf128d9","status":"passed","time":{"start":1659874977000,"stop":1659874987965,"duration":10965}},{"uid":"74ee52337a2d01b2","status":"passed","time":{"start":1659496050000,"stop":1659496060417,"duration":10417}},{"uid":"89ca4a2be446ce6d","status":"passed","time":{"start":1659496083000,"stop":1659496093897,"duration":10897}},{"uid":"5a88319a2c6ccf0d","status":"passed","time":{"start":1659451501000,"stop":1659451511840,"duration":10840}},{"uid":"192a2406bf19822","status":"passed","time":{"start":1659450916000,"stop":1659450926936,"duration":10936}},{"uid":"42dd74920cdaa73c","status":"passed","time":{"start":1659451524000,"stop":1659451535359,"duration":11359}},{"uid":"bbf2bcb388703ac9","status":"passed","time":{"start":1659447298000,"stop":1659447308968,"duration":10968}},{"uid":"734c570b11324124","status":"passed","time":{"start":1659447116000,"stop":1659447126412,"duration":10412}},{"uid":"1e9190532732496f","status":"passed","time":{"start":1659448205000,"stop":1659448215725,"duration":10725}}]},"Istio Suite:Istio Suite#[It] Bookinfo tests Bookinfo Scenario 3 without egress and with ingress Should verify if Ping works successfully from Productpage to details endpoint":{"statistic":{"failed":2,"broken":0,"skipped":15,"passed":1,"unknown":0,"total":18},"items":[{"uid":"cfd01154eb5d7503","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808584000,"stop":1660808584000,"duration":0}},{"uid":"440547e835fcb4e1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660804840000,"stop":1660804840000,"duration":0}},{"uid":"1773c128fb7a7e26","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660804470000,"stop":1660804470000,"duration":0}},{"uid":"4040138f21e54883","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660748849000,"stop":1660748849000,"duration":0}},{"uid":"b20ea4a4d75f9b2","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660748706000,"stop":1660748706000,"duration":0}},{"uid":"dbe401b69047a170","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660748558000,"stop":1660748558000,"duration":0}},{"uid":"f36717638064dd62","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660636759000,"stop":1660636759000,"duration":0}},{"uid":"ff081d7a3827884","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660636482000,"stop":1660636482000,"duration":0}},{"uid":"e424237d1ed686ba","status":"failed","statusDetails":"Unexpected non-nil/non-zero argument at index 1:\n\t<*shell.ErrWithCmdOutput>: &shell.ErrWithCmdOutput{Underlying:(*exec.ExitError)(0xc00068d660), Output:(*shell.output)(0xc00043e978)}","time":{"start":1660632006000,"stop":1660632072577,"duration":66577}},{"uid":"7f21a1b2ee1443dc","status":"passed","time":{"start":1660624273000,"stop":1660624331988,"duration":58988}},{"uid":"5eb6b6a6d59db877","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660624220000,"stop":1660624220000,"duration":0}},{"uid":"61ef7c7892c4faab","status":"failed","statusDetails":"Unexpected non-nil/non-zero argument at index 1:\n\t<*shell.ErrWithCmdOutput>: &shell.ErrWithCmdOutput{Underlying:(*exec.ExitError)(0xc00079c060), Output:(*shell.output)(0xc0006ba7f8)}","time":{"start":1660453205000,"stop":1660453273203,"duration":68203}},{"uid":"307288b2cae7873","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660452554000,"stop":1660452554000,"duration":0}},{"uid":"3ffe498a305da8c3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660452412000,"stop":1660452412000,"duration":0}},{"uid":"2217b37ac9378225","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660415422000,"stop":1660415422000,"duration":0}},{"uid":"8a5a2f9d58960336","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660298968000,"stop":1660298968000,"duration":0}},{"uid":"59490bd18d68755f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295522000,"stop":1660295522000,"duration":0}},{"uid":"f8e326d61b2d24e5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293414000,"stop":1660293414000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice Config Negative Scenario tests Slice Config Negative Scenario Creation Should fail while creating sliceconfig with invalid clusters":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":39,"unknown":0,"total":39},"items":[{"uid":"1c6fd09974e0dd56","status":"passed","time":{"start":1660808797000,"stop":1660808797210,"duration":210}},{"uid":"fd58156bdd2eeda9","status":"passed","time":{"start":1660299182000,"stop":1660299182184,"duration":184}},{"uid":"2a85415c006c6122","status":"passed","time":{"start":1660295740000,"stop":1660295740187,"duration":187}},{"uid":"ee7e944f1af25c9b","status":"passed","time":{"start":1660293657000,"stop":1660293657357,"duration":357}},{"uid":"da5cb7d723fd8f4f","status":"passed","time":{"start":1660113628000,"stop":1660113628291,"duration":291}},{"uid":"48badc49b55e6bcd","status":"passed","time":{"start":1660104898000,"stop":1660104898409,"duration":409}},{"uid":"b96df0ef959fd578","status":"passed","time":{"start":1660067259000,"stop":1660067259174,"duration":174}},{"uid":"b5cac23adb5289fe","status":"passed","time":{"start":1660047551000,"stop":1660047551264,"duration":264}},{"uid":"78899f06b4b0e667","status":"passed","time":{"start":1659982549000,"stop":1659982549395,"duration":395}},{"uid":"ddfde65d3047cf6b","status":"passed","time":{"start":1659970182000,"stop":1659970182210,"duration":210}},{"uid":"2e6bc7e5b307543f","status":"passed","time":{"start":1659944495000,"stop":1659944495309,"duration":309}},{"uid":"b4d68a4a3f0c2680","status":"passed","time":{"start":1659874977000,"stop":1659874977259,"duration":259}},{"uid":"3e74f451885c0c08","status":"passed","time":{"start":1659496050000,"stop":1659496050238,"duration":238}},{"uid":"646d016f32e1da33","status":"passed","time":{"start":1659496083000,"stop":1659496083377,"duration":377}},{"uid":"4e6c3b50899e6449","status":"passed","time":{"start":1659451501000,"stop":1659451501507,"duration":507}},{"uid":"6fbe6290cff2c8d1","status":"passed","time":{"start":1659450916000,"stop":1659450916301,"duration":301}},{"uid":"8e87201815ae6d1b","status":"passed","time":{"start":1659451524000,"stop":1659451524237,"duration":237}},{"uid":"672854f9e9ea8c6f","status":"passed","time":{"start":1659447298000,"stop":1659447298449,"duration":449}},{"uid":"36e161dd2f69c2e","status":"passed","time":{"start":1659447116000,"stop":1659447116212,"duration":212}},{"uid":"ce1acedaf786d4fb","status":"passed","time":{"start":1659448205000,"stop":1659448205246,"duration":246}}]},"Worker Suite:Worker Suite#[It] Intracluster slice tests Slice with netpol should remove network policy resource from application namespace once the isolationEnabled is set to false":{"statistic":{"failed":1,"broken":0,"skipped":24,"passed":14,"unknown":0,"total":39},"items":[{"uid":"7384bf6b99d34c2f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"db0298cc8d1fe4b6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"43c67000a11a2e4c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"364a094eb57ec9ff","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"e5efa63f8f60e74","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"8feb3246cec61f5a","status":"passed","time":{"start":1660104898000,"stop":1660104905082,"duration":7082}},{"uid":"10295f1f4963b982","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"b43b7d108b078b7d","status":"passed","time":{"start":1660047551000,"stop":1660047557836,"duration":6836}},{"uid":"4c71a33499abdf6c","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000a44660>: {\n        Underlying: <*exec.ExitError | 0xc00014ed80>{\n            ProcessState: {\n                pid: 7392,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 83422},\n                    Stime: {Sec: 0, Usec: 31779},\n                    Maxrss: 42544,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2522,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 148,\n                    Nivcsw: 163,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: ServiceExportConfig: Forbidden: The SliceConfig can only be deleted after all the service export configs are deleted for the slice.\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: ServiceExportConfig: Forbidden: The SliceConfig can only be deleted after all the service export configs are deleted for the slice.\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: ServiceExportConfig: Forbidden: The SliceConfig can only be deleted after all the service export configs are deleted for the slice.\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: ServiceExportConfig: Forbidden: The SliceConfig can only be deleted after all the service export configs are deleted for the slice.\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: ServiceExportConfig: Forbidden: The SliceConfig can only be deleted after all the service export configs are deleted for the slice.\noccurred","time":{"start":1659982549000,"stop":1659982558710,"duration":9710}},{"uid":"8c55b6e8a27891a2","status":"passed","time":{"start":1659970182000,"stop":1659970188871,"duration":6871}},{"uid":"a8f89acf60f00a65","status":"passed","time":{"start":1659944495000,"stop":1659944504977,"duration":9977}},{"uid":"720d85b8f5c728fc","status":"passed","time":{"start":1659874977000,"stop":1659874983803,"duration":6803}},{"uid":"a8f494a04a1a6a0b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496050000,"stop":1659496050000,"duration":0}},{"uid":"6b2e149b3f483171","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496083000,"stop":1659496083000,"duration":0}},{"uid":"15527b71eda095ed","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"b59f25d1d2bcd5ca","status":"passed","time":{"start":1659450916000,"stop":1659450925129,"duration":9129}},{"uid":"30e8aa9f71c4fc5d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"83efdde840eee843","status":"passed","time":{"start":1659447298000,"stop":1659447304843,"duration":6843}},{"uid":"630cd32864a9d27","status":"passed","time":{"start":1659447116000,"stop":1659447123133,"duration":7133}},{"uid":"e1371e4e21df0476","status":"passed","time":{"start":1659448205000,"stop":1659448211878,"duration":6878}}]},"Empty Suite:Empty Suite#[It] Hub Creation tests Hub Installtion Test Senarios Should fail if Hub is installed again in the same cluster":{"statistic":{"failed":0,"broken":0,"skipped":16,"passed":30,"unknown":0,"total":46},"items":[{"uid":"f6904b785b79886","status":"passed","time":{"start":1660828377000,"stop":1660828380314,"duration":3314}},{"uid":"e050e10cad13d0b","status":"passed","time":{"start":1660807615000,"stop":1660807617861,"duration":2861}},{"uid":"85331458a0cf0a4c","status":"passed","time":{"start":1660803821000,"stop":1660803824311,"duration":3311}},{"uid":"f5969f353d2b7516","status":"passed","time":{"start":1660805590000,"stop":1660805592911,"duration":2911}},{"uid":"cd62309ad74ac09a","status":"passed","time":{"start":1660803559000,"stop":1660803561522,"duration":2522}},{"uid":"d2e0c70a08e98b0e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660802965000,"stop":1660802965000,"duration":0}},{"uid":"1dcf143d254706e7","status":"passed","time":{"start":1660798299000,"stop":1660798302334,"duration":3334}},{"uid":"c4148fd7b7f3eaee","status":"passed","time":{"start":1660747846000,"stop":1660747850753,"duration":4753}},{"uid":"b35b44a4bc8c699d","status":"passed","time":{"start":1660747702000,"stop":1660747705976,"duration":3976}},{"uid":"e350535b116b83a5","status":"passed","time":{"start":1660747595000,"stop":1660747598242,"duration":3242}},{"uid":"9bcba789713c4f82","status":"passed","time":{"start":1660747499000,"stop":1660747501607,"duration":2607}},{"uid":"168be2cffafa0d11","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660747927000,"stop":1660747927000,"duration":0}},{"uid":"2fc6fb9be9e002cd","status":"passed","time":{"start":1660722528000,"stop":1660722531610,"duration":3610}},{"uid":"e2e90ce51abba1a1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660711963000,"stop":1660711963000,"duration":0}},{"uid":"f71547cacee01069","status":"passed","time":{"start":1660635769000,"stop":1660635772794,"duration":3794}},{"uid":"a23e15f025923d17","status":"passed","time":{"start":1660635489000,"stop":1660635492304,"duration":3304}},{"uid":"8d8498d9ba02b4c4","status":"passed","time":{"start":1660635683000,"stop":1660635687048,"duration":4048}},{"uid":"84fad47469745c5e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660635796000,"stop":1660635796000,"duration":0}},{"uid":"6c1c50b4f000541f","status":"passed","time":{"start":1660631001000,"stop":1660631003626,"duration":2626}},{"uid":"b29c0bf3972bd469","status":"passed","time":{"start":1660623317000,"stop":1660623320288,"duration":3288}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests Iperf App connectivity Verify Connectivity between iperf-server and iperf-client":{"statistic":{"failed":0,"broken":0,"skipped":32,"passed":7,"unknown":0,"total":39},"items":[{"uid":"b2dd8250a0c30e23","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"ccb1ebe6dc37989","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"53f69498bc56e46a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"fdd33692af83777","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"c9e1f1608059fd00","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"cb80154ecd076dda","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"a7837f3cbb91d7ad","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"a7aac38e3105a7ba","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}},{"uid":"5eba677be11c7ac3","status":"passed","time":{"start":1659982549000,"stop":1659982556001,"duration":7001}},{"uid":"35fca064f4533a39","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659970182000,"stop":1659970182000,"duration":0}},{"uid":"82e1ff76cbf2f999","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659944495000,"stop":1659944495000,"duration":0}},{"uid":"23bb2287ab847206","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659874977000,"stop":1659874977000,"duration":0}},{"uid":"83ca3f1756ba941a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496050000,"stop":1659496050000,"duration":0}},{"uid":"e75e36307e73f88f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496083000,"stop":1659496083000,"duration":0}},{"uid":"36d72517ffd74f70","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"da6ba38caba6ccdc","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659450916000,"stop":1659450916000,"duration":0}},{"uid":"993f846fefafacd7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"a97fbaa4b5478b05","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447298000,"stop":1659447298000,"duration":0}},{"uid":"87ffd5b6dfddd1c5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447116000,"stop":1659447116000,"duration":0}},{"uid":"8858a4548cf0356f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448205000,"stop":1659448205000,"duration":0}}]},"Istio Suite:Istio Suite#[It] Bookinfo tests Bookinfo Scenario 2 with egress and without ingress Should verify if Ping works successfully from Productpage to details endpoint":{"statistic":{"failed":1,"broken":0,"skipped":14,"passed":3,"unknown":0,"total":18},"items":[{"uid":"9054513a86e7eccb","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808584000,"stop":1660808584000,"duration":0}},{"uid":"e425aaf8fca7fbd9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660804840000,"stop":1660804840000,"duration":0}},{"uid":"bb05a276c7bc21c8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660804470000,"stop":1660804470000,"duration":0}},{"uid":"c1e341a3022d2686","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660748849000,"stop":1660748849000,"duration":0}},{"uid":"6473730888da54bb","status":"passed","time":{"start":1660748706000,"stop":1660748767157,"duration":61157}},{"uid":"4942be40e7ed48a5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660748558000,"stop":1660748558000,"duration":0}},{"uid":"db21f017b2571d3a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660636759000,"stop":1660636759000,"duration":0}},{"uid":"99485c4d0a969c60","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660636482000,"stop":1660636482000,"duration":0}},{"uid":"8b4c7ef206c7ece3","status":"passed","time":{"start":1660632006000,"stop":1660632065472,"duration":59472}},{"uid":"5ce585a446632ddc","status":"failed","statusDetails":"Unexpected non-nil/non-zero argument at index 1:\n\t<*shell.ErrWithCmdOutput>: &shell.ErrWithCmdOutput{Underlying:(*exec.ExitError)(0xc000696820), Output:(*shell.output)(0xc00081bd40)}","time":{"start":1660624273000,"stop":1660624327550,"duration":54550}},{"uid":"f007692b7d103455","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660624220000,"stop":1660624220000,"duration":0}},{"uid":"4d17e05c43b3dbf2","status":"passed","time":{"start":1660453205000,"stop":1660453263961,"duration":58961}},{"uid":"21ae17debddf3db7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660452554000,"stop":1660452554000,"duration":0}},{"uid":"2e76bd48515cc336","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660452412000,"stop":1660452412000,"duration":0}},{"uid":"1b39f69bdb1f466e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660415422000,"stop":1660415422000,"duration":0}},{"uid":"92bcd465b9d9d9d2","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660298968000,"stop":1660298968000,"duration":0}},{"uid":"9690c9c922012794","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295522000,"stop":1660295522000,"duration":0}},{"uid":"5f90302fde6e3f9a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293414000,"stop":1660293414000,"duration":0}}]},"Istio Suite:Istio Suite#[It] Bookinfo tests Bookinfo Scenario 4 with egress and ingress Should install Details, Reviews, Ratings and Serviceexport on worker cluster-2":{"statistic":{"failed":0,"broken":0,"skipped":17,"passed":1,"unknown":0,"total":18},"items":[{"uid":"602877674acaf897","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808584000,"stop":1660808584000,"duration":0}},{"uid":"7793e3eb71c60dae","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660804840000,"stop":1660804840000,"duration":0}},{"uid":"2f63cc2b8c9df004","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660804470000,"stop":1660804470000,"duration":0}},{"uid":"e83eec2a95b23357","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660748849000,"stop":1660748849000,"duration":0}},{"uid":"5f2dd273a6721c03","status":"passed","time":{"start":1660748706000,"stop":1660748741322,"duration":35322}},{"uid":"7e1368aa817d2ab6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660748558000,"stop":1660748558000,"duration":0}},{"uid":"64d62bc9755d6c01","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660636759000,"stop":1660636759000,"duration":0}},{"uid":"e4c725ed87994a67","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660636482000,"stop":1660636482000,"duration":0}},{"uid":"d15829cd8e7f842c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660632006000,"stop":1660632006000,"duration":0}},{"uid":"7a71b2e94d22f5c6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660624273000,"stop":1660624273000,"duration":0}},{"uid":"1ef19d73aa9cceff","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660624220000,"stop":1660624220000,"duration":0}},{"uid":"be1a7e832eb984ae","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660453205000,"stop":1660453205000,"duration":0}},{"uid":"c6f3850fe629092c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660452554000,"stop":1660452554000,"duration":0}},{"uid":"9aa6974697c66772","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660452412000,"stop":1660452412000,"duration":0}},{"uid":"51b59886437c3742","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660415422000,"stop":1660415422000,"duration":0}},{"uid":"2a692a613efe5ab7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660298968000,"stop":1660298968000,"duration":0}},{"uid":"fb10c77fac9b5cf2","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295522000,"stop":1660295522000,"duration":0}},{"uid":"4f6f1aacb0b2b5ce","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293414000,"stop":1660293414000,"duration":0}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests slice with netpol Should contain application namespaces":{"statistic":{"failed":0,"broken":0,"skipped":29,"passed":10,"unknown":0,"total":39},"items":[{"uid":"6aa2e3bb35b78e32","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"f196b21f6a4e329a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"35d37fa1d1af956a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"a826aa154151a134","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"df21d7568dd7599d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"c247d6ba54f52876","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"1d6194afce7554b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"1235243d85ca5cea","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}},{"uid":"535159053f975706","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659982549000,"stop":1659982549000,"duration":0}},{"uid":"30acfb68f2137adb","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659970182000,"stop":1659970182000,"duration":0}},{"uid":"1a13aeaeaec63028","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659944495000,"stop":1659944495000,"duration":0}},{"uid":"d93213caebf850b3","status":"passed","time":{"start":1659874977000,"stop":1659874977151,"duration":151}},{"uid":"c5ba31a95e455ab8","status":"passed","time":{"start":1659496050000,"stop":1659496050128,"duration":128}},{"uid":"c95259b90823a79d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496083000,"stop":1659496083000,"duration":0}},{"uid":"d254c7cf2ed1d560","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"c7f96b2b233913d4","status":"passed","time":{"start":1659450916000,"stop":1659450916359,"duration":359}},{"uid":"64596d60711b3954","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"23b5cfc05a3b6727","status":"passed","time":{"start":1659447298000,"stop":1659447298235,"duration":235}},{"uid":"1655cb16e97ce896","status":"passed","time":{"start":1659447116000,"stop":1659447116126,"duration":126}},{"uid":"99f576424aca310c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448205000,"stop":1659448205000,"duration":0}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests Iperf App connectivity should have gateway pod running":{"statistic":{"failed":1,"broken":0,"skipped":24,"passed":14,"unknown":0,"total":39},"items":[{"uid":"ca752e72eaa1ba83","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"70a7d27a99b2b60b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"3eeb746fa3a2651","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"fbb2d3f303832d5e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"2861f06811efcc4a","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1660113628000,"stop":1660113810641,"duration":182641}},{"uid":"2435eab733dae1c3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"1255de5c12de14b1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"fd7704c54474e0f0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}},{"uid":"249074e117f93794","status":"passed","time":{"start":1659982549000,"stop":1659982585327,"duration":36327}},{"uid":"d169c26865f6bbb7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659970182000,"stop":1659970182000,"duration":0}},{"uid":"37e3cd1e6b2a7f58","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659944495000,"stop":1659944495000,"duration":0}},{"uid":"699a0a58f888de65","status":"passed","time":{"start":1659874977000,"stop":1659874977043,"duration":43}},{"uid":"1177505f1df6a7ba","status":"passed","time":{"start":1659496050000,"stop":1659496050048,"duration":48}},{"uid":"568dfa885ec9ac9a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496083000,"stop":1659496083000,"duration":0}},{"uid":"8f24e815e07f8581","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"8c3a94f498c5caf7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659450916000,"stop":1659450916000,"duration":0}},{"uid":"9fd2b3e0720b13ef","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"970f18afbe85c363","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447298000,"stop":1659447298000,"duration":0}},{"uid":"94e2ebd5289e3996","status":"passed","time":{"start":1659447116000,"stop":1659447116032,"duration":32}},{"uid":"c4e39d0392284a94","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448205000,"stop":1659448205000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice Config Negative Scenario tests Slice Config Negative Scenario Update Should fail while updating sliceconfig with invalid values on slice subnet":{"statistic":{"failed":0,"broken":0,"skipped":4,"passed":35,"unknown":0,"total":39},"items":[{"uid":"153893885e5371cd","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"e363da716206e0a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"59208380cb3fb644","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"ab82f83cf80120a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"a7c3da3595e9f755","status":"passed","time":{"start":1660113628000,"stop":1660113628214,"duration":214}},{"uid":"96793728a6e27a30","status":"passed","time":{"start":1660104898000,"stop":1660104898211,"duration":211}},{"uid":"dc3e813d284a352a","status":"passed","time":{"start":1660067259000,"stop":1660067259395,"duration":395}},{"uid":"356d03c3c897fa1b","status":"passed","time":{"start":1660047551000,"stop":1660047551432,"duration":432}},{"uid":"ce9987f5ff87d7ba","status":"passed","time":{"start":1659982549000,"stop":1659982549740,"duration":740}},{"uid":"5d724ccc782abd42","status":"passed","time":{"start":1659970182000,"stop":1659970182471,"duration":471}},{"uid":"3eb784e1301fbf5e","status":"passed","time":{"start":1659944495000,"stop":1659944495528,"duration":528}},{"uid":"f2eee1de6a193726","status":"passed","time":{"start":1659874977000,"stop":1659874977462,"duration":462}},{"uid":"44bd298d16b818e6","status":"passed","time":{"start":1659496050000,"stop":1659496050360,"duration":360}},{"uid":"35fcf1431f5ec517","status":"passed","time":{"start":1659496083000,"stop":1659496083474,"duration":474}},{"uid":"7160f6bf40aa2cf6","status":"passed","time":{"start":1659451501000,"stop":1659451501406,"duration":406}},{"uid":"63178a893a1ea710","status":"passed","time":{"start":1659450916000,"stop":1659450916570,"duration":570}},{"uid":"689edf0a40b9c2ea","status":"passed","time":{"start":1659451524000,"stop":1659451524371,"duration":371}},{"uid":"db7b915489b89837","status":"passed","time":{"start":1659447298000,"stop":1659447298428,"duration":428}},{"uid":"ae751069e4a72188","status":"passed","time":{"start":1659447116000,"stop":1659447116421,"duration":421}},{"uid":"15dacae28907463f","status":"passed","time":{"start":1659448205000,"stop":1659448205694,"duration":694}}]},"Worker Suite:Worker Suite#[It] Slice Config Negative Scenario tests Slice Config Negative Scenario Creation Should fail while creating sliceconfig with with invalid DSCP class in qosprofiledetails":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":39,"unknown":0,"total":39},"items":[{"uid":"cd51e04c4119c18b","status":"passed","time":{"start":1660808797000,"stop":1660808797285,"duration":285}},{"uid":"5df03f75f30172d4","status":"passed","time":{"start":1660299182000,"stop":1660299182244,"duration":244}},{"uid":"6bc63fee86082900","status":"passed","time":{"start":1660295740000,"stop":1660295740239,"duration":239}},{"uid":"7eac490dc838f235","status":"passed","time":{"start":1660293657000,"stop":1660293657320,"duration":320}},{"uid":"32ed2535f1490246","status":"passed","time":{"start":1660113628000,"stop":1660113628295,"duration":295}},{"uid":"13e3eb7ce308bc4c","status":"passed","time":{"start":1660104898000,"stop":1660104898243,"duration":243}},{"uid":"e49f09d80140a40b","status":"passed","time":{"start":1660067259000,"stop":1660067259219,"duration":219}},{"uid":"49d64be485e17926","status":"passed","time":{"start":1660047551000,"stop":1660047551349,"duration":349}},{"uid":"f8edf56e8d5df854","status":"passed","time":{"start":1659982549000,"stop":1659982549526,"duration":526}},{"uid":"4ff3fc24efcc9987","status":"passed","time":{"start":1659970182000,"stop":1659970182257,"duration":257}},{"uid":"3ca2bfe14018086","status":"passed","time":{"start":1659944495000,"stop":1659944495370,"duration":370}},{"uid":"17758bfb37752799","status":"passed","time":{"start":1659874977000,"stop":1659874977403,"duration":403}},{"uid":"39b71752fa07c658","status":"passed","time":{"start":1659496050000,"stop":1659496050287,"duration":287}},{"uid":"ef4c607459eba344","status":"passed","time":{"start":1659496083000,"stop":1659496083452,"duration":452}},{"uid":"fb73f3c33a6c676e","status":"passed","time":{"start":1659451501000,"stop":1659451501621,"duration":621}},{"uid":"94bf21b209bcf354","status":"passed","time":{"start":1659450916000,"stop":1659450916315,"duration":315}},{"uid":"99877107e20ce419","status":"passed","time":{"start":1659451524000,"stop":1659451524415,"duration":415}},{"uid":"4a7756fc3f17c390","status":"passed","time":{"start":1659447298000,"stop":1659447298546,"duration":546}},{"uid":"9070b03cabf6da8c","status":"passed","time":{"start":1659447116000,"stop":1659447116285,"duration":285}},{"uid":"fb766d2595a33a64","status":"passed","time":{"start":1659448205000,"stop":1659448205328,"duration":328}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests Iperf App connectivity Should install iperf-sleep on client cluster":{"statistic":{"failed":7,"broken":0,"skipped":25,"passed":7,"unknown":0,"total":39},"items":[{"uid":"e3fa65532ee144ca","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"70d33360ebb66a96","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"fc7652fbc8491091","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"9f03ee2327d70c75","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"b29aed39f4f9ab9a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"ace7e813770a747f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"952e2e1421f79e7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"ae18ebba5777f587","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}},{"uid":"510284485a2fff87","status":"passed","time":{"start":1659982549000,"stop":1659982550090,"duration":1090}},{"uid":"466344827719a82f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659970182000,"stop":1659970182000,"duration":0}},{"uid":"154cb936a3753e5d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659944495000,"stop":1659944495000,"duration":0}},{"uid":"e7495404ab2acfcf","status":"failed","statusDetails":"Timed out after 120.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1659874977000,"stop":1659875099985,"duration":122985}},{"uid":"a61df080062ca9a6","status":"failed","statusDetails":"Timed out after 120.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1659496050000,"stop":1659496173270,"duration":123270}},{"uid":"52194caef4d8c838","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496083000,"stop":1659496083000,"duration":0}},{"uid":"7a0765ea8cd914ac","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"25e9166eff99b05a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659450916000,"stop":1659450916000,"duration":0}},{"uid":"b402cc5cc3b41952","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"1735c40150944cf","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447298000,"stop":1659447298000,"duration":0}},{"uid":"bf8f923cbbe8dbad","status":"failed","statusDetails":"Timed out after 120.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1659447116000,"stop":1659447239709,"duration":123709}},{"uid":"8961f60c5a71307","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448205000,"stop":1659448205000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice creation scenarios: when slice red is installed install slice blue workers should have vl3 routers from both slices":{"statistic":{"failed":1,"broken":0,"skipped":27,"passed":11,"unknown":0,"total":39},"items":[{"uid":"1d16c0e27c4410cb","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"8629b3ff1550e495","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"97f1ae79f85deec5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"9918f695e52359f3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"a18f7f1ce2d18999","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"93bff7dfa90cf20d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"4ff872de7cc98ab1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"f5ca1143b95130ca","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}},{"uid":"d3127f715cb5b818","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659982549000,"stop":1659982549000,"duration":0}},{"uid":"9ec6abe3ace13022","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659970182000,"stop":1659970182000,"duration":0}},{"uid":"213c0844d879d369","status":"passed","time":{"start":1659944495000,"stop":1659944495053,"duration":53}},{"uid":"2ff9a8b29adfd93","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659874977000,"stop":1659874977000,"duration":0}},{"uid":"69c7820d0e162af5","status":"passed","time":{"start":1659496050000,"stop":1659496053077,"duration":3077}},{"uid":"e24f3b1085693d7f","status":"passed","time":{"start":1659496083000,"stop":1659496085052,"duration":2052}},{"uid":"20344023803e3742","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"922f978314c2dab9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659450916000,"stop":1659450916000,"duration":0}},{"uid":"f6013e34bd66aa3b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"a68e4ee9c47d1386","status":"passed","time":{"start":1659447298000,"stop":1659447300041,"duration":2041}},{"uid":"198d61d2b91c0f26","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447116000,"stop":1659447116000,"duration":0}},{"uid":"aa22f1587132edc3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448205000,"stop":1659448205000,"duration":0}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Update Should fail when Project is applied with service account name as blank in Write users":{"statistic":{"failed":7,"broken":0,"skipped":0,"passed":114,"unknown":0,"total":121},"items":[{"uid":"6e10ebc9b82fd36a","status":"passed","time":{"start":1660828517000,"stop":1660828521942,"duration":4942}},{"uid":"f43807e680550c21","status":"passed","time":{"start":1660807774000,"stop":1660807779728,"duration":5728}},{"uid":"9c793f7f75bc7a1a","status":"passed","time":{"start":1660803998000,"stop":1660804003890,"duration":5890}},{"uid":"50972d95eb82efb1","status":"passed","time":{"start":1660805737000,"stop":1660805742838,"duration":5838}},{"uid":"19ade5f0c8162980","status":"passed","time":{"start":1660803696000,"stop":1660803700632,"duration":4632}},{"uid":"ff4e5f1a669f15a2","status":"passed","time":{"start":1660798468000,"stop":1660798473818,"duration":5818}},{"uid":"4c0fa0e3aabb61f3","status":"passed","time":{"start":1660748009000,"stop":1660748014029,"duration":5029}},{"uid":"e531b83c32800bfc","status":"passed","time":{"start":1660747865000,"stop":1660747869895,"duration":4895}},{"uid":"98082ade224f943","status":"passed","time":{"start":1660747735000,"stop":1660747739841,"duration":4841}},{"uid":"f2f04ccfc6684026","status":"passed","time":{"start":1660747654000,"stop":1660747658701,"duration":4701}},{"uid":"62e42b1458242ae8","status":"passed","time":{"start":1660722683000,"stop":1660722687839,"duration":4839}},{"uid":"51839b896087d5a","status":"passed","time":{"start":1660635916000,"stop":1660635921874,"duration":5874}},{"uid":"602a5487283ce3e6","status":"passed","time":{"start":1660635666000,"stop":1660635671882,"duration":5882}},{"uid":"f1c201e394c4531","status":"passed","time":{"start":1660635860000,"stop":1660635866065,"duration":6065}},{"uid":"a5246d6f5fe329b3","status":"passed","time":{"start":1660631164000,"stop":1660631168762,"duration":4762}},{"uid":"347ebac5641775bb","status":"passed","time":{"start":1660623463000,"stop":1660623467860,"duration":4860}},{"uid":"9d4a5199dd1bfe0d","status":"passed","time":{"start":1660623384000,"stop":1660623388689,"duration":4689}},{"uid":"515c34f7be0f6936","status":"passed","time":{"start":1660623360000,"stop":1660623364634,"duration":4634}},{"uid":"539825a3f69f1ec1","status":"passed","time":{"start":1660452377000,"stop":1660452381468,"duration":4468}},{"uid":"8ceaf750261430f6","status":"passed","time":{"start":1660451870000,"stop":1660451875721,"duration":5721}}]},"Empty Suite:Empty Suite#[It] Hub Creation tests Hub Installtion Test Senarios Should successfully pass if Cert is installed first and then Hub":{"statistic":{"failed":16,"broken":0,"skipped":0,"passed":30,"unknown":0,"total":46},"items":[{"uid":"7e9b8ac58578e811","status":"passed","time":{"start":1660828377000,"stop":1660828465157,"duration":88157}},{"uid":"9e5ccc730fe2e8a8","status":"passed","time":{"start":1660807615000,"stop":1660807652328,"duration":37328}},{"uid":"15c44ca29f9d3c46","status":"passed","time":{"start":1660803821000,"stop":1660803870084,"duration":49084}},{"uid":"a5c209b18806cbde","status":"passed","time":{"start":1660805590000,"stop":1660805620517,"duration":30517}},{"uid":"1019666c8f79ad50","status":"passed","time":{"start":1660803559000,"stop":1660803645360,"duration":86360}},{"uid":"84238f52c56b53c9","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000e150>: {\n        Underlying: <*exec.ExitError | 0xc0004d6000>{\n            ProcessState: {\n                pid: 5944,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 1, Usec: 423691},\n                    Stime: {Sec: 0, Usec: 234159},\n                    Maxrss: 79704,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4793,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12840,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 16619,\n                    Nivcsw: 4422,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-0.2.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 26 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 26 resources with timeout of 2m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-0.2.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 26 resource(s)\n    wait.go:48: [debug] beginning wait for 26 resources with timeout of 2m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-controller\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-manager-metrics-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-controller-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-editor-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"workerslicegateways.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"serviceexportconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"clusters.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"projects.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workerserviceimports.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"sliceconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workersliceconfigs.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-serving-cert\" Certificate\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-selfsigned-issuer\" Issuer\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-validating-webhook-configuration\" ValidatingWebhookConfiguration\n    uninstall.go:144: [debug] purge requested for kubeslice-controller\n    Error: INSTALLATION FAILED: release kubeslice-controller failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-controller failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1660802965000,"stop":1660803107938,"duration":142938}},{"uid":"f2acc7120cf1bb35","status":"passed","time":{"start":1660798299000,"stop":1660798346679,"duration":47679}},{"uid":"d98e7bd26792265f","status":"passed","time":{"start":1660747846000,"stop":1660747930696,"duration":84696}},{"uid":"c3a6d02510f34211","status":"passed","time":{"start":1660747702000,"stop":1660747795985,"duration":93985}},{"uid":"aa9abf7a40798569","status":"passed","time":{"start":1660747595000,"stop":1660747632974,"duration":37974}},{"uid":"43bd8d248ea08673","status":"passed","time":{"start":1660747499000,"stop":1660747536364,"duration":37364}},{"uid":"962ab22b3d3e9dac","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000e090>: {\n        Underlying: <*exec.ExitError | 0xc0004e0000>{\n            ProcessState: {\n                pid: 5999,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 1, Usec: 266944},\n                    Stime: {Sec: 0, Usec: 152693},\n                    Maxrss: 78976,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3923,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12840,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 14803,\n                    Nivcsw: 3843,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-0.2.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 26 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 26 resources with timeout of 2m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-0.2.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 26 resource(s)\n    wait.go:48: [debug] beginning wait for 26 resources with timeout of 2m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-controller\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-manager-metrics-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-controller-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-editor-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"workerslicegateways.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"clusters.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"projects.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"serviceexportconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"sliceconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workerserviceimports.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workersliceconfigs.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-serving-cert\" Certificate\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-selfsigned-issuer\" Issuer\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-validating-webhook-configuration\" ValidatingWebhookConfiguration\n    uninstall.go:144: [debug] purge requested for kubeslice-controller\n    Error: INSTALLATION FAILED: release kubeslice-controller failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-controller failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1660747927000,"stop":1660748077240,"duration":150240}},{"uid":"48fb327ae572ec06","status":"passed","time":{"start":1660722528000,"stop":1660722628011,"duration":100011}},{"uid":"269b2e9365b7b83c","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00042a048>: {\n        Underlying: <*exec.ExitError | 0xc00040c000>{\n            ProcessState: {\n                pid: 5934,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 1, Usec: 302999},\n                    Stime: {Sec: 0, Usec: 158068},\n                    Maxrss: 82400,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4432,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12840,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 16099,\n                    Nivcsw: 3847,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-0.2.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 26 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 26 resources with timeout of 2m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-0.2.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 26 resource(s)\n    wait.go:48: [debug] beginning wait for 26 resources with timeout of 2m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-controller\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-manager-metrics-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-controller-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-editor-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"workerslicegateways.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"clusters.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"projects.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"serviceexportconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"sliceconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workerserviceimports.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workersliceconfigs.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-serving-cert\" Certificate\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-selfsigned-issuer\" Issuer\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-validating-webhook-configuration\" ValidatingWebhookConfiguration\n    uninstall.go:144: [debug] purge requested for kubeslice-controller\n    Error: INSTALLATION FAILED: release kubeslice-controller failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-controller failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1660711963000,"stop":1660712111575,"duration":148575}},{"uid":"26299e0a95060e2f","status":"passed","time":{"start":1660635769000,"stop":1660635813739,"duration":44739}},{"uid":"d1e74c7e02613125","status":"passed","time":{"start":1660635489000,"stop":1660635538552,"duration":49552}},{"uid":"3b5b3e32f1256c55","status":"passed","time":{"start":1660635683000,"stop":1660635792188,"duration":109188}},{"uid":"4c9ad09ff79fcfc7","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000e138>: {\n        Underlying: <*exec.ExitError | 0xc000496000>{\n            ProcessState: {\n                pid: 5958,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 1, Usec: 341274},\n                    Stime: {Sec: 0, Usec: 212593},\n                    Maxrss: 80844,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3333,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12840,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 15473,\n                    Nivcsw: 3994,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-0.2.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 26 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 26 resources with timeout of 2m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-0.2.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 26 resource(s)\n    wait.go:48: [debug] beginning wait for 26 resources with timeout of 2m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-controller\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-manager-metrics-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-controller-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-editor-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"workerslicegateways.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"clusters.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"projects.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"serviceexportconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"sliceconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workerserviceimports.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workersliceconfigs.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-serving-cert\" Certificate\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-selfsigned-issuer\" Issuer\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-validating-webhook-configuration\" ValidatingWebhookConfiguration\n    uninstall.go:144: [debug] purge requested for kubeslice-controller\n    Error: INSTALLATION FAILED: release kubeslice-controller failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-controller failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1660635796000,"stop":1660635937895,"duration":141895}},{"uid":"fb49fa0f0ae24206","status":"passed","time":{"start":1660631001000,"stop":1660631102299,"duration":101299}},{"uid":"3a820a869c9e165","status":"passed","time":{"start":1660623317000,"stop":1660623410329,"duration":93329}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests slice with netpol should contain the allowed namespaces":{"statistic":{"failed":0,"broken":0,"skipped":29,"passed":10,"unknown":0,"total":39},"items":[{"uid":"d7977b35335146da","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"fd77bbe4f1f4eb1b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"c92421f5075bac7c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"d9e6c0f77e63520","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"1ec5cbb3af9551d4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"a8cac207d479ac21","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"2189a2fd8853074e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"f1df71237ae45d4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}},{"uid":"1f38300a9af4105e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659982549000,"stop":1659982549000,"duration":0}},{"uid":"4a3b39b6bf2fa331","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659970182000,"stop":1659970182000,"duration":0}},{"uid":"84fd0e33325d5198","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659944495000,"stop":1659944495000,"duration":0}},{"uid":"9f50d9da2d0bb7b8","status":"passed","time":{"start":1659874977000,"stop":1659874977167,"duration":167}},{"uid":"9b8147a59780fcb5","status":"passed","time":{"start":1659496050000,"stop":1659496050154,"duration":154}},{"uid":"ed7a8fe0e7f80fdc","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496083000,"stop":1659496083000,"duration":0}},{"uid":"106604f9c1af435","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"5e259450ea477653","status":"passed","time":{"start":1659450916000,"stop":1659450916259,"duration":259}},{"uid":"cd102e93ce051232","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"a403c2ddc6316ef2","status":"passed","time":{"start":1659447298000,"stop":1659447298198,"duration":198}},{"uid":"92f651ad40aa21fa","status":"passed","time":{"start":1659447116000,"stop":1659447116125,"duration":125}},{"uid":"a1463043b5599998","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448205000,"stop":1659448205000,"duration":0}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Deletion Should fail when deleting a project that does not exist":{"statistic":{"failed":5,"broken":0,"skipped":0,"passed":116,"unknown":0,"total":121},"items":[{"uid":"c0b39d267e3f3109","status":"passed","time":{"start":1660828517000,"stop":1660828524855,"duration":7855}},{"uid":"233e688b1f1eead8","status":"passed","time":{"start":1660807774000,"stop":1660807781617,"duration":7617}},{"uid":"1bf133e392b791ce","status":"passed","time":{"start":1660803998000,"stop":1660804006636,"duration":8636}},{"uid":"38da1b8b9e6ec530","status":"passed","time":{"start":1660805737000,"stop":1660805745394,"duration":8394}},{"uid":"ff40039d35e94182","status":"passed","time":{"start":1660803696000,"stop":1660803704273,"duration":8273}},{"uid":"b00a3a74b0b9697a","status":"passed","time":{"start":1660798468000,"stop":1660798476615,"duration":8615}},{"uid":"b41b5d2e8b4feb58","status":"passed","time":{"start":1660748009000,"stop":1660748017175,"duration":8175}},{"uid":"4f3d850ee462bc76","status":"passed","time":{"start":1660747865000,"stop":1660747872788,"duration":7788}},{"uid":"ab3caa3c0434c733","status":"passed","time":{"start":1660747735000,"stop":1660747742715,"duration":7715}},{"uid":"73029b263d777525","status":"passed","time":{"start":1660747654000,"stop":1660747662317,"duration":8317}},{"uid":"e57ad465ee750283","status":"passed","time":{"start":1660722683000,"stop":1660722690730,"duration":7730}},{"uid":"d29eb7025f081ee0","status":"passed","time":{"start":1660635916000,"stop":1660635923825,"duration":7825}},{"uid":"aebfbb21d87d5df3","status":"passed","time":{"start":1660635666000,"stop":1660635674830,"duration":8830}},{"uid":"b024cedaefb82ce5","status":"passed","time":{"start":1660635860000,"stop":1660635867836,"duration":7836}},{"uid":"a741936452f3eb46","status":"passed","time":{"start":1660631164000,"stop":1660631171571,"duration":7571}},{"uid":"f0490ea2481396ad","status":"passed","time":{"start":1660623463000,"stop":1660623470774,"duration":7774}},{"uid":"b280e91d32dd9557","status":"passed","time":{"start":1660623384000,"stop":1660623391568,"duration":7568}},{"uid":"62734d35c4b57eae","status":"passed","time":{"start":1660623360000,"stop":1660623367599,"duration":7599}},{"uid":"293346984bd643b2","status":"passed","time":{"start":1660452377000,"stop":1660452384736,"duration":7736}},{"uid":"dc4b19a5e7d840a9","status":"passed","time":{"start":1660451870000,"stop":1660451877611,"duration":7611}}]},"Worker Suite:Worker Suite#[It] Hub netpol scenarios: Slice Configs Netpol scenarios Should successfully deploy slice for valid namespace and valid clusters in allowedNamespaces of sliceconfigs manifest":{"statistic":{"failed":9,"broken":0,"skipped":4,"passed":26,"unknown":0,"total":39},"items":[{"uid":"78d9515ea7494cc9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"bda88e86652e4d4c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"400ce18d643c9e5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"ffce12e2a6be2727","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"531f2cd9e5077f58","status":"failed","statusDetails":"Expected\n    <[]string | len:0, cap:0>: nil\nto equal\n    <[]string | len:1, cap:1>: [\"kube-system\"]","time":{"start":1660113628000,"stop":1660113628535,"duration":535}},{"uid":"56aa6694efc8c246","status":"failed","statusDetails":"Expected\n    <[]string | len:0, cap:0>: nil\nto equal\n    <[]string | len:1, cap:1>: [\"kube-system\"]","time":{"start":1660104898000,"stop":1660104898627,"duration":627}},{"uid":"727e6e902469b4f9","status":"passed","time":{"start":1660067259000,"stop":1660067259539,"duration":539}},{"uid":"3ea312df00826f38","status":"passed","time":{"start":1660047551000,"stop":1660047551447,"duration":447}},{"uid":"7d6ec9711cff326e","status":"passed","time":{"start":1659982549000,"stop":1659982550136,"duration":1136}},{"uid":"51ee76f4032baa9a","status":"passed","time":{"start":1659970182000,"stop":1659970182548,"duration":548}},{"uid":"a195397df16ca08c","status":"passed","time":{"start":1659944495000,"stop":1659944495605,"duration":605}},{"uid":"e4c11bc42fce7c05","status":"passed","time":{"start":1659874977000,"stop":1659874977537,"duration":537}},{"uid":"b86acdaec62bc845","status":"passed","time":{"start":1659496050000,"stop":1659496050326,"duration":326}},{"uid":"e43f950617c7e444","status":"passed","time":{"start":1659496083000,"stop":1659496083591,"duration":591}},{"uid":"5e32d67a88629605","status":"failed","statusDetails":"Expected\n    <[]string | len:0, cap:0>: nil\nto equal\n    <[]string | len:1, cap:1>: [\"kube-system\"]","time":{"start":1659451501000,"stop":1659451501608,"duration":608}},{"uid":"f4a219e9153ab9ef","status":"passed","time":{"start":1659450916000,"stop":1659450916423,"duration":423}},{"uid":"b1cc0a57c1d0f7b0","status":"passed","time":{"start":1659451524000,"stop":1659451524680,"duration":680}},{"uid":"812ea94f932d5965","status":"passed","time":{"start":1659447298000,"stop":1659447298361,"duration":361}},{"uid":"42e3d3722f30ba19","status":"passed","time":{"start":1659447116000,"stop":1659447116389,"duration":389}},{"uid":"222cea2b21cac4b7","status":"passed","time":{"start":1659448205000,"stop":1659448205568,"duration":568}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Creation Should create Multiple Projects in controller using valid manifest":{"statistic":{"failed":5,"broken":0,"skipped":0,"passed":119,"unknown":0,"total":124},"items":[{"uid":"e18f2289fb9d913d","status":"passed","time":{"start":1660828517000,"stop":1660828524836,"duration":7836}},{"uid":"8b6c7d8b8078358a","status":"passed","time":{"start":1660807774000,"stop":1660807781618,"duration":7618}},{"uid":"e4b5ebe7fc55d8f7","status":"passed","time":{"start":1660803998000,"stop":1660804005748,"duration":7748}},{"uid":"cd282f53d3339c50","status":"passed","time":{"start":1660805737000,"stop":1660805744669,"duration":7669}},{"uid":"8a17ff95f405d0bc","status":"passed","time":{"start":1660803696000,"stop":1660803703585,"duration":7585}},{"uid":"e58194e6ed4fdfde","status":"passed","time":{"start":1660798468000,"stop":1660798475686,"duration":7686}},{"uid":"2b584b0bb6522446","status":"passed","time":{"start":1660748009000,"stop":1660748017273,"duration":8273}},{"uid":"72c7839acfc22dae","status":"passed","time":{"start":1660747865000,"stop":1660747872818,"duration":7818}},{"uid":"fa4b85a3bb8b8c0e","status":"passed","time":{"start":1660747735000,"stop":1660747742726,"duration":7726}},{"uid":"f0f13637f7b97f04","status":"passed","time":{"start":1660747654000,"stop":1660747661591,"duration":7591}},{"uid":"4ce8bcf5e03ed590","status":"passed","time":{"start":1660722683000,"stop":1660722690686,"duration":7686}},{"uid":"d01f0c4765d274a8","status":"passed","time":{"start":1660635916000,"stop":1660635923773,"duration":7773}},{"uid":"bf421a47a2d7eda3","status":"passed","time":{"start":1660635666000,"stop":1660635673729,"duration":7729}},{"uid":"e7835fbb04706645","status":"passed","time":{"start":1660635860000,"stop":1660635867857,"duration":7857}},{"uid":"9bf22865a06fabd3","status":"passed","time":{"start":1660631164000,"stop":1660631171560,"duration":7560}},{"uid":"55e6ea3a5b2e78df","status":"passed","time":{"start":1660623463000,"stop":1660623470784,"duration":7784}},{"uid":"60ae457813454551","status":"passed","time":{"start":1660623384000,"stop":1660623391583,"duration":7583}},{"uid":"32da06d42ae379a5","status":"passed","time":{"start":1660623360000,"stop":1660623367611,"duration":7611}},{"uid":"bffcea3e8daeb879","status":"passed","time":{"start":1660452377000,"stop":1660452384694,"duration":7694}},{"uid":"3778e536aaaec27b","status":"passed","time":{"start":1660451870000,"stop":1660451877636,"duration":7636}}]},"Worker Suite:Worker Suite#[It] Slice Config Negative Scenario tests Slice Config Negative Scenario Creation Should fail while creating sliceconfig with invalid values on slice subnet":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":39,"unknown":0,"total":39},"items":[{"uid":"4dc867951f6583de","status":"passed","time":{"start":1660808797000,"stop":1660808797206,"duration":206}},{"uid":"7911afcd026fa615","status":"passed","time":{"start":1660299182000,"stop":1660299182192,"duration":192}},{"uid":"f52baeb43e0e1058","status":"passed","time":{"start":1660295740000,"stop":1660295740187,"duration":187}},{"uid":"1e68498eb43c5a17","status":"passed","time":{"start":1660293657000,"stop":1660293657273,"duration":273}},{"uid":"c046516ad4b3191a","status":"passed","time":{"start":1660113628000,"stop":1660113628403,"duration":403}},{"uid":"2a6b0223922ffa5d","status":"passed","time":{"start":1660104898000,"stop":1660104898388,"duration":388}},{"uid":"420942bc34bbbd50","status":"passed","time":{"start":1660067259000,"stop":1660067259222,"duration":222}},{"uid":"23627fff15ec10a0","status":"passed","time":{"start":1660047551000,"stop":1660047551314,"duration":314}},{"uid":"e1d0d8fc7c77748e","status":"passed","time":{"start":1659982549000,"stop":1659982549498,"duration":498}},{"uid":"160506ba5dcbd77a","status":"passed","time":{"start":1659970182000,"stop":1659970182200,"duration":200}},{"uid":"17904ef9501a8845","status":"passed","time":{"start":1659944495000,"stop":1659944495310,"duration":310}},{"uid":"6835329ee47e1603","status":"passed","time":{"start":1659874977000,"stop":1659874977254,"duration":254}},{"uid":"61788abb29ec968e","status":"passed","time":{"start":1659496050000,"stop":1659496050250,"duration":250}},{"uid":"b442481458c9b723","status":"passed","time":{"start":1659496083000,"stop":1659496083279,"duration":279}},{"uid":"36c3b66dc708c916","status":"passed","time":{"start":1659451501000,"stop":1659451501498,"duration":498}},{"uid":"b59d16f356eeaa6f","status":"passed","time":{"start":1659450916000,"stop":1659450916392,"duration":392}},{"uid":"d03b96d20b511421","status":"passed","time":{"start":1659451524000,"stop":1659451524236,"duration":236}},{"uid":"e3c78fa2357ad6d2","status":"passed","time":{"start":1659447298000,"stop":1659447298525,"duration":525}},{"uid":"86cae6f53c76bf88","status":"passed","time":{"start":1659447116000,"stop":1659447116246,"duration":246}},{"uid":"a92276c8fbe148b4","status":"passed","time":{"start":1659448205000,"stop":1659448205288,"duration":288}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Update Should fail when Project is Applied with service account name as combination of special characters in Write users":{"statistic":{"failed":7,"broken":0,"skipped":0,"passed":114,"unknown":0,"total":121},"items":[{"uid":"2832dcfc8b93287f","status":"passed","time":{"start":1660828517000,"stop":1660828521905,"duration":4905}},{"uid":"6b0c6d4766f62488","status":"passed","time":{"start":1660807774000,"stop":1660807778857,"duration":4857}},{"uid":"238a426dfd49b9f3","status":"passed","time":{"start":1660803998000,"stop":1660804002886,"duration":4886}},{"uid":"94681535e53c0a2f","status":"passed","time":{"start":1660805737000,"stop":1660805741725,"duration":4725}},{"uid":"93d6f44b0a9a58a3","status":"passed","time":{"start":1660803696000,"stop":1660803701695,"duration":5695}},{"uid":"415392323277cbb3","status":"passed","time":{"start":1660798468000,"stop":1660798472869,"duration":4869}},{"uid":"8a0ca268cfcba95e","status":"passed","time":{"start":1660748009000,"stop":1660748014092,"duration":5092}},{"uid":"a7269d3aaeced72e","status":"passed","time":{"start":1660747865000,"stop":1660747869984,"duration":4984}},{"uid":"861b62892bc0dbdb","status":"passed","time":{"start":1660747735000,"stop":1660747740893,"duration":5893}},{"uid":"321691a6be697e8","status":"passed","time":{"start":1660747654000,"stop":1660747659681,"duration":5681}},{"uid":"df4582cc963dc5c7","status":"passed","time":{"start":1660722683000,"stop":1660722687903,"duration":4903}},{"uid":"31868e862cca0418","status":"passed","time":{"start":1660635916000,"stop":1660635920864,"duration":4864}},{"uid":"25188f823187cb0e","status":"passed","time":{"start":1660635666000,"stop":1660635670984,"duration":4984}},{"uid":"bcea4717153270b7","status":"passed","time":{"start":1660635860000,"stop":1660635865065,"duration":5065}},{"uid":"5d084e56df35f42","status":"passed","time":{"start":1660631164000,"stop":1660631169656,"duration":5656}},{"uid":"cd1f977919ad1af","status":"passed","time":{"start":1660623463000,"stop":1660623468863,"duration":5863}},{"uid":"c6837685981f4d8a","status":"passed","time":{"start":1660623384000,"stop":1660623388663,"duration":4663}},{"uid":"86e71d1eb2bfdef","status":"passed","time":{"start":1660623360000,"stop":1660623365664,"duration":5664}},{"uid":"4358d0a7c5aa443d","status":"passed","time":{"start":1660452377000,"stop":1660452382833,"duration":5833}},{"uid":"8ee884d24a6d0166","status":"passed","time":{"start":1660451870000,"stop":1660451874781,"duration":4781}}]},"Worker Suite:Worker Suite#[It] Slice validation Slice with MEDIUM QOS should have gateway pod running":{"statistic":{"failed":1,"broken":0,"skipped":3,"passed":5,"unknown":0,"total":9},"items":[{"uid":"b3b6e4a8fa0c91c1","status":"passed","time":{"start":1659168739000,"stop":1659168739027,"duration":27}},{"uid":"8230e5aa7484ed6b","status":"passed","time":{"start":1659164084000,"stop":1659164084038,"duration":38}},{"uid":"a6c5b4296cbc546a","status":"passed","time":{"start":1659160188000,"stop":1659160188098,"duration":98}},{"uid":"b68db7ec5e46e791","status":"passed","time":{"start":1659119724000,"stop":1659119724043,"duration":43}},{"uid":"c82a0259af00b654","status":"passed","time":{"start":1659116511000,"stop":1659116511028,"duration":28}},{"uid":"a7cc472df3f4a042","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659109470000,"stop":1659109470000,"duration":0}},{"uid":"1b5e1219b3c47024","status":"failed","statusDetails":"Timed out after 180.004s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1659106836000,"stop":1659107016302,"duration":180302}},{"uid":"da28f429be91085e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659101522000,"stop":1659101522000,"duration":0}},{"uid":"5fb9844f703d68ba","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659082816000,"stop":1659082816000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Attach/Deattach Scenario Attach cluster from slice  when Deattach cluster from slice  should have vl3 router running on attached cluster":{"statistic":{"failed":1,"broken":0,"skipped":26,"passed":3,"unknown":0,"total":30},"items":[{"uid":"2ec179450bf101f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"d15540ac1143177f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"5a0c683f56f7bd2b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"7d62d68a8eda47b0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"3289fe47f5013d48","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"735a5e166091b1a4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"d0f24a524844458e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"694071bc5a3889df","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}},{"uid":"3654934211e0aee7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659982549000,"stop":1659982549000,"duration":0}},{"uid":"3c530b5d5c7d9c27","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659970182000,"stop":1659970182000,"duration":0}},{"uid":"5e7377a4071080bb","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659944495000,"stop":1659944495000,"duration":0}},{"uid":"32a6a4f83c6183ff","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659874977000,"stop":1659874977000,"duration":0}},{"uid":"c39f03e99cd43672","status":"passed","time":{"start":1659496050000,"stop":1659496050004,"duration":4}},{"uid":"c2c3fe78f84cd62d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496083000,"stop":1659496083000,"duration":0}},{"uid":"17262dac39e31aaa","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"e6940ede380b704","status":"passed","time":{"start":1659450916000,"stop":1659450916010,"duration":10}},{"uid":"6af39d089f161a6c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"51a330b9d3865d0b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447298000,"stop":1659447298000,"duration":0}},{"uid":"76a70ede7b0c3c08","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447116000,"stop":1659447116000,"duration":0}},{"uid":"8ad83e4468e229ce","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448205000,"stop":1659448205000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting nsm-kernel-forwarder pod Iperf cleanup":{"statistic":{"failed":0,"broken":0,"skipped":39,"passed":0,"unknown":0,"total":39},"items":[{"uid":"b5f1e5596a991cf5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"44569541dcf147a3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"11fda8d81cec246a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"f731ca1e4707a20d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"f44600f22877f25","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"52f42cba52cf9c10","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"c69073c6064b5f56","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"5c14f502ae6b7d0f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}},{"uid":"76aba77214a9e69f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659982549000,"stop":1659982549000,"duration":0}},{"uid":"8dc20c5bb402487f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659970182000,"stop":1659970182000,"duration":0}},{"uid":"3019686f91deff55","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659944495000,"stop":1659944495000,"duration":0}},{"uid":"57a51c0c230f0d42","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659874977000,"stop":1659874977000,"duration":0}},{"uid":"56635be81460e006","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496050000,"stop":1659496050000,"duration":0}},{"uid":"ea9546db01da9460","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496083000,"stop":1659496083000,"duration":0}},{"uid":"a1dabe74d7f35002","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"da517c8d81bc9190","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659450916000,"stop":1659450916000,"duration":0}},{"uid":"88ea2970231c1c92","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"33f4a46a9a804c52","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447298000,"stop":1659447298000,"duration":0}},{"uid":"5a92b248513dce25","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447116000,"stop":1659447116000,"duration":0}},{"uid":"e73501cc652415d9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448205000,"stop":1659448205000,"duration":0}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Deletion Should Delete an existing project successfully":{"statistic":{"failed":5,"broken":0,"skipped":0,"passed":116,"unknown":0,"total":121},"items":[{"uid":"efd0f2a4658e3995","status":"passed","time":{"start":1660828517000,"stop":1660828521812,"duration":4812}},{"uid":"6c40616c3a23e3a6","status":"passed","time":{"start":1660807774000,"stop":1660807778618,"duration":4618}},{"uid":"c1859bafc2999657","status":"passed","time":{"start":1660803998000,"stop":1660804002729,"duration":4729}},{"uid":"a014c537d101e268","status":"passed","time":{"start":1660805737000,"stop":1660805741627,"duration":4627}},{"uid":"6b6ae0143e5ed7d4","status":"passed","time":{"start":1660803696000,"stop":1660803700612,"duration":4612}},{"uid":"e6dd9c9dcdfda127","status":"passed","time":{"start":1660798468000,"stop":1660798472794,"duration":4794}},{"uid":"25a4dc12416d27be","status":"passed","time":{"start":1660748009000,"stop":1660748014463,"duration":5463}},{"uid":"80672f122ac5cc6c","status":"passed","time":{"start":1660747865000,"stop":1660747869728,"duration":4728}},{"uid":"10d2f285f35b752b","status":"passed","time":{"start":1660747735000,"stop":1660747739700,"duration":4700}},{"uid":"ba5e6529cc0d1db1","status":"passed","time":{"start":1660747654000,"stop":1660747658581,"duration":4581}},{"uid":"26ab9e3e4bdd96e0","status":"passed","time":{"start":1660722683000,"stop":1660722687707,"duration":4707}},{"uid":"a7a530d94d608f63","status":"passed","time":{"start":1660635916000,"stop":1660635920759,"duration":4759}},{"uid":"c4215d4beab580a7","status":"passed","time":{"start":1660635666000,"stop":1660635670721,"duration":4721}},{"uid":"726ace15e88c255f","status":"passed","time":{"start":1660635860000,"stop":1660635864885,"duration":4885}},{"uid":"25e6dd769fe0e182","status":"passed","time":{"start":1660631164000,"stop":1660631168569,"duration":4569}},{"uid":"852585c227865d8d","status":"passed","time":{"start":1660623463000,"stop":1660623467746,"duration":4746}},{"uid":"d2079d61d11c63e5","status":"passed","time":{"start":1660623384000,"stop":1660623388616,"duration":4616}},{"uid":"676ff5c20df55d6d","status":"passed","time":{"start":1660623360000,"stop":1660623364590,"duration":4590}},{"uid":"a552129804662ba4","status":"passed","time":{"start":1660452377000,"stop":1660452382689,"duration":5689}},{"uid":"2aec800fe0c757cf","status":"passed","time":{"start":1660451870000,"stop":1660451874591,"duration":4591}}]},"Istio Suite:Istio Suite#[It] Bookinfo tests Bookinfo Scenario 4 with egress and ingress Should install Productpage on worker cluster-1":{"statistic":{"failed":17,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":18},"items":[{"uid":"4523ab7e888723b3","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0001b87b0>: {\n        Underlying: <*exec.ExitError | 0xc0007f2580>{\n            ProcessState: {\n                pid: 6863,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 176695},\n                    Stime: {Sec: 0, Usec: 60151},\n                    Maxrss: 80800,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 9849,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 383,\n                    Nivcsw: 516,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicewithegressingress\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicewithegressingress\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicewithegressingress\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicewithegressingress\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicewithegressingress\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660808584000,"stop":1660808584269,"duration":269}},{"uid":"697197ee8998d22c","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 3\nto equal\n    <int>: 1","time":{"start":1660804840000,"stop":1660805020473,"duration":180473}},{"uid":"f71c5548911456a5","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1660804470000,"stop":1660804650637,"duration":180637}},{"uid":"fb5d2975c725097f","status":"failed","statusDetails":"Timed out after 180.002s.\nExpected\n    <int>: 3\nto equal\n    <int>: 1","time":{"start":1660748849000,"stop":1660749029516,"duration":180516}},{"uid":"f9a001de396df6e8","status":"passed","time":{"start":1660748706000,"stop":1660748738556,"duration":32556}},{"uid":"3045a0c24b9a6e0c","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1660748558000,"stop":1660748738409,"duration":180409}},{"uid":"8921d0d1b4334bb1","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1660636759000,"stop":1660636939576,"duration":180576}},{"uid":"72d185804e5c2463","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1660636482000,"stop":1660636695473,"duration":213473}},{"uid":"c7d43915d530c509","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1660632006000,"stop":1660632211771,"duration":205771}},{"uid":"67f1873265200c7f","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1660624273000,"stop":1660624454152,"duration":181152}},{"uid":"f9ee85318b11112b","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1660624220000,"stop":1660624400279,"duration":180279}},{"uid":"8f7a980688accf30","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1660453205000,"stop":1660453395343,"duration":190343}},{"uid":"1408d8bed9830855","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1660452554000,"stop":1660452734361,"duration":180361}},{"uid":"e46a7d6931e31e9b","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1660452412000,"stop":1660452592406,"duration":180406}},{"uid":"cafb49a4ab333aac","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1660415422000,"stop":1660415602270,"duration":180270}},{"uid":"fba46226c5c394e7","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00098ca38>: {\n        Underlying: <*exec.ExitError | 0xc0009c8d60>{\n            ProcessState: {\n                pid: 6912,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 141298},\n                    Stime: {Sec: 0, Usec: 56519},\n                    Maxrss: 84420,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 12465,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 501,\n                    Nivcsw: 385,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicewithegressingress\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicewithegressingress\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicewithegressingress\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicewithegressingress\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicewithegressingress\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660298968000,"stop":1660298968190,"duration":190}},{"uid":"36328c631d83e5f8","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000478d68>: {\n        Underlying: <*exec.ExitError | 0xc0004fb780>{\n            ProcessState: {\n                pid: 6958,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 147870},\n                    Stime: {Sec: 0, Usec: 49290},\n                    Maxrss: 77616,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 12627,\n                    Majflt: 1,\n                    Nswap: 0,\n                    Inblock: 264,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 384,\n                    Nivcsw: 337,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicewithegressingress\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicewithegressingress\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicewithegressingress\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicewithegressingress\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicewithegressingress\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660295522000,"stop":1660295522183,"duration":183}},{"uid":"80960138a349ed19","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00079c360>: {\n        Underlying: <*exec.ExitError | 0xc000472a20>{\n            ProcessState: {\n                pid: 6869,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 194567},\n                    Stime: {Sec: 0, Usec: 47649},\n                    Maxrss: 82844,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 7124,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 333,\n                    Nivcsw: 368,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicewithegressingress\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicewithegressingress\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicewithegressingress\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicewithegressingress\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicewithegressingress\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660293414000,"stop":1660293414240,"duration":240}}]},"Empty Suite:Empty Suite#[AfterSuite]":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":46,"unknown":0,"total":46},"items":[{"uid":"536fc72af94f11f0","status":"passed","time":{"start":1660828377000,"stop":1660828377000,"duration":0}},{"uid":"4230ed0c6630d4b7","status":"passed","time":{"start":1660807615000,"stop":1660807615000,"duration":0}},{"uid":"2eb5bf8d17627bd4","status":"passed","time":{"start":1660803821000,"stop":1660803821000,"duration":0}},{"uid":"9413e2aa70e03fe5","status":"passed","time":{"start":1660805590000,"stop":1660805590000,"duration":0}},{"uid":"892016adfb4fea46","status":"passed","time":{"start":1660803559000,"stop":1660803559000,"duration":0}},{"uid":"8325d043f591cba2","status":"passed","time":{"start":1660802965000,"stop":1660802965000,"duration":0}},{"uid":"cc46431f7094c4c6","status":"passed","time":{"start":1660798299000,"stop":1660798299000,"duration":0}},{"uid":"5dafb59cd4e6f001","status":"passed","time":{"start":1660747846000,"stop":1660747846000,"duration":0}},{"uid":"62ee180b5f34d7d6","status":"passed","time":{"start":1660747702000,"stop":1660747702000,"duration":0}},{"uid":"c4127c9a59a9492f","status":"passed","time":{"start":1660747595000,"stop":1660747595000,"duration":0}},{"uid":"9eae34cc098f0671","status":"passed","time":{"start":1660747499000,"stop":1660747499000,"duration":0}},{"uid":"8cfc11734efe195e","status":"passed","time":{"start":1660747927000,"stop":1660747927001,"duration":1}},{"uid":"44ebfb80cbe0e4d0","status":"passed","time":{"start":1660722528000,"stop":1660722528000,"duration":0}},{"uid":"1ab115da060b1687","status":"passed","time":{"start":1660711963000,"stop":1660711963000,"duration":0}},{"uid":"267fc021ff1c29fa","status":"passed","time":{"start":1660635769000,"stop":1660635769000,"duration":0}},{"uid":"34623f67479cabe1","status":"passed","time":{"start":1660635489000,"stop":1660635489000,"duration":0}},{"uid":"8b6280a658c931ed","status":"passed","time":{"start":1660635683000,"stop":1660635683000,"duration":0}},{"uid":"4cfabf65d33a68b3","status":"passed","time":{"start":1660635796000,"stop":1660635796000,"duration":0}},{"uid":"474dca6ae6adacd","status":"passed","time":{"start":1660631001000,"stop":1660631001000,"duration":0}},{"uid":"3b0a99152012cfe8","status":"passed","time":{"start":1660623317000,"stop":1660623317000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice Config Negative Scenario tests Slice Config Negative Scenario Update Should update while deploying sliceconfig with existing slice name":{"statistic":{"failed":4,"broken":0,"skipped":0,"passed":35,"unknown":0,"total":39},"items":[{"uid":"98e8309932e9a69d","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000989a70>: {\n        Underlying: <*exec.ExitError | 0xc000711960>{\n            ProcessState: {\n                pid: 7197,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 157560},\n                    Stime: {Sec: 0, Usec: 24240},\n                    Maxrss: 85672,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3933,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 490,\n                    Nivcsw: 221,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"fireredslice\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660808797000,"stop":1660808797263,"duration":263}},{"uid":"cc42e741da377427","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00066c150>: {\n        Underlying: <*exec.ExitError | 0xc0007e03a0>{\n            ProcessState: {\n                pid: 7100,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 148423},\n                    Stime: {Sec: 0, Usec: 42406},\n                    Maxrss: 79908,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 5968,\n                    Majflt: 1,\n                    Nswap: 0,\n                    Inblock: 256,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 463,\n                    Nivcsw: 352,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"fireredslice\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660299182000,"stop":1660299182252,"duration":252}},{"uid":"1f9f92f7f1e1c193","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00013b410>: {\n        Underlying: <*exec.ExitError | 0xc0008192c0>{\n            ProcessState: {\n                pid: 7141,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 149363},\n                    Stime: {Sec: 0, Usec: 35144},\n                    Maxrss: 80492,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 5329,\n                    Majflt: 3,\n                    Nswap: 0,\n                    Inblock: 648,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 494,\n                    Nivcsw: 204,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"fireredslice\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660295740000,"stop":1660295740251,"duration":251}},{"uid":"c26e5880f162ac68","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000eb28>: {\n        Underlying: <*exec.ExitError | 0xc00085a580>{\n            ProcessState: {\n                pid: 7204,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 169453},\n                    Stime: {Sec: 0, Usec: 63052},\n                    Maxrss: 84912,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 5388,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 422,\n                    Nivcsw: 309,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"fireredslice\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660293657000,"stop":1660293657291,"duration":291}},{"uid":"e1088d93cfb7d6c6","status":"passed","time":{"start":1660113628000,"stop":1660113628487,"duration":487}},{"uid":"3bf78a3f3ee4de48","status":"passed","time":{"start":1660104898000,"stop":1660104898510,"duration":510}},{"uid":"27407775e2e36805","status":"passed","time":{"start":1660067259000,"stop":1660067259578,"duration":578}},{"uid":"7d2b2902410b0503","status":"passed","time":{"start":1660047551000,"stop":1660047551724,"duration":724}},{"uid":"a36e599b98daf010","status":"passed","time":{"start":1659982549000,"stop":1659982550101,"duration":1101}},{"uid":"26f6948d71d7d1b3","status":"passed","time":{"start":1659970182000,"stop":1659970182592,"duration":592}},{"uid":"3d161a5475ac8a39","status":"passed","time":{"start":1659944495000,"stop":1659944495643,"duration":643}},{"uid":"b8e93b0b10a14592","status":"passed","time":{"start":1659874977000,"stop":1659874977814,"duration":814}},{"uid":"1d5c91043c275f53","status":"passed","time":{"start":1659496050000,"stop":1659496050508,"duration":508}},{"uid":"8f99755c905c768c","status":"passed","time":{"start":1659496083000,"stop":1659496083782,"duration":782}},{"uid":"35e510bdb15c348e","status":"passed","time":{"start":1659451501000,"stop":1659451502030,"duration":1030}},{"uid":"7226c2869e8379d4","status":"passed","time":{"start":1659450916000,"stop":1659450916879,"duration":879}},{"uid":"93b0635f72459da8","status":"passed","time":{"start":1659451524000,"stop":1659451524635,"duration":635}},{"uid":"d508846ad4b42898","status":"passed","time":{"start":1659447298000,"stop":1659447298586,"duration":586}},{"uid":"f8254281bb3f0d06","status":"passed","time":{"start":1659447116000,"stop":1659447116857,"duration":857}},{"uid":"a88714c4351883c0","status":"passed","time":{"start":1659448205000,"stop":1659448205771,"duration":771}}]},"Worker Suite:Worker Suite#[AfterSuite]":{"statistic":{"failed":7,"broken":12,"skipped":0,"passed":133,"unknown":0,"total":152},"items":[{"uid":"2a46450d3d51329b","status":"passed","time":{"start":1660829689000,"stop":1660829993413,"duration":304413}},{"uid":"9f0b2fa3e2b260f0","status":"passed","time":{"start":1660808797000,"stop":1660808824766,"duration":27766}},{"uid":"9c43643321cf901f","status":"passed","time":{"start":1660806889000,"stop":1660807191907,"duration":302907}},{"uid":"32a584d68209ec0e","status":"passed","time":{"start":1660803717000,"stop":1660803719208,"duration":2208}},{"uid":"dc46e31d783df59d","status":"passed","time":{"start":1660799617000,"stop":1660799920054,"duration":303054}},{"uid":"fe73eac91b7edef7","status":"passed","time":{"start":1660748778000,"stop":1660749081033,"duration":303033}},{"uid":"ef7047fd39251c5b","status":"passed","time":{"start":1660748668000,"stop":1660748669815,"duration":1815}},{"uid":"2d2514b89e74bf92","status":"passed","time":{"start":1660723841000,"stop":1660724144192,"duration":303192}},{"uid":"6fb7a0212335c761","status":"passed","time":{"start":1660712720000,"stop":1660712721870,"duration":1870}},{"uid":"41ccd39c12ce4ec8","status":"passed","time":{"start":1660636971000,"stop":1660637275358,"duration":304358}},{"uid":"d5a4f4ee34f7c69b","status":"passed","time":{"start":1660636570000,"stop":1660636571874,"duration":1874}},{"uid":"c4bc9084247032ff","status":"passed","time":{"start":1660624452000,"stop":1660624755543,"duration":303543}},{"uid":"b755db242b9cbd47","status":"passed","time":{"start":1660624148000,"stop":1660624149986,"duration":1986}},{"uid":"92ca7ad522d8771b","status":"passed","time":{"start":1660457696000,"stop":1660457698432,"duration":2432}},{"uid":"d9b159a5b26c7a9d","status":"passed","time":{"start":1660453993000,"stop":1660454295907,"duration":302907}},{"uid":"f8787cb469a089d1","status":"passed","time":{"start":1660411114000,"stop":1660411417512,"duration":303512}},{"uid":"f4a3fb39c67f09f3","status":"passed","time":{"start":1660373247000,"stop":1660373550426,"duration":303426}},{"uid":"81f0ae54b88a6b72","status":"passed","time":{"start":1660371814000,"stop":1660372117393,"duration":303393}},{"uid":"9e5e4b81894d68c8","status":"passed","time":{"start":1660365282000,"stop":1660365587575,"duration":305575}},{"uid":"992e0ed922da8673","status":"passed","time":{"start":1660362429000,"stop":1660362734894,"duration":305894}}]},"Worker Suite:Worker Suite#[It] Attach/Deattach Scenario Attach cluster from slice  when Deattach cluster from slice  should have gateway pod deleted from deattached cluster":{"statistic":{"failed":0,"broken":0,"skipped":27,"passed":3,"unknown":0,"total":30},"items":[{"uid":"542b877b460cdf3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"878085e3f2803815","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"2c6d1f100e96027a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"e30fcef856ba383b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"a07b40b4b758dab8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"5f73c8f35795ada","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"6d34a78e20f38e3d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"7803f71bb08dc67b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}},{"uid":"1a11755b98b63b69","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659982549000,"stop":1659982549000,"duration":0}},{"uid":"77e97ca1b13682c0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659970182000,"stop":1659970182000,"duration":0}},{"uid":"6e175a3dd7eed76c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659944495000,"stop":1659944495000,"duration":0}},{"uid":"5a8cf36d17005672","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659874977000,"stop":1659874977000,"duration":0}},{"uid":"c1b6b67e5de2a6b6","status":"passed","time":{"start":1659496050000,"stop":1659496050003,"duration":3}},{"uid":"395cd9faac31fe60","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496083000,"stop":1659496083000,"duration":0}},{"uid":"30cae155000ba6cf","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"84c70712657d4677","status":"passed","time":{"start":1659450916000,"stop":1659450916006,"duration":6}},{"uid":"6155591c65f5f356","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"b5ae6ec168fe4afa","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447298000,"stop":1659447298000,"duration":0}},{"uid":"b8b38178490dbc91","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447116000,"stop":1659447116000,"duration":0}},{"uid":"f0dbe3c680dd1432","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448205000,"stop":1659448205000,"duration":0}}]},"Empty Suite:Empty Suite#[It] Hub Creation tests Hub Installtion Test Senarios Should fail if Hub is installed before installing Cert":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":46,"unknown":0,"total":46},"items":[{"uid":"4c7efd53c3ab49","status":"passed","time":{"start":1660828377000,"stop":1660828378654,"duration":1654}},{"uid":"e16ae29206a339cb","status":"passed","time":{"start":1660807615000,"stop":1660807616750,"duration":1750}},{"uid":"36995448b6f2be52","status":"passed","time":{"start":1660803821000,"stop":1660803823531,"duration":2531}},{"uid":"968e5a4250ff67ce","status":"passed","time":{"start":1660805590000,"stop":1660805591842,"duration":1842}},{"uid":"d7ec90b44ac68272","status":"passed","time":{"start":1660803559000,"stop":1660803560319,"duration":1319}},{"uid":"d9747e56eb87e80f","status":"passed","time":{"start":1660802965000,"stop":1660802966681,"duration":1681}},{"uid":"755d9a94ef9912a8","status":"passed","time":{"start":1660798299000,"stop":1660798300877,"duration":1877}},{"uid":"af230bf1daf312cb","status":"passed","time":{"start":1660747846000,"stop":1660747848358,"duration":2358}},{"uid":"e99e43cc5a43fac3","status":"passed","time":{"start":1660747702000,"stop":1660747704120,"duration":2120}},{"uid":"4f3ec8a3a74266f1","status":"passed","time":{"start":1660747595000,"stop":1660747596825,"duration":1825}},{"uid":"f57bd488f2aeadc8","status":"passed","time":{"start":1660747499000,"stop":1660747500626,"duration":1626}},{"uid":"c633a2ef14216fd0","status":"passed","time":{"start":1660747927000,"stop":1660747928813,"duration":1813}},{"uid":"22136bf41e2ac97f","status":"passed","time":{"start":1660722528000,"stop":1660722529746,"duration":1746}},{"uid":"77fa513a8d4dadd7","status":"passed","time":{"start":1660711963000,"stop":1660711964878,"duration":1878}},{"uid":"5b5a5156de02137c","status":"passed","time":{"start":1660635769000,"stop":1660635771417,"duration":2417}},{"uid":"1c836f406b29e463","status":"passed","time":{"start":1660635489000,"stop":1660635491146,"duration":2146}},{"uid":"3bb859345979481e","status":"passed","time":{"start":1660635683000,"stop":1660635685019,"duration":2019}},{"uid":"8520af5257385c41","status":"passed","time":{"start":1660635796000,"stop":1660635797464,"duration":1464}},{"uid":"b30fd60d5a774b61","status":"passed","time":{"start":1660631001000,"stop":1660631002434,"duration":1434}},{"uid":"beeb1bb449bd6804","status":"passed","time":{"start":1660623317000,"stop":1660623318767,"duration":1767}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests Iperf App connectivity Should install iperf-server on server cluster":{"statistic":{"failed":0,"broken":0,"skipped":32,"passed":7,"unknown":0,"total":39},"items":[{"uid":"6d5da9b9ff2f0faf","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"2a7fce59b0c9ebd5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"bbb3aa694ee8e679","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"e9df34f49568d398","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"50234fff618b0fdd","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"c8b848eb04b8c53f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"168abb95e985e3b5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"bb9ebf09c77d6533","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}},{"uid":"1d8a3c288b98caa4","status":"passed","time":{"start":1659982549000,"stop":1659982580633,"duration":31633}},{"uid":"3597f86b72eec6a4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659970182000,"stop":1659970182000,"duration":0}},{"uid":"461f0e7c750d5488","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659944495000,"stop":1659944495000,"duration":0}},{"uid":"ccce999e5ec2b275","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659874977000,"stop":1659874977000,"duration":0}},{"uid":"27b355b863ba245e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496050000,"stop":1659496050000,"duration":0}},{"uid":"3c70b4028996091c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496083000,"stop":1659496083000,"duration":0}},{"uid":"490905978d350b02","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"3ca7b409931c8f32","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659450916000,"stop":1659450916000,"duration":0}},{"uid":"7029556ee23fa00b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"ac9bca7f9f51e13b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447298000,"stop":1659447298000,"duration":0}},{"uid":"a6314b9c001dd6c9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447116000,"stop":1659447116000,"duration":0}},{"uid":"d6365664de1a2f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448205000,"stop":1659448205000,"duration":0}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests slice with netpol should remove network policy resource from application namespace once the isolationEnabled is set to false":{"statistic":{"failed":0,"broken":3,"skipped":29,"passed":7,"unknown":0,"total":39},"items":[{"uid":"fc298b6286f3394a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"49819d8bc146893","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"5f78d7a5d06f9620","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"888f79017b08484e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"c4ecdae0e74c8965","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"d666d99d343a5072","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"25f9d43772a122a1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"1d614e2ef6c93c5e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}},{"uid":"d372fb91f1a15f2b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659982549000,"stop":1659982549000,"duration":0}},{"uid":"149eef822efba33d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659970182000,"stop":1659970182000,"duration":0}},{"uid":"e20f4ec7e66dc3fb","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659944495000,"stop":1659944495000,"duration":0}},{"uid":"3e725336bbb60660","status":"broken","statusDetails":"runtime error: invalid memory address or nil pointer dereference","time":{"start":1659874977000,"stop":1659874996220,"duration":19220}},{"uid":"9ad3cbb10a056f9c","status":"passed","time":{"start":1659496050000,"stop":1659496065221,"duration":15221}},{"uid":"289f60f3cd624e96","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496083000,"stop":1659496083000,"duration":0}},{"uid":"2c1136a6022a8261","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"378ff2223d9af915","status":"passed","time":{"start":1659450916000,"stop":1659450932280,"duration":16280}},{"uid":"d7a0fdcc7dc09098","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"6d7f184fa5e0fe54","status":"broken","statusDetails":"runtime error: invalid memory address or nil pointer dereference","time":{"start":1659447298000,"stop":1659447328053,"duration":30053}},{"uid":"f2f8be5e7860eeec","status":"passed","time":{"start":1659447116000,"stop":1659447129404,"duration":13404}},{"uid":"aa3d7207a2c6634c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448205000,"stop":1659448205000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice creation scenarios: when slice red is installed workers should get attached to slice":{"statistic":{"failed":4,"broken":0,"skipped":0,"passed":35,"unknown":0,"total":39},"items":[{"uid":"720c89f065c115e0","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0007f0fc0>: {\n        Underlying: <*exec.ExitError | 0xc00081faa0>{\n            ProcessState: {\n                pid: 7019,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 203396},\n                    Stime: {Sec: 0, Usec: 122805},\n                    Maxrss: 74888,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 7668,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 597,\n                    Nivcsw: 744,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-red\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660808797000,"stop":1660808798270,"duration":1270}},{"uid":"c6bd852ade0c5aac","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0005ccf18>: {\n        Underlying: <*exec.ExitError | 0xc000715fc0>{\n            ProcessState: {\n                pid: 7062,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 133368},\n                    Stime: {Sec: 0, Usec: 68705},\n                    Maxrss: 83528,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 10888,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 604,\n                    Nivcsw: 531,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-red\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660299182000,"stop":1660299183078,"duration":1078}},{"uid":"2a5c21ab42b102e","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004ca930>: {\n        Underlying: <*exec.ExitError | 0xc000158d40>{\n            ProcessState: {\n                pid: 7235,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 137512},\n                    Stime: {Sec: 0, Usec: 42017},\n                    Maxrss: 83500,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3187,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 342,\n                    Nivcsw: 329,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-red\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660295740000,"stop":1660295740165,"duration":165}},{"uid":"5405f180368d0a6","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00084ca68>: {\n        Underlying: <*exec.ExitError | 0xc0008a1ee0>{\n            ProcessState: {\n                pid: 7244,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 213829},\n                    Stime: {Sec: 0, Usec: 38878},\n                    Maxrss: 82620,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 5847,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 445,\n                    Nivcsw: 461,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-red\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660293657000,"stop":1660293657261,"duration":261}},{"uid":"b78277e7475b8a33","status":"passed","time":{"start":1660113628000,"stop":1660113628340,"duration":340}},{"uid":"1da6b1671cd9086d","status":"passed","time":{"start":1660104898000,"stop":1660104898386,"duration":386}},{"uid":"49a8fbddbe632ff4","status":"passed","time":{"start":1660067259000,"stop":1660067259534,"duration":534}},{"uid":"5d15b2fb7d385713","status":"passed","time":{"start":1660047551000,"stop":1660047551685,"duration":685}},{"uid":"7b33ca5f146d6115","status":"passed","time":{"start":1659982549000,"stop":1659982552234,"duration":3234}},{"uid":"dda26ef6bd87456d","status":"passed","time":{"start":1659970182000,"stop":1659970184807,"duration":2807}},{"uid":"a72143e7dca53e27","status":"passed","time":{"start":1659944495000,"stop":1659944497946,"duration":2946}},{"uid":"5c45c80189be2453","status":"passed","time":{"start":1659874977000,"stop":1659874980196,"duration":3196}},{"uid":"98e8766b87f1f9ac","status":"passed","time":{"start":1659496050000,"stop":1659496053016,"duration":3016}},{"uid":"9529399196f130eb","status":"passed","time":{"start":1659496083000,"stop":1659496086630,"duration":3630}},{"uid":"1742215a1316d1ce","status":"passed","time":{"start":1659451501000,"stop":1659451503470,"duration":2470}},{"uid":"e9c29467b8fe7a53","status":"passed","time":{"start":1659450916000,"stop":1659450919065,"duration":3065}},{"uid":"7f14fcefaa265e0b","status":"passed","time":{"start":1659451524000,"stop":1659451527004,"duration":3004}},{"uid":"4b46bb22383a44f1","status":"passed","time":{"start":1659447298000,"stop":1659447301835,"duration":3835}},{"uid":"eb29a8b2afe14bec","status":"passed","time":{"start":1659447116000,"stop":1659447118699,"duration":2699}},{"uid":"dbeb581be7925275","status":"passed","time":{"start":1659448205000,"stop":1659448205705,"duration":705}}]},"Worker Suite:Worker Suite#[It] Slice validation Slice with LOW QOS should have gateway pod running":{"statistic":{"failed":1,"broken":0,"skipped":2,"passed":6,"unknown":0,"total":9},"items":[{"uid":"37109e6a1fcf8e95","status":"passed","time":{"start":1659168739000,"stop":1659168769082,"duration":30082}},{"uid":"174f63e4d95b0be8","status":"passed","time":{"start":1659164084000,"stop":1659164109099,"duration":25099}},{"uid":"a62f8935b4aa5bb8","status":"passed","time":{"start":1659160188000,"stop":1659160208045,"duration":20045}},{"uid":"b0ef192b3b3a9205","status":"passed","time":{"start":1659119724000,"stop":1659119749068,"duration":25068}},{"uid":"8e90a32bd8a8113b","status":"passed","time":{"start":1659116511000,"stop":1659116511012,"duration":12}},{"uid":"f41770fc4484150","status":"passed","time":{"start":1659109470000,"stop":1659109470028,"duration":28}},{"uid":"8ad77caf63959cf7","status":"failed","statusDetails":"Timed out after 180.004s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1659106836000,"stop":1659107016285,"duration":180285}},{"uid":"17f1dcc7e00985cc","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659101522000,"stop":1659101522000,"duration":0}},{"uid":"6423839b465e949a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659082816000,"stop":1659082816000,"duration":0}}]},"Istio Suite:Istio Suite#[It] Bookinfo tests Bookinfo Scenario 2 with egress and without ingress Should install Productpage on worker cluster-1":{"statistic":{"failed":14,"broken":0,"skipped":0,"passed":4,"unknown":0,"total":18},"items":[{"uid":"702649d3ca10c9a9","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0006014d0>: {\n        Underlying: <*exec.ExitError | 0xc0006daac0>{\n            ProcessState: {\n                pid: 6873,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 210973},\n                    Stime: {Sec: 0, Usec: 47287},\n                    Maxrss: 83712,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 10082,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 311,\n                    Nivcsw: 391,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicewithegresswithoutingress\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicewithegresswithoutingress\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicewithegresswithoutingress\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicewithegresswithoutingress\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicewithegresswithoutingress\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660808584000,"stop":1660808584238,"duration":238}},{"uid":"fa872be25165c0c4","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1660804840000,"stop":1660805020431,"duration":180431}},{"uid":"9e088ae666306e7b","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1660804470000,"stop":1660804650206,"duration":180206}},{"uid":"50022c25126afcdc","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 4\nto equal\n    <int>: 1","time":{"start":1660748849000,"stop":1660749029584,"duration":180584}},{"uid":"b865e053eea960c3","status":"passed","time":{"start":1660748706000,"stop":1660748867102,"duration":161102}},{"uid":"d8cba86856e915d9","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1660748558000,"stop":1660748738883,"duration":180883}},{"uid":"470ab1d06861dba1","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1660636759000,"stop":1660636939746,"duration":180746}},{"uid":"9d5cc6ba980682a4","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1660636482000,"stop":1660636662325,"duration":180325}},{"uid":"f698afe2533d2ed0","status":"passed","time":{"start":1660632006000,"stop":1660632107479,"duration":101479}},{"uid":"5ca8972a75a7b652","status":"passed","time":{"start":1660624273000,"stop":1660624300150,"duration":27150}},{"uid":"4c6cf5714934bd5b","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 4\nto equal\n    <int>: 1","time":{"start":1660624220000,"stop":1660624400257,"duration":180257}},{"uid":"7f43a4eb72fbf40f","status":"passed","time":{"start":1660453205000,"stop":1660453235892,"duration":30892}},{"uid":"249c487d89495be","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 3\nto equal\n    <int>: 1","time":{"start":1660452554000,"stop":1660452734313,"duration":180313}},{"uid":"9c1fa26834225945","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1660452412000,"stop":1660452592292,"duration":180292}},{"uid":"a40b26fccf4b220d","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1660415422000,"stop":1660415602313,"duration":180313}},{"uid":"d320efd92d6660c6","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000e7e0>: {\n        Underlying: <*exec.ExitError | 0xc000608920>{\n            ProcessState: {\n                pid: 6907,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 139730},\n                    Stime: {Sec: 0, Usec: 49316},\n                    Maxrss: 82376,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 12023,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 287,\n                    Nivcsw: 277,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicewithegresswithoutingress\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicewithegresswithoutingress\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicewithegresswithoutingress\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicewithegresswithoutingress\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicewithegresswithoutingress\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660298968000,"stop":1660298968184,"duration":184}},{"uid":"7d561f586a05400","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00080e678>: {\n        Underlying: <*exec.ExitError | 0xc0008703e0>{\n            ProcessState: {\n                pid: 6942,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 179506},\n                    Stime: {Sec: 0, Usec: 47013},\n                    Maxrss: 77228,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 12633,\n                    Majflt: 2,\n                    Nswap: 0,\n                    Inblock: 72,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 389,\n                    Nivcsw: 633,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicewithegresswithoutingress\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicewithegresswithoutingress\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicewithegresswithoutingress\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicewithegresswithoutingress\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicewithegresswithoutingress\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660295522000,"stop":1660295522245,"duration":245}},{"uid":"d0c2d828024d375d","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00079c018>: {\n        Underlying: <*exec.ExitError | 0xc000472040>{\n            ProcessState: {\n                pid: 6859,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 182054},\n                    Stime: {Sec: 0, Usec: 41720},\n                    Maxrss: 76948,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 6497,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 530,\n                    Nivcsw: 391,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicewithegresswithoutingress\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicewithegresswithoutingress\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicewithegresswithoutingress\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicewithegresswithoutingress\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicewithegresswithoutingress\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660293414000,"stop":1660293414245,"duration":245}}]},"Worker Suite:Worker Suite#[It] Namespace reconcile test scenarios: Offboard slice get detached from app ns in cluster objects":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":37,"unknown":0,"total":39},"items":[{"uid":"45c329870f108767","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"71b954fdb737555e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"8df457d3ee0f61ca","status":"passed","time":{"start":1660295740000,"stop":1660295740138,"duration":138}},{"uid":"784494d70f4ed812","status":"passed","time":{"start":1660293657000,"stop":1660293657191,"duration":191}},{"uid":"24e36ef2c7ba214e","status":"passed","time":{"start":1660113628000,"stop":1660113628144,"duration":144}},{"uid":"9d12c51634e50f5e","status":"passed","time":{"start":1660104898000,"stop":1660104898170,"duration":170}},{"uid":"928412d23ea7283e","status":"passed","time":{"start":1660067259000,"stop":1660067259292,"duration":292}},{"uid":"698a909a502fe02b","status":"passed","time":{"start":1660047551000,"stop":1660047551231,"duration":231}},{"uid":"6a1097204066860e","status":"passed","time":{"start":1659982549000,"stop":1659982549526,"duration":526}},{"uid":"d926ea3cc7538073","status":"passed","time":{"start":1659970182000,"stop":1659970182175,"duration":175}},{"uid":"323860827362ece7","status":"passed","time":{"start":1659944495000,"stop":1659944495326,"duration":326}},{"uid":"3ee2323f677f1a91","status":"passed","time":{"start":1659874977000,"stop":1659874977320,"duration":320}},{"uid":"1e3120e38a62ebb5","status":"passed","time":{"start":1659496050000,"stop":1659496050135,"duration":135}},{"uid":"c30cbf485972bdc1","status":"passed","time":{"start":1659496083000,"stop":1659496083179,"duration":179}},{"uid":"7ea06c6e7d2a0c5f","status":"passed","time":{"start":1659451501000,"stop":1659451501161,"duration":161}},{"uid":"c7a56f4a04b914e4","status":"passed","time":{"start":1659450916000,"stop":1659450916385,"duration":385}},{"uid":"e79ef4f4c6e49eef","status":"passed","time":{"start":1659451524000,"stop":1659451524436,"duration":436}},{"uid":"a5830c870df8e938","status":"passed","time":{"start":1659447298000,"stop":1659447298147,"duration":147}},{"uid":"c682ef4b1f1027b9","status":"passed","time":{"start":1659447116000,"stop":1659447116222,"duration":222}},{"uid":"4066b6cfa4c392df","status":"passed","time":{"start":1659448205000,"stop":1659448205148,"duration":148}}]},"Hub Suite:Hub Suite#[It] Cluster negative tests Worker Cluster Registration with Wrong endpoint":{"statistic":{"failed":17,"broken":0,"skipped":0,"passed":107,"unknown":0,"total":124},"items":[{"uid":"36d1b912d524515e","status":"passed","time":{"start":1660828517000,"stop":1660828659204,"duration":142204}},{"uid":"4442d58b92621e7e","status":"passed","time":{"start":1660807774000,"stop":1660807913506,"duration":139506}},{"uid":"c53772045318bf00","status":"passed","time":{"start":1660803998000,"stop":1660804132846,"duration":134846}},{"uid":"faa62f538d3ebaf7","status":"passed","time":{"start":1660805737000,"stop":1660805876253,"duration":139253}},{"uid":"84fdd18f6e6e9414","status":"passed","time":{"start":1660803696000,"stop":1660803830933,"duration":134933}},{"uid":"6f840db46a140daf","status":"passed","time":{"start":1660798468000,"stop":1660798603537,"duration":135537}},{"uid":"662af5ded0902df7","status":"passed","time":{"start":1660748009000,"stop":1660748146054,"duration":137054}},{"uid":"81dc00ba501b1d46","status":"passed","time":{"start":1660747865000,"stop":1660748006923,"duration":141923}},{"uid":"644a8ef1609eddc","status":"passed","time":{"start":1660747735000,"stop":1660747872204,"duration":137204}},{"uid":"75fd65ffeb86c4b4","status":"passed","time":{"start":1660747654000,"stop":1660747790458,"duration":136458}},{"uid":"bd3587a554e2afcb","status":"passed","time":{"start":1660722683000,"stop":1660722823402,"duration":140402}},{"uid":"92b46a15b8d5ec61","status":"passed","time":{"start":1660635916000,"stop":1660636052101,"duration":136101}},{"uid":"6ea17c9c83204b12","status":"passed","time":{"start":1660635666000,"stop":1660635801108,"duration":135108}},{"uid":"2e9188b527315ea3","status":"passed","time":{"start":1660635860000,"stop":1660635996228,"duration":136228}},{"uid":"34f2d14889165f90","status":"passed","time":{"start":1660631164000,"stop":1660631303734,"duration":139734}},{"uid":"38235cb153354523","status":"passed","time":{"start":1660623463000,"stop":1660623600001,"duration":137001}},{"uid":"aefb30246430b7a6","status":"passed","time":{"start":1660623384000,"stop":1660623523909,"duration":139909}},{"uid":"95913bb6271cb257","status":"passed","time":{"start":1660623360000,"stop":1660623495925,"duration":135925}},{"uid":"aadc87dc0e2231c3","status":"passed","time":{"start":1660452377000,"stop":1660452515724,"duration":138724}},{"uid":"21a91d916e052cc7","status":"passed","time":{"start":1660451870000,"stop":1660452004464,"duration":134464}}]},"Worker Suite:Worker Suite#[It] Slice Creation scenarios Slice with LOW QOS should reconcile low qos slice":{"statistic":{"failed":0,"broken":0,"skipped":12,"passed":18,"unknown":0,"total":30},"items":[{"uid":"ec479d989181e3c9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"9b20e25c2a5d3171","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"82688c46bcfce6c4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"76cae015ddb4460f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"c024270115cc5970","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"cda97b10df661bed","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"3bdde8547ea9cff","status":"passed","time":{"start":1660067259000,"stop":1660067259352,"duration":352}},{"uid":"3ee95c4806144a62","status":"passed","time":{"start":1660047551000,"stop":1660047551514,"duration":514}},{"uid":"bd86fd7e1923fbd5","status":"passed","time":{"start":1659982549000,"stop":1659982549724,"duration":724}},{"uid":"185b4e098271da1a","status":"passed","time":{"start":1659970182000,"stop":1659970182334,"duration":334}},{"uid":"934e980d89e28e49","status":"passed","time":{"start":1659944495000,"stop":1659944495501,"duration":501}},{"uid":"4d6a1e6c84f1b2bb","status":"passed","time":{"start":1659874977000,"stop":1659874977320,"duration":320}},{"uid":"dfddbc956563f8af","status":"passed","time":{"start":1659496050000,"stop":1659496050313,"duration":313}},{"uid":"2fb0eff080ceec37","status":"passed","time":{"start":1659496083000,"stop":1659496083358,"duration":358}},{"uid":"18ee77134f9d6e2d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"d44dcd6a5b884b4b","status":"passed","time":{"start":1659450916000,"stop":1659450916449,"duration":449}},{"uid":"f3291dfb8626c61b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"1106a8282b6e9d54","status":"passed","time":{"start":1659447298000,"stop":1659447298374,"duration":374}},{"uid":"11724a2eb5ec452","status":"passed","time":{"start":1659447116000,"stop":1659447116638,"duration":638}},{"uid":"d4b1319ab6182251","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448205000,"stop":1659448205000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Hub netpol scenarios: Slice Configs Netpol scenarios Should successfully deploy for valid namespace creating clusters with * in both applicationNamespace and allowedNamespace":{"statistic":{"failed":0,"broken":0,"skipped":13,"passed":26,"unknown":0,"total":39},"items":[{"uid":"4bce15fce59422e2","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"944b083da0bff0ae","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"3e52cce1f61a860b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"cd3844150232c60c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"106a7e6a0eedaa5e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"8968ac8a980ba084","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"cde58556223c78a8","status":"passed","time":{"start":1660067259000,"stop":1660067259696,"duration":696}},{"uid":"1817a460312e12e5","status":"passed","time":{"start":1660047551000,"stop":1660047551474,"duration":474}},{"uid":"1acd960545e5fc6","status":"passed","time":{"start":1659982549000,"stop":1659982550116,"duration":1116}},{"uid":"d613384647c52e98","status":"passed","time":{"start":1659970182000,"stop":1659970182403,"duration":403}},{"uid":"2f1f10872d9174c5","status":"passed","time":{"start":1659944495000,"stop":1659944495795,"duration":795}},{"uid":"6886dcbc27580230","status":"passed","time":{"start":1659874977000,"stop":1659874977469,"duration":469}},{"uid":"5ecb611a60e9b3cd","status":"passed","time":{"start":1659496050000,"stop":1659496050375,"duration":375}},{"uid":"476a63ecbd11329b","status":"passed","time":{"start":1659496083000,"stop":1659496083517,"duration":517}},{"uid":"2a122e065760bbe0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"cbe3ccdf5d91f0b7","status":"passed","time":{"start":1659450916000,"stop":1659450916474,"duration":474}},{"uid":"89df494b1e4f0cd9","status":"passed","time":{"start":1659451524000,"stop":1659451524775,"duration":775}},{"uid":"6586a1b645b9aefc","status":"passed","time":{"start":1659447298000,"stop":1659447298377,"duration":377}},{"uid":"e5974338295322e1","status":"passed","time":{"start":1659447116000,"stop":1659447116451,"duration":451}},{"uid":"db368f376661734","status":"passed","time":{"start":1659448205000,"stop":1659448205531,"duration":531}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Creation Should update Project while applying valid manifest with existing Project name":{"statistic":{"failed":5,"broken":0,"skipped":0,"passed":119,"unknown":0,"total":124},"items":[{"uid":"6f43c86a117d0450","status":"passed","time":{"start":1660828517000,"stop":1660828521786,"duration":4786}},{"uid":"22cbb71ca00cfc42","status":"passed","time":{"start":1660807774000,"stop":1660807778593,"duration":4593}},{"uid":"1f68f0eeeec7fa8c","status":"passed","time":{"start":1660803998000,"stop":1660804002691,"duration":4691}},{"uid":"52ea11121a505808","status":"passed","time":{"start":1660805737000,"stop":1660805741543,"duration":4543}},{"uid":"638c7c62565bbe3b","status":"passed","time":{"start":1660803696000,"stop":1660803700507,"duration":4507}},{"uid":"ab50fb75e7897c16","status":"passed","time":{"start":1660798468000,"stop":1660798472603,"duration":4603}},{"uid":"4a436519ab50069d","status":"passed","time":{"start":1660748009000,"stop":1660748013913,"duration":4913}},{"uid":"62f6bc4bbba8b4af","status":"passed","time":{"start":1660747865000,"stop":1660747869748,"duration":4748}},{"uid":"139ca78b485f3711","status":"passed","time":{"start":1660747735000,"stop":1660747739739,"duration":4739}},{"uid":"38b189b9aa1d618d","status":"passed","time":{"start":1660747654000,"stop":1660747658537,"duration":4537}},{"uid":"d475e407cb136cf8","status":"passed","time":{"start":1660722683000,"stop":1660722687669,"duration":4669}},{"uid":"464a741bb6879d91","status":"passed","time":{"start":1660635916000,"stop":1660635920682,"duration":4682}},{"uid":"9373e7978728eabc","status":"passed","time":{"start":1660635666000,"stop":1660635670622,"duration":4622}},{"uid":"f9be49bada1dfcfd","status":"passed","time":{"start":1660635860000,"stop":1660635864728,"duration":4728}},{"uid":"a14fd04bb8cc7ba7","status":"passed","time":{"start":1660631164000,"stop":1660631168603,"duration":4603}},{"uid":"589c0d1f339deb77","status":"passed","time":{"start":1660623463000,"stop":1660623467673,"duration":4673}},{"uid":"782443afd8b55eb5","status":"passed","time":{"start":1660623384000,"stop":1660623388568,"duration":4568}},{"uid":"d109c0a097fb3480","status":"passed","time":{"start":1660623360000,"stop":1660623364506,"duration":4506}},{"uid":"3c6e3ec9543a8e12","status":"passed","time":{"start":1660452377000,"stop":1660452381729,"duration":4729}},{"uid":"8115ca8d6dc6762d","status":"passed","time":{"start":1660451870000,"stop":1660451874563,"duration":4563}}]},"Worker Suite:Worker Suite#[BeforeSuite]":{"statistic":{"failed":113,"broken":0,"skipped":0,"passed":39,"unknown":0,"total":152},"items":[{"uid":"3bd322c44dad3a90","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004bfd40>: {\n        Underlying: <*exec.ExitError | 0xc00014e9a0>{\n            ProcessState: {\n                pid: 6821,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 173427},\n                    Stime: {Sec: 0, Usec: 25829},\n                    Maxrss: 51204,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3204,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 352,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 677,\n                    Nivcsw: 335,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"v1.8.0\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\",\n                        \"\",\n                        \"Error: INSTALLATION FAILED: cannot re-use a name that is still in use\",\n                        \"helm.go:84: [debug] cannot re-use a name that is still in use\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).availableName\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:483\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:197\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:264\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:125\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                        \"INSTALLATION FAILED\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:127\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insec...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"v1.8.0\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\n    \n    Error: INSTALLATION FAILED: cannot re-use a name that is still in use\n    helm.go:84: [debug] cannot re-use a name that is still in use\n    helm.sh/helm/v3/pkg/action.(*Install).availableName\n    \thelm.sh/helm/v3/pkg/action/install.go:483\n    helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\n    \thelm.sh/helm/v3/pkg/action/install.go:197\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:264\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:125\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1660829689000,"stop":1660829689419,"duration":419}},{"uid":"efd7dac96df22f88","status":"passed","time":{"start":1660808797000,"stop":1660808947591,"duration":150591}},{"uid":"a60254f5a4fbd239","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000352a98>: {\n        Underlying: <*exec.ExitError | 0xc000424680>{\n            ProcessState: {\n                pid: 6888,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 148369},\n                    Stime: {Sec: 0, Usec: 31793},\n                    Maxrss: 51428,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3241,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 24,\n                    Oublock: 352,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 704,\n                    Nivcsw: 653,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"v1.8.0\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\",\n                        \"\",\n                        \"Error: INSTALLATION FAILED: cannot re-use a name that is still in use\",\n                        \"helm.go:84: [debug] cannot re-use a name that is still in use\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).availableName\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:483\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:197\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:264\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:125\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                        \"INSTALLATION FAILED\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:127\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is inse...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"v1.8.0\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\n    \n    Error: INSTALLATION FAILED: cannot re-use a name that is still in use\n    helm.go:84: [debug] cannot re-use a name that is still in use\n    helm.sh/helm/v3/pkg/action.(*Install).availableName\n    \thelm.sh/helm/v3/pkg/action/install.go:483\n    helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\n    \thelm.sh/helm/v3/pkg/action/install.go:197\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:264\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:125\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1660806889000,"stop":1660806889426,"duration":426}},{"uid":"9ccb409f7b36b307","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004ce138>: {\n        Underlying: <*exec.ExitError | 0xc00046e260>{\n            ProcessState: {\n                pid: 6087,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 150922},\n                    Stime: {Sec: 0, Usec: 8384},\n                    Maxrss: 54152,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2712,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 352,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 565,\n                    Nivcsw: 288,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"v1.8.0\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\",\n                        \"\",\n                        \"Error: INSTALLATION FAILED: cannot re-use a name that is still in use\",\n                        \"helm.go:84: [debug] cannot re-use a name that is still in use\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).availableName\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:483\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:197\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:264\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:125\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                        \"INSTALLATION FAILED\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:127\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insecu...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"v1.8.0\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\n    \n    Error: INSTALLATION FAILED: cannot re-use a name that is still in use\n    helm.go:84: [debug] cannot re-use a name that is still in use\n    helm.sh/helm/v3/pkg/action.(*Install).availableName\n    \thelm.sh/helm/v3/pkg/action/install.go:483\n    helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\n    \thelm.sh/helm/v3/pkg/action/install.go:197\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:264\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:125\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1660803717000,"stop":1660803717531,"duration":531}},{"uid":"10379dacd730e805","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0001cfa40>: {\n        Underlying: <*exec.ExitError | 0xc0004a21c0>{\n            ProcessState: {\n                pid: 6855,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 131521},\n                    Stime: {Sec: 0, Usec: 26304},\n                    Maxrss: 53148,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2749,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 24,\n                    Oublock: 352,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 850,\n                    Nivcsw: 313,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"v1.8.0\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\",\n                        \"\",\n                        \"Error: INSTALLATION FAILED: cannot re-use a name that is still in use\",\n                        \"helm.go:84: [debug] cannot re-use a name that is still in use\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).availableName\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:483\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:197\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:264\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:125\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                        \"INSTALLATION FAILED\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:127\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is inse...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"v1.8.0\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\n    \n    Error: INSTALLATION FAILED: cannot re-use a name that is still in use\n    helm.go:84: [debug] cannot re-use a name that is still in use\n    helm.sh/helm/v3/pkg/action.(*Install).availableName\n    \thelm.sh/helm/v3/pkg/action/install.go:483\n    helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\n    \thelm.sh/helm/v3/pkg/action/install.go:197\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:264\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:125\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1660799617000,"stop":1660799617416,"duration":416}},{"uid":"de772b693987552c","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004cc4c8>: {\n        Underlying: <*exec.ExitError | 0xc00046c7e0>{\n            ProcessState: {\n                pid: 6906,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 127406},\n                    Stime: {Sec: 0, Usec: 20549},\n                    Maxrss: 56468,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2185,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 352,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 749,\n                    Nivcsw: 311,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"v1.8.0\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\",\n                        \"\",\n                        \"Error: INSTALLATION FAILED: cannot re-use a name that is still in use\",\n                        \"helm.go:84: [debug] cannot re-use a name that is still in use\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).availableName\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:483\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:197\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:264\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:125\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                        \"INSTALLATION FAILED\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:127\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insec...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"v1.8.0\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\n    \n    Error: INSTALLATION FAILED: cannot re-use a name that is still in use\n    helm.go:84: [debug] cannot re-use a name that is still in use\n    helm.sh/helm/v3/pkg/action.(*Install).availableName\n    \thelm.sh/helm/v3/pkg/action/install.go:483\n    helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\n    \thelm.sh/helm/v3/pkg/action/install.go:197\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:264\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:125\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1660748778000,"stop":1660748778387,"duration":387}},{"uid":"4a9e5639b2590737","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004a6018>: {\n        Underlying: <*exec.ExitError | 0xc000326040>{\n            ProcessState: {\n                pid: 6140,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 120304},\n                    Stime: {Sec: 0, Usec: 24890},\n                    Maxrss: 54708,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2185,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 352,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 734,\n                    Nivcsw: 545,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"v1.8.0\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\",\n                        \"\",\n                        \"Error: INSTALLATION FAILED: cannot re-use a name that is still in use\",\n                        \"helm.go:84: [debug] cannot re-use a name that is still in use\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).availableName\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:483\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:197\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:264\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:125\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                        \"INSTALLATION FAILED\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:127\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insec...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"v1.8.0\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\n    \n    Error: INSTALLATION FAILED: cannot re-use a name that is still in use\n    helm.go:84: [debug] cannot re-use a name that is still in use\n    helm.sh/helm/v3/pkg/action.(*Install).availableName\n    \thelm.sh/helm/v3/pkg/action/install.go:483\n    helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\n    \thelm.sh/helm/v3/pkg/action/install.go:197\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:264\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:125\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1660748668000,"stop":1660748668362,"duration":362}},{"uid":"7c42c149abc65fb8","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000e120>: {\n        Underlying: <*exec.ExitError | 0xc0004d4040>{\n            ProcessState: {\n                pid: 6894,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 167246},\n                    Stime: {Sec: 0, Usec: 29086},\n                    Maxrss: 51884,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2325,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 352,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 798,\n                    Nivcsw: 360,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"v1.8.0\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\",\n                        \"\",\n                        \"Error: INSTALLATION FAILED: cannot re-use a name that is still in use\",\n                        \"helm.go:84: [debug] cannot re-use a name that is still in use\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).availableName\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:483\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:197\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:264\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:125\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                        \"INSTALLATION FAILED\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:127\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insec...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"v1.8.0\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\n    \n    Error: INSTALLATION FAILED: cannot re-use a name that is still in use\n    helm.go:84: [debug] cannot re-use a name that is still in use\n    helm.sh/helm/v3/pkg/action.(*Install).availableName\n    \thelm.sh/helm/v3/pkg/action/install.go:483\n    helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\n    \thelm.sh/helm/v3/pkg/action/install.go:197\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:264\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:125\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1660723841000,"stop":1660723841521,"duration":521}},{"uid":"bdbc89db8a4a8ebd","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000348a08>: {\n        Underlying: <*exec.ExitError | 0xc00039c6e0>{\n            ProcessState: {\n                pid: 6071,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 131281},\n                    Stime: {Sec: 0, Usec: 19306},\n                    Maxrss: 54304,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2224,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 352,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 557,\n                    Nivcsw: 334,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"v1.8.0\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\",\n                        \"\",\n                        \"Error: INSTALLATION FAILED: cannot re-use a name that is still in use\",\n                        \"helm.go:84: [debug] cannot re-use a name that is still in use\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).availableName\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:483\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:197\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:264\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:125\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                        \"INSTALLATION FAILED\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:127\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insec...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"v1.8.0\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\n    \n    Error: INSTALLATION FAILED: cannot re-use a name that is still in use\n    helm.go:84: [debug] cannot re-use a name that is still in use\n    helm.sh/helm/v3/pkg/action.(*Install).availableName\n    \thelm.sh/helm/v3/pkg/action/install.go:483\n    helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\n    \thelm.sh/helm/v3/pkg/action/install.go:197\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:264\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:125\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1660712720000,"stop":1660712720444,"duration":444}},{"uid":"1b831a57b5633989","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0000d7938>: {\n        Underlying: <*exec.ExitError | 0xc00040a140>{\n            ProcessState: {\n                pid: 6912,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 180348},\n                    Stime: {Sec: 0, Usec: 29245},\n                    Maxrss: 51556,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3356,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 352,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 631,\n                    Nivcsw: 529,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"v1.8.0\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\",\n                        \"\",\n                        \"Error: INSTALLATION FAILED: cannot re-use a name that is still in use\",\n                        \"helm.go:84: [debug] cannot re-use a name that is still in use\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).availableName\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:483\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:197\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:264\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:125\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                        \"INSTALLATION FAILED\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:127\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insec...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"v1.8.0\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\n    \n    Error: INSTALLATION FAILED: cannot re-use a name that is still in use\n    helm.go:84: [debug] cannot re-use a name that is still in use\n    helm.sh/helm/v3/pkg/action.(*Install).availableName\n    \thelm.sh/helm/v3/pkg/action/install.go:483\n    helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\n    \thelm.sh/helm/v3/pkg/action/install.go:197\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:264\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:125\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1660636971000,"stop":1660636971503,"duration":503}},{"uid":"7a14d5a3b66b1822","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000e210>: {\n        Underlying: <*exec.ExitError | 0xc00014d360>{\n            ProcessState: {\n                pid: 6097,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 121364},\n                    Stime: {Sec: 0, Usec: 22066},\n                    Maxrss: 54272,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2713,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 352,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 499,\n                    Nivcsw: 226,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"v1.8.0\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\",\n                        \"\",\n                        \"Error: INSTALLATION FAILED: cannot re-use a name that is still in use\",\n                        \"helm.go:84: [debug] cannot re-use a name that is still in use\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).availableName\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:483\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:197\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:264\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:125\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                        \"INSTALLATION FAILED\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:127\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insec...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"v1.8.0\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\n    \n    Error: INSTALLATION FAILED: cannot re-use a name that is still in use\n    helm.go:84: [debug] cannot re-use a name that is still in use\n    helm.sh/helm/v3/pkg/action.(*Install).availableName\n    \thelm.sh/helm/v3/pkg/action/install.go:483\n    helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\n    \thelm.sh/helm/v3/pkg/action/install.go:197\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:264\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:125\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1660636570000,"stop":1660636570270,"duration":270}},{"uid":"4791d32ce0dccbf6","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0005cc5b8>: {\n        Underlying: <*exec.ExitError | 0xc00045e0e0>{\n            ProcessState: {\n                pid: 6899,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 123926},\n                    Stime: {Sec: 0, Usec: 27983},\n                    Maxrss: 52284,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2179,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 352,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 733,\n                    Nivcsw: 308,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"v1.8.0\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\",\n                        \"\",\n                        \"Error: INSTALLATION FAILED: cannot re-use a name that is still in use\",\n                        \"helm.go:84: [debug] cannot re-use a name that is still in use\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).availableName\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:483\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:197\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:264\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:125\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                        \"INSTALLATION FAILED\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:127\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insec...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"v1.8.0\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\n    \n    Error: INSTALLATION FAILED: cannot re-use a name that is still in use\n    helm.go:84: [debug] cannot re-use a name that is still in use\n    helm.sh/helm/v3/pkg/action.(*Install).availableName\n    \thelm.sh/helm/v3/pkg/action/install.go:483\n    helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\n    \thelm.sh/helm/v3/pkg/action/install.go:197\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:264\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:125\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1660624452000,"stop":1660624452426,"duration":426}},{"uid":"1597b71f5ab51f69","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004cfd40>: {\n        Underlying: <*exec.ExitError | 0xc00014c9c0>{\n            ProcessState: {\n                pid: 6115,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 122337},\n                    Stime: {Sec: 0, Usec: 28545},\n                    Maxrss: 54472,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2277,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 352,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 730,\n                    Nivcsw: 326,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"v1.8.0\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\",\n                        \"\",\n                        \"Error: INSTALLATION FAILED: cannot re-use a name that is still in use\",\n                        \"helm.go:84: [debug] cannot re-use a name that is still in use\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).availableName\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:483\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:197\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:264\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:125\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                        \"INSTALLATION FAILED\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:127\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insec...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"v1.8.0\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\n    \n    Error: INSTALLATION FAILED: cannot re-use a name that is still in use\n    helm.go:84: [debug] cannot re-use a name that is still in use\n    helm.sh/helm/v3/pkg/action.(*Install).availableName\n    \thelm.sh/helm/v3/pkg/action/install.go:483\n    helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\n    \thelm.sh/helm/v3/pkg/action/install.go:197\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:264\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:125\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1660624148000,"stop":1660624148391,"duration":391}},{"uid":"c4d7a2366682c660","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000e1c8>: {\n        Underlying: <*exec.ExitError | 0xc00014c640>{\n            ProcessState: {\n                pid: 6101,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 142132},\n                    Stime: {Sec: 0, Usec: 32487},\n                    Maxrss: 54384,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3192,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 352,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 787,\n                    Nivcsw: 428,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"v1.8.0\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\",\n                        \"\",\n                        \"Error: INSTALLATION FAILED: cannot re-use a name that is still in use\",\n                        \"helm.go:84: [debug] cannot re-use a name that is still in use\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).availableName\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:483\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:197\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:264\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:125\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                        \"INSTALLATION FAILED\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:127\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insec...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"v1.8.0\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\n    \n    Error: INSTALLATION FAILED: cannot re-use a name that is still in use\n    helm.go:84: [debug] cannot re-use a name that is still in use\n    helm.sh/helm/v3/pkg/action.(*Install).availableName\n    \thelm.sh/helm/v3/pkg/action/install.go:483\n    helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\n    \thelm.sh/helm/v3/pkg/action/install.go:197\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:264\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:125\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1660457696000,"stop":1660457696442,"duration":442}},{"uid":"407eae6d653b59ee","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00018d248>: {\n        Underlying: <*exec.ExitError | 0xc00038a300>{\n            ProcessState: {\n                pid: 6893,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 133895},\n                    Stime: {Sec: 0, Usec: 16229},\n                    Maxrss: 54036,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2164,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 352,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 623,\n                    Nivcsw: 321,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"v1.8.0\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\",\n                        \"\",\n                        \"Error: INSTALLATION FAILED: cannot re-use a name that is still in use\",\n                        \"helm.go:84: [debug] cannot re-use a name that is still in use\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).availableName\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:483\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:197\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:264\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:125\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                        \"INSTALLATION FAILED\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:127\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insec...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"v1.8.0\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\n    \n    Error: INSTALLATION FAILED: cannot re-use a name that is still in use\n    helm.go:84: [debug] cannot re-use a name that is still in use\n    helm.sh/helm/v3/pkg/action.(*Install).availableName\n    \thelm.sh/helm/v3/pkg/action/install.go:483\n    helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\n    \thelm.sh/helm/v3/pkg/action/install.go:197\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:264\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:125\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1660453993000,"stop":1660453993462,"duration":462}},{"uid":"98dca474fbd40f63","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000614a50>: {\n        Underlying: <*exec.ExitError | 0xc0003f54a0>{\n            ProcessState: {\n                pid: 6855,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 162875},\n                    Stime: {Sec: 0, Usec: 23835},\n                    Maxrss: 54228,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2941,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 24,\n                    Oublock: 352,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 818,\n                    Nivcsw: 272,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"v1.8.0\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\",\n                        \"\",\n                        \"Error: INSTALLATION FAILED: cannot re-use a name that is still in use\",\n                        \"helm.go:84: [debug] cannot re-use a name that is still in use\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).availableName\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:483\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:197\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:264\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:125\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                        \"INSTALLATION FAILED\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:127\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is inse...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"v1.8.0\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\n    \n    Error: INSTALLATION FAILED: cannot re-use a name that is still in use\n    helm.go:84: [debug] cannot re-use a name that is still in use\n    helm.sh/helm/v3/pkg/action.(*Install).availableName\n    \thelm.sh/helm/v3/pkg/action/install.go:483\n    helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\n    \thelm.sh/helm/v3/pkg/action/install.go:197\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:264\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:125\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1660411114000,"stop":1660411114545,"duration":545}},{"uid":"811b22a98a925350","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000460b70>: {\n        Underlying: <*exec.ExitError | 0xc00046c280>{\n            ProcessState: {\n                pid: 6859,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 147355},\n                    Stime: {Sec: 0, Usec: 26791},\n                    Maxrss: 52576,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2279,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 352,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 754,\n                    Nivcsw: 277,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"v1.8.0\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\",\n                        \"\",\n                        \"Error: INSTALLATION FAILED: cannot re-use a name that is still in use\",\n                        \"helm.go:84: [debug] cannot re-use a name that is still in use\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).availableName\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:483\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:197\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:264\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:125\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                        \"INSTALLATION FAILED\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:127\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insec...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"v1.8.0\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\n    \n    Error: INSTALLATION FAILED: cannot re-use a name that is still in use\n    helm.go:84: [debug] cannot re-use a name that is still in use\n    helm.sh/helm/v3/pkg/action.(*Install).availableName\n    \thelm.sh/helm/v3/pkg/action/install.go:483\n    helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\n    \thelm.sh/helm/v3/pkg/action/install.go:197\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:264\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:125\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1660373247000,"stop":1660373247367,"duration":367}},{"uid":"dbac6c2fb8653028","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004b1968>: {\n        Underlying: <*exec.ExitError | 0xc000074640>{\n            ProcessState: {\n                pid: 6901,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 160989},\n                    Stime: {Sec: 0, Usec: 15706},\n                    Maxrss: 55920,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2034,\n                    Majflt: 1,\n                    Nswap: 0,\n                    Inblock: 128,\n                    Oublock: 352,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 791,\n                    Nivcsw: 401,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"v1.8.0\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\",\n                        \"\",\n                        \"Error: INSTALLATION FAILED: cannot re-use a name that is still in use\",\n                        \"helm.go:84: [debug] cannot re-use a name that is still in use\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).availableName\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:483\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:197\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:264\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:125\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                        \"INSTALLATION FAILED\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:127\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is ins...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"v1.8.0\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\n    \n    Error: INSTALLATION FAILED: cannot re-use a name that is still in use\n    helm.go:84: [debug] cannot re-use a name that is still in use\n    helm.sh/helm/v3/pkg/action.(*Install).availableName\n    \thelm.sh/helm/v3/pkg/action/install.go:483\n    helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\n    \thelm.sh/helm/v3/pkg/action/install.go:197\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:264\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:125\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1660371814000,"stop":1660371814633,"duration":633}},{"uid":"ee33676df3c29e3e","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000265ea8>: {\n        Underlying: <*exec.ExitError | 0xc000074580>{\n            ProcessState: {\n                pid: 6928,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 166699},\n                    Stime: {Sec: 0, Usec: 23814},\n                    Maxrss: 54244,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2230,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 24,\n                    Oublock: 352,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 1042,\n                    Nivcsw: 931,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"install.go:178: [debug] Original chart version: \\\"v1.8.0\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\",\n                        \"\",\n                        \"Error: INSTALLATION FAILED: cannot re-use a name that is still in use\",\n                        \"helm.go:84: [debug] cannot re-use a name that is still in use\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).availableName\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:483\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:197\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:264\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:125\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                        \"INSTALLATION FAILED\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:127\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"install.go:178: [debug] Original chart version: \\\"v1.8.0\\\"\",\n                    \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\",\n                    \"\",\n                    \"Error: INSTALLATION FAILED: cannot re-use a name that is still in use\",\n                    \"helm.go:84: [debug] cannot re-use a name that is still i...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; install.go:178: [debug] Original chart version: \"v1.8.0\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\n    \n    Error: INSTALLATION FAILED: cannot re-use a name that is still in use\n    helm.go:84: [debug] cannot re-use a name that is still in use\n    helm.sh/helm/v3/pkg/action.(*Install).availableName\n    \thelm.sh/helm/v3/pkg/action/install.go:483\n    helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\n    \thelm.sh/helm/v3/pkg/action/install.go:197\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:264\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:125\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1660365282000,"stop":1660365282506,"duration":506}},{"uid":"9579d4456ccf2dd8","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004be258>: {\n        Underlying: <*exec.ExitError | 0xc00014e800>{\n            ProcessState: {\n                pid: 6896,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 174126},\n                    Stime: {Sec: 0, Usec: 46433},\n                    Maxrss: 54328,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2684,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 352,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 777,\n                    Nivcsw: 853,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"install.go:178: [debug] Original chart version: \\\"v1.8.0\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\",\n                        \"\",\n                        \"Error: INSTALLATION FAILED: cannot re-use a name that is still in use\",\n                        \"helm.go:84: [debug] cannot re-use a name that is still in use\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).availableName\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:483\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:197\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:264\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:125\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                        \"INSTALLATION FAILED\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:127\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"install.go:178: [debug] Original chart version: \\\"v1.8.0\\\"\",\n                    \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\",\n                    \"\",\n                    \"Error: INSTALLATION FAILED: cannot re-use a name that is still in use\",\n                    \"helm.go:84: [debug] cannot re-use a name that is still in ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; install.go:178: [debug] Original chart version: \"v1.8.0\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\n    \n    Error: INSTALLATION FAILED: cannot re-use a name that is still in use\n    helm.go:84: [debug] cannot re-use a name that is still in use\n    helm.sh/helm/v3/pkg/action.(*Install).availableName\n    \thelm.sh/helm/v3/pkg/action/install.go:483\n    helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\n    \thelm.sh/helm/v3/pkg/action/install.go:197\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:264\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:125\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1660362429000,"stop":1660362429631,"duration":631}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting iperf-server pod Check ping between iperf-server and iperf-client after iperf-server pod restart":{"statistic":{"failed":0,"broken":0,"skipped":39,"passed":0,"unknown":0,"total":39},"items":[{"uid":"d7b8b124d5be2f76","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"c118fdb8c9f167cc","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"27834db9efcc7feb","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"90e61660d13f0a96","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"73958c213a534876","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"269d2763f0b7651a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"8e3191ea549b4b71","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"93d3c2492084715e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}},{"uid":"29077ae9cb07db6a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659982549000,"stop":1659982549000,"duration":0}},{"uid":"1c72f0991f86e2c6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659970182000,"stop":1659970182000,"duration":0}},{"uid":"4c036a8e62cd942a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659944495000,"stop":1659944495000,"duration":0}},{"uid":"724038d5d23dcff9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659874977000,"stop":1659874977000,"duration":0}},{"uid":"2db80c5c3f6afdea","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496050000,"stop":1659496050000,"duration":0}},{"uid":"ad8b2f4fc2b3c448","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496083000,"stop":1659496083000,"duration":0}},{"uid":"a49ddc57c488716","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"c36dfcc3b13bdcf5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659450916000,"stop":1659450916000,"duration":0}},{"uid":"14c7f9dcf55459a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"6afa278afecbd7b6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447298000,"stop":1659447298000,"duration":0}},{"uid":"970e5a37e53c1627","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447116000,"stop":1659447116000,"duration":0}},{"uid":"c668d1307ce5f731","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448205000,"stop":1659448205000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice creation scenarios: when slice red is installed workers should have gateway pod running":{"statistic":{"failed":0,"broken":0,"skipped":27,"passed":12,"unknown":0,"total":39},"items":[{"uid":"6917e7d4f39e53eb","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"ef34e1212f4a0ae2","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"c8346067fdbf0ab1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"96cfcd29034b975a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"ce4a08a1314c8cc5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"e04f21649a41ac9b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"49fd7587906d710a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"a867e929ed7b9bcf","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}},{"uid":"c68e7a949a0f8139","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659982549000,"stop":1659982549000,"duration":0}},{"uid":"2ecd29f09aa98c5f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659970182000,"stop":1659970182000,"duration":0}},{"uid":"fb6832e1e6e29a12","status":"passed","time":{"start":1659944495000,"stop":1659944495030,"duration":30}},{"uid":"151367ac16e08ad6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659874977000,"stop":1659874977000,"duration":0}},{"uid":"ad42895f0f513d94","status":"passed","time":{"start":1659496050000,"stop":1659496080133,"duration":30133}},{"uid":"dc15a74958854782","status":"passed","time":{"start":1659496083000,"stop":1659496103112,"duration":20112}},{"uid":"20632b5e0ae6afae","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"cf1d01eb267cd56","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659450916000,"stop":1659450916000,"duration":0}},{"uid":"ef12351cf493beb4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"e4eec20157befe4d","status":"passed","time":{"start":1659447298000,"stop":1659447366237,"duration":68237}},{"uid":"99724be27d73d2c9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447116000,"stop":1659447116000,"duration":0}},{"uid":"55814213c1d3f90f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448205000,"stop":1659448205000,"duration":0}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Update Should update successfully when Project Applied while removing a service account name in Read users":{"statistic":{"failed":7,"broken":0,"skipped":0,"passed":114,"unknown":0,"total":121},"items":[{"uid":"ac889b558949fd9","status":"passed","time":{"start":1660828517000,"stop":1660828522072,"duration":5072}},{"uid":"85eabc6b10e50c65","status":"passed","time":{"start":1660807774000,"stop":1660807778591,"duration":4591}},{"uid":"df5e3c767c90e2b6","status":"passed","time":{"start":1660803998000,"stop":1660804002823,"duration":4823}},{"uid":"bea8856bbbcf0988","status":"passed","time":{"start":1660805737000,"stop":1660805741760,"duration":4760}},{"uid":"172c56ef481884ed","status":"passed","time":{"start":1660803696000,"stop":1660803701484,"duration":5484}},{"uid":"4a11e4240a108540","status":"passed","time":{"start":1660798468000,"stop":1660798472673,"duration":4673}},{"uid":"c2a127b59947ab43","status":"passed","time":{"start":1660748009000,"stop":1660748014448,"duration":5448}},{"uid":"b9ea12d9ccfc14bd","status":"passed","time":{"start":1660747865000,"stop":1660747869814,"duration":4814}},{"uid":"dad10807a4d86ff8","status":"passed","time":{"start":1660747735000,"stop":1660747739733,"duration":4733}},{"uid":"52f47fe2ae3292ca","status":"passed","time":{"start":1660747654000,"stop":1660747659420,"duration":5420}},{"uid":"7befe4e1783a5f6","status":"passed","time":{"start":1660722683000,"stop":1660722687872,"duration":4872}},{"uid":"74f930f058d1d3ae","status":"passed","time":{"start":1660635916000,"stop":1660635920925,"duration":4925}},{"uid":"9397aa5ce323647b","status":"passed","time":{"start":1660635666000,"stop":1660635670812,"duration":4812}},{"uid":"12529bbc42d0a818","status":"passed","time":{"start":1660635860000,"stop":1660635864967,"duration":4967}},{"uid":"cf4ac761984d5326","status":"passed","time":{"start":1660631164000,"stop":1660631169383,"duration":5383}},{"uid":"3cbb0b08ebe34bca","status":"passed","time":{"start":1660623463000,"stop":1660623467862,"duration":4862}},{"uid":"6ad2b413dd3b35ac","status":"passed","time":{"start":1660623384000,"stop":1660623389369,"duration":5369}},{"uid":"b7adf7bcbce39c27","status":"passed","time":{"start":1660623360000,"stop":1660623365446,"duration":5446}},{"uid":"8c492fb6027eb0d5","status":"passed","time":{"start":1660452377000,"stop":1660452381968,"duration":4968}},{"uid":"9f23e50d09a074d2","status":"passed","time":{"start":1660451870000,"stop":1660451874544,"duration":4544}}]},"Worker Suite:Worker Suite#[It] NodeRestartSlice Testing node restart should re-establish connection on node restart":{"statistic":{"failed":0,"broken":0,"skipped":30,"passed":0,"unknown":0,"total":30},"items":[{"uid":"91e1ff4203d606b3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"d9a11fb6de248829","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"8bb7628b8ba05538","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"3c3e5cabf7832d7b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"5c48cb060fe5b172","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"a0e7ad08cc242e71","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"e788046be4165370","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"db48046b039b4aa7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}},{"uid":"6fcbc28b4eaed6e9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659982549000,"stop":1659982549000,"duration":0}},{"uid":"5f2162242f2e886d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659970182000,"stop":1659970182000,"duration":0}},{"uid":"56c9f62414fd4a9d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659944495000,"stop":1659944495000,"duration":0}},{"uid":"201556a2f06ca154","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659874977000,"stop":1659874977000,"duration":0}},{"uid":"c83e4eb5ec7a436e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496050000,"stop":1659496050000,"duration":0}},{"uid":"e0f43405c6aee648","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496083000,"stop":1659496083000,"duration":0}},{"uid":"f6cc0dc15447168","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"206af700116ea313","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659450916000,"stop":1659450916000,"duration":0}},{"uid":"54cf17c19cd1289c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"d742ebbf5a414e6b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447298000,"stop":1659447298000,"duration":0}},{"uid":"81572141d096d69b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447116000,"stop":1659447116000,"duration":0}},{"uid":"4a893ba3b4a339bd","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448205000,"stop":1659448205000,"duration":0}}]},"Hub Suite:Hub Suite#[It]  QosProfile tests QosProfile delete validation Delete qosProfile positive scenario":{"statistic":{"failed":0,"broken":0,"skipped":116,"passed":0,"unknown":0,"total":116},"items":[{"uid":"4b0b3c6d308b3d8f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660828517000,"stop":1660828517000,"duration":0}},{"uid":"88a98dc5b8b36183","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660807774000,"stop":1660807774000,"duration":0}},{"uid":"f035476c2e7693a0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660803998000,"stop":1660803998000,"duration":0}},{"uid":"c7314ddec11eaa8f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660805737000,"stop":1660805737000,"duration":0}},{"uid":"e582eac821d42231","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660803696000,"stop":1660803696000,"duration":0}},{"uid":"afa4e84d5ad4456a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660798468000,"stop":1660798468000,"duration":0}},{"uid":"90be350e2e7ae4ec","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660748009000,"stop":1660748009000,"duration":0}},{"uid":"9ffab6ff0d08057b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660747865000,"stop":1660747865000,"duration":0}},{"uid":"5c57ba5e5fb34aa9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660747735000,"stop":1660747735000,"duration":0}},{"uid":"959db51aab9fa455","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660747654000,"stop":1660747654000,"duration":0}},{"uid":"86968fafbb92f4f2","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660722683000,"stop":1660722683000,"duration":0}},{"uid":"a5b0ff3b789914bf","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660635916000,"stop":1660635916000,"duration":0}},{"uid":"efaf971d14584bd8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660635666000,"stop":1660635666000,"duration":0}},{"uid":"ffabf6f42859b1d6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660635860000,"stop":1660635860000,"duration":0}},{"uid":"51fe603f8567bca1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660631164000,"stop":1660631164000,"duration":0}},{"uid":"d0f203cc3f5671f8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660623463000,"stop":1660623463000,"duration":0}},{"uid":"48387e0ac6c68ebc","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660623384000,"stop":1660623384000,"duration":0}},{"uid":"22917b0d8b633ca6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660623360000,"stop":1660623360000,"duration":0}},{"uid":"5d4f4ff306f15f4b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660452377000,"stop":1660452377000,"duration":0}},{"uid":"ccde316ab69ed800","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660451870000,"stop":1660451870000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Hub netpol scenarios: Slice Configs Netpol scenarios Should fail while Deleting Slice without removing the namespaceIsolationProfile part in slice configs":{"statistic":{"failed":0,"broken":0,"skipped":13,"passed":26,"unknown":0,"total":39},"items":[{"uid":"4d35a03346a35521","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"d7affabc70874cf5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"952c17cd76c850bd","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"cba509b284082409","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"afe7880f99255487","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"349aee741123efd3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"bda3aabdfcaab334","status":"passed","time":{"start":1660067259000,"stop":1660067259112,"duration":112}},{"uid":"883524a3e0e84518","status":"passed","time":{"start":1660047551000,"stop":1660047551102,"duration":102}},{"uid":"91cbc227a85fc1ca","status":"passed","time":{"start":1659982549000,"stop":1659982549278,"duration":278}},{"uid":"a099f350ef435649","status":"passed","time":{"start":1659970182000,"stop":1659970182092,"duration":92}},{"uid":"d12d26cac3536fb7","status":"passed","time":{"start":1659944495000,"stop":1659944495135,"duration":135}},{"uid":"829e0817fa336760","status":"passed","time":{"start":1659874977000,"stop":1659874977096,"duration":96}},{"uid":"9318b5335d66291e","status":"passed","time":{"start":1659496050000,"stop":1659496050071,"duration":71}},{"uid":"49f06b443176dd2f","status":"passed","time":{"start":1659496083000,"stop":1659496083151,"duration":151}},{"uid":"3c969bc0619c24e7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"50cbbcaec9ec25a0","status":"passed","time":{"start":1659450916000,"stop":1659450916095,"duration":95}},{"uid":"853847a4e0aabcdf","status":"passed","time":{"start":1659451524000,"stop":1659451524099,"duration":99}},{"uid":"226e0b16cc6f34fc","status":"passed","time":{"start":1659447298000,"stop":1659447298085,"duration":85}},{"uid":"2dd95b143d31686","status":"passed","time":{"start":1659447116000,"stop":1659447116103,"duration":103}},{"uid":"85e8aabfa263ec84","status":"passed","time":{"start":1659448205000,"stop":1659448205137,"duration":137}}]},"Worker Suite:Worker Suite#[It] Slice deletion tests: Slice is installed & validated Slice object is deleted from hub should remove vl3 router from spoke":{"statistic":{"failed":31,"broken":0,"skipped":0,"passed":8,"unknown":0,"total":39},"items":[{"uid":"61e696478a276c40","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004961b0>: {\n        Underlying: <*exec.ExitError | 0xc000662060>{\n            ProcessState: {\n                pid: 7054,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 179753},\n                    Stime: {Sec: 0, Usec: 35950},\n                    Maxrss: 82292,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 6378,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 363,\n                    Nivcsw: 330,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-red\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660808797000,"stop":1660808797218,"duration":218}},{"uid":"98e093a2c5568254","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0005cc378>: {\n        Underlying: <*exec.ExitError | 0xc000720f00>{\n            ProcessState: {\n                pid: 7129,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 148879},\n                    Stime: {Sec: 0, Usec: 39178},\n                    Maxrss: 81224,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 6084,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 415,\n                    Nivcsw: 350,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-red\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660299182000,"stop":1660299182183,"duration":183}},{"uid":"a2418f3c9820c060","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0003d30e0>: {\n        Underlying: <*exec.ExitError | 0xc000709b40>{\n            ProcessState: {\n                pid: 7161,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 158288},\n                    Stime: {Sec: 0, Usec: 41456},\n                    Maxrss: 82336,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 5777,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 314,\n                    Nivcsw: 345,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-red\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660295740000,"stop":1660295740197,"duration":197}},{"uid":"cd109c43ac4ad673","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0008e8138>: {\n        Underlying: <*exec.ExitError | 0xc0004c2f00>{\n            ProcessState: {\n                pid: 7249,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 186089},\n                    Stime: {Sec: 0, Usec: 64726},\n                    Maxrss: 83836,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4991,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 596,\n                    Nivcsw: 491,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-red\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660293657000,"stop":1660293657257,"duration":257}},{"uid":"f8a6b9a3bda62f3","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 3\nto equal\n    <int>: 1","time":{"start":1660113628000,"stop":1660113688522,"duration":60522}},{"uid":"41c28d0a63c60f32","status":"failed","statusDetails":"Timed out after 120.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1660104898000,"stop":1660105018717,"duration":120717}},{"uid":"faa2b3f81db94314","status":"failed","statusDetails":"Timed out after 120.001s.\nExpected\n    <int>: 1\nto equal\n    <int>: 0","time":{"start":1660067259000,"stop":1660067394447,"duration":135447}},{"uid":"4b6f6352fa75b6fe","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 3\nto equal\n    <int>: 1","time":{"start":1660047551000,"stop":1660047612720,"duration":61720}},{"uid":"363d4a36387567e0","status":"failed","statusDetails":"Timed out after 120.000s.\nExpected\n    <int>: 1\nto equal\n    <int>: 0","time":{"start":1659982549000,"stop":1659982672024,"duration":123024}},{"uid":"df8bf4f9d2daafcb","status":"passed","time":{"start":1659970182000,"stop":1659970200079,"duration":18079}},{"uid":"222f5671504f5d04","status":"failed","statusDetails":"Timed out after 120.000s.\nExpected\n    <int>: 1\nto equal\n    <int>: 0","time":{"start":1659944495000,"stop":1659944649698,"duration":154698}},{"uid":"8839ecdb832b8a01","status":"passed","time":{"start":1659874977000,"stop":1659875010032,"duration":33032}},{"uid":"70b52fade9421f17","status":"failed","statusDetails":"Timed out after 120.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1659496050000,"stop":1659496172442,"duration":122442}},{"uid":"87086431c21db65c","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 3\nto equal\n    <int>: 1","time":{"start":1659496083000,"stop":1659496144892,"duration":61892}},{"uid":"c2fd07ad7e2241b6","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1659451501000,"stop":1659451562996,"duration":61996}},{"uid":"96e73c69b1850357","status":"passed","time":{"start":1659450916000,"stop":1659450994022,"duration":78022}},{"uid":"cfa1c868a9413b1b","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1659451524000,"stop":1659451586160,"duration":62160}},{"uid":"5c0a78000ba61832","status":"passed","time":{"start":1659447298000,"stop":1659447321143,"duration":23143}},{"uid":"cbea9a0d981ad7a3","status":"passed","time":{"start":1659447116000,"stop":1659447162061,"duration":46061}},{"uid":"ba41f21e460a46b2","status":"failed","statusDetails":"Timed out after 120.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1659448205000,"stop":1659448349898,"duration":144898}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests slice with netpol should have gateway pod running":{"statistic":{"failed":1,"broken":0,"skipped":27,"passed":11,"unknown":0,"total":39},"items":[{"uid":"9ccf4b19a13242d5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"7f25c723508252b3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"9214189e190f1652","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"7ee2fa807ec44da0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"d75e2c660ebd79d8","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1660113628000,"stop":1660113822741,"duration":194741}},{"uid":"f9f595d398047778","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"a72d51888be345ba","status":"passed","time":{"start":1660067259000,"stop":1660067259010,"duration":10}},{"uid":"539dd318245b41e1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}},{"uid":"e5a328f0c074de7d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659982549000,"stop":1659982549000,"duration":0}},{"uid":"703a25fd5b578a18","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659970182000,"stop":1659970182000,"duration":0}},{"uid":"e02e754f6da995eb","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659944495000,"stop":1659944495000,"duration":0}},{"uid":"cbff7e3721535a0e","status":"passed","time":{"start":1659874977000,"stop":1659875007124,"duration":30124}},{"uid":"14b3b0e88672c386","status":"passed","time":{"start":1659496050000,"stop":1659496082127,"duration":32127}},{"uid":"5a2dfb615bf7eabe","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496083000,"stop":1659496083000,"duration":0}},{"uid":"8ac39ad2745b4bf9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"58e0d3b34d9cf21e","status":"passed","time":{"start":1659450916000,"stop":1659450972343,"duration":56343}},{"uid":"c069523638914937","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"fa0378ee58601d81","status":"passed","time":{"start":1659447298000,"stop":1659447298014,"duration":14}},{"uid":"e13085f8833772bf","status":"passed","time":{"start":1659447116000,"stop":1659447146148,"duration":30148}},{"uid":"279c9740b6bbce96","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448205000,"stop":1659448205000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice Config Negative Scenario tests Slice Config Negative Scenario Update Should fail while updating sliceconfig with with invalid DSCP class in qosprofiledetails":{"statistic":{"failed":1,"broken":0,"skipped":4,"passed":34,"unknown":0,"total":39},"items":[{"uid":"1f05c560c987e04f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"5a83bd996154abc7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"b6cbd68a04bc9afc","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"3d6ed365e8eab724","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"1123c64f65947f2f","status":"passed","time":{"start":1660113628000,"stop":1660113628359,"duration":359}},{"uid":"c2bbf1c2465db96e","status":"passed","time":{"start":1660104898000,"stop":1660104898511,"duration":511}},{"uid":"1817a5b1dca82056","status":"passed","time":{"start":1660067259000,"stop":1660067259309,"duration":309}},{"uid":"23037119b86b1c6f","status":"passed","time":{"start":1660047551000,"stop":1660047552326,"duration":1326}},{"uid":"a3bbdca48e2de4fa","status":"passed","time":{"start":1659982549000,"stop":1659982550447,"duration":1447}},{"uid":"5dae4f57370144ec","status":"passed","time":{"start":1659970182000,"stop":1659970182578,"duration":578}},{"uid":"f31d5bbd01606ed","status":"passed","time":{"start":1659944495000,"stop":1659944495984,"duration":984}},{"uid":"95f26a6ec4c4026b","status":"passed","time":{"start":1659874977000,"stop":1659874977811,"duration":811}},{"uid":"853561a32391d1b3","status":"passed","time":{"start":1659496050000,"stop":1659496050537,"duration":537}},{"uid":"1ed821a7e4febfe3","status":"passed","time":{"start":1659496083000,"stop":1659496083825,"duration":825}},{"uid":"189b26cad1f5c6b0","status":"passed","time":{"start":1659451501000,"stop":1659451501933,"duration":933}},{"uid":"293e85ea61fa2596","status":"passed","time":{"start":1659450916000,"stop":1659450916622,"duration":622}},{"uid":"e067886a38c8e886","status":"passed","time":{"start":1659451524000,"stop":1659451524736,"duration":736}},{"uid":"384d004e1737474","status":"passed","time":{"start":1659447298000,"stop":1659447298799,"duration":799}},{"uid":"d9c49a12fddd1caa","status":"passed","time":{"start":1659447116000,"stop":1659447117009,"duration":1009}},{"uid":"f17d19fff35677af","status":"passed","time":{"start":1659448205000,"stop":1659448206226,"duration":1226}}]},"Worker Suite:Worker Suite#[It] Intracluster slice tests Slice with netpol should have vl3 router running":{"statistic":{"failed":18,"broken":0,"skipped":0,"passed":21,"unknown":0,"total":39},"items":[{"uid":"4d3469b67813ea59","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000496af8>: {\n        Underlying: <*exec.ExitError | 0xc0006ffee0>{\n            ProcessState: {\n                pid: 7034,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 168832},\n                    Stime: {Sec: 0, Usec: 64317},\n                    Maxrss: 80856,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 6535,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 375,\n                    Nivcsw: 309,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660808797000,"stop":1660808797484,"duration":484}},{"uid":"2478d1e6452ddce2","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0005cc9f0>: {\n        Underlying: <*exec.ExitError | 0xc000721de0>{\n            ProcessState: {\n                pid: 7213,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 145623},\n                    Stime: {Sec: 0, Usec: 47229},\n                    Maxrss: 85716,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 8175,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 266,\n                    Nivcsw: 403,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660299182000,"stop":1660299182380,"duration":380}},{"uid":"4a198e46afaf19c2","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00044b0c8>: {\n        Underlying: <*exec.ExitError | 0xc000806dc0>{\n            ProcessState: {\n                pid: 7190,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 117535},\n                    Stime: {Sec: 0, Usec: 39178},\n                    Maxrss: 82876,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3815,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 434,\n                    Nivcsw: 254,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660295740000,"stop":1660295740316,"duration":316}},{"uid":"35024475e4e76e70","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00084c780>: {\n        Underlying: <*exec.ExitError | 0xc0008a14a0>{\n            ProcessState: {\n                pid: 7223,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 177523},\n                    Stime: {Sec: 0, Usec: 33814},\n                    Maxrss: 80248,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4303,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 410,\n                    Nivcsw: 356,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660293657000,"stop":1660293657433,"duration":433}},{"uid":"8695f0e807b3dc0a","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 3\nto equal\n    <int>: 1","time":{"start":1660113628000,"stop":1660113697033,"duration":69033}},{"uid":"2a6c5000e4ecf209","status":"passed","time":{"start":1660104898000,"stop":1660104911531,"duration":13531}},{"uid":"134545b34a325aa4","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 2\nto equal\n    <int>: 1","time":{"start":1660067259000,"stop":1660067327837,"duration":68837}},{"uid":"1c906c1908f5806c","status":"passed","time":{"start":1660047551000,"stop":1660047564042,"duration":13042}},{"uid":"530d28c3da1c8fbd","status":"passed","time":{"start":1659982549000,"stop":1659982554283,"duration":5283}},{"uid":"cecedf8545ea7d97","status":"passed","time":{"start":1659970182000,"stop":1659970182776,"duration":776}},{"uid":"e133129792e6b7f1","status":"passed","time":{"start":1659944495000,"stop":1659944495675,"duration":675}},{"uid":"7b1d5934f03e2ce3","status":"passed","time":{"start":1659874977000,"stop":1659874979193,"duration":2193}},{"uid":"a734d4aaae4912e","status":"passed","time":{"start":1659496050000,"stop":1659496050196,"duration":196}},{"uid":"7ece1f268fe05f22","status":"passed","time":{"start":1659496083000,"stop":1659496083332,"duration":332}},{"uid":"ca4b9edaa4586262","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1659451501000,"stop":1659451568038,"duration":67038}},{"uid":"7f71746b6da6edfa","status":"passed","time":{"start":1659450916000,"stop":1659450916544,"duration":544}},{"uid":"5ea01e835e95bdcb","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1659451524000,"stop":1659451592812,"duration":68812}},{"uid":"d9dbf78d8f0d8e07","status":"passed","time":{"start":1659447298000,"stop":1659447299324,"duration":1324}},{"uid":"34204c28b21bd3b9","status":"passed","time":{"start":1659447116000,"stop":1659447116666,"duration":666}},{"uid":"9df4236d25e4e996","status":"passed","time":{"start":1659448205000,"stop":1659448205238,"duration":238}}]},"Empty Suite:Empty Suite#[It] Hub Deletion tests Hub Uninstalltion Test Scenarios Should pass if project is uninstalled first and then hub":{"statistic":{"failed":16,"broken":0,"skipped":0,"passed":30,"unknown":0,"total":46},"items":[{"uid":"958ddf4d619b09ef","status":"passed","time":{"start":1660828377000,"stop":1660828423429,"duration":46429}},{"uid":"89e079b17c2fe792","status":"passed","time":{"start":1660807615000,"stop":1660807731988,"duration":116988}},{"uid":"55ed9da423b28a6f","status":"passed","time":{"start":1660803821000,"stop":1660803942275,"duration":121275}},{"uid":"41249896efc18271","status":"passed","time":{"start":1660805590000,"stop":1660805701445,"duration":111445}},{"uid":"e747398e64d97fed","status":"passed","time":{"start":1660803559000,"stop":1660803604499,"duration":45499}},{"uid":"6f08f92e4faca770","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000e150>: {\n        Underlying: <*exec.ExitError | 0xc0004d6000>{\n            ProcessState: {\n                pid: 5978,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 1, Usec: 373143},\n                    Stime: {Sec: 0, Usec: 249278},\n                    Maxrss: 87500,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3802,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12840,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 16717,\n                    Nivcsw: 4168,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-0.2.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 26 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 26 resources with timeout of 2m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-0.2.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 26 resource(s)\n    wait.go:48: [debug] beginning wait for 26 resources with timeout of 2m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-controller\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-manager-metrics-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-controller-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-editor-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"workerslicegateways.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"clusters.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"projects.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"serviceexportconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"sliceconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workerserviceimports.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workersliceconfigs.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-serving-cert\" Certificate\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-selfsigned-issuer\" Issuer\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-validating-webhook-configuration\" ValidatingWebhookConfiguration\n    uninstall.go:144: [debug] purge requested for kubeslice-controller\n    Error: INSTALLATION FAILED: release kubeslice-controller failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-controller failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1660802965000,"stop":1660803158561,"duration":193561}},{"uid":"62850bfc288563b8","status":"passed","time":{"start":1660798299000,"stop":1660798414628,"duration":115628}},{"uid":"47288603022610cf","status":"passed","time":{"start":1660747846000,"stop":1660747916470,"duration":70470}},{"uid":"c316027157d3f619","status":"passed","time":{"start":1660747702000,"stop":1660747764822,"duration":62822}},{"uid":"d75e1933ff8c7117","status":"passed","time":{"start":1660747595000,"stop":1660747690957,"duration":95957}},{"uid":"b934e1f94a7fe69e","status":"passed","time":{"start":1660747499000,"stop":1660747611773,"duration":112773}},{"uid":"d1eaae9adefdbec0","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00013a768>: {\n        Underlying: <*exec.ExitError | 0xc0003a6040>{\n            ProcessState: {\n                pid: 6030,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 1, Usec: 282184},\n                    Stime: {Sec: 0, Usec: 169766},\n                    Maxrss: 77684,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 5771,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12840,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 14843,\n                    Nivcsw: 3922,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-0.2.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 26 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 26 resources with timeout of 2m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-0.2.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 26 resource(s)\n    wait.go:48: [debug] beginning wait for 26 resources with timeout of 2m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-controller\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-manager-metrics-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-controller-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-editor-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"workerslicegateways.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"clusters.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"projects.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"serviceexportconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"sliceconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workerserviceimports.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workersliceconfigs.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-serving-cert\" Certificate\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-selfsigned-issuer\" Issuer\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-validating-webhook-configuration\" ValidatingWebhookConfiguration\n    uninstall.go:144: [debug] purge requested for kubeslice-controller\n    Error: INSTALLATION FAILED: release kubeslice-controller failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-controller failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1660747927000,"stop":1660748125519,"duration":198519}},{"uid":"25142a2b85dce3d0","status":"passed","time":{"start":1660722528000,"stop":1660722577671,"duration":49671}},{"uid":"bc5debd2fe57502a","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0003e8060>: {\n        Underlying: <*exec.ExitError | 0xc000386060>{\n            ProcessState: {\n                pid: 5965,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 1, Usec: 261899},\n                    Stime: {Sec: 0, Usec: 184668},\n                    Maxrss: 79164,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4909,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12840,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 14819,\n                    Nivcsw: 3340,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-0.2.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 26 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 26 resources with timeout of 2m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-0.2.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 26 resource(s)\n    wait.go:48: [debug] beginning wait for 26 resources with timeout of 2m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-controller\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-manager-metrics-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-controller-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-editor-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"workerslicegateways.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"clusters.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"projects.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"serviceexportconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"sliceconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workerserviceimports.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workersliceconfigs.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-serving-cert\" Certificate\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-selfsigned-issuer\" Issuer\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-validating-webhook-configuration\" ValidatingWebhookConfiguration\n    uninstall.go:144: [debug] purge requested for kubeslice-controller\n    Error: INSTALLATION FAILED: release kubeslice-controller failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-controller failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1660711963000,"stop":1660712158346,"duration":195346}},{"uid":"eab5556afa9a110f","status":"passed","time":{"start":1660635769000,"stop":1660635864610,"duration":95610}},{"uid":"a76aff333e60e643","status":"passed","time":{"start":1660635489000,"stop":1660635610252,"duration":121252}},{"uid":"248ef0ccc1c7f884","status":"passed","time":{"start":1660635683000,"stop":1660635744973,"duration":61973}},{"uid":"154560720cab90b3","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000e138>: {\n        Underlying: <*exec.ExitError | 0xc000426060>{\n            ProcessState: {\n                pid: 5992,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 1, Usec: 404457},\n                    Stime: {Sec: 0, Usec: 200001},\n                    Maxrss: 78644,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 10056,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12840,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 16453,\n                    Nivcsw: 4322,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-0.2.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 26 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 26 resources with timeout of 2m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                       ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-0.2.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 26 resource(s)\n    wait.go:48: [debug] beginning wait for 26 resources with timeout of 2m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-controller\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-manager-metrics-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-controller-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-editor-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"workerslicegateways.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"clusters.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"projects.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"serviceexportconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"sliceconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workerserviceimports.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workersliceconfigs.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-serving-cert\" Certificate\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-selfsigned-issuer\" Issuer\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-validating-webhook-configuration\" ValidatingWebhookConfiguration\n    uninstall.go:144: [debug] purge requested for kubeslice-controller\n    Error: INSTALLATION FAILED: release kubeslice-controller failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-controller failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1660635796000,"stop":1660636009414,"duration":213414}},{"uid":"4e4fd98cf1c0f0","status":"passed","time":{"start":1660631001000,"stop":1660631058538,"duration":57538}},{"uid":"57c8a57f2fa56886","status":"passed","time":{"start":1660623317000,"stop":1660623363555,"duration":46555}}]},"Worker Suite:Worker Suite#[It] Hub netpol scenarios: Slice Configs Netpol scenarios Should fail when deploying with invalid clusters in both applicationNamespace and allowedNamespace":{"statistic":{"failed":0,"broken":0,"skipped":13,"passed":26,"unknown":0,"total":39},"items":[{"uid":"24dfc86c653369c0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"1398910f30c53639","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"fe009663d3aa37aa","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"e45fe61d57c6ebff","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"97a9135b25dc1db1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"cc77ad471859959d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"4e512b6ecc5ca831","status":"passed","time":{"start":1660067259000,"stop":1660067259951,"duration":951}},{"uid":"d04536ee29a113d2","status":"passed","time":{"start":1660047551000,"stop":1660047551847,"duration":847}},{"uid":"65c6d5adb8d816e1","status":"passed","time":{"start":1659982549000,"stop":1659982551208,"duration":2208}},{"uid":"16bca63d16632b71","status":"passed","time":{"start":1659970182000,"stop":1659970182845,"duration":845}},{"uid":"eb238d493077b714","status":"passed","time":{"start":1659944495000,"stop":1659944496129,"duration":1129}},{"uid":"53e9bdbd7b973b2e","status":"passed","time":{"start":1659874977000,"stop":1659874977733,"duration":733}},{"uid":"a8f2ff274bfcb57e","status":"passed","time":{"start":1659496050000,"stop":1659496050633,"duration":633}},{"uid":"ea5305b8bfabad47","status":"passed","time":{"start":1659496083000,"stop":1659496084020,"duration":1020}},{"uid":"dfc37b36f0d3643f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"82e47dbf4d2f55c0","status":"passed","time":{"start":1659450916000,"stop":1659450916740,"duration":740}},{"uid":"1c857c61369cb7be","status":"passed","time":{"start":1659451524000,"stop":1659451525012,"duration":1012}},{"uid":"e1eb2f64ae44238d","status":"passed","time":{"start":1659447298000,"stop":1659447298695,"duration":695}},{"uid":"f23fe3e1bcd60a67","status":"passed","time":{"start":1659447116000,"stop":1659447116831,"duration":831}},{"uid":"cd78b242a915b6cb","status":"passed","time":{"start":1659448205000,"stop":1659448206157,"duration":1157}}]},"Worker Suite:Worker Suite#[It] Intracluster slice tests Iperf connectivity Verify Connectivity between iperf-server and iperf-client":{"statistic":{"failed":1,"broken":0,"skipped":33,"passed":5,"unknown":0,"total":39},"items":[{"uid":"e8fa2bd80b7fb2f1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"2562db67c3bdcf3f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"3b5c0947583c57f8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"7fc120c049725ee4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"77dbe44598075993","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"6d4e04110e2a88ec","status":"passed","time":{"start":1660104898000,"stop":1660104904754,"duration":6754}},{"uid":"c8a1aaa7af80547a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"6722ccd10c859182","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}},{"uid":"a8c49b95007f5f95","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659982549000,"stop":1659982549000,"duration":0}},{"uid":"d665e2ecc605a8d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659970182000,"stop":1659970182000,"duration":0}},{"uid":"84274267ff16eb3c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659944495000,"stop":1659944495000,"duration":0}},{"uid":"d0f1d009a5c9426f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659874977000,"stop":1659874977000,"duration":0}},{"uid":"1d9b542e08465503","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496050000,"stop":1659496050000,"duration":0}},{"uid":"185199e10f0eab04","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496083000,"stop":1659496083000,"duration":0}},{"uid":"f6f24f1fa300dcbe","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"9562505a882387b4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659450916000,"stop":1659450916000,"duration":0}},{"uid":"453e5f5e2531620c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"e6b65184d9cebec5","status":"passed","time":{"start":1659447298000,"stop":1659447312923,"duration":14923}},{"uid":"2fb2eb4ba1e9faf9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447116000,"stop":1659447116000,"duration":0}},{"uid":"fe569fd1070cd3d3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448205000,"stop":1659448205000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Hub netpol scenarios: Slice Configs Netpol scenarios Should fail when deploying with invalid namespace in both applicationNamespace and allowedNamespace":{"statistic":{"failed":0,"broken":0,"skipped":13,"passed":26,"unknown":0,"total":39},"items":[{"uid":"400f724cc595a88a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"c00eec3ebea0edae","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"f2285aa25e713989","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"e39401ceb5e72756","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"eb939a8da12694af","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"9c399eb1f6d9191c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"cd09dd02ce7594c8","status":"passed","time":{"start":1660067259000,"stop":1660067259765,"duration":765}},{"uid":"27c52a30ecc3a82d","status":"passed","time":{"start":1660047551000,"stop":1660047551497,"duration":497}},{"uid":"a1e489095f262be9","status":"passed","time":{"start":1659982549000,"stop":1659982550401,"duration":1401}},{"uid":"6628aa031d37d368","status":"passed","time":{"start":1659970182000,"stop":1659970182667,"duration":667}},{"uid":"a00e32029eb941a2","status":"passed","time":{"start":1659944495000,"stop":1659944495760,"duration":760}},{"uid":"d29704c48c6d8ccf","status":"passed","time":{"start":1659874977000,"stop":1659874977598,"duration":598}},{"uid":"503c485190b7244b","status":"passed","time":{"start":1659496050000,"stop":1659496050441,"duration":441}},{"uid":"d2a83b1f6930c1c","status":"passed","time":{"start":1659496083000,"stop":1659496083693,"duration":693}},{"uid":"431605936bbed0dc","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"f978a69218ba4613","status":"passed","time":{"start":1659450916000,"stop":1659450916752,"duration":752}},{"uid":"4b44f6ce4e273a5e","status":"passed","time":{"start":1659451524000,"stop":1659451524879,"duration":879}},{"uid":"92014a6ebeabb14f","status":"passed","time":{"start":1659447298000,"stop":1659447298440,"duration":440}},{"uid":"a3f635cffa641b1d","status":"passed","time":{"start":1659447116000,"stop":1659447116577,"duration":577}},{"uid":"15591f7fba9052e","status":"passed","time":{"start":1659448205000,"stop":1659448205725,"duration":725}}]},"Hub Suite:Hub Suite#[AfterSuite]":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":168,"unknown":0,"total":168},"items":[{"uid":"88741c6df9fecd1f","status":"passed","time":{"start":1660828517000,"stop":1660828520421,"duration":3421}},{"uid":"7f9df8a62475d818","status":"passed","time":{"start":1660807774000,"stop":1660807776583,"duration":2583}},{"uid":"3fa1a98d3db30a97","status":"passed","time":{"start":1660803998000,"stop":1660804000937,"duration":2937}},{"uid":"f51d834ed8a98482","status":"passed","time":{"start":1660805737000,"stop":1660805739425,"duration":2425}},{"uid":"9bdc6d5177366d9","status":"passed","time":{"start":1660803696000,"stop":1660803698033,"duration":2033}},{"uid":"9ad178ccf09d339a","status":"passed","time":{"start":1660803304000,"stop":1660803305830,"duration":1830}},{"uid":"f38a631c1dcfe2db","status":"passed","time":{"start":1660798468000,"stop":1660798470625,"duration":2625}},{"uid":"f0a4a028de3883c3","status":"passed","time":{"start":1660748009000,"stop":1660748013250,"duration":4250}},{"uid":"a03409d5369c1b19","status":"passed","time":{"start":1660747865000,"stop":1660747868051,"duration":3051}},{"uid":"3e1a23a3287b0cce","status":"passed","time":{"start":1660747735000,"stop":1660747737634,"duration":2634}},{"uid":"c85fcdcc19a67593","status":"passed","time":{"start":1660747654000,"stop":1660747656230,"duration":2230}},{"uid":"f12ce234e4170853","status":"passed","time":{"start":1660748277000,"stop":1660748278465,"duration":1465}},{"uid":"aa692e80963d6145","status":"passed","time":{"start":1660722683000,"stop":1660722685924,"duration":2924}},{"uid":"51238c330736e77f","status":"passed","time":{"start":1660712309000,"stop":1660712310503,"duration":1503}},{"uid":"fda9059886ae4d35","status":"passed","time":{"start":1660635916000,"stop":1660635919096,"duration":3096}},{"uid":"d25a3802e02ff77c","status":"passed","time":{"start":1660635666000,"stop":1660635668961,"duration":2961}},{"uid":"2a5a824269d147e9","status":"passed","time":{"start":1660635860000,"stop":1660635863347,"duration":3347}},{"uid":"b215e78ce61eb001","status":"passed","time":{"start":1660636153000,"stop":1660636154624,"duration":1624}},{"uid":"a79521c68c382a88","status":"passed","time":{"start":1660631164000,"stop":1660631166040,"duration":2040}},{"uid":"3a661937056bab46","status":"passed","time":{"start":1660623463000,"stop":1660623466086,"duration":3086}}]},"Hub Suite:Hub Suite#[It] Cluster CR tests Cluster CR validation Creates cluster secrets":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":124,"unknown":0,"total":124},"items":[{"uid":"dfa57ed179d8f628","status":"passed","time":{"start":1660828517000,"stop":1660828521549,"duration":4549}},{"uid":"29081834d7667bd3","status":"passed","time":{"start":1660807774000,"stop":1660807778203,"duration":4203}},{"uid":"b62bed7bc3008204","status":"passed","time":{"start":1660803998000,"stop":1660804001635,"duration":3635}},{"uid":"d88a011981b21278","status":"passed","time":{"start":1660805737000,"stop":1660805740466,"duration":3466}},{"uid":"63a72290c5219fe4","status":"passed","time":{"start":1660803696000,"stop":1660803699415,"duration":3415}},{"uid":"d053ba184944682f","status":"passed","time":{"start":1660798468000,"stop":1660798471572,"duration":3572}},{"uid":"f3e40701a5598662","status":"passed","time":{"start":1660748009000,"stop":1660748012659,"duration":3659}},{"uid":"e32a995bda463925","status":"passed","time":{"start":1660747865000,"stop":1660747868553,"duration":3553}},{"uid":"7abe0f68f630e898","status":"passed","time":{"start":1660747735000,"stop":1660747738600,"duration":3600}},{"uid":"49d35c63a045d123","status":"passed","time":{"start":1660747654000,"stop":1660747657457,"duration":3457}},{"uid":"f3f4acec97f111bf","status":"passed","time":{"start":1660722683000,"stop":1660722686569,"duration":3569}},{"uid":"798b2865c742a3a9","status":"passed","time":{"start":1660635916000,"stop":1660635919555,"duration":3555}},{"uid":"fdac20a8239b988f","status":"passed","time":{"start":1660635666000,"stop":1660635670408,"duration":4408}},{"uid":"7a823b33cba9aa06","status":"passed","time":{"start":1660635860000,"stop":1660635868901,"duration":8901}},{"uid":"54df5d012c3dffe2","status":"passed","time":{"start":1660631164000,"stop":1660631173335,"duration":9335}},{"uid":"2cd4e3e9b1071a64","status":"passed","time":{"start":1660623463000,"stop":1660623467381,"duration":4381}},{"uid":"dd45d45b6a96a66","status":"passed","time":{"start":1660623384000,"stop":1660623387402,"duration":3402}},{"uid":"d04426efbc465670","status":"passed","time":{"start":1660623360000,"stop":1660623363452,"duration":3452}},{"uid":"41053e3c753e23dd","status":"passed","time":{"start":1660452377000,"stop":1660452381037,"duration":4037}},{"uid":"b43f3d3bf14e2f3a","status":"passed","time":{"start":1660451870000,"stop":1660451873485,"duration":3485}}]},"Istio Suite:Istio Suite#[It] Bookinfo tests Bookinfo Scenario 2 with egress and without ingress Should install Details, Reviews, Ratings and Serviceexport on worker cluster-2":{"statistic":{"failed":0,"broken":0,"skipped":14,"passed":4,"unknown":0,"total":18},"items":[{"uid":"e72d8e0d22de67b6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808584000,"stop":1660808584000,"duration":0}},{"uid":"c62c275c9b07370e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660804840000,"stop":1660804840000,"duration":0}},{"uid":"a470888e3fdd8831","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660804470000,"stop":1660804470000,"duration":0}},{"uid":"a51073ef79834484","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660748849000,"stop":1660748849000,"duration":0}},{"uid":"6c3168b81732d0f1","status":"passed","time":{"start":1660748706000,"stop":1660748803662,"duration":97662}},{"uid":"2482fba91dfb6bdf","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660748558000,"stop":1660748558000,"duration":0}},{"uid":"fa6d4f6badc40626","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660636759000,"stop":1660636759000,"duration":0}},{"uid":"1385293a0b7319fb","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660636482000,"stop":1660636482000,"duration":0}},{"uid":"b8a78ddd56c6fe13","status":"passed","time":{"start":1660632006000,"stop":1660632084172,"duration":78172}},{"uid":"cbb6be300396083a","status":"passed","time":{"start":1660624273000,"stop":1660624305673,"duration":32673}},{"uid":"224987343dfb56be","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660624220000,"stop":1660624220000,"duration":0}},{"uid":"82bfba96d42ae043","status":"passed","time":{"start":1660453205000,"stop":1660453240444,"duration":35444}},{"uid":"85edb49bdab69d25","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660452554000,"stop":1660452554000,"duration":0}},{"uid":"35d41f82da083eb6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660452412000,"stop":1660452412000,"duration":0}},{"uid":"f19921c145921682","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660415422000,"stop":1660415422000,"duration":0}},{"uid":"7d7760d0f7b012c5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660298968000,"stop":1660298968000,"duration":0}},{"uid":"97ae211991c1ae4a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295522000,"stop":1660295522000,"duration":0}},{"uid":"375f90ca953b764b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293414000,"stop":1660293414000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Hub netpol scenarios: Slice Configs Netpol scenarios Should remove successfully while Deleting Slice after removing the applicationNamespace in namespaceIsolationProfile part in slice configs":{"statistic":{"failed":0,"broken":0,"skipped":13,"passed":26,"unknown":0,"total":39},"items":[{"uid":"53c3b1560abc31f5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"2ecb17c64020439a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"c52a50f99fbbb084","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"c1d173ab677fb8dd","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"8dede603bdf60bf","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"ee48266bbafa92a5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"91c3685558aaa853","status":"passed","time":{"start":1660067259000,"stop":1660067269623,"duration":10623}},{"uid":"c034f5d350da7db5","status":"passed","time":{"start":1660047551000,"stop":1660047561652,"duration":10652}},{"uid":"29575edc2e725031","status":"passed","time":{"start":1659982549000,"stop":1659982560771,"duration":11771}},{"uid":"3d4038dd3c38ec45","status":"passed","time":{"start":1659970182000,"stop":1659970192804,"duration":10804}},{"uid":"1dfa514fd214225c","status":"passed","time":{"start":1659944495000,"stop":1659944505902,"duration":10902}},{"uid":"a0e2ff4f6735c742","status":"passed","time":{"start":1659874977000,"stop":1659874987725,"duration":10725}},{"uid":"24f937bf3fbdaa29","status":"passed","time":{"start":1659496050000,"stop":1659496060723,"duration":10723}},{"uid":"823c84e41cd22093","status":"passed","time":{"start":1659496083000,"stop":1659496103819,"duration":20819}},{"uid":"e0d4aae4a83a55ca","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"57ad33fb0eaa5f9","status":"passed","time":{"start":1659450916000,"stop":1659450926683,"duration":10683}},{"uid":"8829c7c1edd56e97","status":"passed","time":{"start":1659451524000,"stop":1659451534686,"duration":10686}},{"uid":"5acd1e6f4b0617c3","status":"passed","time":{"start":1659447298000,"stop":1659447308701,"duration":10701}},{"uid":"bc96cf1e27ca1d80","status":"passed","time":{"start":1659447116000,"stop":1659447126608,"duration":10608}},{"uid":"d2c1891e6a4347cd","status":"passed","time":{"start":1659448205000,"stop":1659448215781,"duration":10781}}]},"Worker Suite:Worker Suite#[It] Intracluster slice tests Slice with netpol should label app ns with kubeslice label":{"statistic":{"failed":0,"broken":0,"skipped":24,"passed":15,"unknown":0,"total":39},"items":[{"uid":"3fc42fab330d955b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"1345357952dd5706","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"4cb071edab3c4f18","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"7eb33bac35287641","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"bfe5d6d1deb00c32","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"5edf25e216623b2f","status":"passed","time":{"start":1660104898000,"stop":1660104898090,"duration":90}},{"uid":"e3057d1f988f5870","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"46644bab25a7659b","status":"passed","time":{"start":1660047551000,"stop":1660047551064,"duration":64}},{"uid":"e3c5a43c4e16d554","status":"passed","time":{"start":1659982549000,"stop":1659982549127,"duration":127}},{"uid":"ee37d770ef63c77c","status":"passed","time":{"start":1659970182000,"stop":1659970182070,"duration":70}},{"uid":"d2faa314c1286ede","status":"passed","time":{"start":1659944495000,"stop":1659944495190,"duration":190}},{"uid":"dbf96b4b039ee320","status":"passed","time":{"start":1659874977000,"stop":1659874977060,"duration":60}},{"uid":"6115a4e440e42165","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496050000,"stop":1659496050000,"duration":0}},{"uid":"b7cc655d84198ebb","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496083000,"stop":1659496083000,"duration":0}},{"uid":"2c3c798b96546973","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"63314094c9aeece","status":"passed","time":{"start":1659450916000,"stop":1659450916105,"duration":105}},{"uid":"b8904633f7bdac32","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"4bbf3c589546d9c8","status":"passed","time":{"start":1659447298000,"stop":1659447298064,"duration":64}},{"uid":"1029d59624cde5de","status":"passed","time":{"start":1659447116000,"stop":1659447116124,"duration":124}},{"uid":"c5991ed4d7bb5695","status":"passed","time":{"start":1659448205000,"stop":1659448205064,"duration":64}}]},"Istio Suite:Istio Suite#[It] Bookinfo tests Bookinfo Scenario 3 without egress and with ingress Should install Productpage on worker cluster-1":{"statistic":{"failed":15,"broken":0,"skipped":0,"passed":3,"unknown":0,"total":18},"items":[{"uid":"eb8c8721b405f0ee","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000600e70>: {\n        Underlying: <*exec.ExitError | 0xc000669980>{\n            ProcessState: {\n                pid: 6858,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 192918},\n                    Stime: {Sec: 0, Usec: 65592},\n                    Maxrss: 82016,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 9777,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 583,\n                    Nivcsw: 531,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicewithoutegresswithingress\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicewithoutegresswithingress\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicewithoutegresswithingress\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicewithoutegresswithingress\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicewithoutegresswithingress\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660808584000,"stop":1660808585187,"duration":1187}},{"uid":"e202326f69ad7085","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1660804840000,"stop":1660805052333,"duration":212333}},{"uid":"378beeba96d2c7e7","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1660804470000,"stop":1660804650301,"duration":180301}},{"uid":"c3dfede9a8b248ad","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1660748849000,"stop":1660749072558,"duration":223558}},{"uid":"6feb37388450eb5e","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004910b0>: {\n        Underlying: <*exec.ExitError | 0xc000700340>{\n            ProcessState: {\n                pid: 7103,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 184207},\n                    Stime: {Sec: 0, Usec: 71171},\n                    Maxrss: 73884,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4042,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 16,\n                    Oublock: 16144,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 436,\n                    Nivcsw: 841,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicewithoutegresswithingress\\\" is invalid: Spec.NamespaceIsolationProfile.ApplicationNamespaces: Invalid value: \\\"bookinfo\\\": The given namespace: bookinfo in cluster cluster1 is already acquired by other slice: slicewithoutegressingress\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicewithoutegresswithingress\\\" is invalid: Spec.NamespaceIsolationProfile.ApplicationNamespaces: Invalid value: \\\"bookinfo\\\": The given namespace: bookinfo in cluster cluster1 is already acquired by other slice: slicewithoutegressingress\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicewithoutegresswithingress\\\" is invalid: Spec.NamespaceIsolationProfile.ApplicationNamespaces: Invalid value: \\\"bookinfo\\\": The given namespace: bookinfo in cluster cluster1 is already acquired by other slice: slicewithoutegressingress\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicewithoutegresswithingress\\\" is invalid: Spec.NamespaceIsolationProfile.ApplicationNamespaces: Invalid value: \\\"bookinfo\\\": The given namespace: bookinfo in cluster cluster1 is already acquired by other slice: slicewithoutegressingress\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicewithoutegresswithingress\" is invalid: Spec.NamespaceIsolationProfile.ApplicationNamespaces: Invalid value: \"bookinfo\": The given namespace: bookinfo in cluster cluster1 is already acquired by other slice: slicewithoutegressingress\noccurred","time":{"start":1660748706000,"stop":1660748706309,"duration":309}},{"uid":"29dc3d057947893e","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1660748558000,"stop":1660748738554,"duration":180554}},{"uid":"c94cfee7be7f37c4","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1660636759000,"stop":1660636939638,"duration":180638}},{"uid":"ea0a23f5e269dff3","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 3\nto equal\n    <int>: 1","time":{"start":1660636482000,"stop":1660636662341,"duration":180341}},{"uid":"c335f45f1c84c894","status":"passed","time":{"start":1660632006000,"stop":1660632030948,"duration":24948}},{"uid":"e32094bbd44754d9","status":"passed","time":{"start":1660624273000,"stop":1660624366597,"duration":93597}},{"uid":"858e7fde2025824c","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 3\nto equal\n    <int>: 1","time":{"start":1660624220000,"stop":1660624400236,"duration":180236}},{"uid":"b69f78f9d2a7b1dd","status":"passed","time":{"start":1660453205000,"stop":1660453235962,"duration":30962}},{"uid":"d673a0fdb5a08d15","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1660452554000,"stop":1660452760057,"duration":206057}},{"uid":"efa92617c5420d31","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1660452412000,"stop":1660452592547,"duration":180547}},{"uid":"7bc501ad973d2c0b","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1660415422000,"stop":1660415602455,"duration":180455}},{"uid":"7ff89ad6ab1816cb","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00098c708>: {\n        Underlying: <*exec.ExitError | 0xc0009c84a0>{\n            ProcessState: {\n                pid: 6902,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 157416},\n                    Stime: {Sec: 0, Usec: 89344},\n                    Maxrss: 79688,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 11386,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 369,\n                    Nivcsw: 466,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicewithoutegresswithingress\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicewithoutegresswithingress\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicewithoutegresswithingress\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicewithoutegresswithingress\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicewithoutegresswithingress\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660298968000,"stop":1660298968222,"duration":222}},{"uid":"c939d164f5bace93","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000478198>: {\n        Underlying: <*exec.ExitError | 0xc0004facc0>{\n            ProcessState: {\n                pid: 6947,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 145497},\n                    Stime: {Sec: 0, Usec: 51120},\n                    Maxrss: 84412,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 12739,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 412,\n                    Nivcsw: 303,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicewithoutegresswithingress\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicewithoutegresswithingress\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicewithoutegresswithingress\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicewithoutegresswithingress\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicewithoutegresswithingress\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660295522000,"stop":1660295522182,"duration":182}},{"uid":"da3974c3889dc656","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0007a0db0>: {\n        Underlying: <*exec.ExitError | 0xc00091b9e0>{\n            ProcessState: {\n                pid: 6874,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 205276},\n                    Stime: {Sec: 0, Usec: 55266},\n                    Maxrss: 82988,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 7705,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 338,\n                    Nivcsw: 468,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicewithoutegresswithingress\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicewithoutegresswithingress\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicewithoutegresswithingress\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicewithoutegresswithingress\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicewithoutegresswithingress\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660293414000,"stop":1660293414298,"duration":298}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Iperf connectivity should onboard deployment in application Namespaces without slice annotations":{"statistic":{"failed":18,"broken":0,"skipped":20,"passed":1,"unknown":0,"total":39},"items":[{"uid":"dd40c43b37cfcc22","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"3fde2ff935cb2ac3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"c46cb01428e4c7ee","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"b18d26fd781c5b52","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"2fd7096c25e04333","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"1209d2fb5dd692a2","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"d6667b5009dc2e81","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 1\nto equal\n    <int>: 2","time":{"start":1660067259000,"stop":1660067440053,"duration":181053}},{"uid":"3e82acd47805ef73","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 1\nto equal\n    <int>: 2","time":{"start":1660047551000,"stop":1660047732291,"duration":181291}},{"uid":"ecbe12fffc6f2fc2","status":"failed","statusDetails":"Timed out after 180.113s.\nExpected\n    <int>: 1\nto equal\n    <int>: 2","time":{"start":1659982549000,"stop":1659982730823,"duration":181823}},{"uid":"515b232973533e68","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 2","time":{"start":1659970182000,"stop":1659970363279,"duration":181279}},{"uid":"231bebb33f179f73","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659944495000,"stop":1659944495000,"duration":0}},{"uid":"a655fa59ce18c093","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659874977000,"stop":1659874977000,"duration":0}},{"uid":"212ff630a85f8f4c","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 2","time":{"start":1659496050000,"stop":1659496230994,"duration":180994}},{"uid":"4f99a9b9d93fc74a","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 1\nto equal\n    <int>: 2","time":{"start":1659496083000,"stop":1659496265526,"duration":182526}},{"uid":"2600f8f8a8665b91","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 2","time":{"start":1659451501000,"stop":1659451681685,"duration":180685}},{"uid":"8a69ccbccdfa5ba0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659450916000,"stop":1659450916000,"duration":0}},{"uid":"3c712671ed8f4c23","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 2","time":{"start":1659451524000,"stop":1659451704899,"duration":180899}},{"uid":"22380fbc43156edb","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447298000,"stop":1659447298000,"duration":0}},{"uid":"65c258ad2f05e8aa","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447116000,"stop":1659447116000,"duration":0}},{"uid":"8d836a9cc9b558e5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448205000,"stop":1659448205000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Namespace reconcile test scenarios: Offboard remove label from app ns":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":37,"unknown":0,"total":39},"items":[{"uid":"bd98bd3c5fa6fa56","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"f32a5516494d50f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"13d279c71956fc1b","status":"passed","time":{"start":1660295740000,"stop":1660295740167,"duration":167}},{"uid":"bc48a1037f396a73","status":"passed","time":{"start":1660293657000,"stop":1660293657204,"duration":204}},{"uid":"c592ef78edda75fd","status":"passed","time":{"start":1660113628000,"stop":1660113628146,"duration":146}},{"uid":"8d7e85c04a75cc84","status":"passed","time":{"start":1660104898000,"stop":1660104898168,"duration":168}},{"uid":"5a057c865089ab3f","status":"passed","time":{"start":1660067259000,"stop":1660067259293,"duration":293}},{"uid":"f84cf2131e852cf7","status":"passed","time":{"start":1660047551000,"stop":1660047551172,"duration":172}},{"uid":"475e46aeb5950aa4","status":"passed","time":{"start":1659982549000,"stop":1659982549580,"duration":580}},{"uid":"c96ba7f9487f77ad","status":"passed","time":{"start":1659970182000,"stop":1659970182169,"duration":169}},{"uid":"c4bcbe9b30eb7fe6","status":"passed","time":{"start":1659944495000,"stop":1659944495330,"duration":330}},{"uid":"7cdb927b81367b62","status":"passed","time":{"start":1659874977000,"stop":1659874977293,"duration":293}},{"uid":"bad66ac4d56419f","status":"passed","time":{"start":1659496050000,"stop":1659496050169,"duration":169}},{"uid":"58723cc69358a373","status":"passed","time":{"start":1659496083000,"stop":1659496083175,"duration":175}},{"uid":"8e848df75fd7ccde","status":"passed","time":{"start":1659451501000,"stop":1659451501164,"duration":164}},{"uid":"a65990e896883e65","status":"passed","time":{"start":1659450916000,"stop":1659450916326,"duration":326}},{"uid":"82cfc5628770e70c","status":"passed","time":{"start":1659451524000,"stop":1659451524311,"duration":311}},{"uid":"1618f9311d8562ba","status":"passed","time":{"start":1659447298000,"stop":1659447298176,"duration":176}},{"uid":"4494683dc7d456ab","status":"passed","time":{"start":1659447116000,"stop":1659447116334,"duration":334}},{"uid":"6b6d098c40028976","status":"passed","time":{"start":1659448205000,"stop":1659448205151,"duration":151}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting nsm-kernel-forwarder pod Check ping between iperf-server and iperf-client after nsm-kernel-forwarder pod restart":{"statistic":{"failed":0,"broken":0,"skipped":39,"passed":0,"unknown":0,"total":39},"items":[{"uid":"aebc1bd80316ba7e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"85487c84e8ca86f8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"ad8edf6be7023e17","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"3e7806c0562b5ac0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"cb93d54740688c87","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"1f9053edbbdc4cec","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"d5adb1757d825c89","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"ff2adf10077a1759","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}},{"uid":"49b5c0effe699e0c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659982549000,"stop":1659982549000,"duration":0}},{"uid":"896402dc7b639bd2","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659970182000,"stop":1659970182000,"duration":0}},{"uid":"ebe8b3e4d7a5106a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659944495000,"stop":1659944495000,"duration":0}},{"uid":"7a8c6b97a9e7a693","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659874977000,"stop":1659874977000,"duration":0}},{"uid":"481e2245ac2762d8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496050000,"stop":1659496050000,"duration":0}},{"uid":"4ba1b9a425ffe5a3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496083000,"stop":1659496083000,"duration":0}},{"uid":"6da10e1560b2d62a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"ac448248b82bac62","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659450916000,"stop":1659450916000,"duration":0}},{"uid":"20a18f47047560b9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"5bcc292d25fb135e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447298000,"stop":1659447298000,"duration":0}},{"uid":"e302a7de03d1c598","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447116000,"stop":1659447116000,"duration":0}},{"uid":"255c055f8fe216cc","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448205000,"stop":1659448205000,"duration":0}}]},"Istio Suite:Istio Suite#[AfterSuite]":{"statistic":{"failed":28,"broken":14,"skipped":0,"passed":4,"unknown":0,"total":46},"items":[{"uid":"71c6fb5835551be1","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000388270>: {\n        Underlying: <*exec.ExitError | 0xc000611820>{\n            ProcessState: {\n                pid: 6805,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 54192},\n                    Stime: {Sec: 0, Usec: 13548},\n                    Maxrss: 39792,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 1887,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 245,\n                    Nivcsw: 139,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    Error: uninstall: Release not loaded: istiod: release: not found\noccurred","time":{"start":1660829454000,"stop":1660829455798,"duration":1798}},{"uid":"616a4aa9daef4670","status":"passed","time":{"start":1660808584000,"stop":1660808615002,"duration":31002}},{"uid":"ac19998fe7665dfa","status":"broken","statusDetails":"interrupted","time":{"start":1660804840000,"stop":1660806302843,"duration":1462843}},{"uid":"16d629160839170f","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000606510>: {\n        Underlying: <*exec.ExitError | 0xc000469dc0>{\n            ProcessState: {\n                pid: 6870,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 32382},\n                    Stime: {Sec: 0, Usec: 17990},\n                    Maxrss: 42988,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2234,\n                    Majflt: 2,\n                    Nswap: 0,\n                    Inblock: 312,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 192,\n                    Nivcsw: 92,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    Error: uninstall: Release not loaded: istiod: release: not found\noccurred","time":{"start":1660806662000,"stop":1660806663188,"duration":1188}},{"uid":"e96bac4b74983397","status":"broken","statusDetails":"interrupted","time":{"start":1660804470000,"stop":1660806146638,"duration":1676638}},{"uid":"e0ab9ecb8dec1b2c","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000133de8>: {\n        Underlying: <*exec.ExitError | 0xc000422900>{\n            ProcessState: {\n                pid: 6071,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 41947},\n                    Stime: {Sec: 0, Usec: 10486},\n                    Maxrss: 39552,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2362,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 188,\n                    Nivcsw: 171,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    Error: uninstall: Release not loaded: istiod: release: not found\noccurred","time":{"start":1660803509000,"stop":1660803509272,"duration":272}},{"uid":"d9333a6686e3d06a","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000381788>: {\n        Underlying: <*exec.ExitError | 0xc0006e4200>{\n            ProcessState: {\n                pid: 6839,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 44963},\n                    Stime: {Sec: 0, Usec: 7493},\n                    Maxrss: 43456,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2507,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 119,\n                    Nivcsw: 70,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    Error: uninstall: Release not loaded: istiod: release: not found\noccurred","time":{"start":1660799371000,"stop":1660799372373,"duration":1373}},{"uid":"949a60e7a6009669","status":"broken","statusDetails":"interrupted","time":{"start":1660748849000,"stop":1660750243567,"duration":1394567}},{"uid":"e40ec55c839c7d8","status":"broken","statusDetails":"interrupted","time":{"start":1660748706000,"stop":1660750374188,"duration":1668188}},{"uid":"e805a81c7d02533a","status":"broken","statusDetails":"interrupted","time":{"start":1660748558000,"stop":1660750133845,"duration":1575845}},{"uid":"9b2c02f746a6683f","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000ef00>: {\n        Underlying: <*exec.ExitError | 0xc0006c7b80>{\n            ProcessState: {\n                pid: 6889,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 37394},\n                    Stime: {Sec: 0, Usec: 9348},\n                    Maxrss: 39944,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2335,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 203,\n                    Nivcsw: 251,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    Error: uninstall: Release not loaded: istiod: release: not found\noccurred","time":{"start":1660748560000,"stop":1660748561210,"duration":1210}},{"uid":"e394156597a8b2f8","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000408d98>: {\n        Underlying: <*exec.ExitError | 0xc0005f14c0>{\n            ProcessState: {\n                pid: 6123,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 47257},\n                    Stime: {Sec: 0, Usec: 3938},\n                    Maxrss: 40324,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 1896,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 296,\n                    Nivcsw: 316,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    Error: uninstall: Release not loaded: istiod: release: not found\noccurred","time":{"start":1660748476000,"stop":1660748476261,"duration":261}},{"uid":"9067e5819d508ef6","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0006b0b28>: {\n        Underlying: <*exec.ExitError | 0xc000685640>{\n            ProcessState: {\n                pid: 6876,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 56013},\n                    Stime: {Sec: 0, Usec: 7468},\n                    Maxrss: 40228,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 1900,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 260,\n                    Nivcsw: 192,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    Error: uninstall: Release not loaded: istiod: release: not found\noccurred","time":{"start":1660723610000,"stop":1660723611446,"duration":1446}},{"uid":"9a63ade9ba816cb6","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0003f1578>: {\n        Underlying: <*exec.ExitError | 0xc00059e6e0>{\n            ProcessState: {\n                pid: 6055,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 41529},\n                    Stime: {Sec: 0, Usec: 0},\n                    Maxrss: 39780,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 1893,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 168,\n                    Nivcsw: 48,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    Error: uninstall: Release not loaded: istiod: release: not found\noccurred","time":{"start":1660712528000,"stop":1660712528204,"duration":204}},{"uid":"b98ec38d3c7245bc","status":"broken","statusDetails":"interrupted","time":{"start":1660636759000,"stop":1660638282443,"duration":1523443}},{"uid":"b36f79ba3630d5fb","status":"broken","statusDetails":"interrupted","time":{"start":1660636482000,"stop":1660638006872,"duration":1524872}},{"uid":"899fb24e3cf73e05","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0001e5668>: {\n        Underlying: <*exec.ExitError | 0xc00078b4a0>{\n            ProcessState: {\n                pid: 6896,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 50144},\n                    Stime: {Sec: 0, Usec: 18804},\n                    Maxrss: 43180,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 1874,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 170,\n                    Nivcsw: 299,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    Error: uninstall: Release not loaded: istiod: release: not found\noccurred","time":{"start":1660636748000,"stop":1660636749530,"duration":1530}},{"uid":"749b290d2e58fdd6","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0008ffe30>: {\n        Underlying: <*exec.ExitError | 0xc000605b20>{\n            ProcessState: {\n                pid: 6080,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 45740},\n                    Stime: {Sec: 0, Usec: 3518},\n                    Maxrss: 42092,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2403,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 234,\n                    Nivcsw: 102,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    Error: uninstall: Release not loaded: istiod: release: not found\noccurred","time":{"start":1660636362000,"stop":1660636362214,"duration":214}},{"uid":"fae240793b34e31","status":"broken","statusDetails":"interrupted","time":{"start":1660632006000,"stop":1660633606156,"duration":1600156}},{"uid":"902329f82fe172ed","status":"broken","statusDetails":"interrupted","time":{"start":1660624273000,"stop":1660625929552,"duration":1656552}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Creation Should fail while creating Project with  project name as blank":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":124,"unknown":0,"total":124},"items":[{"uid":"408ace620a57a2af","status":"passed","time":{"start":1660828517000,"stop":1660828517268,"duration":268}},{"uid":"69534394bfd6962e","status":"passed","time":{"start":1660807774000,"stop":1660807774220,"duration":220}},{"uid":"9bae4228f35e6069","status":"passed","time":{"start":1660803998000,"stop":1660803998267,"duration":267}},{"uid":"fe8d9c498368ef26","status":"passed","time":{"start":1660805737000,"stop":1660805737202,"duration":202}},{"uid":"18202c2113cfca76","status":"passed","time":{"start":1660803696000,"stop":1660803696194,"duration":194}},{"uid":"57ed42ee9c517b38","status":"passed","time":{"start":1660798468000,"stop":1660798468254,"duration":254}},{"uid":"47ff46d8300f8495","status":"passed","time":{"start":1660748009000,"stop":1660748009354,"duration":354}},{"uid":"bfdde25b0b6de017","status":"passed","time":{"start":1660747865000,"stop":1660747865265,"duration":265}},{"uid":"ee259aa39a25b648","status":"passed","time":{"start":1660747735000,"stop":1660747735263,"duration":263}},{"uid":"70d7fdcc124f71b7","status":"passed","time":{"start":1660747654000,"stop":1660747654229,"duration":229}},{"uid":"8df1404c06751cf1","status":"passed","time":{"start":1660722683000,"stop":1660722683274,"duration":274}},{"uid":"6457a17f62a4c5d5","status":"passed","time":{"start":1660635916000,"stop":1660635916255,"duration":255}},{"uid":"7ccc83344b36bcb","status":"passed","time":{"start":1660635666000,"stop":1660635666280,"duration":280}},{"uid":"355d79fb9c24ad92","status":"passed","time":{"start":1660635860000,"stop":1660635860293,"duration":293}},{"uid":"c023b45b00745863","status":"passed","time":{"start":1660631164000,"stop":1660631164193,"duration":193}},{"uid":"5c7523b6f3a699bc","status":"passed","time":{"start":1660623463000,"stop":1660623463280,"duration":280}},{"uid":"1d5756fce4209b57","status":"passed","time":{"start":1660623384000,"stop":1660623384178,"duration":178}},{"uid":"ff3b482f7f147b42","status":"passed","time":{"start":1660623360000,"stop":1660623360215,"duration":215}},{"uid":"f6c90e21e538819e","status":"passed","time":{"start":1660452377000,"stop":1660452377278,"duration":278}},{"uid":"8bcf98f9f176b8b0","status":"passed","time":{"start":1660451870000,"stop":1660451870219,"duration":219}}]},"Worker Suite:Worker Suite#[It] Slice Creation scenarios Slice with MEDIUM QOS should have gateway pod running":{"statistic":{"failed":4,"broken":0,"skipped":8,"passed":18,"unknown":0,"total":30},"items":[{"uid":"6b1267010d9a1c54","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"672d3c8786f95f79","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"be8ca8ee7c8b0fe7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"9032582f8f13f746","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"bd69a0674582e107","status":"failed","statusDetails":"Timed out after 180.004s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1660113628000,"stop":1660113808378,"duration":180378}},{"uid":"741098da46901686","status":"failed","statusDetails":"Timed out after 180.004s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1660104898000,"stop":1660105078192,"duration":180192}},{"uid":"b9eec9322c615330","status":"passed","time":{"start":1660067259000,"stop":1660067314130,"duration":55130}},{"uid":"af70513e880982f5","status":"passed","time":{"start":1660047551000,"stop":1660047571086,"duration":20086}},{"uid":"314decebdc74be0","status":"passed","time":{"start":1659982549000,"stop":1659982564116,"duration":15116}},{"uid":"8db7ec436bace433","status":"passed","time":{"start":1659970182000,"stop":1659970267207,"duration":85207}},{"uid":"f0aa9620e97bdf2a","status":"passed","time":{"start":1659944495000,"stop":1659944565226,"duration":70226}},{"uid":"7472208dc64691e7","status":"passed","time":{"start":1659874977000,"stop":1659875042154,"duration":65154}},{"uid":"39c2fec0ae0a8a95","status":"passed","time":{"start":1659496050000,"stop":1659496095111,"duration":45111}},{"uid":"74022a8e4572a784","status":"passed","time":{"start":1659496083000,"stop":1659496103054,"duration":20054}},{"uid":"aca437232317c056","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"a2eaaf0de29fac99","status":"passed","time":{"start":1659450916000,"stop":1659450941084,"duration":25084}},{"uid":"b92ab2ba7e2fe05","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"603367f12194637","status":"passed","time":{"start":1659447298000,"stop":1659447333064,"duration":35064}},{"uid":"b42fc16ddac01857","status":"passed","time":{"start":1659447116000,"stop":1659447151117,"duration":35117}},{"uid":"c571dfc4b376bac","status":"failed","statusDetails":"Timed out after 180.004s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1659448205000,"stop":1659448385271,"duration":180271}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests Iperf App connectivity Verify Iperf traffic":{"statistic":{"failed":0,"broken":0,"skipped":32,"passed":7,"unknown":0,"total":39},"items":[{"uid":"528127249e998e0a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"a0bc99e2d05b09c8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"ffc67f6897dc058f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"a194cb0c4891fb5d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"641fe44b3bdaeb15","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"d19d5ebb840dfb4f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"90100b52a9767d0b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"f5feb2f611247554","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}},{"uid":"6c0fe63647f97c8a","status":"passed","time":{"start":1659982549000,"stop":1659982610912,"duration":61912}},{"uid":"b9710d8c2178aa33","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659970182000,"stop":1659970182000,"duration":0}},{"uid":"a5051b74f896dbb0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659944495000,"stop":1659944495000,"duration":0}},{"uid":"10a127afc20b747c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659874977000,"stop":1659874977000,"duration":0}},{"uid":"11a8b2cd8b141cc4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496050000,"stop":1659496050000,"duration":0}},{"uid":"98bc7359231964ba","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496083000,"stop":1659496083000,"duration":0}},{"uid":"97362200db55efc5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"9fabdb0144309f20","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659450916000,"stop":1659450916000,"duration":0}},{"uid":"eda5aba09a87b585","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"422d95be88aef462","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447298000,"stop":1659447298000,"duration":0}},{"uid":"5dcb0a2807e8881a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447116000,"stop":1659447116000,"duration":0}},{"uid":"6e07759cf7c004c1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448205000,"stop":1659448205000,"duration":0}}]},"Worker Suite:Worker Suite#[It] NodeRestartSlice Testing node restart should have vl3 router running":{"statistic":{"failed":22,"broken":0,"skipped":0,"passed":8,"unknown":0,"total":30},"items":[{"uid":"e6f7ca5e15c401fc","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1660808797000,"stop":1660808857312,"duration":60312}},{"uid":"9707232065015d73","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1660299182000,"stop":1660299242232,"duration":60232}},{"uid":"ef52de3f0fa8e531","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1660295740000,"stop":1660295800296,"duration":60296}},{"uid":"4e31aeeec9603f23","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1660293657000,"stop":1660293718457,"duration":61457}},{"uid":"6c1e22e56abddc59","status":"passed","time":{"start":1660113628000,"stop":1660113629169,"duration":1169}},{"uid":"b640df5e971a0db2","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 2\nto equal\n    <int>: 1","time":{"start":1660104898000,"stop":1660104960822,"duration":62822}},{"uid":"d00e68b993ab4788","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1660067259000,"stop":1660067321581,"duration":62581}},{"uid":"c33b3f5965b650ec","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1660047551000,"stop":1660047611568,"duration":60568}},{"uid":"b1e5ef14f3683a9d","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1659982549000,"stop":1659982612188,"duration":63188}},{"uid":"16ea56aba9a47d07","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1659970182000,"stop":1659970244642,"duration":62642}},{"uid":"6e2a0f54d6b2e2e2","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1659944495000,"stop":1659944557958,"duration":62958}},{"uid":"addc8e564feecc57","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <int>: 2\nto equal\n    <int>: 1","time":{"start":1659874977000,"stop":1659875039856,"duration":62856}},{"uid":"96407de582dc8a1d","status":"passed","time":{"start":1659496050000,"stop":1659496053130,"duration":3130}},{"uid":"2bd6e89e54b21bab","status":"passed","time":{"start":1659496083000,"stop":1659496085231,"duration":2231}},{"uid":"83ff197bc09c9c5a","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1659451501000,"stop":1659451563892,"duration":62892}},{"uid":"9ac703d735ddfdb9","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1659450916000,"stop":1659450976645,"duration":60645}},{"uid":"e0c541f67e7d9756","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1659451524000,"stop":1659451586765,"duration":62765}},{"uid":"da43ed19f9613155","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1659447298000,"stop":1659447360620,"duration":62620}},{"uid":"5cc13e3aac5f58f1","status":"passed","time":{"start":1659447116000,"stop":1659447118224,"duration":2224}},{"uid":"2a8612777758ed03","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 3\nto equal\n    <int>: 1","time":{"start":1659448205000,"stop":1659448267976,"duration":62976}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests slice with netpol Should label all application namespaces with kubeslice namespace":{"statistic":{"failed":0,"broken":0,"skipped":29,"passed":10,"unknown":0,"total":39},"items":[{"uid":"c5a6f9b1e5589237","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"ea0f571e1703d74c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"6add127231c55114","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"7837df298b71b402","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"569ccbb531b9abc3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"22e718ca030d26df","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"3857180907e76007","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"67ee87bb3198af92","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}},{"uid":"403d70710b91dc8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659982549000,"stop":1659982549000,"duration":0}},{"uid":"dec92fde9f73d64f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659970182000,"stop":1659970182000,"duration":0}},{"uid":"ed85f7f5eb8f6bc7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659944495000,"stop":1659944495000,"duration":0}},{"uid":"54a86c6bfc9b6d72","status":"passed","time":{"start":1659874977000,"stop":1659874977148,"duration":148}},{"uid":"30e9a9d997ef4688","status":"passed","time":{"start":1659496050000,"stop":1659496050139,"duration":139}},{"uid":"f81f67cea2e0304b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496083000,"stop":1659496083000,"duration":0}},{"uid":"4b1fea54f6f64bb9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"e49b868bfb6a2953","status":"passed","time":{"start":1659450916000,"stop":1659450916260,"duration":260}},{"uid":"48c72a15bbff68f5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"26847aaab7706cd7","status":"passed","time":{"start":1659447298000,"stop":1659447298219,"duration":219}},{"uid":"eae0b162075194cf","status":"passed","time":{"start":1659447116000,"stop":1659447116137,"duration":137}},{"uid":"a3056aab6bc1cb13","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448205000,"stop":1659448205000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice Creation scenarios Slice with MEDIUM QOS should reconcile medium qos slice":{"statistic":{"failed":0,"broken":0,"skipped":12,"passed":18,"unknown":0,"total":30},"items":[{"uid":"2f1612e3bfd1d243","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"3746b2ba88c57bd5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"91900d866d8d3c2b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"57daac34bf55c1af","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"5ba4f84babc0480a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"9ef9632d99cd5c01","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"e716b45d51b81686","status":"passed","time":{"start":1660067259000,"stop":1660067259509,"duration":509}},{"uid":"150e6c7673077fbd","status":"passed","time":{"start":1660047551000,"stop":1660047551461,"duration":461}},{"uid":"c51130d08eef9594","status":"passed","time":{"start":1659982549000,"stop":1659982550042,"duration":1042}},{"uid":"3cb95c89d23e1acb","status":"passed","time":{"start":1659970182000,"stop":1659970182324,"duration":324}},{"uid":"246b3d843e07aba4","status":"passed","time":{"start":1659944495000,"stop":1659944495499,"duration":499}},{"uid":"60c55cb8baf95399","status":"passed","time":{"start":1659874977000,"stop":1659874977400,"duration":400}},{"uid":"557294ffe7d5b72b","status":"passed","time":{"start":1659496050000,"stop":1659496050330,"duration":330}},{"uid":"d32002dc559f0163","status":"passed","time":{"start":1659496083000,"stop":1659496083505,"duration":505}},{"uid":"ece47bd48dea4cae","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"57f071e5e850192","status":"passed","time":{"start":1659450916000,"stop":1659450916408,"duration":408}},{"uid":"490f343779140d7f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"76d4318ce77a623f","status":"passed","time":{"start":1659447298000,"stop":1659447298301,"duration":301}},{"uid":"d60e2ea00e0c1712","status":"passed","time":{"start":1659447116000,"stop":1659447116484,"duration":484}},{"uid":"165fdb58f13b18aa","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448205000,"stop":1659448205000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Iperf connectivity Verify Connectivity between iperf-server and iperf-client":{"statistic":{"failed":1,"broken":0,"skipped":38,"passed":0,"unknown":0,"total":39},"items":[{"uid":"7c90237538704011","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"9c9283b2f17a3ebc","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"ed0ba7f9b7b79cff","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"5be50ad18496c366","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"b04bcacbaa34becd","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"1fbdf937900a2a80","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"63f31b30f54bd7e4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"79a6b75a1e593857","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}},{"uid":"9f940cdab7e2e09","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659982549000,"stop":1659982549000,"duration":0}},{"uid":"9eacca028fa18ed2","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659970182000,"stop":1659970182000,"duration":0}},{"uid":"489ed7a1245efa1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659944495000,"stop":1659944495000,"duration":0}},{"uid":"40b80cc2fb0c403","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659874977000,"stop":1659874977000,"duration":0}},{"uid":"50271fbbf5702175","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496050000,"stop":1659496050000,"duration":0}},{"uid":"c1e0c5d8e8cb29c8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496083000,"stop":1659496083000,"duration":0}},{"uid":"aed1670f527a1e73","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"a84445354affae75","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659450916000,"stop":1659450916000,"duration":0}},{"uid":"dbe62a3cedbfa3e6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"727f043ed905c18c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447298000,"stop":1659447298000,"duration":0}},{"uid":"d1dfbcdc6a688d99","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447116000,"stop":1659447116000,"duration":0}},{"uid":"30b44fe10693cf2e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448205000,"stop":1659448205000,"duration":0}}]},"Worker Suite:Worker Suite#[It] NodeRestartSlice Testing node restart iperf connectivity across multi cluster":{"statistic":{"failed":8,"broken":0,"skipped":22,"passed":0,"unknown":0,"total":30},"items":[{"uid":"27799419333af5b1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"322ffeb50fc37847","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"c123415bfa37b331","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"37637f232c472e3f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"c40ac25977c86333","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <bool>: false\nto be true","time":{"start":1660113628000,"stop":1660113688454,"duration":60454}},{"uid":"cc87a733b2c3b626","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"8b10849df7b7784c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"92a4cbe8f2ecc3e3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}},{"uid":"ab20c80024237ae4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659982549000,"stop":1659982549000,"duration":0}},{"uid":"58a6c30bd1570603","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659970182000,"stop":1659970182000,"duration":0}},{"uid":"96f1b4a11a1b3aaa","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659944495000,"stop":1659944495000,"duration":0}},{"uid":"4c1b284d48408fbd","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659874977000,"stop":1659874977000,"duration":0}},{"uid":"b05022508c6db13d","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <bool>: false\nto be true","time":{"start":1659496050000,"stop":1659496112426,"duration":62426}},{"uid":"8ce9054290a1b90e","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <bool>: false\nto be true","time":{"start":1659496083000,"stop":1659496145658,"duration":62658}},{"uid":"bd36be5e7f75aafb","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"b093d0e7d364fdcc","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659450916000,"stop":1659450916000,"duration":0}},{"uid":"7ad701133e3dab39","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"dd4284be57569697","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447298000,"stop":1659447298000,"duration":0}},{"uid":"e9b3bef974962fb6","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <bool>: false\nto be true","time":{"start":1659447116000,"stop":1659447176433,"duration":60433}},{"uid":"4c1ee93a3bb85218","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448205000,"stop":1659448205000,"duration":0}}]},"Hub Suite:Hub Suite#[It] Cluster negative tests Worker Cluster Registration with Wrong token":{"statistic":{"failed":0,"broken":0,"skipped":17,"passed":107,"unknown":0,"total":124},"items":[{"uid":"f95dfa8efb9c3059","status":"passed","time":{"start":1660828517000,"stop":1660828648448,"duration":131448}},{"uid":"a962820f26f693d6","status":"passed","time":{"start":1660807774000,"stop":1660807904129,"duration":130129}},{"uid":"466f8649ced8557e","status":"passed","time":{"start":1660803998000,"stop":1660804133414,"duration":135414}},{"uid":"585795b88a1170d1","status":"passed","time":{"start":1660805737000,"stop":1660805871518,"duration":134518}},{"uid":"8752bb1b9437815f","status":"passed","time":{"start":1660803696000,"stop":1660803830511,"duration":134511}},{"uid":"cf1bb61bbb89fced","status":"passed","time":{"start":1660798468000,"stop":1660798597945,"duration":129945}},{"uid":"7c323b86093c1ac0","status":"passed","time":{"start":1660748009000,"stop":1660748140516,"duration":131516}},{"uid":"bfaca18354a0d3f3","status":"passed","time":{"start":1660747865000,"stop":1660748000492,"duration":135492}},{"uid":"8573bf45f518ff33","status":"passed","time":{"start":1660747735000,"stop":1660747865304,"duration":130304}},{"uid":"24150a85f9a139b2","status":"passed","time":{"start":1660747654000,"stop":1660747788519,"duration":134519}},{"uid":"85decdc6d651c6cd","status":"passed","time":{"start":1660722683000,"stop":1660722817986,"duration":134986}},{"uid":"5c4c8b52b937260f","status":"passed","time":{"start":1660635916000,"stop":1660636051821,"duration":135821}},{"uid":"7fc934d0547bcbb1","status":"passed","time":{"start":1660635666000,"stop":1660635796286,"duration":130286}},{"uid":"8c9ef4ab2890183e","status":"passed","time":{"start":1660635860000,"stop":1660635991243,"duration":131243}},{"uid":"ff2094819d738758","status":"passed","time":{"start":1660631164000,"stop":1660631293293,"duration":129293}},{"uid":"bc31a3786e2b338f","status":"passed","time":{"start":1660623463000,"stop":1660623593126,"duration":130126}},{"uid":"e2d5700280c6d0b3","status":"passed","time":{"start":1660623384000,"stop":1660623518673,"duration":134673}},{"uid":"b1dc58cbd22db88c","status":"passed","time":{"start":1660623360000,"stop":1660623489330,"duration":129330}},{"uid":"d93199a2291ee48","status":"passed","time":{"start":1660452377000,"stop":1660452509052,"duration":132052}},{"uid":"a4ef3c7221544848","status":"passed","time":{"start":1660451870000,"stop":1660452004277,"duration":134277}}]},"Worker Suite:Worker Suite#[It] Slice Creation scenarios Slice with MEDIUM QOS should have vl3 router running":{"statistic":{"failed":8,"broken":0,"skipped":0,"passed":22,"unknown":0,"total":30},"items":[{"uid":"e96433d83a0cae1","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004dd020>: {\n        Underlying: <*exec.ExitError | 0xc0007d47e0>{\n            ProcessState: {\n                pid: 7217,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 130784},\n                    Stime: {Sec: 0, Usec: 54493},\n                    Maxrss: 89336,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3769,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 261,\n                    Nivcsw: 265,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicemediumqos\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660808797000,"stop":1660808797260,"duration":260}},{"uid":"d5c79c81ac0886da","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0005cc258>: {\n        Underlying: <*exec.ExitError | 0xc0007209c0>{\n            ProcessState: {\n                pid: 7120,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 133532},\n                    Stime: {Sec: 0, Usec: 34460},\n                    Maxrss: 85304,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 6881,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 377,\n                    Nivcsw: 265,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicemediumqos\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660299182000,"stop":1660299182227,"duration":227}},{"uid":"6454ddafda446f0c","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0000cf878>: {\n        Underlying: <*exec.ExitError | 0xc00078ba20>{\n            ProcessState: {\n                pid: 7175,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 117176},\n                    Stime: {Sec: 0, Usec: 56418},\n                    Maxrss: 86588,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 6539,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 340,\n                    Nivcsw: 285,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicemediumqos\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660295740000,"stop":1660295740257,"duration":257}},{"uid":"964570ca7b2786e5","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00084d1b8>: {\n        Underlying: <*exec.ExitError | 0xc00047a5a0>{\n            ProcessState: {\n                pid: 7263,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 164428},\n                    Stime: {Sec: 0, Usec: 50593},\n                    Maxrss: 83572,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4337,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 404,\n                    Nivcsw: 363,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicemediumqos\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660293657000,"stop":1660293657302,"duration":302}},{"uid":"e38253dca24054bf","status":"passed","time":{"start":1660113628000,"stop":1660113632531,"duration":4531}},{"uid":"343fb057e308248e","status":"passed","time":{"start":1660104898000,"stop":1660104901714,"duration":3714}},{"uid":"7c86720bcb17df48","status":"passed","time":{"start":1660067259000,"stop":1660067263669,"duration":4669}},{"uid":"8d6b9f872be941b2","status":"passed","time":{"start":1660047551000,"stop":1660047556593,"duration":5593}},{"uid":"4ce983ee76a077d1","status":"passed","time":{"start":1659982549000,"stop":1659982556616,"duration":7616}},{"uid":"880190ff7de57490","status":"passed","time":{"start":1659970182000,"stop":1659970187260,"duration":5260}},{"uid":"d6789f77bf7c3e98","status":"passed","time":{"start":1659944495000,"stop":1659944501407,"duration":6407}},{"uid":"28dd605da7d0715","status":"passed","time":{"start":1659874977000,"stop":1659874980749,"duration":3749}},{"uid":"124ec6feabc6bea5","status":"passed","time":{"start":1659496050000,"stop":1659496053591,"duration":3591}},{"uid":"245770b0b64e24b6","status":"passed","time":{"start":1659496083000,"stop":1659496087546,"duration":4546}},{"uid":"ac6a05b8a9137709","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1659451501000,"stop":1659451561819,"duration":60819}},{"uid":"18a6f537a2dee605","status":"passed","time":{"start":1659450916000,"stop":1659450921872,"duration":5872}},{"uid":"c04a8f6ad9f09ae9","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1659451524000,"stop":1659451584543,"duration":60543}},{"uid":"712bdba7ba597a31","status":"passed","time":{"start":1659447298000,"stop":1659447302601,"duration":4601}},{"uid":"64b73e70e4709cee","status":"passed","time":{"start":1659447116000,"stop":1659447120992,"duration":4992}},{"uid":"f5179fa4c3bdc493","status":"passed","time":{"start":1659448205000,"stop":1659448209618,"duration":4618}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Iperf connectivity Verify Iperf traffic":{"statistic":{"failed":0,"broken":0,"skipped":39,"passed":0,"unknown":0,"total":39},"items":[{"uid":"72e95de472e49e4e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"a1b002457298af83","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"bd9d6c5987d793f4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"1cc07f65e0a26f0a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"74b0c4a68123bb79","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"4bd7a17867fd253d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"df9833c409829e1a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"3c3b6674a8ab1e1a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}},{"uid":"aa80ac5a4fb50875","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659982549000,"stop":1659982549000,"duration":0}},{"uid":"90a38c60b8396fa3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659970182000,"stop":1659970182000,"duration":0}},{"uid":"32fafc6d1906cf8d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659944495000,"stop":1659944495000,"duration":0}},{"uid":"47994d772fcba1c0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659874977000,"stop":1659874977000,"duration":0}},{"uid":"9b1b1e1655e0c561","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496050000,"stop":1659496050000,"duration":0}},{"uid":"adfed59c32fe81d3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496083000,"stop":1659496083000,"duration":0}},{"uid":"d6d67eb717698198","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"7f4a98e403823f55","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659450916000,"stop":1659450916000,"duration":0}},{"uid":"9fbd45a658c67d16","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"1bba72c4190a9aa5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447298000,"stop":1659447298000,"duration":0}},{"uid":"d0300741100a5695","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447116000,"stop":1659447116000,"duration":0}},{"uid":"a15cde49066aac2d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448205000,"stop":1659448205000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice validation Slice with LOW QOS should have vl3 router running":{"statistic":{"failed":2,"broken":0,"skipped":0,"passed":7,"unknown":0,"total":9},"items":[{"uid":"46aca11723b30ab4","status":"passed","time":{"start":1659168739000,"stop":1659168753412,"duration":14412}},{"uid":"23c755beac37ab66","status":"passed","time":{"start":1659164084000,"stop":1659164087272,"duration":3272}},{"uid":"88103c88c0b75f1c","status":"passed","time":{"start":1659160188000,"stop":1659160191197,"duration":3197}},{"uid":"3d2e40ff3ef5d0e5","status":"passed","time":{"start":1659119724000,"stop":1659119725567,"duration":1567}},{"uid":"502e7078fbd0aa84","status":"passed","time":{"start":1659116511000,"stop":1659116511230,"duration":230}},{"uid":"b2daf468d54fc42b","status":"passed","time":{"start":1659109470000,"stop":1659109470577,"duration":577}},{"uid":"e5cba288b1a3ee92","status":"passed","time":{"start":1659106836000,"stop":1659106864747,"duration":28747}},{"uid":"9676d1b60703e180","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1659101522000,"stop":1659101582708,"duration":60708}},{"uid":"3834bb8efced8f8","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 4\nto equal\n    <int>: 1","time":{"start":1659082816000,"stop":1659082876703,"duration":60703}}]},"Hub Suite:Hub Suite#[It]  QosProfile tests QosProfile Create validation Create qos profile- validation of BandwidthCeilingKbps > BandwidthGuaranteedKbps":{"statistic":{"failed":0,"broken":0,"skipped":111,"passed":5,"unknown":0,"total":116},"items":[{"uid":"b68214c265a2b17","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660828517000,"stop":1660828517000,"duration":0}},{"uid":"fdea751746c59664","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660807774000,"stop":1660807774000,"duration":0}},{"uid":"9d8a3ce4dc510132","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660803998000,"stop":1660803998000,"duration":0}},{"uid":"c1c2d5ebb95dd223","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660805737000,"stop":1660805737000,"duration":0}},{"uid":"a0f8748f52def9b4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660803696000,"stop":1660803696000,"duration":0}},{"uid":"45ee483cc7f1d735","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660798468000,"stop":1660798468000,"duration":0}},{"uid":"2d0c684c031dbfae","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660748009000,"stop":1660748009000,"duration":0}},{"uid":"d9c495e72ca14a82","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660747865000,"stop":1660747865000,"duration":0}},{"uid":"e5ea2b7789957b63","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660747735000,"stop":1660747735000,"duration":0}},{"uid":"c8305c50d63096f7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660747654000,"stop":1660747654000,"duration":0}},{"uid":"7fb814562d715b97","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660722683000,"stop":1660722683000,"duration":0}},{"uid":"d58b6690e9a17215","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660635916000,"stop":1660635916000,"duration":0}},{"uid":"9f06123fd4dd6fc5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660635666000,"stop":1660635666000,"duration":0}},{"uid":"11e31cbc006e2d68","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660635860000,"stop":1660635860000,"duration":0}},{"uid":"a85ec17f52068fe3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660631164000,"stop":1660631164000,"duration":0}},{"uid":"f984baab20846de9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660623463000,"stop":1660623463000,"duration":0}},{"uid":"e3a9e4dac96122b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660623384000,"stop":1660623384000,"duration":0}},{"uid":"bcd51b0a819155ae","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660623360000,"stop":1660623360000,"duration":0}},{"uid":"32eb2569048acbd0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660452377000,"stop":1660452377000,"duration":0}},{"uid":"ceb51abc677edd20","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660451870000,"stop":1660451870000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Attach/Deattach Scenario Attach cluster from slice  when Deattach cluster from slice  should have vl3 router deleted from deattach cluster":{"statistic":{"failed":25,"broken":1,"skipped":0,"passed":4,"unknown":0,"total":30},"items":[{"uid":"dd97ab75259c6dfc","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000f938>: {\n        Underlying: <*exec.ExitError | 0xc0006ff640>{\n            ProcessState: {\n                pid: 7024,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 191010},\n                    Stime: {Sec: 0, Usec: 62370},\n                    Maxrss: 83420,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 7217,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 485,\n                    Nivcsw: 457,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicetwocluster\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660808797000,"stop":1660808797245,"duration":245}},{"uid":"3b25b12501699b75","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000442ed0>: {\n        Underlying: <*exec.ExitError | 0xc0006ef680>{\n            ProcessState: {\n                pid: 7247,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 132812},\n                    Stime: {Sec: 0, Usec: 30989},\n                    Maxrss: 93756,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3766,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 372,\n                    Nivcsw: 261,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicetwocluster\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660299182000,"stop":1660299182166,"duration":166}},{"uid":"7b396665c2d82388","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004cadb0>: {\n        Underlying: <*exec.ExitError | 0xc0007937c0>{\n            ProcessState: {\n                pid: 7105,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 196941},\n                    Stime: {Sec: 0, Usec: 74317},\n                    Maxrss: 77184,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4905,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 421,\n                    Nivcsw: 556,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicetwocluster\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660295740000,"stop":1660295741128,"duration":1128}},{"uid":"835871cd8b9f80b","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000f770>: {\n        Underlying: <*exec.ExitError | 0xc00085b300>{\n            ProcessState: {\n                pid: 7218,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 171448},\n                    Stime: {Sec: 0, Usec: 55907},\n                    Maxrss: 91464,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 7067,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 484,\n                    Nivcsw: 452,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicetwocluster\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660293657000,"stop":1660293657268,"duration":268}},{"uid":"75aedaf35c3f7f72","status":"failed","statusDetails":"Timed out after 180.004s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1660113628000,"stop":1660113808241,"duration":180241}},{"uid":"b9f986577f69cc6c","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1660104898000,"stop":1660105081644,"duration":183644}},{"uid":"f9394230d84aa75f","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 3\nto equal\n    <int>: 1","time":{"start":1660067259000,"stop":1660067319210,"duration":60210}},{"uid":"b5795c2e99782bba","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <int>: 4\nto equal\n    <int>: 1","time":{"start":1660047551000,"stop":1660047611447,"duration":60447}},{"uid":"5dfea7eeb7b7746","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 2\nto equal\n    <int>: 1","time":{"start":1659982549000,"stop":1659982646211,"duration":97211}},{"uid":"f7403465a20a1ee9","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 3\nto equal\n    <int>: 1","time":{"start":1659970182000,"stop":1659970242235,"duration":60235}},{"uid":"a47b4af4ddb3d5ff","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 3\nto equal\n    <int>: 1","time":{"start":1659944495000,"stop":1659944555311,"duration":60311}},{"uid":"feaf5ec21a88c59e","status":"broken","statusDetails":"runtime error: invalid memory address or nil pointer dereference","time":{"start":1659874977000,"stop":1659874977334,"duration":334}},{"uid":"6f5ceffbf9614acc","status":"passed","time":{"start":1659496050000,"stop":1659496071077,"duration":21077}},{"uid":"10f58fe5b924960e","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0008c0018>: {\n        Underlying: <*exec.ExitError | 0xc000880000>{\n            ProcessState: {\n                pid: 7197,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 54634},\n                    Stime: {Sec: 0, Usec: 22764},\n                    Maxrss: 43780,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3367,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 221,\n                    Nivcsw: 142,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicetwocluster\\\" not found\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicetwocluster\\\" not found\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicetwocluster\\\" not found\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicetwocluster\\\" not found\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (NotFound): slice.networking.kubeslice.io \"slicetwocluster\" not found\noccurred","time":{"start":1659496083000,"stop":1659496083788,"duration":788}},{"uid":"6a8bb385dc65c2f4","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1659451501000,"stop":1659451561389,"duration":60389}},{"uid":"4bf6c4ba19bba614","status":"passed","time":{"start":1659450916000,"stop":1659450996988,"duration":80988}},{"uid":"1df4795bd2dec759","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1659451524000,"stop":1659451584157,"duration":60157}},{"uid":"62edf51a2bf1168","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <int>: 1\nto equal\n    <int>: 0","time":{"start":1659447298000,"stop":1659447361412,"duration":63412}},{"uid":"dd9b9b783b5a4ba6","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0006c36f8>: {\n        Underlying: <*exec.ExitError | 0xc0009bcb40>{\n            ProcessState: {\n                pid: 8065,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 66541},\n                    Stime: {Sec: 0, Usec: 11090},\n                    Maxrss: 44300,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2467,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 260,\n                    Nivcsw: 237,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicetwocluster\\\" not found\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicetwocluster\\\" not found\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicetwocluster\\\" not found\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicetwocluster\\\" not found\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (NotFound): slice.networking.kubeslice.io \"slicetwocluster\" not found\noccurred","time":{"start":1659447116000,"stop":1659447116652,"duration":652}},{"uid":"c94791318239a8df","status":"failed","statusDetails":"Timed out after 180.004s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1659448205000,"stop":1659448385427,"duration":180427}}]},"Hub Suite:Hub Suite#[BeforeSuite]":{"statistic":{"failed":44,"broken":0,"skipped":0,"passed":124,"unknown":0,"total":168},"items":[{"uid":"c3456f611106744f","status":"passed","time":{"start":1660828517000,"stop":1660828615332,"duration":98332}},{"uid":"9ae2496830dff615","status":"passed","time":{"start":1660807774000,"stop":1660807861048,"duration":87048}},{"uid":"ae10736c3347280","status":"passed","time":{"start":1660803998000,"stop":1660804091796,"duration":93796}},{"uid":"980ae8f6181c5b72","status":"passed","time":{"start":1660805737000,"stop":1660805838755,"duration":101755}},{"uid":"704576524fe79147","status":"passed","time":{"start":1660803696000,"stop":1660803793419,"duration":97419}},{"uid":"90ef3c80c847da7f","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0002c0168>: {\n        Underlying: <*exec.ExitError | 0xc00030c000>{\n            ProcessState: {\n                pid: 6012,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 1, Usec: 415939},\n                    Stime: {Sec: 0, Usec: 212617},\n                    Maxrss: 80724,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3614,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12840,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 15874,\n                    Nivcsw: 3902,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-0.2.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 26 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 26 resources with timeout of 2m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-0.2.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 26 resource(s)\n    wait.go:48: [debug] beginning wait for 26 resources with timeout of 2m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-controller\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-manager-metrics-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-controller-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-editor-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"workerslicegateways.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"serviceexportconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"clusters.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"projects.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workerserviceimports.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"sliceconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workersliceconfigs.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-serving-cert\" Certificate\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-selfsigned-issuer\" Issuer\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-validating-webhook-configuration\" ValidatingWebhookConfiguration\n    uninstall.go:144: [debug] purge requested for kubeslice-controller\n    Error: INSTALLATION FAILED: release kubeslice-controller failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-controller failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1660803304000,"stop":1660803506886,"duration":202886}},{"uid":"527c9cb80c48e0b","status":"passed","time":{"start":1660798468000,"stop":1660798559984,"duration":91984}},{"uid":"d9e14426d3f540f","status":"passed","time":{"start":1660748009000,"stop":1660748104006,"duration":95006}},{"uid":"3bb0459eff0b3bba","status":"passed","time":{"start":1660747865000,"stop":1660747979053,"duration":114053}},{"uid":"ee1ce548d9694283","status":"passed","time":{"start":1660747735000,"stop":1660747826763,"duration":91763}},{"uid":"cf37f97206785be7","status":"passed","time":{"start":1660747654000,"stop":1660747745601,"duration":91601}},{"uid":"77a61d70a91da284","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0000c6048>: {\n        Underlying: <*exec.ExitError | 0xc00048a2c0>{\n            ProcessState: {\n                pid: 6065,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 1, Usec: 239583},\n                    Stime: {Sec: 0, Usec: 224593},\n                    Maxrss: 82176,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 9147,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12840,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 15076,\n                    Nivcsw: 3787,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-0.2.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 26 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 26 resources with timeout of 2m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-0.2.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 26 resource(s)\n    wait.go:48: [debug] beginning wait for 26 resources with timeout of 2m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-controller\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-manager-metrics-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-controller-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-editor-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"workerslicegateways.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"clusters.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"projects.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"serviceexportconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"sliceconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workerserviceimports.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workersliceconfigs.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-serving-cert\" Certificate\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-selfsigned-issuer\" Issuer\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-validating-webhook-configuration\" ValidatingWebhookConfiguration\n    uninstall.go:144: [debug] purge requested for kubeslice-controller\n    Error: INSTALLATION FAILED: release kubeslice-controller failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-controller failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1660748277000,"stop":1660748473995,"duration":196995}},{"uid":"7fdac0eaa5149371","status":"passed","time":{"start":1660722683000,"stop":1660722781326,"duration":98326}},{"uid":"8b7142c0b08d1220","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00018c108>: {\n        Underlying: <*exec.ExitError | 0xc000194000>{\n            ProcessState: {\n                pid: 5999,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 1, Usec: 270380},\n                    Stime: {Sec: 0, Usec: 180881},\n                    Maxrss: 78536,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 6875,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12840,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 15340,\n                    Nivcsw: 4662,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-0.2.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 26 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 26 resources with timeout of 2m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-0.2.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 26 resource(s)\n    wait.go:48: [debug] beginning wait for 26 resources with timeout of 2m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-controller\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-manager-metrics-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-controller-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-editor-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"workerslicegateways.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"clusters.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"projects.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"serviceexportconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"sliceconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workerserviceimports.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workersliceconfigs.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-serving-cert\" Certificate\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-selfsigned-issuer\" Issuer\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-validating-webhook-configuration\" ValidatingWebhookConfiguration\n    uninstall.go:144: [debug] purge requested for kubeslice-controller\n    Error: INSTALLATION FAILED: release kubeslice-controller failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-controller failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1660712309000,"stop":1660712526737,"duration":217737}},{"uid":"9fd1e143850c8dbd","status":"passed","time":{"start":1660635916000,"stop":1660636005125,"duration":89125}},{"uid":"80f0b32ade3689dd","status":"passed","time":{"start":1660635666000,"stop":1660635764288,"duration":98288}},{"uid":"ce35f22d826b7c72","status":"passed","time":{"start":1660635860000,"stop":1660635942848,"duration":82848}},{"uid":"63e796f2eb67643f","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00014c048>: {\n        Underlying: <*exec.ExitError | 0xc00038c000>{\n            ProcessState: {\n                pid: 6024,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 1, Usec: 311180},\n                    Stime: {Sec: 0, Usec: 278518},\n                    Maxrss: 84496,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 8228,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12840,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 16390,\n                    Nivcsw: 4473,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-0.2.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 26 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 26 resources with timeout of 2m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-0.2.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 26 resource(s)\n    wait.go:48: [debug] beginning wait for 26 resources with timeout of 2m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-controller\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-manager-metrics-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-controller-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-editor-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"workerslicegateways.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"serviceexportconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"clusters.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"projects.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workerserviceimports.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"sliceconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workersliceconfigs.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-serving-cert\" Certificate\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-selfsigned-issuer\" Issuer\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-validating-webhook-configuration\" ValidatingWebhookConfiguration\n    uninstall.go:144: [debug] purge requested for kubeslice-controller\n    Error: INSTALLATION FAILED: release kubeslice-controller failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-controller failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1660636153000,"stop":1660636360695,"duration":207695}},{"uid":"38606e88cd3451ab","status":"passed","time":{"start":1660631164000,"stop":1660631266097,"duration":102097}},{"uid":"292053b567613087","status":"passed","time":{"start":1660623463000,"stop":1660623545430,"duration":82430}}]},"Worker Suite:Worker Suite#[It] Slice creation scenarios: when slice red is installed workers should have vl3 router pods running":{"statistic":{"failed":23,"broken":0,"skipped":4,"passed":12,"unknown":0,"total":39},"items":[{"uid":"bba39c270532456","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"a721720e8cf1c1e1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"43f281e297cc3e49","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"e1a9928d14cd30e3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"a00e71172c2c972f","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 3\nto equal\n    <int>: 1","time":{"start":1660113628000,"stop":1660113688206,"duration":60206}},{"uid":"b00389e53c48f1bc","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 4\nto equal\n    <int>: 1","time":{"start":1660104898000,"stop":1660104958219,"duration":60219}},{"uid":"f86ec96bb86b5ca1","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 4\nto equal\n    <int>: 1","time":{"start":1660067259000,"stop":1660067319170,"duration":60170}},{"uid":"f42a9323b9da382f","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 4\nto equal\n    <int>: 1","time":{"start":1660047551000,"stop":1660047611173,"duration":60173}},{"uid":"96f4cdf3d853cbf4","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 3\nto equal\n    <int>: 1","time":{"start":1659982549000,"stop":1659982609489,"duration":60489}},{"uid":"a1b9429af1fefd86","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 2\nto equal\n    <int>: 1","time":{"start":1659970182000,"stop":1659970242243,"duration":60243}},{"uid":"d38fc465464d3b03","status":"passed","time":{"start":1659944495000,"stop":1659944495037,"duration":37}},{"uid":"fb0adbe226e7eebc","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 4\nto equal\n    <int>: 1","time":{"start":1659874977000,"stop":1659875037171,"duration":60171}},{"uid":"161d6ba71ae99227","status":"passed","time":{"start":1659496050000,"stop":1659496052034,"duration":2034}},{"uid":"709610fc56f0eaf9","status":"passed","time":{"start":1659496083000,"stop":1659496101155,"duration":18155}},{"uid":"3561c55d9522702c","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1659451501000,"stop":1659451561303,"duration":60303}},{"uid":"20632dd0710f130d","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <int>: 3\nto equal\n    <int>: 1","time":{"start":1659450916000,"stop":1659450976330,"duration":60330}},{"uid":"7f55e6b2fdce0ad2","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1659451524000,"stop":1659451584630,"duration":60630}},{"uid":"868cb5385e377f11","status":"passed","time":{"start":1659447298000,"stop":1659447316620,"duration":18620}},{"uid":"bcc04a83a2a9ca14","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 2\nto equal\n    <int>: 1","time":{"start":1659447116000,"stop":1659447176209,"duration":60209}},{"uid":"d77b66ff284f9d70","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <int>: 4\nto equal\n    <int>: 1","time":{"start":1659448205000,"stop":1659448265264,"duration":60264}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests Iperf App connectivity Application namespaces should be isolated":{"statistic":{"failed":7,"broken":0,"skipped":32,"passed":0,"unknown":0,"total":39},"items":[{"uid":"d97c93c657a33794","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"9725a16fe4e8c7af","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"1c93cf49c8f0cdca","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"eb9932417827ecc4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"4fc9286750e399a5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"6d3f1759fe352220","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"2aa3f287d14b3323","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"e500e1e7697014b0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}},{"uid":"6dafe66778e09cf6","status":"failed","statusDetails":"Expected\n    <bool>: true\nto equal\n    <bool>: false","time":{"start":1659982549000,"stop":1659982607206,"duration":58206}},{"uid":"8ad464614fa56159","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659970182000,"stop":1659970182000,"duration":0}},{"uid":"957c5eb6879ca2dc","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659944495000,"stop":1659944495000,"duration":0}},{"uid":"8d77c83742c71467","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659874977000,"stop":1659874977000,"duration":0}},{"uid":"b947f1c580de7e5f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496050000,"stop":1659496050000,"duration":0}},{"uid":"3c2dbc7ec3cb362b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496083000,"stop":1659496083000,"duration":0}},{"uid":"222a8d9257f10a02","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"ca88c92d44c3d43a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659450916000,"stop":1659450916000,"duration":0}},{"uid":"ca25fbf18bd36441","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"868353a5dd249005","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447298000,"stop":1659447298000,"duration":0}},{"uid":"1efc0bb70e3ee8e4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447116000,"stop":1659447116000,"duration":0}},{"uid":"9ca811ff865e1c89","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448205000,"stop":1659448205000,"duration":0}}]},"Istio Suite:Istio Suite#[It] Bookinfo tests Bookinfo Scenario 3 without egress and with ingress Should install Details, Reviews, Ratings and Serviceexport on worker cluster-2":{"statistic":{"failed":0,"broken":0,"skipped":15,"passed":3,"unknown":0,"total":18},"items":[{"uid":"9a0d46baa5e41327","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808584000,"stop":1660808584000,"duration":0}},{"uid":"962c65957bc2a45f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660804840000,"stop":1660804840000,"duration":0}},{"uid":"57b5d900975f1e9c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660804470000,"stop":1660804470000,"duration":0}},{"uid":"e1ea78eff30023e0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660748849000,"stop":1660748849000,"duration":0}},{"uid":"8f2b1d4fcf657667","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660748706000,"stop":1660748706000,"duration":0}},{"uid":"2a42327ad6b760ab","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660748558000,"stop":1660748558000,"duration":0}},{"uid":"6b026780935f455e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660636759000,"stop":1660636759000,"duration":0}},{"uid":"6a0e22a75baccb2f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660636482000,"stop":1660636482000,"duration":0}},{"uid":"e0bdda717d05de41","status":"passed","time":{"start":1660632006000,"stop":1660632039041,"duration":33041}},{"uid":"8fc3076e7305324","status":"passed","time":{"start":1660624273000,"stop":1660624357585,"duration":84585}},{"uid":"41f6aafa3c4c68c8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660624220000,"stop":1660624220000,"duration":0}},{"uid":"1705f074849e75a7","status":"passed","time":{"start":1660453205000,"stop":1660453244063,"duration":39063}},{"uid":"800b88a4334f0276","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660452554000,"stop":1660452554000,"duration":0}},{"uid":"2c0208a2d9e28092","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660452412000,"stop":1660452412000,"duration":0}},{"uid":"70b5f5e20c96c5af","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660415422000,"stop":1660415422000,"duration":0}},{"uid":"bd8971646e307d7e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660298968000,"stop":1660298968000,"duration":0}},{"uid":"3310fd72e22ab0c0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295522000,"stop":1660295522000,"duration":0}},{"uid":"8acd73a04b93cdd9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293414000,"stop":1660293414000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice deletion tests: Slice is installed & validated Slice object is deleted from hub should remove gateway pod from spoke":{"statistic":{"failed":0,"broken":0,"skipped":31,"passed":8,"unknown":0,"total":39},"items":[{"uid":"5c3f709f3f5fe60e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"929fcfa78387f8af","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"2a9ef5d74ac33d05","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"fe7b4d71a5e23fcd","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"5327462f8ff4e436","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"7d7598ffdfd14785","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"e41c10057cdaa0b4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"ea78d1c0af1b8979","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}},{"uid":"cd61a4c5d6e3c81c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659982549000,"stop":1659982549000,"duration":0}},{"uid":"2ca53197a2dfc03d","status":"passed","time":{"start":1659970182000,"stop":1659970182011,"duration":11}},{"uid":"f38f3492bc4b244d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659944495000,"stop":1659944495000,"duration":0}},{"uid":"eb28c6c3b5ac162c","status":"passed","time":{"start":1659874977000,"stop":1659874979024,"duration":2024}},{"uid":"a8441c605f162d21","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496050000,"stop":1659496050000,"duration":0}},{"uid":"c5be3bdf46b67c32","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496083000,"stop":1659496083000,"duration":0}},{"uid":"5def494e5dfa3f01","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"d8617e93d29c6af","status":"passed","time":{"start":1659450916000,"stop":1659450916009,"duration":9}},{"uid":"a01c7449c4bab27d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"3a60b3b6d50e4293","status":"passed","time":{"start":1659447298000,"stop":1659447298018,"duration":18}},{"uid":"c0b127fd5935a97","status":"passed","time":{"start":1659447116000,"stop":1659447118029,"duration":2029}},{"uid":"aef61c0b1d6f4ea6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448205000,"stop":1659448205000,"duration":0}}]},"Hub Suite:Hub Suite#[It] Cluster negative tests Worker Cluster Registration with Wrong ca.cert":{"statistic":{"failed":0,"broken":0,"skipped":17,"passed":107,"unknown":0,"total":124},"items":[{"uid":"2d1c6f81a6240008","status":"passed","time":{"start":1660828517000,"stop":1660828653840,"duration":136840}},{"uid":"e2852e5737219099","status":"passed","time":{"start":1660807774000,"stop":1660807910105,"duration":136105}},{"uid":"e5a05dd951ab8be6","status":"passed","time":{"start":1660803998000,"stop":1660804129344,"duration":131344}},{"uid":"98064971e2ea030c","status":"passed","time":{"start":1660805737000,"stop":1660805872462,"duration":135462}},{"uid":"a631093bdd2ce1d4","status":"passed","time":{"start":1660803696000,"stop":1660803831026,"duration":135026}},{"uid":"4dd66627ebf87e29","status":"passed","time":{"start":1660798468000,"stop":1660798597687,"duration":129687}},{"uid":"6cfca75f28316893","status":"passed","time":{"start":1660748009000,"stop":1660748147176,"duration":138176}},{"uid":"a4cced84937f18d7","status":"passed","time":{"start":1660747865000,"stop":1660747995795,"duration":130795}},{"uid":"77c98ae7d4d2fe44","status":"passed","time":{"start":1660747735000,"stop":1660747865527,"duration":130527}},{"uid":"f8342c8a45afef67","status":"passed","time":{"start":1660747654000,"stop":1660747789756,"duration":135756}},{"uid":"409715286a945a47","status":"passed","time":{"start":1660722683000,"stop":1660722812964,"duration":129964}},{"uid":"38c21aa747eea358","status":"passed","time":{"start":1660635916000,"stop":1660636053464,"duration":137464}},{"uid":"df9af8432a1d8f22","status":"passed","time":{"start":1660635666000,"stop":1660635796098,"duration":130098}},{"uid":"956b25b836b4b210","status":"passed","time":{"start":1660635860000,"stop":1660635997353,"duration":137353}},{"uid":"c187855bf8baf8d9","status":"passed","time":{"start":1660631164000,"stop":1660631293433,"duration":129433}},{"uid":"725a9863586a4b48","status":"passed","time":{"start":1660623463000,"stop":1660623594020,"duration":131020}},{"uid":"465953213f20aa8d","status":"passed","time":{"start":1660623384000,"stop":1660623519765,"duration":135765}},{"uid":"73fa0530c382a33a","status":"passed","time":{"start":1660623360000,"stop":1660623489515,"duration":129515}},{"uid":"d6a90e3ed07efa04","status":"passed","time":{"start":1660452377000,"stop":1660452507897,"duration":130897}},{"uid":"6a268f6e7892633b","status":"passed","time":{"start":1660451870000,"stop":1660451999847,"duration":129847}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Creation Should fail while creating service accounts as combination of special characters":{"statistic":{"failed":5,"broken":0,"skipped":0,"passed":119,"unknown":0,"total":124},"items":[{"uid":"7bc725aef8923dca","status":"passed","time":{"start":1660828517000,"stop":1660828517319,"duration":319}},{"uid":"d522de50c6d58d93","status":"passed","time":{"start":1660807774000,"stop":1660807774255,"duration":255}},{"uid":"91ab365f4c703259","status":"passed","time":{"start":1660803998000,"stop":1660803998306,"duration":306}},{"uid":"f4e5bd71ce1907ae","status":"passed","time":{"start":1660805737000,"stop":1660805737248,"duration":248}},{"uid":"b070d448b45deac4","status":"passed","time":{"start":1660803696000,"stop":1660803696213,"duration":213}},{"uid":"7e593b00aec67844","status":"passed","time":{"start":1660798468000,"stop":1660798468277,"duration":277}},{"uid":"a4e4f20717cff338","status":"passed","time":{"start":1660748009000,"stop":1660748009396,"duration":396}},{"uid":"c97c4818e6ff94fb","status":"passed","time":{"start":1660747865000,"stop":1660747865244,"duration":244}},{"uid":"638e254e9d74023e","status":"passed","time":{"start":1660747735000,"stop":1660747735290,"duration":290}},{"uid":"5cc295b8b5deb5b2","status":"passed","time":{"start":1660747654000,"stop":1660747654227,"duration":227}},{"uid":"ab81588bb4e1cef6","status":"passed","time":{"start":1660722683000,"stop":1660722683267,"duration":267}},{"uid":"7ddde2f442718d88","status":"passed","time":{"start":1660635916000,"stop":1660635916289,"duration":289}},{"uid":"952320033ba82b3d","status":"passed","time":{"start":1660635666000,"stop":1660635666259,"duration":259}},{"uid":"662a6332d3d0df29","status":"passed","time":{"start":1660635860000,"stop":1660635860294,"duration":294}},{"uid":"dda7be7a5f35b7d","status":"passed","time":{"start":1660631164000,"stop":1660631164218,"duration":218}},{"uid":"705c7fbc0e8c4e0c","status":"passed","time":{"start":1660623463000,"stop":1660623463284,"duration":284}},{"uid":"51ecb8688e915212","status":"passed","time":{"start":1660623384000,"stop":1660623384208,"duration":208}},{"uid":"c025c48cdf996bd1","status":"passed","time":{"start":1660623360000,"stop":1660623360234,"duration":234}},{"uid":"fae910cb6ef9993f","status":"passed","time":{"start":1660452377000,"stop":1660452377338,"duration":338}},{"uid":"e5216c9dd9fa1c81","status":"passed","time":{"start":1660451870000,"stop":1660451870231,"duration":231}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting nsm-kernel-forwarder pod Should restart nsm-kernel-forwarder pod":{"statistic":{"failed":0,"broken":0,"skipped":39,"passed":0,"unknown":0,"total":39},"items":[{"uid":"84e51ba34eca9079","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"3fe899c8a904648b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"24c1d41b52cd0959","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"a9c8f0c2915b4ccf","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"88de0417b8c590c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"8c4d338133296a87","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"1a9fba1a18cfb37b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"be3b4ae9d6ba3d61","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}},{"uid":"f3e3a50375e11f8e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659982549000,"stop":1659982549000,"duration":0}},{"uid":"e422917c6e20a0ce","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659970182000,"stop":1659970182000,"duration":0}},{"uid":"78d530d9c466458a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659944495000,"stop":1659944495000,"duration":0}},{"uid":"13d446a1ff1670a7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659874977000,"stop":1659874977000,"duration":0}},{"uid":"4dabd04863b110eb","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496050000,"stop":1659496050000,"duration":0}},{"uid":"e6563d7c1c9d5780","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496083000,"stop":1659496083000,"duration":0}},{"uid":"b348a625d25e2449","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"78d7949da523b81d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659450916000,"stop":1659450916000,"duration":0}},{"uid":"4151a1cde5221116","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"b853f6d33ed920de","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447298000,"stop":1659447298000,"duration":0}},{"uid":"4342fa2c914fd39c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447116000,"stop":1659447116000,"duration":0}},{"uid":"bd5f3cea55997272","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448205000,"stop":1659448205000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting nsm-manager pod Check ping between iperf-server and iperf-client after nsm-manager pod restart":{"statistic":{"failed":0,"broken":0,"skipped":39,"passed":0,"unknown":0,"total":39},"items":[{"uid":"4f7746b37e6c2587","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"1e22290a9c5deab2","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"4040b4d0da3ab12d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"2b1b10be263ccda4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"5c30549487d0ea09","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"6ea7cf1fd8397dbe","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"9c041fb0e17fd02c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"78cd510da75148c6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}},{"uid":"6641048f7e846c73","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659982549000,"stop":1659982549000,"duration":0}},{"uid":"900b3a47b1a7b84c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659970182000,"stop":1659970182000,"duration":0}},{"uid":"b049cdf76a474a04","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659944495000,"stop":1659944495000,"duration":0}},{"uid":"2aac0f693e869d88","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659874977000,"stop":1659874977000,"duration":0}},{"uid":"4079753729c9fab9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496050000,"stop":1659496050000,"duration":0}},{"uid":"f610e4a78c4cc661","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496083000,"stop":1659496083000,"duration":0}},{"uid":"7cad46dc35e05df7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"5cf858e00fa369e2","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659450916000,"stop":1659450916000,"duration":0}},{"uid":"5a7f153872854ded","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"f3406e9d98c05d50","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447298000,"stop":1659447298000,"duration":0}},{"uid":"5a4315b44449ff05","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447116000,"stop":1659447116000,"duration":0}},{"uid":"ef28204785b4f6b5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448205000,"stop":1659448205000,"duration":0}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Creation Should fail while creating Project with project name as combination of special characters":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":124,"unknown":0,"total":124},"items":[{"uid":"d5751ea82fd9ec1c","status":"passed","time":{"start":1660828517000,"stop":1660828517283,"duration":283}},{"uid":"bfcfe33cc004043f","status":"passed","time":{"start":1660807774000,"stop":1660807774236,"duration":236}},{"uid":"3b2524a555d8061b","status":"passed","time":{"start":1660803998000,"stop":1660803998298,"duration":298}},{"uid":"5d51edbe06687165","status":"passed","time":{"start":1660805737000,"stop":1660805737209,"duration":209}},{"uid":"78bea81a2faec405","status":"passed","time":{"start":1660803696000,"stop":1660803696199,"duration":199}},{"uid":"84a8c3d9685e3410","status":"passed","time":{"start":1660798468000,"stop":1660798468255,"duration":255}},{"uid":"b121c276d451fca8","status":"passed","time":{"start":1660748009000,"stop":1660748009361,"duration":361}},{"uid":"3c16744bfb3cc0df","status":"passed","time":{"start":1660747865000,"stop":1660747865264,"duration":264}},{"uid":"f7766d413f48d882","status":"passed","time":{"start":1660747735000,"stop":1660747735268,"duration":268}},{"uid":"3a4a5c25c181d56f","status":"passed","time":{"start":1660747654000,"stop":1660747654197,"duration":197}},{"uid":"241b54c3af862bcf","status":"passed","time":{"start":1660722683000,"stop":1660722683256,"duration":256}},{"uid":"5e7ac54771134dc9","status":"passed","time":{"start":1660635916000,"stop":1660635916258,"duration":258}},{"uid":"c4e88fa1bcd562d0","status":"passed","time":{"start":1660635666000,"stop":1660635666263,"duration":263}},{"uid":"f7fd099779fbe936","status":"passed","time":{"start":1660635860000,"stop":1660635860275,"duration":275}},{"uid":"1c593b3b265af57e","status":"passed","time":{"start":1660631164000,"stop":1660631164223,"duration":223}},{"uid":"e9e6445e2db33c19","status":"passed","time":{"start":1660623463000,"stop":1660623463273,"duration":273}},{"uid":"a7c0b4c429031afe","status":"passed","time":{"start":1660623384000,"stop":1660623384180,"duration":180}},{"uid":"76302f4d2d54bfd9","status":"passed","time":{"start":1660623360000,"stop":1660623360195,"duration":195}},{"uid":"d46939d7b80cc0a3","status":"passed","time":{"start":1660452377000,"stop":1660452377271,"duration":271}},{"uid":"6d4b52dd304eebbe","status":"passed","time":{"start":1660451870000,"stop":1660451870222,"duration":222}}]},"Istio Suite:Istio Suite#[It] Bookinfo tests Bookinfo Scenario 2 with egress and without ingress Should verify if Curl Traffic works on Productpage test":{"statistic":{"failed":0,"broken":0,"skipped":14,"passed":4,"unknown":0,"total":18},"items":[{"uid":"10ab4ab6efb3e047","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808584000,"stop":1660808584000,"duration":0}},{"uid":"bb3b750b76123552","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660804840000,"stop":1660804840000,"duration":0}},{"uid":"74c07af07035c571","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660804470000,"stop":1660804470000,"duration":0}},{"uid":"4b1d37eec3313659","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660748849000,"stop":1660748849000,"duration":0}},{"uid":"c91d97c8b923afa3","status":"passed","time":{"start":1660748706000,"stop":1660748706540,"duration":540}},{"uid":"7c094946f31334d0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660748558000,"stop":1660748558000,"duration":0}},{"uid":"61e9c3c82b7830fb","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660636759000,"stop":1660636759000,"duration":0}},{"uid":"2f2a374f3d78bf48","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660636482000,"stop":1660636482000,"duration":0}},{"uid":"6aecb73808b6ab83","status":"passed","time":{"start":1660632006000,"stop":1660632006449,"duration":449}},{"uid":"6a5ee77445802c88","status":"passed","time":{"start":1660624273000,"stop":1660624274547,"duration":1547}},{"uid":"188c3e29505136fd","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660624220000,"stop":1660624220000,"duration":0}},{"uid":"d08cc7678afbb297","status":"passed","time":{"start":1660453205000,"stop":1660453205619,"duration":619}},{"uid":"f5fa27419ce9c9cf","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660452554000,"stop":1660452554000,"duration":0}},{"uid":"195e40e3cae82d3e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660452412000,"stop":1660452412000,"duration":0}},{"uid":"5f0a1aa09b2a5a16","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660415422000,"stop":1660415422000,"duration":0}},{"uid":"4ce49971af97d00e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660298968000,"stop":1660298968000,"duration":0}},{"uid":"293de2ead5c36a59","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295522000,"stop":1660295522000,"duration":0}},{"uid":"e29fdc2bf911f66b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293414000,"stop":1660293414000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Hub netpol scenarios: Slice Configs Netpol scenarios Should fail while creating namespaces with same name in both allowedNamespace and applicationNamespace":{"statistic":{"failed":0,"broken":0,"skipped":13,"passed":26,"unknown":0,"total":39},"items":[{"uid":"94198843ab048a4b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"f73da2f7d60ef140","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"ca06e6e756849081","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"86c8b7e171227643","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"b295c3bc52481d14","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"cacb1bf0c65dc3b6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"b18e8be3034e07a0","status":"passed","time":{"start":1660067259000,"stop":1660067259327,"duration":327}},{"uid":"a599479aa6ba95f3","status":"passed","time":{"start":1660047551000,"stop":1660047551345,"duration":345}},{"uid":"9bc6bca39f0a7d59","status":"passed","time":{"start":1659982549000,"stop":1659982549628,"duration":628}},{"uid":"2498530d4846db75","status":"passed","time":{"start":1659970182000,"stop":1659970182322,"duration":322}},{"uid":"832517f5de86c819","status":"passed","time":{"start":1659944495000,"stop":1659944495386,"duration":386}},{"uid":"2ff4039d50974e18","status":"passed","time":{"start":1659874977000,"stop":1659874977283,"duration":283}},{"uid":"a029f32e5c86f789","status":"passed","time":{"start":1659496050000,"stop":1659496050168,"duration":168}},{"uid":"41d5bc15d27e6a38","status":"passed","time":{"start":1659496083000,"stop":1659496083365,"duration":365}},{"uid":"c1b0ca2f6a405bf2","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"f1743a0ff792d686","status":"passed","time":{"start":1659450916000,"stop":1659450916297,"duration":297}},{"uid":"469ecc33613c0103","status":"passed","time":{"start":1659451524000,"stop":1659451524404,"duration":404}},{"uid":"9425d7edb3cc2f5","status":"passed","time":{"start":1659447298000,"stop":1659447298229,"duration":229}},{"uid":"7ca373fc25b70669","status":"passed","time":{"start":1659447116000,"stop":1659447116350,"duration":350}},{"uid":"b98e46c025868716","status":"passed","time":{"start":1659448205000,"stop":1659448205280,"duration":280}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting iperf-server pod Should restart iperf-server pod":{"statistic":{"failed":0,"broken":0,"skipped":39,"passed":0,"unknown":0,"total":39},"items":[{"uid":"414d751d0312a41f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"b0b94777d2cb702e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"caba256e1cc2bbef","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"431365f80d15803","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"34ba068544ff09c7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"475d48b671990726","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"fd93fcd5dcf92398","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"e81f0f8c906863c6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}},{"uid":"74ad7195449d0090","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659982549000,"stop":1659982549000,"duration":0}},{"uid":"16aba6005575c616","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659970182000,"stop":1659970182000,"duration":0}},{"uid":"770b3e272b6fbe5c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659944495000,"stop":1659944495000,"duration":0}},{"uid":"af4ecae8c2e4f678","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659874977000,"stop":1659874977000,"duration":0}},{"uid":"c5400e164577490c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496050000,"stop":1659496050000,"duration":0}},{"uid":"c1002e7e0f88e284","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496083000,"stop":1659496083000,"duration":0}},{"uid":"7520b702322fd05c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"ca57d9641bf170b2","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659450916000,"stop":1659450916000,"duration":0}},{"uid":"eea836a2fc70917e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"5331423252706652","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447298000,"stop":1659447298000,"duration":0}},{"uid":"9ff8601400b33284","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447116000,"stop":1659447116000,"duration":0}},{"uid":"9c87ed8fb261e00c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448205000,"stop":1659448205000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting worker-operator pod Should restart worker-operator pod":{"statistic":{"failed":0,"broken":0,"skipped":39,"passed":0,"unknown":0,"total":39},"items":[{"uid":"f01d1a92ba60e192","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"d5729991e9b3cb90","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"e46d1ce3a1d9ef6c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"f196b038b33e8198","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"bbd2eacb60b58a3f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"bf242a58aac28486","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"cb543711d9edb312","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"be4c88760885ea1b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}},{"uid":"109b4eef62c498bd","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659982549000,"stop":1659982549000,"duration":0}},{"uid":"7f40a8b1881c8c47","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659970182000,"stop":1659970182000,"duration":0}},{"uid":"1daac68858ec0398","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659944495000,"stop":1659944495000,"duration":0}},{"uid":"d611d4d272ab3305","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659874977000,"stop":1659874977000,"duration":0}},{"uid":"66b7895ed7b4b752","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496050000,"stop":1659496050000,"duration":0}},{"uid":"43fd7a55012e8361","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496083000,"stop":1659496083000,"duration":0}},{"uid":"9f09c751a09c57f0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"8f31bdf379b8c4b9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659450916000,"stop":1659450916000,"duration":0}},{"uid":"16c5f501f46907d4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"fe2217ea1fbe8e76","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447298000,"stop":1659447298000,"duration":0}},{"uid":"2a149e7f9fbfd44a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447116000,"stop":1659447116000,"duration":0}},{"uid":"5c625a61c957d866","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448205000,"stop":1659448205000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice Creation scenarios Slice with LOW QOS should have vl3 router running":{"statistic":{"failed":8,"broken":0,"skipped":0,"passed":22,"unknown":0,"total":30},"items":[{"uid":"90c863483faf9005","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004dc828>: {\n        Underlying: <*exec.ExitError | 0xc0007d42c0>{\n            ProcessState: {\n                pid: 7207,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 155805},\n                    Stime: {Sec: 0, Usec: 67375},\n                    Maxrss: 80652,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4200,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 487,\n                    Nivcsw: 354,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicelowqos\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660808797000,"stop":1660808797280,"duration":280}},{"uid":"70e33a27619371c","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0005cc138>: {\n        Underlying: <*exec.ExitError | 0xc000720480>{\n            ProcessState: {\n                pid: 7110,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 130640},\n                    Stime: {Sec: 0, Usec: 39917},\n                    Maxrss: 85196,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 6182,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 293,\n                    Nivcsw: 295,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicelowqos\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660299182000,"stop":1660299182230,"duration":230}},{"uid":"890837a5f9a4b54b","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00044a018>: {\n        Underlying: <*exec.ExitError | 0xc000806160>{\n            ProcessState: {\n                pid: 7166,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 142515},\n                    Stime: {Sec: 0, Usec: 43546},\n                    Maxrss: 79220,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4888,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 376,\n                    Nivcsw: 286,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicelowqos\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660295740000,"stop":1660295740243,"duration":243}},{"uid":"7b6af30713ee0624","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00084ce28>: {\n        Underlying: <*exec.ExitError | 0xc000074920>{\n            ProcessState: {\n                pid: 7254,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 200962},\n                    Stime: {Sec: 0, Usec: 64459},\n                    Maxrss: 80860,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 7546,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 442,\n                    Nivcsw: 427,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicelowqos\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660293657000,"stop":1660293657334,"duration":334}},{"uid":"94c4e83ac5902d5","status":"passed","time":{"start":1660113628000,"stop":1660113631562,"duration":3562}},{"uid":"2f74166b31149e41","status":"passed","time":{"start":1660104898000,"stop":1660104902701,"duration":4701}},{"uid":"90ab2f86812d46bd","status":"passed","time":{"start":1660067259000,"stop":1660067262335,"duration":3335}},{"uid":"83ca3d46dbaac9da","status":"passed","time":{"start":1660047551000,"stop":1660047554342,"duration":3342}},{"uid":"6d80b4ff5f31083a","status":"passed","time":{"start":1659982549000,"stop":1659982553592,"duration":4592}},{"uid":"86d596614a5d7b54","status":"passed","time":{"start":1659970182000,"stop":1659970186679,"duration":4679}},{"uid":"fba8cf35a94aaafd","status":"passed","time":{"start":1659944495000,"stop":1659944524059,"duration":29059}},{"uid":"38a742fa72b5bcfb","status":"passed","time":{"start":1659874977000,"stop":1659874980291,"duration":3291}},{"uid":"2b04f2855ad7c278","status":"passed","time":{"start":1659496050000,"stop":1659496075448,"duration":25448}},{"uid":"82b45c9001fb3a6a","status":"passed","time":{"start":1659496083000,"stop":1659496087249,"duration":4249}},{"uid":"33b73272ae51e309","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1659451501000,"stop":1659451561498,"duration":60498}},{"uid":"525f0d7132df5920","status":"passed","time":{"start":1659450916000,"stop":1659450952639,"duration":36639}},{"uid":"c331e1b7d91ad959","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1659451524000,"stop":1659451584358,"duration":60358}},{"uid":"edc7dd8185a598bb","status":"passed","time":{"start":1659447298000,"stop":1659447301305,"duration":3305}},{"uid":"45c09e1f261b960","status":"passed","time":{"start":1659447116000,"stop":1659447138934,"duration":22934}},{"uid":"44c564d73bc0f695","status":"passed","time":{"start":1659448205000,"stop":1659448209454,"duration":4454}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests slice with netpol should install slice on each worker cluster with correct namespaceisolationprofile":{"statistic":{"failed":0,"broken":1,"skipped":28,"passed":10,"unknown":0,"total":39},"items":[{"uid":"7f42d02ee97e57ba","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"77bb8a277c2f216","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"5b93153683a52b0f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"e7dcda63f7952683","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"ec51288878fc5a11","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"76ba8125e909631a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"90307464dcdff2ea","status":"broken","statusDetails":"runtime error: invalid memory address or nil pointer dereference","time":{"start":1660067259000,"stop":1660067272200,"duration":13200}},{"uid":"6cbfa9832ad737b5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}},{"uid":"403ffcb6c198bd03","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659982549000,"stop":1659982549000,"duration":0}},{"uid":"fa9b719fe109b074","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659970182000,"stop":1659970182000,"duration":0}},{"uid":"d5aae204fb47f7fc","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659944495000,"stop":1659944495000,"duration":0}},{"uid":"59c02aef97f9e552","status":"passed","time":{"start":1659874977000,"stop":1659874977132,"duration":132}},{"uid":"9987cde71c4b5d60","status":"passed","time":{"start":1659496050000,"stop":1659496050134,"duration":134}},{"uid":"5b454783fba4bc6c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496083000,"stop":1659496083000,"duration":0}},{"uid":"4971df4f3f5485c9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"e8f20504e5a02d03","status":"passed","time":{"start":1659450916000,"stop":1659450916255,"duration":255}},{"uid":"afd3f0f77a6bb672","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"cf49f17dfff515d4","status":"passed","time":{"start":1659447298000,"stop":1659447300329,"duration":2329}},{"uid":"22b7bf83539454f","status":"passed","time":{"start":1659447116000,"stop":1659447116154,"duration":154}},{"uid":"54522e06a9c70b1f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448205000,"stop":1659448205000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Namespace reconcile test scenarios: Delete deboarded app ns gets deleted":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":37,"unknown":0,"total":39},"items":[{"uid":"8baaf6138f0968e6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"8667aa84b2de2312","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"b75ab78ef085c5e5","status":"passed","time":{"start":1660295740000,"stop":1660295750533,"duration":10533}},{"uid":"eff33a8e00776e17","status":"passed","time":{"start":1660293657000,"stop":1660293667716,"duration":10716}},{"uid":"2831a67a404a7357","status":"passed","time":{"start":1660113628000,"stop":1660113638487,"duration":10487}},{"uid":"8f676df91669f63","status":"passed","time":{"start":1660104898000,"stop":1660104908574,"duration":10574}},{"uid":"cd93f51a1ab83ad3","status":"passed","time":{"start":1660067259000,"stop":1660067269534,"duration":10534}},{"uid":"13fcd3859cc66725","status":"passed","time":{"start":1660047551000,"stop":1660047562318,"duration":11318}},{"uid":"b9ac4c54f7882360","status":"passed","time":{"start":1659982549000,"stop":1659982560511,"duration":11511}},{"uid":"7078296c0949d1b2","status":"passed","time":{"start":1659970182000,"stop":1659970193563,"duration":11563}},{"uid":"82b61b916305481b","status":"passed","time":{"start":1659944495000,"stop":1659944505858,"duration":10858}},{"uid":"e3e4c9cca4c428fa","status":"passed","time":{"start":1659874977000,"stop":1659874987822,"duration":10822}},{"uid":"302f5fc2e67c7319","status":"passed","time":{"start":1659496050000,"stop":1659496060456,"duration":10456}},{"uid":"b49a5854e44e3829","status":"passed","time":{"start":1659496083000,"stop":1659496093545,"duration":10545}},{"uid":"cb718d0bfdd9f774","status":"passed","time":{"start":1659451501000,"stop":1659451511554,"duration":10554}},{"uid":"2e6fea764baab7e0","status":"passed","time":{"start":1659450916000,"stop":1659450927053,"duration":11053}},{"uid":"3df2d0f6ed5e9fd3","status":"passed","time":{"start":1659451524000,"stop":1659451535530,"duration":11530}},{"uid":"ced8dd2e69aa5a8","status":"passed","time":{"start":1659447298000,"stop":1659447308599,"duration":10599}},{"uid":"bd596f220a630598","status":"passed","time":{"start":1659447116000,"stop":1659447126500,"duration":10500}},{"uid":"c79eb12720ea847d","status":"passed","time":{"start":1659448205000,"stop":1659448215609,"duration":10609}}]},"Istio Suite:Istio Suite#[It] Bookinfo tests Bookinfo Scenario 1 without egress and ingress Should install Productpage on worker cluster-1":{"statistic":{"failed":14,"broken":0,"skipped":0,"passed":4,"unknown":0,"total":18},"items":[{"uid":"76fda372e9457a6e","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0006011a0>: {\n        Underlying: <*exec.ExitError | 0xc0006da240>{\n            ProcessState: {\n                pid: 6868,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 168402},\n                    Stime: {Sec: 0, Usec: 64153},\n                    Maxrss: 77772,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 8372,\n                    Majflt: 1,\n                    Nswap: 0,\n                    Inblock: 112,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 421,\n                    Nivcsw: 411,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicewithoutegressingress\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicewithoutegressingress\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicewithoutegressingress\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicewithoutegressingress\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicewithoutegressingress\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660808584000,"stop":1660808584237,"duration":237}},{"uid":"ba0db05345783323","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 4\nto equal\n    <int>: 1","time":{"start":1660804840000,"stop":1660805020406,"duration":180406}},{"uid":"d30390f34ca90f0d","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1660804470000,"stop":1660804650444,"duration":180444}},{"uid":"6f709e38d8a2980e","status":"failed","statusDetails":"Timed out after 180.002s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1660748849000,"stop":1660749029594,"duration":180594}},{"uid":"312de3a613513a5e","status":"passed","time":{"start":1660748706000,"stop":1660748727700,"duration":21700}},{"uid":"72eba15a90f956f4","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1660748558000,"stop":1660748738563,"duration":180563}},{"uid":"f5e1237117d6b332","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1660636759000,"stop":1660636939419,"duration":180419}},{"uid":"b9a6471c5c57e5c4","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 4\nto equal\n    <int>: 1","time":{"start":1660636482000,"stop":1660636662822,"duration":180822}},{"uid":"cb39f278d4eabb65","status":"passed","time":{"start":1660632006000,"stop":1660632027007,"duration":21007}},{"uid":"2a49cd9a3e307384","status":"passed","time":{"start":1660624273000,"stop":1660624291676,"duration":18676}},{"uid":"44ff2579c99eec10","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1660624220000,"stop":1660624425751,"duration":205751}},{"uid":"67f952fc045bf4a7","status":"passed","time":{"start":1660453205000,"stop":1660453394126,"duration":189126}},{"uid":"3db48f8e8a7240f9","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 4\nto equal\n    <int>: 1","time":{"start":1660452554000,"stop":1660452734411,"duration":180411}},{"uid":"20e3b38b712acca2","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1660452412000,"stop":1660452592754,"duration":180754}},{"uid":"8db78ef4eadff978","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1660415422000,"stop":1660415602555,"duration":180555}},{"uid":"4663b832c396a05c","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000e318>: {\n        Underlying: <*exec.ExitError | 0xc000608040>{\n            ProcessState: {\n                pid: 6897,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 186990},\n                    Stime: {Sec: 0, Usec: 56910},\n                    Maxrss: 81616,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 11952,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 361,\n                    Nivcsw: 491,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicewithoutegressingress\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicewithoutegressingress\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicewithoutegressingress\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicewithoutegressingress\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicewithoutegressingress\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660298968000,"stop":1660298968220,"duration":220}},{"uid":"339e01b43ee4a1b8","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00080e9a8>: {\n        Underlying: <*exec.ExitError | 0xc000870ca0>{\n            ProcessState: {\n                pid: 6952,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 168687},\n                    Stime: {Sec: 0, Usec: 58844},\n                    Maxrss: 78824,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 11658,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 459,\n                    Nivcsw: 462,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicewithoutegressingress\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicewithoutegressingress\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicewithoutegressingress\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicewithoutegressingress\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicewithoutegressingress\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660295522000,"stop":1660295522201,"duration":201}},{"uid":"9009b3e1ffe5bb45","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0007a0a80>: {\n        Underlying: <*exec.ExitError | 0xc00091b120>{\n            ProcessState: {\n                pid: 6864,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 195725},\n                    Stime: {Sec: 0, Usec: 47932},\n                    Maxrss: 83840,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 7210,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 296,\n                    Nivcsw: 287,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicewithoutegressingress\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicewithoutegressingress\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicewithoutegressingress\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicewithoutegressingress\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicewithoutegressingress\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660293414000,"stop":1660293414233,"duration":233}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting mesh-dns pod Check ping between iperf-server and iperf-client after mesh-dns pod restart":{"statistic":{"failed":0,"broken":0,"skipped":39,"passed":0,"unknown":0,"total":39},"items":[{"uid":"4d75542c9f4f09a1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"4662c5300922074d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"5a44f16fcfd50051","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"3bee2a48eb5c6e6a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"9a90dd3bcc877f76","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"d67a0bffcb44f2a0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"5ebbcb010527076","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"2664ad03adc8ecca","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}},{"uid":"6a7c247e6bf8fd66","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659982549000,"stop":1659982549000,"duration":0}},{"uid":"37ac04dd656c246f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659970182000,"stop":1659970182000,"duration":0}},{"uid":"be287ec5f222a53b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659944495000,"stop":1659944495000,"duration":0}},{"uid":"2bf4cd3d8954f19d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659874977000,"stop":1659874977000,"duration":0}},{"uid":"8f4690fb2dbbbf06","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496050000,"stop":1659496050000,"duration":0}},{"uid":"1a7a52786a0a8e13","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496083000,"stop":1659496083000,"duration":0}},{"uid":"12dfb30642958397","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"ff5e7d4726cbb999","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659450916000,"stop":1659450916000,"duration":0}},{"uid":"890b78ee163eae11","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"c84b9a00f55c6c8d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447298000,"stop":1659447298000,"duration":0}},{"uid":"62beff7c1453800e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447116000,"stop":1659447116000,"duration":0}},{"uid":"7777d458be9e5e75","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448205000,"stop":1659448205000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice creation scenarios: when slice red is installed install slice blue workers should get attached to slice blue":{"statistic":{"failed":0,"broken":0,"skipped":27,"passed":12,"unknown":0,"total":39},"items":[{"uid":"a1c7b24a9868efce","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"b0b113a3f84633da","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"9292148a6305abe5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"e9b50a3c93bd90dd","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"915e33568f1b60f0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"6318749ab84150f0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"20ab880307c7c7a8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"ca501e21f13e3dbf","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}},{"uid":"4660aacdce62685a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659982549000,"stop":1659982549000,"duration":0}},{"uid":"f3c33ef94876bd2b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659970182000,"stop":1659970182000,"duration":0}},{"uid":"ecf08bf8622b2ee","status":"passed","time":{"start":1659944495000,"stop":1659944497277,"duration":2277}},{"uid":"1db3c3d39b20d8bc","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659874977000,"stop":1659874977000,"duration":0}},{"uid":"49f77de0f662eb1b","status":"passed","time":{"start":1659496050000,"stop":1659496050459,"duration":459}},{"uid":"5f8b1d4a52c83682","status":"passed","time":{"start":1659496083000,"stop":1659496084771,"duration":1771}},{"uid":"1701bfa31be17b24","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"d081d016ea6b2cab","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659450916000,"stop":1659450916000,"duration":0}},{"uid":"8f4eaacdd311b8c4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"fabdac3c5410891e","status":"passed","time":{"start":1659447298000,"stop":1659447299728,"duration":1728}},{"uid":"827d13e1354407a4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447116000,"stop":1659447116000,"duration":0}},{"uid":"810f052b5bf059c8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448205000,"stop":1659448205000,"duration":0}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests slice with netpol should have vl3 router running":{"statistic":{"failed":27,"broken":0,"skipped":0,"passed":12,"unknown":0,"total":39},"items":[{"uid":"3271f69ef5bca0e5","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0001266f0>: {\n        Underlying: <*exec.ExitError | 0xc000663480>{\n            ProcessState: {\n                pid: 7142,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 166188},\n                    Stime: {Sec: 0, Usec: 47482},\n                    Maxrss: 86340,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4052,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 391,\n                    Nivcsw: 257,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660808797000,"stop":1660808797552,"duration":552}},{"uid":"974d621c049fb879","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000350948>: {\n        Underlying: <*exec.ExitError | 0xc000828e60>{\n            ProcessState: {\n                pid: 7252,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 133880},\n                    Stime: {Sec: 0, Usec: 44626},\n                    Maxrss: 82964,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2723,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 590,\n                    Nivcsw: 512,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660299182000,"stop":1660299182459,"duration":459}},{"uid":"c662752d8114479a","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004ca618>: {\n        Underlying: <*exec.ExitError | 0xc00065fc60>{\n            ProcessState: {\n                pid: 7215,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 141036},\n                    Stime: {Sec: 0, Usec: 43094},\n                    Maxrss: 74312,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3629,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 312,\n                    Nivcsw: 567,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660295740000,"stop":1660295740433,"duration":433}},{"uid":"f16388458dc45ba2","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000649860>: {\n        Underlying: <*exec.ExitError | 0xc000828ea0>{\n            ProcessState: {\n                pid: 7148,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 192477},\n                    Stime: {Sec: 0, Usec: 55604},\n                    Maxrss: 73856,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 5400,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 430,\n                    Nivcsw: 736,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660293657000,"stop":1660293657539,"duration":539}},{"uid":"9006817b9c332608","status":"passed","time":{"start":1660113628000,"stop":1660113649243,"duration":21243}},{"uid":"e827510464e03727","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 4\nto equal\n    <int>: 1","time":{"start":1660104898000,"stop":1660105093414,"duration":195414}},{"uid":"d064732320ea6e1e","status":"passed","time":{"start":1660067259000,"stop":1660067259349,"duration":349}},{"uid":"7e2022ee47a9ac83","status":"failed","statusDetails":"Unexpected error:\n    <*errors.StatusError | 0xc00069a280>: {\n        ErrStatus: {\n            TypeMeta: {Kind: \"\", APIVersion: \"\"},\n            ListMeta: {\n                SelfLink: \"\",\n                ResourceVersion: \"\",\n                Continue: \"\",\n                RemainingItemCount: nil,\n            },\n            Status: \"Failure\",\n            Message: \"namespaces \\\"iperf\\\" already exists\",\n            Reason: \"AlreadyExists\",\n            Details: {Name: \"iperf\", Group: \"\", Kind: \"namespaces\", UID: \"\", Causes: nil, RetryAfterSeconds: 0},\n            Code: 409,\n        },\n    }\n    namespaces \"iperf\" already exists\noccurred","time":{"start":1660047551000,"stop":1660047559604,"duration":8604}},{"uid":"228eb1717e3625a5","status":"failed","statusDetails":"Unexpected error:\n    <*errors.StatusError | 0xc000394280>: {\n        ErrStatus: {\n            TypeMeta: {Kind: \"\", APIVersion: \"\"},\n            ListMeta: {\n                SelfLink: \"\",\n                ResourceVersion: \"\",\n                Continue: \"\",\n                RemainingItemCount: nil,\n            },\n            Status: \"Failure\",\n            Message: \"namespaces \\\"iperf\\\" already exists\",\n            Reason: \"AlreadyExists\",\n            Details: {Name: \"iperf\", Group: \"\", Kind: \"namespaces\", UID: \"\", Causes: nil, RetryAfterSeconds: 0},\n            Code: 409,\n        },\n    }\n    namespaces \"iperf\" already exists\noccurred","time":{"start":1659982549000,"stop":1659982559138,"duration":10138}},{"uid":"7a13ad7261e61eeb","status":"failed","statusDetails":"Unexpected error:\n    <*errors.StatusError | 0xc0006e0dc0>: {\n        ErrStatus: {\n            TypeMeta: {Kind: \"\", APIVersion: \"\"},\n            ListMeta: {\n                SelfLink: \"\",\n                ResourceVersion: \"\",\n                Continue: \"\",\n                RemainingItemCount: nil,\n            },\n            Status: \"Failure\",\n            Message: \"namespaces \\\"iperf\\\" already exists\",\n            Reason: \"AlreadyExists\",\n            Details: {Name: \"iperf\", Group: \"\", Kind: \"namespaces\", UID: \"\", Causes: nil, RetryAfterSeconds: 0},\n            Code: 409,\n        },\n    }\n    namespaces \"iperf\" already exists\noccurred","time":{"start":1659970182000,"stop":1659970190617,"duration":8617}},{"uid":"14d087b532cce9ff","status":"failed","statusDetails":"Unexpected error:\n    <*errors.StatusError | 0xc000623b80>: {\n        ErrStatus: {\n            TypeMeta: {Kind: \"\", APIVersion: \"\"},\n            ListMeta: {\n                SelfLink: \"\",\n                ResourceVersion: \"\",\n                Continue: \"\",\n                RemainingItemCount: nil,\n            },\n            Status: \"Failure\",\n            Message: \"namespaces \\\"iperf\\\" already exists\",\n            Reason: \"AlreadyExists\",\n            Details: {Name: \"iperf\", Group: \"\", Kind: \"namespaces\", UID: \"\", Causes: nil, RetryAfterSeconds: 0},\n            Code: 409,\n        },\n    }\n    namespaces \"iperf\" already exists\noccurred","time":{"start":1659944495000,"stop":1659944504718,"duration":9718}},{"uid":"b62d7ebb05e5a750","status":"passed","time":{"start":1659874977000,"stop":1659874981259,"duration":4259}},{"uid":"4a93dfd562fc842c","status":"passed","time":{"start":1659496050000,"stop":1659496052423,"duration":2423}},{"uid":"61fb6ffe68b4292d","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 4\nto equal\n    <int>: 1","time":{"start":1659496083000,"stop":1659496278462,"duration":195462}},{"uid":"1d9ce49b513f6cd2","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1659451501000,"stop":1659451697487,"duration":196487}},{"uid":"5665676d1af1e9e","status":"passed","time":{"start":1659450916000,"stop":1659450916458,"duration":458}},{"uid":"e3ec03f44d19d720","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1659451524000,"stop":1659451720050,"duration":196050}},{"uid":"2077c6886480d7c0","status":"passed","time":{"start":1659447298000,"stop":1659447298229,"duration":229}},{"uid":"4461fd0744d8df7","status":"passed","time":{"start":1659447116000,"stop":1659447120325,"duration":4325}},{"uid":"85380ea922ac0c39","status":"failed","statusDetails":"Unexpected error:\n    <*errors.StatusError | 0xc000a152c0>: {\n        ErrStatus: {\n            TypeMeta: {Kind: \"\", APIVersion: \"\"},\n            ListMeta: {\n                SelfLink: \"\",\n                ResourceVersion: \"\",\n                Continue: \"\",\n                RemainingItemCount: nil,\n            },\n            Status: \"Failure\",\n            Message: \"namespaces \\\"iperf\\\" already exists\",\n            Reason: \"AlreadyExists\",\n            Details: {Name: \"iperf\", Group: \"\", Kind: \"namespaces\", UID: \"\", Causes: nil, RetryAfterSeconds: 0},\n            Code: 409,\n        },\n    }\n    namespaces \"iperf\" already exists\noccurred","time":{"start":1659448205000,"stop":1659448208194,"duration":3194}}]},"Empty Suite:Empty Suite#[BeforeSuite]":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":46,"unknown":0,"total":46},"items":[{"uid":"565c6f8be922d16b","status":"passed","time":{"start":1660828377000,"stop":1660828377000,"duration":0}},{"uid":"332be947dda4bc85","status":"passed","time":{"start":1660807615000,"stop":1660807615000,"duration":0}},{"uid":"12af38ed20f38a6f","status":"passed","time":{"start":1660803821000,"stop":1660803821000,"duration":0}},{"uid":"1a62543976d0c38f","status":"passed","time":{"start":1660805590000,"stop":1660805590000,"duration":0}},{"uid":"4439f42fb3326f30","status":"passed","time":{"start":1660803559000,"stop":1660803559000,"duration":0}},{"uid":"bfc0a5aea6cb652b","status":"passed","time":{"start":1660802965000,"stop":1660802965000,"duration":0}},{"uid":"72eacea2bbb27842","status":"passed","time":{"start":1660798299000,"stop":1660798299000,"duration":0}},{"uid":"c6f8e1a244731743","status":"passed","time":{"start":1660747846000,"stop":1660747846002,"duration":2}},{"uid":"43b6cc6a45404ad9","status":"passed","time":{"start":1660747702000,"stop":1660747702000,"duration":0}},{"uid":"dbd1a8f902696814","status":"passed","time":{"start":1660747595000,"stop":1660747595000,"duration":0}},{"uid":"c590801e12a920bb","status":"passed","time":{"start":1660747499000,"stop":1660747499000,"duration":0}},{"uid":"eab0b70f754875b8","status":"passed","time":{"start":1660747927000,"stop":1660747927000,"duration":0}},{"uid":"92fc234fe051573","status":"passed","time":{"start":1660722528000,"stop":1660722528000,"duration":0}},{"uid":"57468a1430e8dec4","status":"passed","time":{"start":1660711963000,"stop":1660711963000,"duration":0}},{"uid":"e0c074de0edccd86","status":"passed","time":{"start":1660635769000,"stop":1660635769000,"duration":0}},{"uid":"529309ee1dde43bf","status":"passed","time":{"start":1660635489000,"stop":1660635489000,"duration":0}},{"uid":"4877465eeba37118","status":"passed","time":{"start":1660635683000,"stop":1660635683000,"duration":0}},{"uid":"f519cad201d950b8","status":"passed","time":{"start":1660635796000,"stop":1660635796000,"duration":0}},{"uid":"52649a4a802711ad","status":"passed","time":{"start":1660631001000,"stop":1660631001000,"duration":0}},{"uid":"6bd0c2f20e6423d1","status":"passed","time":{"start":1660623317000,"stop":1660623317000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice validation Slice with MEDIUM QOS should have vl3 router running":{"statistic":{"failed":3,"broken":0,"skipped":0,"passed":6,"unknown":0,"total":9},"items":[{"uid":"24dbffb43308dcfe","status":"passed","time":{"start":1659168739000,"stop":1659168739689,"duration":689}},{"uid":"32d8b5c697e6c847","status":"passed","time":{"start":1659164084000,"stop":1659164084631,"duration":631}},{"uid":"49c7996737d1d261","status":"passed","time":{"start":1659160188000,"stop":1659160188690,"duration":690}},{"uid":"dad5cb3f70e541bb","status":"passed","time":{"start":1659119724000,"stop":1659119724538,"duration":538}},{"uid":"3696a0b392e3f2eb","status":"passed","time":{"start":1659116511000,"stop":1659116511419,"duration":419}},{"uid":"df9e0e068ede27d4","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 3\nto equal\n    <int>: 1","time":{"start":1659109470000,"stop":1659109530750,"duration":60750}},{"uid":"b166459c6ae18a80","status":"passed","time":{"start":1659106836000,"stop":1659106836774,"duration":774}},{"uid":"55bf1b7112ff65f6","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1659101522000,"stop":1659101582765,"duration":60765}},{"uid":"6b9bc19dfb598b06","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <int>: 4\nto equal\n    <int>: 1","time":{"start":1659082816000,"stop":1659082876914,"duration":60914}}]},"Worker Suite:Worker Suite#[It] Slice validation Slice with MEDIUM QOS should reconcile medium qos slice":{"statistic":{"failed":5,"broken":0,"skipped":4,"passed":0,"unknown":0,"total":9},"items":[{"uid":"531abd47defaec11","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004a9788>: {\n        Underlying: <*exec.ExitError | 0xc00040d860>{\n            ProcessState: {\n                pid: 7260,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 72127},\n                    Stime: {Sec: 0, Usec: 18031},\n                    Maxrss: 43584,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4297,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 230,\n                    Nivcsw: 359,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicemediumqos\\\" not found\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicemediumqos\\\" not found\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicemediumqos\\\" not found\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicemediumqos\\\" not found\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (NotFound): slice.networking.kubeslice.io \"slicemediumqos\" not found\noccurred","time":{"start":1659168739000,"stop":1659168740414,"duration":1414}},{"uid":"46130730b1f11a1a","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00087c120>: {\n        Underlying: <*exec.ExitError | 0xc00069c5e0>{\n            ProcessState: {\n                pid: 7406,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 74896},\n                    Stime: {Sec: 0, Usec: 12482},\n                    Maxrss: 43348,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3354,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 199,\n                    Nivcsw: 297,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicemediumqos\\\" not found\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicemediumqos\\\" not found\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicemediumqos\\\" not found\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicemediumqos\\\" not found\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (NotFound): slice.networking.kubeslice.io \"slicemediumqos\" not found\noccurred","time":{"start":1659164084000,"stop":1659164085098,"duration":1098}},{"uid":"cdf275d41a8481c7","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00049ad20>: {\n        Underlying: <*exec.ExitError | 0xc000843c20>{\n            ProcessState: {\n                pid: 7452,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 44988},\n                    Stime: {Sec: 0, Usec: 22494},\n                    Maxrss: 41588,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2497,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 210,\n                    Nivcsw: 113,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicemediumqos\\\" not found\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicemediumqos\\\" not found\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicemediumqos\\\" not found\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicemediumqos\\\" not found\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (NotFound): slice.networking.kubeslice.io \"slicemediumqos\" not found\noccurred","time":{"start":1659160188000,"stop":1659160188732,"duration":732}},{"uid":"7f1c6ad507808e9d","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000347c38>: {\n        Underlying: <*exec.ExitError | 0xc000778740>{\n            ProcessState: {\n                pid: 8030,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 59177},\n                    Stime: {Sec: 0, Usec: 12680},\n                    Maxrss: 42032,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2537,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 257,\n                    Nivcsw: 140,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicemediumqos\\\" not found\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicemediumqos\\\" not found\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicemediumqos\\\" not found\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicemediumqos\\\" not found\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (NotFound): slice.networking.kubeslice.io \"slicemediumqos\" not found\noccurred","time":{"start":1659119724000,"stop":1659119725126,"duration":1126}},{"uid":"88a740814282788f","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004e9560>: {\n        Underlying: <*exec.ExitError | 0xc0004510c0>{\n            ProcessState: {\n                pid: 7542,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 66414},\n                    Stime: {Sec: 0, Usec: 15626},\n                    Maxrss: 45792,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2977,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 225,\n                    Nivcsw: 318,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicemediumqos\\\" not found\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicemediumqos\\\" not found\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicemediumqos\\\" not found\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicemediumqos\\\" not found\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (NotFound): slice.networking.kubeslice.io \"slicemediumqos\" not found\noccurred","time":{"start":1659116511000,"stop":1659116511874,"duration":874}},{"uid":"f1ae2dd55b9e07f8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659109470000,"stop":1659109470000,"duration":0}},{"uid":"afe8c318d5877363","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659106836000,"stop":1659106836000,"duration":0}},{"uid":"cffa1fbdfce28b40","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659101522000,"stop":1659101522000,"duration":0}},{"uid":"ce0be5d753f333ab","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659082816000,"stop":1659082816000,"duration":0}}]},"Hub Suite:Hub Suite#[It] Cluster negative tests Worker Cluster Registration with Wrong clustername":{"statistic":{"failed":0,"broken":0,"skipped":17,"passed":107,"unknown":0,"total":124},"items":[{"uid":"e0f1490e36dfcb8c","status":"passed","time":{"start":1660828517000,"stop":1660828705722,"duration":188722}},{"uid":"4a319e36b33925e4","status":"passed","time":{"start":1660807774000,"stop":1660807958573,"duration":184573}},{"uid":"72e184f8e6eae9fc","status":"passed","time":{"start":1660803998000,"stop":1660804186699,"duration":188699}},{"uid":"12dd00ba3d4fd992","status":"passed","time":{"start":1660805737000,"stop":1660805921514,"duration":184514}},{"uid":"c181b51b20be171f","status":"passed","time":{"start":1660803696000,"stop":1660803831917,"duration":135917}},{"uid":"551e51dd3f9765b1","status":"passed","time":{"start":1660798468000,"stop":1660798652153,"duration":184153}},{"uid":"b43a2c8bd456a55","status":"passed","time":{"start":1660748009000,"stop":1660748187527,"duration":178527}},{"uid":"1195c44e73677923","status":"passed","time":{"start":1660747865000,"stop":1660748021213,"duration":156213}},{"uid":"51cb6562d2be93c9","status":"passed","time":{"start":1660747735000,"stop":1660747925531,"duration":190531}},{"uid":"4db80b91b2d1aa45","status":"passed","time":{"start":1660747654000,"stop":1660747835371,"duration":181371}},{"uid":"66c114fa4aaf9694","status":"passed","time":{"start":1660722683000,"stop":1660722871253,"duration":188253}},{"uid":"d9f570532dd59f09","status":"passed","time":{"start":1660635916000,"stop":1660636105188,"duration":189188}},{"uid":"57d039792e791673","status":"passed","time":{"start":1660635666000,"stop":1660635854363,"duration":188363}},{"uid":"f97f9707682e0949","status":"passed","time":{"start":1660635860000,"stop":1660636019600,"duration":159600}},{"uid":"7021dd83e05aacda","status":"passed","time":{"start":1660631164000,"stop":1660631351350,"duration":187350}},{"uid":"14c3337438fac0c9","status":"passed","time":{"start":1660623463000,"stop":1660623651764,"duration":188764}},{"uid":"80bd257330427523","status":"passed","time":{"start":1660623384000,"stop":1660623571711,"duration":187711}},{"uid":"2d5c1baa7dd66a2b","status":"passed","time":{"start":1660623360000,"stop":1660623501802,"duration":141802}},{"uid":"583110929decdff3","status":"passed","time":{"start":1660452377000,"stop":1660452566554,"duration":189554}},{"uid":"91ba9caad38bb115","status":"passed","time":{"start":1660451870000,"stop":1660451925235,"duration":55235}}]},"Istio Suite:Istio Suite#[It] Bookinfo tests Bookinfo Scenario 1 without egress and ingress Should install Details, Reviews, Ratings and Serviceexport on worker cluster-2":{"statistic":{"failed":1,"broken":0,"skipped":14,"passed":3,"unknown":0,"total":18},"items":[{"uid":"34785ef689fcdd91","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808584000,"stop":1660808584000,"duration":0}},{"uid":"74031168cf0706ce","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660804840000,"stop":1660804840000,"duration":0}},{"uid":"61eecd02e34c16d6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660804470000,"stop":1660804470000,"duration":0}},{"uid":"7108bb5b53583b18","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660748849000,"stop":1660748849000,"duration":0}},{"uid":"a40a07e3526c29c9","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1660748706000,"stop":1660748791693,"duration":85693}},{"uid":"829ef660667be168","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660748558000,"stop":1660748558000,"duration":0}},{"uid":"cb4b4e8312e5f55e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660636759000,"stop":1660636759000,"duration":0}},{"uid":"a0fc0f58e3772833","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660636482000,"stop":1660636482000,"duration":0}},{"uid":"78e24875e138334e","status":"passed","time":{"start":1660632006000,"stop":1660632039609,"duration":33609}},{"uid":"82f1c555495069f3","status":"passed","time":{"start":1660624273000,"stop":1660624307875,"duration":34875}},{"uid":"c88e8e82d20891c4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660624220000,"stop":1660624220000,"duration":0}},{"uid":"40f372dc5c728b1b","status":"passed","time":{"start":1660453205000,"stop":1660453306642,"duration":101642}},{"uid":"79d221c400ded796","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660452554000,"stop":1660452554000,"duration":0}},{"uid":"a08002738cc5a266","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660452412000,"stop":1660452412000,"duration":0}},{"uid":"d1daef45d7c191a6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660415422000,"stop":1660415422000,"duration":0}},{"uid":"7f28d114ab93e186","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660298968000,"stop":1660298968000,"duration":0}},{"uid":"e45343586a5f6400","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295522000,"stop":1660295522000,"duration":0}},{"uid":"80590ac7b0a8832c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293414000,"stop":1660293414000,"duration":0}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Update Should fail when Project is Applied with service account name as blank in Read users":{"statistic":{"failed":7,"broken":0,"skipped":0,"passed":114,"unknown":0,"total":121},"items":[{"uid":"eca5f612237403e4","status":"passed","time":{"start":1660828517000,"stop":1660828522898,"duration":5898}},{"uid":"eee82ae02fb02173","status":"passed","time":{"start":1660807774000,"stop":1660807779807,"duration":5807}},{"uid":"98d73f968231e571","status":"passed","time":{"start":1660803998000,"stop":1660804002906,"duration":4906}},{"uid":"8fc8eda014a29820","status":"passed","time":{"start":1660805737000,"stop":1660805742731,"duration":5731}},{"uid":"772cc4236414679d","status":"passed","time":{"start":1660803696000,"stop":1660803700734,"duration":4734}},{"uid":"8cda4d3c8c03fb97","status":"passed","time":{"start":1660798468000,"stop":1660798473804,"duration":5804}},{"uid":"6fdb8eadae9af526","status":"passed","time":{"start":1660748009000,"stop":1660748015085,"duration":6085}},{"uid":"6ebfec86c5a2188a","status":"passed","time":{"start":1660747865000,"stop":1660747870958,"duration":5958}},{"uid":"7ced273e70bf00a8","status":"passed","time":{"start":1660747735000,"stop":1660747739822,"duration":4822}},{"uid":"7eff822553832853","status":"passed","time":{"start":1660747654000,"stop":1660747658669,"duration":4669}},{"uid":"69757f59a53d9813","status":"passed","time":{"start":1660722683000,"stop":1660722687958,"duration":4958}},{"uid":"5589cc0e6712b24f","status":"passed","time":{"start":1660635916000,"stop":1660635921859,"duration":5859}},{"uid":"a3964dc5d3385369","status":"passed","time":{"start":1660635666000,"stop":1660635670887,"duration":4887}},{"uid":"58cb350a038400c0","status":"passed","time":{"start":1660635860000,"stop":1660635864936,"duration":4936}},{"uid":"94130334f970b5c3","status":"passed","time":{"start":1660631164000,"stop":1660631168691,"duration":4691}},{"uid":"34e4ac640e3fa36d","status":"passed","time":{"start":1660623463000,"stop":1660623467876,"duration":4876}},{"uid":"cd6bdd70ed71b463","status":"passed","time":{"start":1660623384000,"stop":1660623389673,"duration":5673}},{"uid":"1555a2533ed33e05","status":"passed","time":{"start":1660623360000,"stop":1660623364631,"duration":4631}},{"uid":"4b78b7262498c1a8","status":"passed","time":{"start":1660452377000,"stop":1660452381869,"duration":4869}},{"uid":"95ac0f9f2e02068a","status":"passed","time":{"start":1660451870000,"stop":1660451875797,"duration":5797}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting mesh-dns pod Should restart mesh-dns pod":{"statistic":{"failed":0,"broken":0,"skipped":39,"passed":0,"unknown":0,"total":39},"items":[{"uid":"492cded192505f05","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"cd3931edeb60e724","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"e69aed4f153b962","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"ce5b72621fa27679","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"be37a4982c5d2ea4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"22b3e07e8562cb","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"6087ee1466eafde6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"24b8df24665b65fc","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}},{"uid":"53c64af72ee9723d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659982549000,"stop":1659982549000,"duration":0}},{"uid":"9838ea87ba03d216","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659970182000,"stop":1659970182000,"duration":0}},{"uid":"df430113d776a1f3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659944495000,"stop":1659944495000,"duration":0}},{"uid":"7824264acb7db787","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659874977000,"stop":1659874977000,"duration":0}},{"uid":"f7f85e0279f839e0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496050000,"stop":1659496050000,"duration":0}},{"uid":"af58336646fdf682","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496083000,"stop":1659496083000,"duration":0}},{"uid":"f5f09aefb76b112d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"dc6080f579816089","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659450916000,"stop":1659450916000,"duration":0}},{"uid":"9b3658bbd778a2e6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"59fe015355d48e66","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447298000,"stop":1659447298000,"duration":0}},{"uid":"b866e7882a56e578","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447116000,"stop":1659447116000,"duration":0}},{"uid":"ac2e9a9a0c725964","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448205000,"stop":1659448205000,"duration":0}}]},"Empty Suite:Empty Suite#[It] Hub Creation tests Hub Installtion Test Senarios Should fail if Cert is installed again in the same cluster":{"statistic":{"failed":0,"broken":0,"skipped":16,"passed":30,"unknown":0,"total":46},"items":[{"uid":"6dd10f4010477415","status":"passed","time":{"start":1660828377000,"stop":1660828377324,"duration":324}},{"uid":"9f02c92084df0770","status":"passed","time":{"start":1660807615000,"stop":1660807615333,"duration":333}},{"uid":"10e6166caacb6a60","status":"passed","time":{"start":1660803821000,"stop":1660803821273,"duration":273}},{"uid":"6c5b82880463a26d","status":"passed","time":{"start":1660805590000,"stop":1660805590247,"duration":247}},{"uid":"d0fb249c9c8b017c","status":"passed","time":{"start":1660803559000,"stop":1660803559330,"duration":330}},{"uid":"4c04c1d7bde2f574","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660802965000,"stop":1660802965000,"duration":0}},{"uid":"7b8980c20d96ff32","status":"passed","time":{"start":1660798299000,"stop":1660798299378,"duration":378}},{"uid":"a64c3cad8c71a96","status":"passed","time":{"start":1660747846000,"stop":1660747846358,"duration":358}},{"uid":"1fa183bc768f03b0","status":"passed","time":{"start":1660747702000,"stop":1660747702358,"duration":358}},{"uid":"955fd4949b8a4c65","status":"passed","time":{"start":1660747595000,"stop":1660747595338,"duration":338}},{"uid":"14eb4c902bbec550","status":"passed","time":{"start":1660747499000,"stop":1660747499329,"duration":329}},{"uid":"c0de0fbe9f7e6530","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660747927000,"stop":1660747927000,"duration":0}},{"uid":"9f83b377aba22abc","status":"passed","time":{"start":1660722528000,"stop":1660722528320,"duration":320}},{"uid":"ebe448f6114bdc69","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660711963000,"stop":1660711963000,"duration":0}},{"uid":"c370039797e1121b","status":"passed","time":{"start":1660635769000,"stop":1660635769303,"duration":303}},{"uid":"6ea4201edfbdcdce","status":"passed","time":{"start":1660635489000,"stop":1660635489341,"duration":341}},{"uid":"cf3ec47c8a0d8379","status":"passed","time":{"start":1660635683000,"stop":1660635683357,"duration":357}},{"uid":"994f594da84ad3e9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660635796000,"stop":1660635796000,"duration":0}},{"uid":"5938dc97ed6f4b87","status":"passed","time":{"start":1660631001000,"stop":1660631001319,"duration":319}},{"uid":"bb94f2bf14381438","status":"passed","time":{"start":1660623317000,"stop":1660623317367,"duration":367}}]},"Worker Suite:Worker Suite#[It] Namespace reconcile test scenarios: Delete deleted app ns entry should get removed from cluster objs":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":37,"unknown":0,"total":39},"items":[{"uid":"a68c16167562932","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"1784a268ba9df126","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"fd817c32325dd5ba","status":"passed","time":{"start":1660295740000,"stop":1660295750929,"duration":10929}},{"uid":"1ccf3fe80a7095e9","status":"passed","time":{"start":1660293657000,"stop":1660293668363,"duration":11363}},{"uid":"5f0118be17c85c16","status":"passed","time":{"start":1660113628000,"stop":1660113638889,"duration":10889}},{"uid":"8df3fb4cbb9def6c","status":"passed","time":{"start":1660104898000,"stop":1660104909074,"duration":11074}},{"uid":"65a0fe8d141d0c32","status":"passed","time":{"start":1660067259000,"stop":1660067269991,"duration":10991}},{"uid":"42d748857c27a54b","status":"passed","time":{"start":1660047551000,"stop":1660047562383,"duration":11383}},{"uid":"b209b72dc0422951","status":"passed","time":{"start":1659982549000,"stop":1659982561359,"duration":12359}},{"uid":"53a9db4c1f7db2f3","status":"passed","time":{"start":1659970182000,"stop":1659970193965,"duration":11965}},{"uid":"66834cb18738a6f0","status":"passed","time":{"start":1659944495000,"stop":1659944506259,"duration":11259}},{"uid":"f12fe9161ce48b7d","status":"passed","time":{"start":1659874977000,"stop":1659874988105,"duration":11105}},{"uid":"c86fdcd27d17cab","status":"passed","time":{"start":1659496050000,"stop":1659496060888,"duration":10888}},{"uid":"4787146e1cbb6641","status":"passed","time":{"start":1659496083000,"stop":1659496094084,"duration":11084}},{"uid":"e60c7e6600ef522b","status":"passed","time":{"start":1659451501000,"stop":1659451511948,"duration":10948}},{"uid":"67d27253ffbd920e","status":"passed","time":{"start":1659450916000,"stop":1659450927991,"duration":11991}},{"uid":"ab220c411c1137d7","status":"passed","time":{"start":1659451524000,"stop":1659451536762,"duration":12762}},{"uid":"2444cc84e12d8565","status":"passed","time":{"start":1659447298000,"stop":1659447308923,"duration":10923}},{"uid":"4e7b8e9a180536a6","status":"passed","time":{"start":1659447116000,"stop":1659447126973,"duration":10973}},{"uid":"46a0d5a4cf2840a2","status":"passed","time":{"start":1659448205000,"stop":1659448216160,"duration":11160}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting nsm-manager pod Should restart nsm-manager pod":{"statistic":{"failed":0,"broken":0,"skipped":39,"passed":0,"unknown":0,"total":39},"items":[{"uid":"5981e303f107ed9c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"d74cfd4282b121d6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"fc366e0a0c3434b1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"b649882982554a75","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"db55039887229fe","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"1c60e4002c5b7046","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"5cce28e3a0c00b9f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"ea20699f2b725d82","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}},{"uid":"48578733b94b5969","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659982549000,"stop":1659982549000,"duration":0}},{"uid":"2b9a2c67aafc1981","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659970182000,"stop":1659970182000,"duration":0}},{"uid":"ef688033641e9cd3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659944495000,"stop":1659944495000,"duration":0}},{"uid":"d13f544a0c920b38","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659874977000,"stop":1659874977000,"duration":0}},{"uid":"3a74c94948d60ce4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496050000,"stop":1659496050000,"duration":0}},{"uid":"d2588fcbbf054d3a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496083000,"stop":1659496083000,"duration":0}},{"uid":"8d235d38f7079c70","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"69a5604d0e834f69","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659450916000,"stop":1659450916000,"duration":0}},{"uid":"51c3833b2de4f5fc","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"5618f416588183c3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447298000,"stop":1659447298000,"duration":0}},{"uid":"3d3d67ba339b1737","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447116000,"stop":1659447116000,"duration":0}},{"uid":"71dc6633c6def77b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448205000,"stop":1659448205000,"duration":0}}]},"Hub Suite:Hub Suite#[It]  QosProfile tests QosProfile Create validation Creates qosprofile":{"statistic":{"failed":111,"broken":0,"skipped":0,"passed":5,"unknown":0,"total":116},"items":[{"uid":"3de8079968828316","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000488270>: {\n        Underlying: <*exec.ExitError | 0xc0000e0b40>{\n            ProcessState: {\n                pid: 6030,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 179216},\n                    Stime: {Sec: 0, Usec: 81073},\n                    Maxrss: 80148,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 6668,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13616,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 1368,\n                    Nivcsw: 641,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile3757589326\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile3757589326\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile3757589326\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile3757589326\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile3757589326\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1660828517000,"stop":1660828530906,"duration":13906}},{"uid":"ee068619e9d1541c","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000131e48>: {\n        Underlying: <*exec.ExitError | 0xc000820080>{\n            ProcessState: {\n                pid: 6725,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 158241},\n                    Stime: {Sec: 0, Usec: 41642},\n                    Maxrss: 75636,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4434,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13344,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 345,\n                    Nivcsw: 309,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile710553501\\\": Internal error occurred: failed calling webhook \\\"msliceqosconfig.kb.io\\\": failed to call webhook: the server could not find the requested resource\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile710553501\\\": Internal error occurred: failed calling webhook \\\"msliceqosconfig.kb.io\\\": failed to call webhook: the server could not find the requested resource\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile710553501\\\": Internal error occurred: failed calling webhook \\\"msliceqosconfig.kb.io\\\": failed to call webhook: the server could not find the requested resource\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile710553501\\\": Internal error occurred: failed calling webhook \\\"msliceqosconfig.kb.io\\\": failed to call webhook: the server could not find the requested resource\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile710553501\": Internal error occurred: failed calling webhook \"msliceqosconfig.kb.io\": failed to call webhook: the server could not find the requested resource\noccurred","time":{"start":1660807774000,"stop":1660807781539,"duration":7539}},{"uid":"ecd9c5fef726d933","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004ee0a8>: {\n        Underlying: <*exec.ExitError | 0xc00039a0c0>{\n            ProcessState: {\n                pid: 6067,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 165435},\n                    Stime: {Sec: 0, Usec: 101099},\n                    Maxrss: 80344,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 8920,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13616,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 973,\n                    Nivcsw: 519,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile3265629551\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile3265629551\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile3265629551\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile3265629551\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile3265629551\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1660803998000,"stop":1660804006801,"duration":8801}},{"uid":"7e0091f4a1b25d26","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00074d680>: {\n        Underlying: <*exec.ExitError | 0xc00068f380>{\n            ProcessState: {\n                pid: 6681,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 144163},\n                    Stime: {Sec: 0, Usec: 74029},\n                    Maxrss: 74860,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 6973,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14104,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 795,\n                    Nivcsw: 445,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile627993536\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile627993536\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile627993536\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile627993536\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile627993536\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1660805737000,"stop":1660805744546,"duration":7546}},{"uid":"ae9b5df5c28612e5","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000133530>: {\n        Underlying: <*exec.ExitError | 0xc00065b1e0>{\n            ProcessState: {\n                pid: 6649,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 128683},\n                    Stime: {Sec: 0, Usec: 55150},\n                    Maxrss: 73264,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3488,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14104,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 898,\n                    Nivcsw: 482,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile2979926826\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile2979926826\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile2979926826\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile2979926826\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile2979926826\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1660803696000,"stop":1660803704132,"duration":8132}},{"uid":"14b352f2fb9c3e08","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000334f90>: {\n        Underlying: <*exec.ExitError | 0xc0006a4240>{\n            ProcessState: {\n                pid: 6272,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 155951},\n                    Stime: {Sec: 0, Usec: 51983},\n                    Maxrss: 82272,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4084,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14104,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 1034,\n                    Nivcsw: 505,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile2138158796\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile2138158796\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile2138158796\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile2138158796\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile2138158796\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1660798468000,"stop":1660798475598,"duration":7598}},{"uid":"21e9f6580885cceb","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0006252a8>: {\n        Underlying: <*exec.ExitError | 0xc000654580>{\n            ProcessState: {\n                pid: 6183,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 248661},\n                    Stime: {Sec: 0, Usec: 110516},\n                    Maxrss: 73896,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 6922,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13616,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 1106,\n                    Nivcsw: 795,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile4150248164\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile4150248164\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile4150248164\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile4150248164\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile4150248164\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1660748009000,"stop":1660748016980,"duration":7980}},{"uid":"c382664291e1c962","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00051a0d8>: {\n        Underlying: <*exec.ExitError | 0xc000692d60>{\n            ProcessState: {\n                pid: 6498,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 181899},\n                    Stime: {Sec: 0, Usec: 71913},\n                    Maxrss: 74156,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3691,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13616,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 970,\n                    Nivcsw: 462,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile3789482925\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile3789482925\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile3789482925\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile3789482925\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile3789482925\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1660747865000,"stop":1660747872679,"duration":7679}},{"uid":"1adbe9880641f8f6","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000fc98>: {\n        Underlying: <*exec.ExitError | 0xc00062a520>{\n            ProcessState: {\n                pid: 6261,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 158501},\n                    Stime: {Sec: 0, Usec: 90572},\n                    Maxrss: 71468,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4831,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13616,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 852,\n                    Nivcsw: 421,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile3089396990\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile3089396990\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile3089396990\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile3089396990\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile3089396990\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1660747735000,"stop":1660747742666,"duration":7666}},{"uid":"b471944abde22b64","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000432030>: {\n        Underlying: <*exec.ExitError | 0xc0003900c0>{\n            ProcessState: {\n                pid: 6649,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 131228},\n                    Stime: {Sec: 0, Usec: 57894},\n                    Maxrss: 82856,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 5026,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14104,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 730,\n                    Nivcsw: 475,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1654548262\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1654548262\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1654548262\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1654548262\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1654548262\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1660747654000,"stop":1660747661478,"duration":7478}},{"uid":"f661b240c299545d","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00073ccd8>: {\n        Underlying: <*exec.ExitError | 0xc00039f120>{\n            ProcessState: {\n                pid: 6541,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 192173},\n                    Stime: {Sec: 0, Usec: 65266},\n                    Maxrss: 75112,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3624,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14104,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 943,\n                    Nivcsw: 358,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile2613102308\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile2613102308\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile2613102308\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile2613102308\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile2613102308\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1660722683000,"stop":1660722691455,"duration":8455}},{"uid":"a335a8c0a59f6272","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000627b00>: {\n        Underlying: <*exec.ExitError | 0xc000671f40>{\n            ProcessState: {\n                pid: 6600,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 151153},\n                    Stime: {Sec: 0, Usec: 86373},\n                    Maxrss: 78528,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4052,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14104,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 1018,\n                    Nivcsw: 472,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile886271800\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile886271800\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile886271800\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile886271800\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile886271800\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1660635916000,"stop":1660635924743,"duration":8743}},{"uid":"81725166b66e7ab2","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000837740>: {\n        Underlying: <*exec.ExitError | 0xc0006dcc00>{\n            ProcessState: {\n                pid: 6545,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 175451},\n                    Stime: {Sec: 0, Usec: 62382},\n                    Maxrss: 78528,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2904,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13616,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 789,\n                    Nivcsw: 328,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1903991852\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1903991852\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1903991852\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1903991852\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1903991852\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1660635666000,"stop":1660635673627,"duration":7627}},{"uid":"bcf48395d81e7ca8","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000625650>: {\n        Underlying: <*exec.ExitError | 0xc0005ae520>{\n            ProcessState: {\n                pid: 6127,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 177031},\n                    Stime: {Sec: 0, Usec: 92730},\n                    Maxrss: 81108,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3465,\n                    Majflt: 1,\n                    Nswap: 0,\n                    Inblock: 200,\n                    Oublock: 14104,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 868,\n                    Nivcsw: 463,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile816827663\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile816827663\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile816827663\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile816827663\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile816827663\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1660635860000,"stop":1660635868721,"duration":8721}},{"uid":"c3e5f55a67568c5f","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0006801c8>: {\n        Underlying: <*exec.ExitError | 0xc0007e6140>{\n            ProcessState: {\n                pid: 6789,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 116927},\n                    Stime: {Sec: 0, Usec: 74053},\n                    Maxrss: 77036,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 8617,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14104,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 766,\n                    Nivcsw: 441,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile589991095\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile589991095\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile589991095\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile589991095\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile589991095\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1660631164000,"stop":1660631172191,"duration":8191}},{"uid":"576827a91a0f1f1a","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004e41e0>: {\n        Underlying: <*exec.ExitError | 0xc0005e0b20>{\n            ProcessState: {\n                pid: 6725,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 184933},\n                    Stime: {Sec: 0, Usec: 86805},\n                    Maxrss: 78052,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3201,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14104,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 921,\n                    Nivcsw: 556,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile651282065\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile651282065\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile651282065\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile651282065\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile651282065\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1660623463000,"stop":1660623470755,"duration":7755}},{"uid":"8ccc7e9f0f746c8d","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004b51e8>: {\n        Underlying: <*exec.ExitError | 0xc00047e340>{\n            ProcessState: {\n                pid: 6097,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 146238},\n                    Stime: {Sec: 0, Usec: 40621},\n                    Maxrss: 69548,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 6447,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13616,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 622,\n                    Nivcsw: 371,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile3060213936\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile3060213936\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile3060213936\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile3060213936\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile3060213936\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1660623384000,"stop":1660623392369,"duration":8369}},{"uid":"cfcd40fc1e8b2f49","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000353ab8>: {\n        Underlying: <*exec.ExitError | 0xc000847320>{\n            ProcessState: {\n                pid: 6573,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 134444},\n                    Stime: {Sec: 0, Usec: 42014},\n                    Maxrss: 82340,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4570,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14104,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 729,\n                    Nivcsw: 346,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile3705132090\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile3705132090\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile3705132090\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile3705132090\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile3705132090\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1660623360000,"stop":1660623368126,"duration":8126}},{"uid":"c66ed51e50488f55","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000131170>: {\n        Underlying: <*exec.ExitError | 0xc0006828e0>{\n            ProcessState: {\n                pid: 6140,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 179497},\n                    Stime: {Sec: 0, Usec: 61192},\n                    Maxrss: 75632,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 5697,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14104,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 1528,\n                    Nivcsw: 510,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile3284955012\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile3284955012\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile3284955012\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile3284955012\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile3284955012\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1660452377000,"stop":1660452385884,"duration":8884}},{"uid":"8940695319743c5e","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00067e108>: {\n        Underlying: <*exec.ExitError | 0xc000194160>{\n            ProcessState: {\n                pid: 6591,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 168725},\n                    Stime: {Sec: 0, Usec: 54835},\n                    Maxrss: 78124,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3686,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14104,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 964,\n                    Nivcsw: 436,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile897531393\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile897531393\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile897531393\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile897531393\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile897531393\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1660451870000,"stop":1660451878274,"duration":8274}}]},"Hub Suite:Hub Suite#[It]  QosProfile tests QosProfile delete validation Delete qos profile - negative scenario":{"statistic":{"failed":116,"broken":0,"skipped":0,"passed":0,"unknown":0,"total":116},"items":[{"uid":"f1c2e5845fded27a","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00044c0a8>: {\n        Underlying: <*exec.ExitError | 0xc0000c8140>{\n            ProcessState: {\n                pid: 6045,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 173241},\n                    Stime: {Sec: 0, Usec: 79088},\n                    Maxrss: 78128,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 6729,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13616,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 947,\n                    Nivcsw: 457,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario2658292079\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario2658292079\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario2658292079\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario2658292079\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario2658292079\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1660828517000,"stop":1660828521711,"duration":4711}},{"uid":"e66ef1c2868a7e5d","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000532ae0>: {\n        Underlying: <*exec.ExitError | 0xc00082acc0>{\n            ProcessState: {\n                pid: 6739,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 126319},\n                    Stime: {Sec: 0, Usec: 48897},\n                    Maxrss: 76864,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3963,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13344,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 383,\n                    Nivcsw: 352,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario1442163578\\\": Internal error occurred: failed calling webhook \\\"msliceqosconfig.kb.io\\\": failed to call webhook: the server could not find the requested resource\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario1442163578\\\": Internal error occurred: failed calling webhook \\\"msliceqosconfig.kb.io\\\": failed to call webhook: the server could not find the requested resource\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario1442163578\\\": Internal error occurred: failed calling webhook \\\"msliceqosconfig.kb.io\\\": failed to call webhook: the server could not find the requested resource\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario1442163578\\\": Internal error occurred: failed calling webhook \\\"msliceqosconfig.kb.io\\\": failed to call webhook: the server could not find the requested resource\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario1442163578\": Internal error occurred: failed calling webhook \"msliceqosconfig.kb.io\": failed to call webhook: the server could not find the requested resource\noccurred","time":{"start":1660807774000,"stop":1660807778483,"duration":4483}},{"uid":"d3dd142d038dd372","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004effe0>: {\n        Underlying: <*exec.ExitError | 0xc0007a4560>{\n            ProcessState: {\n                pid: 6082,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 202686},\n                    Stime: {Sec: 0, Usec: 71536},\n                    Maxrss: 76028,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 5687,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 13616,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 1103,\n                    Nivcsw: 500,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario562073034\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario562073034\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario562073034\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario562073034\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario562073034\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1660803998000,"stop":1660804002669,"duration":4669}},{"uid":"131c249187e903f","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0003a0e40>: {\n        Underlying: <*exec.ExitError | 0xc00013f800>{\n            ProcessState: {\n                pid: 6696,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 163458},\n                    Stime: {Sec: 0, Usec: 48595},\n                    Maxrss: 76896,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3975,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14104,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 836,\n                    Nivcsw: 646,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario2496284242\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario2496284242\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario2496284242\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario2496284242\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario2496284242\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1660805737000,"stop":1660805741521,"duration":4521}},{"uid":"9abace91f61384c1","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004c0198>: {\n        Underlying: <*exec.ExitError | 0xc0007dc0e0>{\n            ProcessState: {\n                pid: 6665,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 148467},\n                    Stime: {Sec: 0, Usec: 43666},\n                    Maxrss: 75768,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3995,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14104,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 890,\n                    Nivcsw: 368,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario2477195818\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario2477195818\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario2477195818\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario2477195818\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario2477195818\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1660803696000,"stop":1660803700462,"duration":4462}},{"uid":"5cd59d9cfdc8ca71","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00086a060>: {\n        Underlying: <*exec.ExitError | 0xc000671100>{\n            ProcessState: {\n                pid: 6287,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 150959},\n                    Stime: {Sec: 0, Usec: 77415},\n                    Maxrss: 85080,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4254,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14104,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 834,\n                    Nivcsw: 422,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario1832941242\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario1832941242\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario1832941242\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario1832941242\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario1832941242\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1660798468000,"stop":1660798472728,"duration":4728}},{"uid":"40b86c940b618ce8","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0006255f0>: {\n        Underlying: <*exec.ExitError | 0xc00063eea0>{\n            ProcessState: {\n                pid: 6199,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 251810},\n                    Stime: {Sec: 0, Usec: 89352},\n                    Maxrss: 76208,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 5849,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13616,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 1148,\n                    Nivcsw: 704,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario724127653\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario724127653\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario724127653\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario724127653\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario724127653\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1660748009000,"stop":1660748013868,"duration":4868}},{"uid":"a996f68c636f1d8b","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00051b1e8>: {\n        Underlying: <*exec.ExitError | 0xc00046f2c0>{\n            ProcessState: {\n                pid: 6514,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 138936},\n                    Stime: {Sec: 0, Usec: 93986},\n                    Maxrss: 71420,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2821,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13616,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 954,\n                    Nivcsw: 438,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario799444851\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario799444851\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario799444851\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario799444851\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario799444851\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1660747865000,"stop":1660747869615,"duration":4615}},{"uid":"f57894a0b7fcc93f","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0003585b8>: {\n        Underlying: <*exec.ExitError | 0xc000491820>{\n            ProcessState: {\n                pid: 6276,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 172999},\n                    Stime: {Sec: 0, Usec: 82380},\n                    Maxrss: 75392,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 7126,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13616,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 898,\n                    Nivcsw: 424,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario3512186592\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario3512186592\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario3512186592\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario3512186592\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario3512186592\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1660747735000,"stop":1660747739623,"duration":4623}},{"uid":"b1aacb869436af1b","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0001917b8>: {\n        Underlying: <*exec.ExitError | 0xc000651240>{\n            ProcessState: {\n                pid: 6664,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 150890},\n                    Stime: {Sec: 0, Usec: 30178},\n                    Maxrss: 77360,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4218,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14104,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 845,\n                    Nivcsw: 458,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario1657140760\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario1657140760\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario1657140760\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario1657140760\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario1657140760\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1660747654000,"stop":1660747658462,"duration":4462}},{"uid":"800245c926f69798","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00048de30>: {\n        Underlying: <*exec.ExitError | 0xc000772f40>{\n            ProcessState: {\n                pid: 6556,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 168924},\n                    Stime: {Sec: 0, Usec: 68374},\n                    Maxrss: 77892,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4677,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14104,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 891,\n                    Nivcsw: 473,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario1914866712\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario1914866712\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario1914866712\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario1914866712\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario1914866712\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1660722683000,"stop":1660722687595,"duration":4595}},{"uid":"48cea03a6f0c568b","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000905398>: {\n        Underlying: <*exec.ExitError | 0xc00057df80>{\n            ProcessState: {\n                pid: 6613,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 160983},\n                    Stime: {Sec: 0, Usec: 73965},\n                    Maxrss: 76172,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4119,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14104,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 638,\n                    Nivcsw: 362,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario697834923\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario697834923\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario697834923\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario697834923\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario697834923\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1660635916000,"stop":1660635920616,"duration":4616}},{"uid":"24e0e65cc1b89a99","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0000cfec0>: {\n        Underlying: <*exec.ExitError | 0xc0007ab900>{\n            ProcessState: {\n                pid: 6560,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 143234},\n                    Stime: {Sec: 0, Usec: 85940},\n                    Maxrss: 87724,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3956,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13616,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 855,\n                    Nivcsw: 466,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario1491373339\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario1491373339\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario1491373339\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario1491373339\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario1491373339\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1660635666000,"stop":1660635670619,"duration":4619}},{"uid":"23e7bc16b0ca8f86","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0005d4528>: {\n        Underlying: <*exec.ExitError | 0xc0006b0dc0>{\n            ProcessState: {\n                pid: 6144,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 183707},\n                    Stime: {Sec: 0, Usec: 81647},\n                    Maxrss: 81112,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3729,\n                    Majflt: 3,\n                    Nswap: 0,\n                    Inblock: 384,\n                    Oublock: 14104,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 1189,\n                    Nivcsw: 705,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario2635656512\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario2635656512\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario2635656512\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario2635656512\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario2635656512\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1660635860000,"stop":1660635864735,"duration":4735}},{"uid":"80c3f9509f41b75c","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0007b0eb8>: {\n        Underlying: <*exec.ExitError | 0xc000708120>{\n            ProcessState: {\n                pid: 6804,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 159999},\n                    Stime: {Sec: 0, Usec: 47999},\n                    Maxrss: 73744,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 7759,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14104,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 753,\n                    Nivcsw: 416,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario4073771651\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario4073771651\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario4073771651\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario4073771651\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario4073771651\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1660631164000,"stop":1660631168474,"duration":4474}},{"uid":"67d5e87c69e1d32d","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004e5200>: {\n        Underlying: <*exec.ExitError | 0xc000390a20>{\n            ProcessState: {\n                pid: 6741,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 163137},\n                    Stime: {Sec: 0, Usec: 68880},\n                    Maxrss: 81512,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3438,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14104,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 813,\n                    Nivcsw: 500,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario1079198400\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario1079198400\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario1079198400\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario1079198400\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario1079198400\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1660623463000,"stop":1660623467614,"duration":4614}},{"uid":"d1346a8ffde0c448","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0006db0e0>: {\n        Underlying: <*exec.ExitError | 0xc000804060>{\n            ProcessState: {\n                pid: 6112,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 142874},\n                    Stime: {Sec: 0, Usec: 67468},\n                    Maxrss: 75112,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 6167,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13616,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 802,\n                    Nivcsw: 487,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario422125737\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario422125737\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario422125737\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario422125737\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario422125737\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1660623384000,"stop":1660623388507,"duration":4507}},{"uid":"b6a012d6f759ed79","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000130870>: {\n        Underlying: <*exec.ExitError | 0xc000710240>{\n            ProcessState: {\n                pid: 6588,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 131382},\n                    Stime: {Sec: 0, Usec: 45162},\n                    Maxrss: 76688,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4310,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14104,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 717,\n                    Nivcsw: 377,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario4011330673\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario4011330673\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario4011330673\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario4011330673\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario4011330673\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1660623360000,"stop":1660623364445,"duration":4445}},{"uid":"f895a5287959b3a2","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004cbf38>: {\n        Underlying: <*exec.ExitError | 0xc000826d80>{\n            ProcessState: {\n                pid: 6155,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 172099},\n                    Stime: {Sec: 0, Usec: 68039},\n                    Maxrss: 75788,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 5323,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14104,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 2182,\n                    Nivcsw: 422,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario1005218118\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario1005218118\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario1005218118\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario1005218118\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario1005218118\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1660452377000,"stop":1660452382395,"duration":5395}},{"uid":"dbc66606718a1d6a","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00067f9f8>: {\n        Underlying: <*exec.ExitError | 0xc000818a00>{\n            ProcessState: {\n                pid: 6607,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 157319},\n                    Stime: {Sec: 0, Usec: 67422},\n                    Maxrss: 69200,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3863,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14104,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 738,\n                    Nivcsw: 376,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario1453353991\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario1453353991\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario1453353991\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario1453353991\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario1453353991\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1660451870000,"stop":1660451874545,"duration":4545}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting iperf-client pod Check ping between iperf-server and iperf-client after iperf-client pod restart":{"statistic":{"failed":0,"broken":0,"skipped":39,"passed":0,"unknown":0,"total":39},"items":[{"uid":"30285a81205946e7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"614d5b7d2c1a71ac","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"17355c47a78ffa8a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"81eb1f9ea8106452","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"13bccfdb1f55ccf","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"5ecdc7763ea8b5fb","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"1c4d34acdc179a9c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"e229c74d458f3317","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}},{"uid":"c73be2c4b27fd8f6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659982549000,"stop":1659982549000,"duration":0}},{"uid":"3a32e9e78162c55","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659970182000,"stop":1659970182000,"duration":0}},{"uid":"358dfcc0dc8b5dce","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659944495000,"stop":1659944495000,"duration":0}},{"uid":"b854455b8c235a4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659874977000,"stop":1659874977000,"duration":0}},{"uid":"e9a0fcdd64c9c343","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496050000,"stop":1659496050000,"duration":0}},{"uid":"278e0b5ae2f71f95","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496083000,"stop":1659496083000,"duration":0}},{"uid":"53c5df1e3f905c3e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"e956951cf4f06639","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659450916000,"stop":1659450916000,"duration":0}},{"uid":"5a3fd0bb4385e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"b1fac386eb77f16c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447298000,"stop":1659447298000,"duration":0}},{"uid":"3bf717aac14ac4d6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447116000,"stop":1659447116000,"duration":0}},{"uid":"147d7caaa1265c2e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448205000,"stop":1659448205000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice Creation scenarios Slice with LOW QOS should have gateway pod running":{"statistic":{"failed":4,"broken":0,"skipped":8,"passed":18,"unknown":0,"total":30},"items":[{"uid":"f91d469747efd535","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"8f2696314b982364","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"2bd1b4515291389b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"56239780991e202d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"6d97e2f3ddbf091a","status":"failed","statusDetails":"Timed out after 180.003s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1660113628000,"stop":1660113808230,"duration":180230}},{"uid":"db984b14975dc419","status":"failed","statusDetails":"Timed out after 180.003s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1660104898000,"stop":1660105078329,"duration":180329}},{"uid":"52d0b1af77db83e0","status":"passed","time":{"start":1660067259000,"stop":1660067304107,"duration":45107}},{"uid":"10963f3710a2b8f1","status":"passed","time":{"start":1660047551000,"stop":1660047576084,"duration":25084}},{"uid":"6a23f74930b7586c","status":"passed","time":{"start":1659982549000,"stop":1659982584171,"duration":35171}},{"uid":"4ef23a2bbe351250","status":"passed","time":{"start":1659970182000,"stop":1659970217082,"duration":35082}},{"uid":"d9c410853aab26ec","status":"passed","time":{"start":1659944495000,"stop":1659944550162,"duration":55162}},{"uid":"4e53c9a27403fe40","status":"passed","time":{"start":1659874977000,"stop":1659875032141,"duration":55141}},{"uid":"687960afb231088a","status":"passed","time":{"start":1659496050000,"stop":1659496120156,"duration":70156}},{"uid":"4869c986697247ea","status":"passed","time":{"start":1659496083000,"stop":1659496108078,"duration":25078}},{"uid":"cc936e6180a68c30","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"4f720b4f88133e92","status":"passed","time":{"start":1659450916000,"stop":1659450996230,"duration":80230}},{"uid":"2cf4f207593ec1a4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"ad8b9a5883d85e98","status":"passed","time":{"start":1659447298000,"stop":1659447343120,"duration":45120}},{"uid":"de0007b1bf0344c8","status":"passed","time":{"start":1659447116000,"stop":1659447156140,"duration":40140}},{"uid":"650dc9f682427718","status":"failed","statusDetails":"Timed out after 180.005s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1659448205000,"stop":1659448385251,"duration":180251}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests slice with netpol Should install networkpolicies in all application namespaces":{"statistic":{"failed":0,"broken":0,"skipped":29,"passed":10,"unknown":0,"total":39},"items":[{"uid":"b754077ad098a7c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"f8b23fccc62594e5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"5f51308b0d424df0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"2bced604e45755f0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"f9790e32a3152ebf","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"abe1a1dc4ce71d5a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"c829bdfb31dc1fa1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"d7a74d3deea8d4ca","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}},{"uid":"39d84f2a4b374136","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659982549000,"stop":1659982549000,"duration":0}},{"uid":"854d70b4e5ac7799","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659970182000,"stop":1659970182000,"duration":0}},{"uid":"5bd79e0e96d204f9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659944495000,"stop":1659944495000,"duration":0}},{"uid":"956e4120761878e0","status":"passed","time":{"start":1659874977000,"stop":1659874977242,"duration":242}},{"uid":"fb5679dba00f6ba2","status":"passed","time":{"start":1659496050000,"stop":1659496050250,"duration":250}},{"uid":"7b9c3d8cd7fbb14a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496083000,"stop":1659496083000,"duration":0}},{"uid":"8234d4f86fdad30e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"d048edf82722d872","status":"passed","time":{"start":1659450916000,"stop":1659450916527,"duration":527}},{"uid":"fba6609343a8d195","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"657393e3b8c9c4f1","status":"passed","time":{"start":1659447298000,"stop":1659447298378,"duration":378}},{"uid":"edde7c7a8b93c34c","status":"passed","time":{"start":1659447116000,"stop":1659447116267,"duration":267}},{"uid":"96bf8586a973df6d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448205000,"stop":1659448205000,"duration":0}}]},"Istio Suite:Istio Suite#[It] Bookinfo tests Bookinfo Scenario 3 without egress and with ingress Should verify if Curl Traffic works on Productpage test":{"statistic":{"failed":0,"broken":0,"skipped":15,"passed":3,"unknown":0,"total":18},"items":[{"uid":"82359bde83251710","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808584000,"stop":1660808584000,"duration":0}},{"uid":"ed61bfbac679399c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660804840000,"stop":1660804840000,"duration":0}},{"uid":"d068fadf3dc0efc1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660804470000,"stop":1660804470000,"duration":0}},{"uid":"f336f54368df40e3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660748849000,"stop":1660748849000,"duration":0}},{"uid":"2669736c56e086f5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660748706000,"stop":1660748706000,"duration":0}},{"uid":"9fda0714530d9bcd","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660748558000,"stop":1660748558000,"duration":0}},{"uid":"bd12fd9224793199","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660636759000,"stop":1660636759000,"duration":0}},{"uid":"2e30445cf19bffed","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660636482000,"stop":1660636482000,"duration":0}},{"uid":"de62c0807cf0ffee","status":"passed","time":{"start":1660632006000,"stop":1660632015436,"duration":9436}},{"uid":"428d050928de631e","status":"passed","time":{"start":1660624273000,"stop":1660624273493,"duration":493}},{"uid":"51f004e0910f073f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660624220000,"stop":1660624220000,"duration":0}},{"uid":"acd73c596f03cdd3","status":"passed","time":{"start":1660453205000,"stop":1660453214646,"duration":9646}},{"uid":"b186860eed2755c0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660452554000,"stop":1660452554000,"duration":0}},{"uid":"1df1756e2f5aeae7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660452412000,"stop":1660452412000,"duration":0}},{"uid":"8077d7dd620d0b70","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660415422000,"stop":1660415422000,"duration":0}},{"uid":"417a8d66e486a602","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660298968000,"stop":1660298968000,"duration":0}},{"uid":"a6d5ce6959878ded","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295522000,"stop":1660295522000,"duration":0}},{"uid":"4dbc1735d502c1e1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293414000,"stop":1660293414000,"duration":0}}]},"Istio Suite:Istio Suite#[It] Bookinfo tests Bookinfo Scenario 4 with egress and ingress Should verify if Ping works successfully from Productpage to details endpoint":{"statistic":{"failed":1,"broken":0,"skipped":17,"passed":0,"unknown":0,"total":18},"items":[{"uid":"f158dfe542f28daf","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808584000,"stop":1660808584000,"duration":0}},{"uid":"57c5d20b5612b84b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660804840000,"stop":1660804840000,"duration":0}},{"uid":"bead9f81d7756cf1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660804470000,"stop":1660804470000,"duration":0}},{"uid":"d12a26cfb0f1cac2","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660748849000,"stop":1660748849000,"duration":0}},{"uid":"399e60556720ae67","status":"failed","statusDetails":"Unexpected non-nil/non-zero argument at index 1:\n\t<*shell.ErrWithCmdOutput>: &shell.ErrWithCmdOutput{Underlying:(*exec.ExitError)(0xc0005d43c0), Output:(*shell.output)(0xc000386240)}","time":{"start":1660748706000,"stop":1660748762977,"duration":56977}},{"uid":"3a5f39ccc000c510","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660748558000,"stop":1660748558000,"duration":0}},{"uid":"ffb998bb83b8bf07","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660636759000,"stop":1660636759000,"duration":0}},{"uid":"9d5f2b9b28068370","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660636482000,"stop":1660636482000,"duration":0}},{"uid":"4114fed992c9a547","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660632006000,"stop":1660632006000,"duration":0}},{"uid":"a166f8a5b8553f24","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660624273000,"stop":1660624273000,"duration":0}},{"uid":"2b8b6a3444798ea9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660624220000,"stop":1660624220000,"duration":0}},{"uid":"1965303b577d18cd","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660453205000,"stop":1660453205000,"duration":0}},{"uid":"5c8b2dd6ea465643","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660452554000,"stop":1660452554000,"duration":0}},{"uid":"1bde09affef625f8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660452412000,"stop":1660452412000,"duration":0}},{"uid":"72e36d3b0eafb98d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660415422000,"stop":1660415422000,"duration":0}},{"uid":"899b636ae35f61b3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660298968000,"stop":1660298968000,"duration":0}},{"uid":"3640342a161b2947","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295522000,"stop":1660295522000,"duration":0}},{"uid":"8b53d53863326310","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293414000,"stop":1660293414000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Hub netpol scenarios: Slice Configs Netpol scenarios Should fail when deploying clusters in namespaceisolationprofile with * and a cluster name in both applicationNamespace and allowedNamespace":{"statistic":{"failed":0,"broken":0,"skipped":13,"passed":26,"unknown":0,"total":39},"items":[{"uid":"122dffff7cf89791","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"20992d36dc97c400","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"75f8e1c62cb14df9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"b7bc0a8f88bcbc15","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"b03d36df7f44a05d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"600d71a75a9351de","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"d07887859a71b553","status":"passed","time":{"start":1660067259000,"stop":1660067259358,"duration":358}},{"uid":"4ed7671a8dd4de62","status":"passed","time":{"start":1660047551000,"stop":1660047551304,"duration":304}},{"uid":"1cd73755f593d30e","status":"passed","time":{"start":1659982549000,"stop":1659982549960,"duration":960}},{"uid":"e7394558af825f00","status":"passed","time":{"start":1659970182000,"stop":1659970182361,"duration":361}},{"uid":"99b393b3444d56ea","status":"passed","time":{"start":1659944495000,"stop":1659944495418,"duration":418}},{"uid":"b31a378ea97b28e3","status":"passed","time":{"start":1659874977000,"stop":1659874977397,"duration":397}},{"uid":"dd6b4aac0dac3ed4","status":"passed","time":{"start":1659496050000,"stop":1659496050232,"duration":232}},{"uid":"b8014ebfb643c74b","status":"passed","time":{"start":1659496083000,"stop":1659496083329,"duration":329}},{"uid":"9c131efb0c4eea64","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"726bee7babff25ae","status":"passed","time":{"start":1659450916000,"stop":1659450916346,"duration":346}},{"uid":"149cdb14074a0f1d","status":"passed","time":{"start":1659451524000,"stop":1659451524442,"duration":442}},{"uid":"a75d7a0deec4b7f6","status":"passed","time":{"start":1659447298000,"stop":1659447298313,"duration":313}},{"uid":"f8b49d107112dd01","status":"passed","time":{"start":1659447116000,"stop":1659447116303,"duration":303}},{"uid":"1783e1607e2794d4","status":"passed","time":{"start":1659448205000,"stop":1659448205464,"duration":464}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Update Should update successfully when Project applied with valid service account name in Write users":{"statistic":{"failed":7,"broken":0,"skipped":0,"passed":114,"unknown":0,"total":121},"items":[{"uid":"e83ac03a28421a05","status":"passed","time":{"start":1660828517000,"stop":1660828522412,"duration":5412}},{"uid":"66050ce34e24e0ff","status":"passed","time":{"start":1660807774000,"stop":1660807779127,"duration":5127}},{"uid":"322eafe0063697e5","status":"passed","time":{"start":1660803998000,"stop":1660804003445,"duration":5445}},{"uid":"9ad3cde2c416c7f3","status":"passed","time":{"start":1660805737000,"stop":1660805742029,"duration":5029}},{"uid":"8428743197e770b0","status":"passed","time":{"start":1660803696000,"stop":1660803700944,"duration":4944}},{"uid":"9c013d2fa31c3348","status":"passed","time":{"start":1660798468000,"stop":1660798473239,"duration":5239}},{"uid":"589650136c48b47d","status":"passed","time":{"start":1660748009000,"stop":1660748013648,"duration":4648}},{"uid":"b6fe0a1fd6ceca7a","status":"passed","time":{"start":1660747865000,"stop":1660747870327,"duration":5327}},{"uid":"1b4bfa2914c2dcc1","status":"passed","time":{"start":1660747735000,"stop":1660747740250,"duration":5250}},{"uid":"e7704cb07ab31acf","status":"passed","time":{"start":1660747654000,"stop":1660747659027,"duration":5027}},{"uid":"9986e9e650e54965","status":"passed","time":{"start":1660722683000,"stop":1660722688222,"duration":5222}},{"uid":"8128e3472d2baf71","status":"passed","time":{"start":1660635916000,"stop":1660635921351,"duration":5351}},{"uid":"5d50c095508a2b25","status":"passed","time":{"start":1660635666000,"stop":1660635671220,"duration":5220}},{"uid":"5c7976caf56ff73","status":"passed","time":{"start":1660635860000,"stop":1660635865442,"duration":5442}},{"uid":"dc380ca94da59af1","status":"passed","time":{"start":1660631164000,"stop":1660631168883,"duration":4883}},{"uid":"fe158e479890dd3a","status":"passed","time":{"start":1660623463000,"stop":1660623468236,"duration":5236}},{"uid":"c07c10f686c6037c","status":"passed","time":{"start":1660623384000,"stop":1660623388924,"duration":4924}},{"uid":"e9ed7d003c4b89ff","status":"passed","time":{"start":1660623360000,"stop":1660623364934,"duration":4934}},{"uid":"6ece6a6e14b9d02f","status":"passed","time":{"start":1660452377000,"stop":1660452382295,"duration":5295}},{"uid":"c23bb742dad3fc08","status":"passed","time":{"start":1660451870000,"stop":1660451875106,"duration":5106}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Update Should fail when Project is Applied with service account name as combination of special characters in Read users":{"statistic":{"failed":7,"broken":0,"skipped":0,"passed":114,"unknown":0,"total":121},"items":[{"uid":"349124cfe7181c53","status":"passed","time":{"start":1660828517000,"stop":1660828521891,"duration":4891}},{"uid":"7a187966ec130cd4","status":"passed","time":{"start":1660807774000,"stop":1660807778752,"duration":4752}},{"uid":"71d91d690eacef73","status":"passed","time":{"start":1660803998000,"stop":1660804003960,"duration":5960}},{"uid":"893adc3c70084b5b","status":"passed","time":{"start":1660805737000,"stop":1660805741740,"duration":4740}},{"uid":"36238b4c299059d2","status":"passed","time":{"start":1660803696000,"stop":1660803701719,"duration":5719}},{"uid":"9a8f11f80760b8be","status":"passed","time":{"start":1660798468000,"stop":1660798472889,"duration":4889}},{"uid":"3b15741bd53a2f38","status":"passed","time":{"start":1660748009000,"stop":1660748014123,"duration":5123}},{"uid":"c61841e9383f9bc4","status":"passed","time":{"start":1660747865000,"stop":1660747869881,"duration":4881}},{"uid":"f1a1d144dd604088","status":"passed","time":{"start":1660747735000,"stop":1660747739870,"duration":4870}},{"uid":"c50ef688cce6899c","status":"passed","time":{"start":1660747654000,"stop":1660747659657,"duration":5657}},{"uid":"975ebad7791ac4c9","status":"passed","time":{"start":1660722683000,"stop":1660722688964,"duration":5964}},{"uid":"b5ce4a28685a1274","status":"passed","time":{"start":1660635916000,"stop":1660635920906,"duration":4906}},{"uid":"f6ed42bf3e4a43fd","status":"passed","time":{"start":1660635666000,"stop":1660635671841,"duration":5841}},{"uid":"40b73cd4a8f93be2","status":"passed","time":{"start":1660635860000,"stop":1660635865943,"duration":5943}},{"uid":"1fa816eb36c6cacd","status":"passed","time":{"start":1660631164000,"stop":1660631169679,"duration":5679}},{"uid":"e6786a82b1ef5e02","status":"passed","time":{"start":1660623463000,"stop":1660623467891,"duration":4891}},{"uid":"ba85c3b7db6131c7","status":"passed","time":{"start":1660623384000,"stop":1660623388694,"duration":4694}},{"uid":"9791ea63060af302","status":"passed","time":{"start":1660623360000,"stop":1660623365644,"duration":5644}},{"uid":"3a2fa5ced1d1f7e8","status":"passed","time":{"start":1660452377000,"stop":1660452382838,"duration":5838}},{"uid":"c0fdec596ed6a9df","status":"passed","time":{"start":1660451870000,"stop":1660451874751,"duration":4751}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Creation Should create Project while using valid manifest":{"statistic":{"failed":5,"broken":0,"skipped":0,"passed":119,"unknown":0,"total":124},"items":[{"uid":"922f62731051d4c2","status":"passed","time":{"start":1660828517000,"stop":1660828525764,"duration":8764}},{"uid":"68ae262827688274","status":"passed","time":{"start":1660807774000,"stop":1660807782541,"duration":8541}},{"uid":"c28bb61bf1955864","status":"passed","time":{"start":1660803998000,"stop":1660804006489,"duration":8489}},{"uid":"ba3f85a779955d08","status":"passed","time":{"start":1660805737000,"stop":1660805744579,"duration":7579}},{"uid":"b9222ed9ab454279","status":"passed","time":{"start":1660803696000,"stop":1660803703536,"duration":7536}},{"uid":"830edb5861deb121","status":"passed","time":{"start":1660798468000,"stop":1660798476481,"duration":8481}},{"uid":"f7439d53ecfad518","status":"passed","time":{"start":1660748009000,"stop":1660748025461,"duration":16461}},{"uid":"ecf6cef8956c9513","status":"passed","time":{"start":1660747865000,"stop":1660747872639,"duration":7639}},{"uid":"3ae202b4c7ec5e12","status":"passed","time":{"start":1660747735000,"stop":1660747743811,"duration":8811}},{"uid":"aec22cb3e92f68fa","status":"passed","time":{"start":1660747654000,"stop":1660747661572,"duration":7572}},{"uid":"d532168d71af3e40","status":"passed","time":{"start":1660722683000,"stop":1660722690669,"duration":7669}},{"uid":"29b30bf344372f03","status":"passed","time":{"start":1660635916000,"stop":1660635924604,"duration":8604}},{"uid":"5baa46393bf8f1dd","status":"passed","time":{"start":1660635666000,"stop":1660635674502,"duration":8502}},{"uid":"d9b9afaeb4038a39","status":"passed","time":{"start":1660635860000,"stop":1660635867750,"duration":7750}},{"uid":"c5aec4c273266348","status":"passed","time":{"start":1660631164000,"stop":1660631171512,"duration":7512}},{"uid":"858ff48f26a57065","status":"passed","time":{"start":1660623463000,"stop":1660623476767,"duration":13767}},{"uid":"253fa66dc2573955","status":"passed","time":{"start":1660623384000,"stop":1660623391530,"duration":7530}},{"uid":"d35ab41c854ce02d","status":"passed","time":{"start":1660623360000,"stop":1660623367518,"duration":7518}},{"uid":"b5128e0d24e5b946","status":"passed","time":{"start":1660452377000,"stop":1660452384623,"duration":7623}},{"uid":"a25aed4aeb09bd8d","status":"passed","time":{"start":1660451870000,"stop":1660451877587,"duration":7587}}]},"Istio Suite:Istio Suite#[It] Bookinfo tests Bookinfo Scenario 1 without egress and ingress Should verify if Curl Traffic works on Productpage test":{"statistic":{"failed":0,"broken":0,"skipped":15,"passed":3,"unknown":0,"total":18},"items":[{"uid":"c018e49768f2bda9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808584000,"stop":1660808584000,"duration":0}},{"uid":"491a9ee2b6523d83","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660804840000,"stop":1660804840000,"duration":0}},{"uid":"c44edce4182724f9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660804470000,"stop":1660804470000,"duration":0}},{"uid":"1c545623a417c4be","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660748849000,"stop":1660748849000,"duration":0}},{"uid":"42cf8dd56b51a98c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660748706000,"stop":1660748706000,"duration":0}},{"uid":"7bf1a9fdbe82dee1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660748558000,"stop":1660748558000,"duration":0}},{"uid":"f293ff9840ba1888","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660636759000,"stop":1660636759000,"duration":0}},{"uid":"7110297db0bda482","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660636482000,"stop":1660636482000,"duration":0}},{"uid":"c982a5d1928a5286","status":"passed","time":{"start":1660632006000,"stop":1660632006494,"duration":494}},{"uid":"dcfed73a6a1c80fe","status":"passed","time":{"start":1660624273000,"stop":1660624282490,"duration":9490}},{"uid":"b94ddd0fa3cbab75","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660624220000,"stop":1660624220000,"duration":0}},{"uid":"97d17d34836112ed","status":"passed","time":{"start":1660453205000,"stop":1660453205501,"duration":501}},{"uid":"5dac05a34d18e13","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660452554000,"stop":1660452554000,"duration":0}},{"uid":"9d09dc6f5734f6a3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660452412000,"stop":1660452412000,"duration":0}},{"uid":"adc37dbd0c88afc7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660415422000,"stop":1660415422000,"duration":0}},{"uid":"8b4c4b897e17a31e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660298968000,"stop":1660298968000,"duration":0}},{"uid":"c3f550e2c7dabc17","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295522000,"stop":1660295522000,"duration":0}},{"uid":"ed75b4a5ff043c3f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293414000,"stop":1660293414000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting iperf-client pod Should restart iperf-client pod":{"statistic":{"failed":0,"broken":0,"skipped":39,"passed":0,"unknown":0,"total":39},"items":[{"uid":"65b51b0637fbc5cb","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"22cbbcb5807ceea7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"3f1bdf2d3340251c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"97eccf445f16f957","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"3771a5d1186d5a76","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"9f15fa0a8bdf138c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"bd27a581f053c534","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"74c77ba825a795ea","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}},{"uid":"fa7e0fde0999af65","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659982549000,"stop":1659982549000,"duration":0}},{"uid":"a7eba0463f8331a5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659970182000,"stop":1659970182000,"duration":0}},{"uid":"15087d2c61f8519d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659944495000,"stop":1659944495000,"duration":0}},{"uid":"576ab208e6a9166b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659874977000,"stop":1659874977000,"duration":0}},{"uid":"ca72aaf31565f0fe","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496050000,"stop":1659496050000,"duration":0}},{"uid":"c431f7869332068d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496083000,"stop":1659496083000,"duration":0}},{"uid":"6bafe7cc23dff0d3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"6ea339cda5774440","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659450916000,"stop":1659450916000,"duration":0}},{"uid":"db5eed0a94715173","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"344c107adbb82c00","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447298000,"stop":1659447298000,"duration":0}},{"uid":"8a380485849242bc","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447116000,"stop":1659447116000,"duration":0}},{"uid":"5b9a95fa94e9f4f4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448205000,"stop":1659448205000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Namespace reconcile test scenarios: Onboard cluster objs should have app ns & attached slice entry":{"statistic":{"failed":2,"broken":0,"skipped":0,"passed":37,"unknown":0,"total":39},"items":[{"uid":"9135abeb8e2cca39","status":"failed","statusDetails":"Expected\n    <int>: 0\nto equal\n    <int>: 2","time":{"start":1660808797000,"stop":1660808818719,"duration":21719}},{"uid":"10cfc04b8b336a0e","status":"failed","statusDetails":"Expected\n    <int>: 0\nto equal\n    <int>: 2","time":{"start":1660299182000,"stop":1660299203653,"duration":21653}},{"uid":"3774be67279f862b","status":"passed","time":{"start":1660295740000,"stop":1660295740137,"duration":137}},{"uid":"841c993985726627","status":"passed","time":{"start":1660293657000,"stop":1660293657168,"duration":168}},{"uid":"fd20553455f463a","status":"passed","time":{"start":1660113628000,"stop":1660113628155,"duration":155}},{"uid":"a122f0fcd157933f","status":"passed","time":{"start":1660104898000,"stop":1660104898158,"duration":158}},{"uid":"5bf00521790334c7","status":"passed","time":{"start":1660067259000,"stop":1660067259351,"duration":351}},{"uid":"ea7a254be89923dc","status":"passed","time":{"start":1660047551000,"stop":1660047551258,"duration":258}},{"uid":"32daea97c2051a5d","status":"passed","time":{"start":1659982549000,"stop":1659982549671,"duration":671}},{"uid":"c03e2dac62f95e1d","status":"passed","time":{"start":1659970182000,"stop":1659970182159,"duration":159}},{"uid":"abcd49dec62e3530","status":"passed","time":{"start":1659944495000,"stop":1659944495409,"duration":409}},{"uid":"b072b14bebdd0ef7","status":"passed","time":{"start":1659874977000,"stop":1659874977351,"duration":351}},{"uid":"b6e2ead0b840609e","status":"passed","time":{"start":1659496050000,"stop":1659496050133,"duration":133}},{"uid":"57f90a9dfaebed06","status":"passed","time":{"start":1659496083000,"stop":1659496083172,"duration":172}},{"uid":"d60e9b0959ec0831","status":"passed","time":{"start":1659451501000,"stop":1659451501136,"duration":136}},{"uid":"50b6c3d323723146","status":"passed","time":{"start":1659450916000,"stop":1659450916327,"duration":327}},{"uid":"2fab010ec05ab108","status":"passed","time":{"start":1659451524000,"stop":1659451524304,"duration":304}},{"uid":"898896a2a3aa0b1d","status":"passed","time":{"start":1659447298000,"stop":1659447298134,"duration":134}},{"uid":"eaa4fa13b2b85a62","status":"passed","time":{"start":1659447116000,"stop":1659447116241,"duration":241}},{"uid":"142cc5f6cbbc2326","status":"passed","time":{"start":1659448205000,"stop":1659448205141,"duration":141}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Iperf connectivity Should install slice":{"statistic":{"failed":20,"broken":0,"skipped":0,"passed":19,"unknown":0,"total":39},"items":[{"uid":"df1a04414ab30732","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000127110>: {\n        Underlying: <*exec.ExitError | 0xc0006ba4e0>{\n            ProcessState: {\n                pid: 7162,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 158077},\n                    Stime: {Sec: 0, Usec: 56745},\n                    Maxrss: 92508,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4012,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 333,\n                    Nivcsw: 533,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660808797000,"stop":1660808797247,"duration":247}},{"uid":"ea42b83a1b9092cf","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00066c3a8>: {\n        Underlying: <*exec.ExitError | 0xc0006471c0>{\n            ProcessState: {\n                pid: 7067,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 128316},\n                    Stime: {Sec: 0, Usec: 44247},\n                    Maxrss: 80264,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 9521,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 412,\n                    Nivcsw: 300,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660299182000,"stop":1660299182197,"duration":197}},{"uid":"eacf557b92ba96b8","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0000ce5b8>: {\n        Underlying: <*exec.ExitError | 0xc0008b8b00>{\n            ProcessState: {\n                pid: 7210,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 132413},\n                    Stime: {Sec: 0, Usec: 46985},\n                    Maxrss: 80556,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3993,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 377,\n                    Nivcsw: 203,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660295740000,"stop":1660295740186,"duration":186}},{"uid":"9dcf6e391f45ea53","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000649ad0>: {\n        Underlying: <*exec.ExitError | 0xc000829900>{\n            ProcessState: {\n                pid: 7169,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 169532},\n                    Stime: {Sec: 0, Usec: 59139},\n                    Maxrss: 82468,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 5967,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 299,\n                    Nivcsw: 316,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660293657000,"stop":1660293657257,"duration":257}},{"uid":"82afcbf8277b6889","status":"failed","statusDetails":"Unexpected error:\n    <*errors.StatusError | 0xc0000f6a00>: {\n        ErrStatus: {\n            TypeMeta: {Kind: \"\", APIVersion: \"\"},\n            ListMeta: {\n                SelfLink: \"\",\n                ResourceVersion: \"\",\n                Continue: \"\",\n                RemainingItemCount: nil,\n            },\n            Status: \"Failure\",\n            Message: \"namespaces \\\"iperf\\\" already exists\",\n            Reason: \"AlreadyExists\",\n            Details: {Name: \"iperf\", Group: \"\", Kind: \"namespaces\", UID: \"\", Causes: nil, RetryAfterSeconds: 0},\n            Code: 409,\n        },\n    }\n    namespaces \"iperf\" already exists\noccurred","time":{"start":1660113628000,"stop":1660113628360,"duration":360}},{"uid":"8898fd59cdc037cd","status":"failed","statusDetails":"Unexpected error:\n    <*errors.StatusError | 0xc00098bf40>: {\n        ErrStatus: {\n            TypeMeta: {Kind: \"\", APIVersion: \"\"},\n            ListMeta: {\n                SelfLink: \"\",\n                ResourceVersion: \"\",\n                Continue: \"\",\n                RemainingItemCount: nil,\n            },\n            Status: \"Failure\",\n            Message: \"namespaces \\\"iperf\\\" already exists\",\n            Reason: \"AlreadyExists\",\n            Details: {Name: \"iperf\", Group: \"\", Kind: \"namespaces\", UID: \"\", Causes: nil, RetryAfterSeconds: 0},\n            Code: 409,\n        },\n    }\n    namespaces \"iperf\" already exists\noccurred","time":{"start":1660104898000,"stop":1660104898528,"duration":528}},{"uid":"f7f482a66199998b","status":"passed","time":{"start":1660067259000,"stop":1660067259240,"duration":240}},{"uid":"16cb0f9377c13402","status":"passed","time":{"start":1660047551000,"stop":1660047551904,"duration":904}},{"uid":"81397e1af8087c56","status":"passed","time":{"start":1659982549000,"stop":1659982550984,"duration":1984}},{"uid":"55f4eddab014d14","status":"passed","time":{"start":1659970182000,"stop":1659970182838,"duration":838}},{"uid":"1a9027a95c501b97","status":"failed","statusDetails":"Unexpected error:\n    <*errors.StatusError | 0xc0006235e0>: {\n        ErrStatus: {\n            TypeMeta: {Kind: \"\", APIVersion: \"\"},\n            ListMeta: {\n                SelfLink: \"\",\n                ResourceVersion: \"\",\n                Continue: \"\",\n                RemainingItemCount: nil,\n            },\n            Status: \"Failure\",\n            Message: \"object is being deleted: namespaces \\\"appns1\\\" already exists\",\n            Reason: \"AlreadyExists\",\n            Details: {Name: \"appns1\", Group: \"\", Kind: \"namespaces\", UID: \"\", Causes: nil, RetryAfterSeconds: 0},\n            Code: 409,\n        },\n    }\n    object is being deleted: namespaces \"appns1\" already exists\noccurred","time":{"start":1659944495000,"stop":1659944495678,"duration":678}},{"uid":"171ce26b91b9b9c2","status":"failed","statusDetails":"Unexpected error:\n    <*errors.StatusError | 0xc0006d1a40>: {\n        ErrStatus: {\n            TypeMeta: {Kind: \"\", APIVersion: \"\"},\n            ListMeta: {\n                SelfLink: \"\",\n                ResourceVersion: \"\",\n                Continue: \"\",\n                RemainingItemCount: nil,\n            },\n            Status: \"Failure\",\n            Message: \"namespaces \\\"iperf\\\" already exists\",\n            Reason: \"AlreadyExists\",\n            Details: {Name: \"iperf\", Group: \"\", Kind: \"namespaces\", UID: \"\", Causes: nil, RetryAfterSeconds: 0},\n            Code: 409,\n        },\n    }\n    namespaces \"iperf\" already exists\noccurred","time":{"start":1659874977000,"stop":1659874977443,"duration":443}},{"uid":"251467de527f8996","status":"passed","time":{"start":1659496050000,"stop":1659496050675,"duration":675}},{"uid":"69b97791bdc803f8","status":"passed","time":{"start":1659496083000,"stop":1659496083628,"duration":628}},{"uid":"4eac7f9f63d3d13d","status":"passed","time":{"start":1659451501000,"stop":1659451502083,"duration":1083}},{"uid":"9b029ad9105096bf","status":"failed","statusDetails":"Unexpected error:\n    <*errors.StatusError | 0xc0006ff220>: {\n        ErrStatus: {\n            TypeMeta: {Kind: \"\", APIVersion: \"\"},\n            ListMeta: {\n                SelfLink: \"\",\n                ResourceVersion: \"\",\n                Continue: \"\",\n                RemainingItemCount: nil,\n            },\n            Status: \"Failure\",\n            Message: \"namespaces \\\"iperf\\\" already exists\",\n            Reason: \"AlreadyExists\",\n            Details: {Name: \"iperf\", Group: \"\", Kind: \"namespaces\", UID: \"\", Causes: nil, RetryAfterSeconds: 0},\n            Code: 409,\n        },\n    }\n    namespaces \"iperf\" already exists\noccurred","time":{"start":1659450916000,"stop":1659450916321,"duration":321}},{"uid":"6772042a3518b2a","status":"passed","time":{"start":1659451524000,"stop":1659451524462,"duration":462}},{"uid":"6dce2773b75aa3cd","status":"failed","statusDetails":"Unexpected error:\n    <*errors.StatusError | 0xc000752000>: {\n        ErrStatus: {\n            TypeMeta: {Kind: \"\", APIVersion: \"\"},\n            ListMeta: {\n                SelfLink: \"\",\n                ResourceVersion: \"\",\n                Continue: \"\",\n                RemainingItemCount: nil,\n            },\n            Status: \"Failure\",\n            Message: \"namespaces \\\"iperf\\\" already exists\",\n            Reason: \"AlreadyExists\",\n            Details: {Name: \"iperf\", Group: \"\", Kind: \"namespaces\", UID: \"\", Causes: nil, RetryAfterSeconds: 0},\n            Code: 409,\n        },\n    }\n    namespaces \"iperf\" already exists\noccurred","time":{"start":1659447298000,"stop":1659447298248,"duration":248}},{"uid":"ef8909c34d0ed3cd","status":"failed","statusDetails":"Unexpected error:\n    <*errors.StatusError | 0xc00043ca00>: {\n        ErrStatus: {\n            TypeMeta: {Kind: \"\", APIVersion: \"\"},\n            ListMeta: {\n                SelfLink: \"\",\n                ResourceVersion: \"\",\n                Continue: \"\",\n                RemainingItemCount: nil,\n            },\n            Status: \"Failure\",\n            Message: \"namespaces \\\"iperf\\\" already exists\",\n            Reason: \"AlreadyExists\",\n            Details: {Name: \"iperf\", Group: \"\", Kind: \"namespaces\", UID: \"\", Causes: nil, RetryAfterSeconds: 0},\n            Code: 409,\n        },\n    }\n    namespaces \"iperf\" already exists\noccurred","time":{"start":1659447116000,"stop":1659447116608,"duration":608}},{"uid":"59f7f8c7e2799c5b","status":"failed","statusDetails":"Unexpected error:\n    <*errors.StatusError | 0xc000342280>: {\n        ErrStatus: {\n            TypeMeta: {Kind: \"\", APIVersion: \"\"},\n            ListMeta: {\n                SelfLink: \"\",\n                ResourceVersion: \"\",\n                Continue: \"\",\n                RemainingItemCount: nil,\n            },\n            Status: \"Failure\",\n            Message: \"object is being deleted: namespaces \\\"appns1\\\" already exists\",\n            Reason: \"AlreadyExists\",\n            Details: {Name: \"appns1\", Group: \"\", Kind: \"namespaces\", UID: \"\", Causes: nil, RetryAfterSeconds: 0},\n            Code: 409,\n        },\n    }\n    object is being deleted: namespaces \"appns1\" already exists\noccurred","time":{"start":1659448205000,"stop":1659448205283,"duration":283}}]},"Worker Suite:Worker Suite#[It] Intracluster slice tests Iperf connectivity Verify Iperf traffic":{"statistic":{"failed":0,"broken":0,"skipped":34,"passed":5,"unknown":0,"total":39},"items":[{"uid":"f628ee148388006b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"ae99bacad219ebc1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"93c952e4bb2b6a94","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"d44bc2219e08aebe","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"93e4a76dfc51a352","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"5279f931074749bd","status":"passed","time":{"start":1660104898000,"stop":1660104967723,"duration":69723}},{"uid":"a60ed165405bfb9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"ac8fbb740fe2a6d8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}},{"uid":"ca3cdc17533b3532","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659982549000,"stop":1659982549000,"duration":0}},{"uid":"ff7453967a47588","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659970182000,"stop":1659970182000,"duration":0}},{"uid":"3444acdca0b6787","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659944495000,"stop":1659944495000,"duration":0}},{"uid":"9d4c37ca7aca3ad8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659874977000,"stop":1659874977000,"duration":0}},{"uid":"43bc1c6c4e266c89","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496050000,"stop":1659496050000,"duration":0}},{"uid":"67aac1a8e1dc2e42","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496083000,"stop":1659496083000,"duration":0}},{"uid":"74dd338d4d7d570b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"4a40569094ff9837","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659450916000,"stop":1659450916000,"duration":0}},{"uid":"b391898554e5bea7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"df2985074c9ee1d6","status":"passed","time":{"start":1659447298000,"stop":1659447368306,"duration":70306}},{"uid":"24fc39a6ff851b8d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447116000,"stop":1659447116000,"duration":0}},{"uid":"576e105b7bbdaf2c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448205000,"stop":1659448205000,"duration":0}}]},"Istio Suite:Istio Suite#[It] Bookinfo tests Bookinfo Scenario 1 without egress and ingress Should verify if Ping works successfully from Productpage to details endpoint":{"statistic":{"failed":1,"broken":0,"skipped":15,"passed":2,"unknown":0,"total":18},"items":[{"uid":"76196c29051974fe","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808584000,"stop":1660808584000,"duration":0}},{"uid":"12cd893412f3b16f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660804840000,"stop":1660804840000,"duration":0}},{"uid":"1a44eb1bad692a4d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660804470000,"stop":1660804470000,"duration":0}},{"uid":"a87577c53f1733c5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660748849000,"stop":1660748849000,"duration":0}},{"uid":"828e64c6c800b82d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660748706000,"stop":1660748706000,"duration":0}},{"uid":"c419e8318e04fc65","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660748558000,"stop":1660748558000,"duration":0}},{"uid":"2fe04e442e0b64d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660636759000,"stop":1660636759000,"duration":0}},{"uid":"4c86a16a81e97d2d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660636482000,"stop":1660636482000,"duration":0}},{"uid":"6249f3f99a63cf2a","status":"passed","time":{"start":1660632006000,"stop":1660632065318,"duration":59318}},{"uid":"a3bed621ffcabc84","status":"failed","statusDetails":"Unexpected non-nil/non-zero argument at index 1:\n\t<*shell.ErrWithCmdOutput>: &shell.ErrWithCmdOutput{Underlying:(*exec.ExitError)(0xc00030dfe0), Output:(*shell.output)(0xc0006925e8)}","time":{"start":1660624273000,"stop":1660624340131,"duration":67131}},{"uid":"3ea6fba99006bded","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660624220000,"stop":1660624220000,"duration":0}},{"uid":"2e08bc6c0f79d785","status":"passed","time":{"start":1660453205000,"stop":1660453265971,"duration":60971}},{"uid":"f02a1feeaee29209","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660452554000,"stop":1660452554000,"duration":0}},{"uid":"aa37cdfff4a89eea","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660452412000,"stop":1660452412000,"duration":0}},{"uid":"99be457e596cba0a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660415422000,"stop":1660415422000,"duration":0}},{"uid":"347ac28c1a6e5547","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660298968000,"stop":1660298968000,"duration":0}},{"uid":"ac8955c49e54c4d6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295522000,"stop":1660295522000,"duration":0}},{"uid":"7b36e51ed116986","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293414000,"stop":1660293414000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Intracluster slice tests Slice with netpol should reconcile netpol config":{"statistic":{"failed":6,"broken":0,"skipped":18,"passed":15,"unknown":0,"total":39},"items":[{"uid":"d9f832b7af9640d8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"a0d675ae50a80275","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"9d5f92a1e32ba1e8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"d616e72d6e91c868","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"ad455162458977d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"ddbbf6a16b8f369a","status":"passed","time":{"start":1660104898000,"stop":1660104900319,"duration":2319}},{"uid":"964d7e47f9ad0048","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"dda066b66ff4e4e","status":"passed","time":{"start":1660047551000,"stop":1660047553273,"duration":2273}},{"uid":"4a6257fa541efb4f","status":"passed","time":{"start":1659982549000,"stop":1659982558004,"duration":9004}},{"uid":"c84e80603dc27b9c","status":"passed","time":{"start":1659970182000,"stop":1659970185573,"duration":3573}},{"uid":"4b15115c840bd5ce","status":"passed","time":{"start":1659944495000,"stop":1659944498821,"duration":3821}},{"uid":"be59dfbef116405d","status":"passed","time":{"start":1659874977000,"stop":1659874987540,"duration":10540}},{"uid":"ea8c0ba3980e3824","status":"failed","statusDetails":"\n\tError Trace:\tnamespace.go:15\n\t            \t\t\t\tt_intra_slice_test.go:98\n\t            \t\t\t\tsuite.go:596\n\t            \t\t\t\tasm_amd64.s:1581\n\tError:      \tReceived unexpected error:\n\t            \tnamespaces \"iperf\" already exists\n\tTest:       \tIntracluster slice tests Slice with netpol should reconcile netpol config\n","time":{"start":1659496050000,"stop":1659496097093,"duration":47093}},{"uid":"be343f482aa846fb","status":"failed","statusDetails":"\n\tError Trace:\tnamespace.go:15\n\t            \t\t\t\tt_intra_slice_test.go:98\n\t            \t\t\t\tsuite.go:596\n\t            \t\t\t\tasm_amd64.s:1581\n\tError:      \tReceived unexpected error:\n\t            \tnamespaces \"iperf\" already exists\n\tTest:       \tIntracluster slice tests Slice with netpol should reconcile netpol config\n","time":{"start":1659496083000,"stop":1659496089748,"duration":6748}},{"uid":"51d01dba53daab9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"f06401c869eecfb1","status":"passed","time":{"start":1659450916000,"stop":1659450919797,"duration":3797}},{"uid":"5b5da0a1cdfe5687","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"e797d8a6a47a327c","status":"passed","time":{"start":1659447298000,"stop":1659447319006,"duration":21006}},{"uid":"49fd959544321e7b","status":"passed","time":{"start":1659447116000,"stop":1659447119635,"duration":3635}},{"uid":"8c2d897cfb759a33","status":"passed","time":{"start":1659448205000,"stop":1659448207401,"duration":2401}}]},"Worker Suite:Worker Suite#[It] Slice creation scenarios: when slice red is installed install slice blue workers should have gateway pods from both slices":{"statistic":{"failed":0,"broken":0,"skipped":28,"passed":11,"unknown":0,"total":39},"items":[{"uid":"7ac2f98b7a731107","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"e93f7d98170f432","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"50896baa337f0314","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"b5c613f710dc5ce3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"2b39c17db0aba308","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"34eb0385ebe89ef8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"f9fe66694047a9a7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"edc7efe3b07c09dc","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}},{"uid":"cc5d4e542e96e425","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659982549000,"stop":1659982549000,"duration":0}},{"uid":"7f37d7b482dd457","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659970182000,"stop":1659970182000,"duration":0}},{"uid":"8332a71818fe843b","status":"passed","time":{"start":1659944495000,"stop":1659944504383,"duration":9383}},{"uid":"fad9629d5a83c829","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659874977000,"stop":1659874977000,"duration":0}},{"uid":"1850f7250c447a17","status":"passed","time":{"start":1659496050000,"stop":1659496080822,"duration":30822}},{"uid":"273a69b09f814b","status":"passed","time":{"start":1659496083000,"stop":1659496105829,"duration":22829}},{"uid":"695fad9ae7bc120b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"b2155d2daa6d5b32","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659450916000,"stop":1659450916000,"duration":0}},{"uid":"1485d347117437c9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"a35c1e3b90aaeff6","status":"passed","time":{"start":1659447298000,"stop":1659447350643,"duration":52643}},{"uid":"6ab6fc46852a951f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447116000,"stop":1659447116000,"duration":0}},{"uid":"acb126f895196dca","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448205000,"stop":1659448205000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Namespace reconcile test scenarios: Onboard label app ns on workers":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":39,"unknown":0,"total":39},"items":[{"uid":"ed236ef7ddc8b5b","status":"passed","time":{"start":1660808797000,"stop":1660808797641,"duration":641}},{"uid":"d727d2d177b96a1c","status":"passed","time":{"start":1660299182000,"stop":1660299182576,"duration":576}},{"uid":"639a60d7406e1ed1","status":"passed","time":{"start":1660295740000,"stop":1660295740624,"duration":624}},{"uid":"e9d26784f4775377","status":"passed","time":{"start":1660293657000,"stop":1660293657867,"duration":867}},{"uid":"92e75ee914ea46c9","status":"passed","time":{"start":1660113628000,"stop":1660113628644,"duration":644}},{"uid":"2a0eef263107f5d8","status":"passed","time":{"start":1660104898000,"stop":1660104898751,"duration":751}},{"uid":"c1326a6b9188664e","status":"passed","time":{"start":1660067259000,"stop":1660067260343,"duration":1343}},{"uid":"7cfb401f88ceeb0e","status":"passed","time":{"start":1660047551000,"stop":1660047552076,"duration":1076}},{"uid":"78389f531eabe6e2","status":"passed","time":{"start":1659982549000,"stop":1659982551207,"duration":2207}},{"uid":"e033e5f928b37871","status":"passed","time":{"start":1659970182000,"stop":1659970182878,"duration":878}},{"uid":"2ebea12dbb57daaf","status":"passed","time":{"start":1659944495000,"stop":1659944496942,"duration":1942}},{"uid":"38ef915aafe41c3a","status":"passed","time":{"start":1659874977000,"stop":1659874978507,"duration":1507}},{"uid":"9984faa860790168","status":"passed","time":{"start":1659496050000,"stop":1659496050690,"duration":690}},{"uid":"e9975b2c6430b06e","status":"passed","time":{"start":1659496083000,"stop":1659496083847,"duration":847}},{"uid":"838ca9c333cd6ef0","status":"passed","time":{"start":1659451501000,"stop":1659451501737,"duration":737}},{"uid":"f96fef4e3233c08a","status":"passed","time":{"start":1659450916000,"stop":1659450917648,"duration":1648}},{"uid":"ce31f053e0fba3b7","status":"passed","time":{"start":1659451524000,"stop":1659451525210,"duration":1210}},{"uid":"15770e8ef45c83ef","status":"passed","time":{"start":1659447298000,"stop":1659447298703,"duration":703}},{"uid":"545a7311c6090c29","status":"passed","time":{"start":1659447116000,"stop":1659447117520,"duration":1520}},{"uid":"e976408103243e1a","status":"passed","time":{"start":1659448205000,"stop":1659448205693,"duration":693}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests Iperf App connectivity should have vl3 router running":{"statistic":{"failed":24,"broken":0,"skipped":0,"passed":15,"unknown":0,"total":39},"items":[{"uid":"4f3f14917e294823","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000989518>: {\n        Underlying: <*exec.ExitError | 0xc000710900>{\n            ProcessState: {\n                pid: 7152,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 159208},\n                    Stime: {Sec: 0, Usec: 55537},\n                    Maxrss: 83908,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4107,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 377,\n                    Nivcsw: 344,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660808797000,"stop":1660808797413,"duration":413}},{"uid":"c6dbabf0998b8d95","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000350ba0>: {\n        Underlying: <*exec.ExitError | 0xc000829380>{\n            ProcessState: {\n                pid: 7262,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 137557},\n                    Stime: {Sec: 0, Usec: 33459},\n                    Maxrss: 86792,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4348,\n                    Majflt: 1,\n                    Nswap: 0,\n                    Inblock: 160,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 505,\n                    Nivcsw: 381,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660299182000,"stop":1660299182351,"duration":351}},{"uid":"ad89085f103f7bb8","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004ca780>: {\n        Underlying: <*exec.ExitError | 0xc000158400>{\n            ProcessState: {\n                pid: 7225,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 140590},\n                    Stime: {Sec: 0, Usec: 37215},\n                    Maxrss: 86032,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3706,\n                    Majflt: 1,\n                    Nswap: 0,\n                    Inblock: 104,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 367,\n                    Nivcsw: 290,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660295740000,"stop":1660295740325,"duration":325}},{"uid":"4a870b9dec063265","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000649998>: {\n        Underlying: <*exec.ExitError | 0xc0008293e0>{\n            ProcessState: {\n                pid: 7158,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 153874},\n                    Stime: {Sec: 0, Usec: 48360},\n                    Maxrss: 82512,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 5046,\n                    Majflt: 2,\n                    Nswap: 0,\n                    Inblock: 392,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 359,\n                    Nivcsw: 315,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660293657000,"stop":1660293657456,"duration":456}},{"uid":"9c1ac4c625de9212","status":"passed","time":{"start":1660113628000,"stop":1660113628487,"duration":487}},{"uid":"8b394d22a22410ed","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 4\nto equal\n    <int>: 1","time":{"start":1660104898000,"stop":1660105081464,"duration":183464}},{"uid":"45c02c19e20732cd","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 2\nto equal\n    <int>: 1","time":{"start":1660067259000,"stop":1660067442133,"duration":183133}},{"uid":"397ec039985cdb94","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 2\nto equal\n    <int>: 1","time":{"start":1660047551000,"stop":1660047731889,"duration":180889}},{"uid":"31e3dd06a5376a65","status":"passed","time":{"start":1659982549000,"stop":1659982549652,"duration":652}},{"uid":"c9a9428c9bf70aab","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 2\nto equal\n    <int>: 1","time":{"start":1659970182000,"stop":1659970365167,"duration":183167}},{"uid":"c9b0b13fc07b9b93","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 2\nto equal\n    <int>: 1","time":{"start":1659944495000,"stop":1659944678278,"duration":183278}},{"uid":"fc3f38909d6dcc50","status":"passed","time":{"start":1659874977000,"stop":1659874977719,"duration":719}},{"uid":"a6acd4401d239c77","status":"passed","time":{"start":1659496050000,"stop":1659496050571,"duration":571}},{"uid":"4618911f61a7a8e","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 4\nto equal\n    <int>: 1","time":{"start":1659496083000,"stop":1659496266318,"duration":183318}},{"uid":"9111b1bffcd7e71a","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1659451501000,"stop":1659451685089,"duration":184089}},{"uid":"570745321da0801b","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 2\nto equal\n    <int>: 1","time":{"start":1659450916000,"stop":1659451104080,"duration":188080}},{"uid":"50d2afb671e492a","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1659451524000,"stop":1659451709724,"duration":185724}},{"uid":"43f4f381032348ff","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 2\nto equal\n    <int>: 1","time":{"start":1659447298000,"stop":1659447483021,"duration":185021}},{"uid":"480bf9ab0343e342","status":"passed","time":{"start":1659447116000,"stop":1659447116804,"duration":804}},{"uid":"fca79682723bfda7","status":"failed","statusDetails":"Unexpected error:\n    <*errors.StatusError | 0xc0007183c0>: {\n        ErrStatus: {\n            TypeMeta: {Kind: \"\", APIVersion: \"\"},\n            ListMeta: {\n                SelfLink: \"\",\n                ResourceVersion: \"\",\n                Continue: \"\",\n                RemainingItemCount: nil,\n            },\n            Status: \"Failure\",\n            Message: \"namespaces \\\"iperf\\\" already exists\",\n            Reason: \"AlreadyExists\",\n            Details: {Name: \"iperf\", Group: \"\", Kind: \"namespaces\", UID: \"\", Causes: nil, RetryAfterSeconds: 0},\n            Code: 409,\n        },\n    }\n    namespaces \"iperf\" already exists\noccurred","time":{"start":1659448205000,"stop":1659448206275,"duration":1275}}]},"Istio Suite:Istio Suite#[It] Bookinfo tests Bookinfo Scenario 4 with egress and ingress Should verify if Curl Traffic works on Productpage test":{"statistic":{"failed":0,"broken":0,"skipped":17,"passed":1,"unknown":0,"total":18},"items":[{"uid":"9b662430d9df0866","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808584000,"stop":1660808584000,"duration":0}},{"uid":"f66911ee5be7d954","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660804840000,"stop":1660804840000,"duration":0}},{"uid":"8f00e86312572130","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660804470000,"stop":1660804470000,"duration":0}},{"uid":"990eeda4a329c381","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660748849000,"stop":1660748849000,"duration":0}},{"uid":"6f29538befbc1a1","status":"passed","time":{"start":1660748706000,"stop":1660748706787,"duration":787}},{"uid":"48cd7dadda96bb92","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660748558000,"stop":1660748558000,"duration":0}},{"uid":"4747dac2f925e8ba","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660636759000,"stop":1660636759000,"duration":0}},{"uid":"2037d001a1b27b1d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660636482000,"stop":1660636482000,"duration":0}},{"uid":"51b187c8a1c7e587","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660632006000,"stop":1660632006000,"duration":0}},{"uid":"364da5ea38953148","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660624273000,"stop":1660624273000,"duration":0}},{"uid":"325f31eae7992d6b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660624220000,"stop":1660624220000,"duration":0}},{"uid":"616b523aa2f2d5ce","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660453205000,"stop":1660453205000,"duration":0}},{"uid":"1e436ce33078ad5a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660452554000,"stop":1660452554000,"duration":0}},{"uid":"f7c8f5fcfbef8294","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660452412000,"stop":1660452412000,"duration":0}},{"uid":"efaf7bac046e828a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660415422000,"stop":1660415422000,"duration":0}},{"uid":"a205cd9c75bef5f3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660298968000,"stop":1660298968000,"duration":0}},{"uid":"2d2b9036f34ee2e9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295522000,"stop":1660295522000,"duration":0}},{"uid":"6f2410fde490546b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293414000,"stop":1660293414000,"duration":0}}]},"Istio Suite:Istio Suite#[BeforeSuite]":{"statistic":{"failed":28,"broken":0,"skipped":0,"passed":18,"unknown":0,"total":46},"items":[{"uid":"f8decf1d585a451","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0007f8018>: {\n        Underlying: <*exec.ExitError | 0xc000074320>{\n            ProcessState: {\n                pid: 6764,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 3, Usec: 202194},\n                    Stime: {Sec: 0, Usec: 490223},\n                    Maxrss: 86772,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 8157,\n                    Majflt: 1,\n                    Nswap: 0,\n                    Inblock: 256,\n                    Oublock: 14168,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 23239,\n                    Nivcsw: 6068,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.2.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD serviceexports.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD serviceimports.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD slice.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD slicegateways.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 44 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 44 resources with timeout of 2m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [deb...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.2.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD serviceexports.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD serviceimports.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD slice.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD slicegateways.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 44 resource(s)\n    wait.go:48: [debug] beginning wait for 44 resources with timeout of 2m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:310: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-svc\" Service\n    client.go:310: [debug] Starting delete for \"jaeger\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:310: [debug] Starting delete for \"jaeger\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook\" Deployment\n    client.go:310: [debug] Starting delete for \"prefix-service\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsm-kernel-forwarder\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:310: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"networkservices.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkserviceendpoints.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservicemanagers.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-cfg\" MutatingWebhookConfiguration\n    uninstall.go:144: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1660829454000,"stop":1660829687964,"duration":233964}},{"uid":"1458757bcf6df0fd","status":"passed","time":{"start":1660808584000,"stop":1660808763135,"duration":179135}},{"uid":"d37466e0259ca532","status":"passed","time":{"start":1660804840000,"stop":1660805070738,"duration":230738}},{"uid":"55e7042eba33c6ba","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00047e060>: {\n        Underlying: <*exec.ExitError | 0xc000898020>{\n            ProcessState: {\n                pid: 6824,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 2, Usec: 262683},\n                    Stime: {Sec: 0, Usec: 360058},\n                    Maxrss: 85624,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 9746,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14168,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 22198,\n                    Nivcsw: 6277,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.2.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD serviceexports.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD serviceimports.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD slice.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD slicegateways.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 44 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 44 resources with timeout of 2m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.2.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD serviceexports.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD serviceimports.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD slice.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD slicegateways.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 44 resource(s)\n    wait.go:48: [debug] beginning wait for 44 resources with timeout of 2m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:310: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-svc\" Service\n    client.go:310: [debug] Starting delete for \"jaeger\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook\" Deployment\n    client.go:310: [debug] Starting delete for \"jaeger\" Deployment\n    client.go:310: [debug] Starting delete for \"prefix-service\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsm-kernel-forwarder\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"networkservices.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkserviceendpoints.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservicemanagers.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:310: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-cfg\" MutatingWebhookConfiguration\n    uninstall.go:144: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1660806662000,"stop":1660806888361,"duration":226361}},{"uid":"bcfb7ab15c8b4219","status":"passed","time":{"start":1660804470000,"stop":1660804658229,"duration":188229}},{"uid":"5ea26b9d4ac83f40","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00047c228>: {\n        Underlying: <*exec.ExitError | 0xc000488040>{\n            ProcessState: {\n                pid: 6054,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 1, Usec: 452993},\n                    Stime: {Sec: 0, Usec: 240781},\n                    Maxrss: 87168,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 7612,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12840,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 16897,\n                    Nivcsw: 4042,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-0.2.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 26 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 26 resources with timeout of 2m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-0.2.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 26 resource(s)\n    wait.go:48: [debug] beginning wait for 26 resources with timeout of 2m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-controller\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-manager-metrics-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-controller-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-editor-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"workerslicegateways.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"clusters.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"sliceconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"projects.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"serviceexportconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workerserviceimports.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workersliceconfigs.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-serving-cert\" Certificate\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-selfsigned-issuer\" Issuer\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-validating-webhook-configuration\" ValidatingWebhookConfiguration\n    uninstall.go:144: [debug] purge requested for kubeslice-controller\n    Error: INSTALLATION FAILED: release kubeslice-controller failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-controller failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1660803509000,"stop":1660803717429,"duration":208429}},{"uid":"7cf46e43445db83d","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000e030>: {\n        Underlying: <*exec.ExitError | 0xc000074000>{\n            ProcessState: {\n                pid: 6798,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 2, Usec: 272072},\n                    Stime: {Sec: 0, Usec: 278396},\n                    Maxrss: 90660,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 9919,\n                    Majflt: 1,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14168,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 22750,\n                    Nivcsw: 4356,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.2.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD serviceexports.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD serviceimports.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD slice.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD slicegateways.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 44 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 44 resources with timeout of 2m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.2.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD serviceexports.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD serviceimports.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD slice.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD slicegateways.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 44 resource(s)\n    wait.go:48: [debug] beginning wait for 44 resources with timeout of 2m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:310: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-svc\" Service\n    client.go:310: [debug] Starting delete for \"jaeger\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook\" Deployment\n    client.go:310: [debug] Starting delete for \"jaeger\" Deployment\n    client.go:310: [debug] Starting delete for \"prefix-service\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsm-kernel-forwarder\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:310: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"networkservices.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkserviceendpoints.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservicemanagers.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-cfg\" MutatingWebhookConfiguration\n    uninstall.go:144: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1660799371000,"stop":1660799615625,"duration":244625}},{"uid":"c21ef7811ca0b874","status":"passed","time":{"start":1660748849000,"stop":1660749094459,"duration":245459}},{"uid":"4ec84a329524d4ed","status":"passed","time":{"start":1660748706000,"stop":1660748930692,"duration":224692}},{"uid":"ac250aa5d54fe2e3","status":"passed","time":{"start":1660748558000,"stop":1660748770327,"duration":212327}},{"uid":"cdd3561cca657e6b","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000398048>: {\n        Underlying: <*exec.ExitError | 0xc00066c000>{\n            ProcessState: {\n                pid: 6847,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 2, Usec: 373184},\n                    Stime: {Sec: 0, Usec: 292584},\n                    Maxrss: 89588,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 16390,\n                    Majflt: 3,\n                    Nswap: 0,\n                    Inblock: 184,\n                    Oublock: 14168,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 20722,\n                    Nivcsw: 4916,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.2.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD serviceexports.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD serviceimports.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD slice.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD slicegateways.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 44 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 44 resources with timeout of 2m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"re...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.2.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD serviceexports.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD serviceimports.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD slice.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD slicegateways.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 44 resource(s)\n    wait.go:48: [debug] beginning wait for 44 resources with timeout of 2m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:310: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"jaeger\" Service\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-svc\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook\" Deployment\n    client.go:310: [debug] Starting delete for \"jaeger\" Deployment\n    client.go:310: [debug] Starting delete for \"prefix-service\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-kernel-forwarder\" DaemonSet\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"networkservices.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkserviceendpoints.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservicemanagers.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-cfg\" MutatingWebhookConfiguration\n    uninstall.go:144: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1660748560000,"stop":1660748776334,"duration":216334}},{"uid":"b371fa7bc36dbafd","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000e090>: {\n        Underlying: <*exec.ExitError | 0xc00045c000>{\n            ProcessState: {\n                pid: 6105,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 1, Usec: 308407},\n                    Stime: {Sec: 0, Usec: 188952},\n                    Maxrss: 78696,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 9543,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12840,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 14898,\n                    Nivcsw: 3997,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-0.2.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 26 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 26 resources with timeout of 2m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-0.2.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 26 resource(s)\n    wait.go:48: [debug] beginning wait for 26 resources with timeout of 2m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-controller\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-manager-metrics-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-controller-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-editor-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"workerslicegateways.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"clusters.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"projects.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"serviceexportconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"sliceconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workerserviceimports.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workersliceconfigs.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-serving-cert\" Certificate\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-selfsigned-issuer\" Issuer\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-validating-webhook-configuration\" ValidatingWebhookConfiguration\n    uninstall.go:144: [debug] purge requested for kubeslice-controller\n    Error: INSTALLATION FAILED: release kubeslice-controller failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-controller failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1660748476000,"stop":1660748667809,"duration":191809}},{"uid":"a5ecc6461c09704b","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0006b0018>: {\n        Underlying: <*exec.ExitError | 0xc00089a000>{\n            ProcessState: {\n                pid: 6835,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 3, Usec: 188800},\n                    Stime: {Sec: 0, Usec: 453005},\n                    Maxrss: 87128,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 8522,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14168,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 24813,\n                    Nivcsw: 7041,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.2.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD serviceexports.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD serviceimports.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD slice.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD slicegateways.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 44 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 44 resources with timeout of 2m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.2.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD serviceexports.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD serviceimports.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD slice.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD slicegateways.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 44 resource(s)\n    wait.go:48: [debug] beginning wait for 44 resources with timeout of 2m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:310: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"jaeger\" Service\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-svc\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook\" Deployment\n    client.go:310: [debug] Starting delete for \"jaeger\" Deployment\n    client.go:310: [debug] Starting delete for \"prefix-service\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-kernel-forwarder\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"networkservices.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkserviceendpoints.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservicemanagers.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-cfg\" MutatingWebhookConfiguration\n    uninstall.go:144: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: client rate limiter Wait returned an error: rate: Wait(n=1) would exceed context deadline\n    helm.go:84: [debug] client rate limiter Wait returned an error: rate: Wait(n=1) would exceed context deadline\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1660723610000,"stop":1660723839675,"duration":229675}},{"uid":"5507bc31b8dbcf49","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000494078>: {\n        Underlying: <*exec.ExitError | 0xc000074000>{\n            ProcessState: {\n                pid: 6038,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 1, Usec: 262633},\n                    Stime: {Sec: 0, Usec: 171417},\n                    Maxrss: 78620,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 7950,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12840,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 14758,\n                    Nivcsw: 3770,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-0.2.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 26 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 26 resources with timeout of 2m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-0.2.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 26 resource(s)\n    wait.go:48: [debug] beginning wait for 26 resources with timeout of 2m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-controller\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-manager-metrics-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-controller-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-editor-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"workerslicegateways.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"clusters.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"projects.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"serviceexportconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"sliceconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workerserviceimports.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workersliceconfigs.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-serving-cert\" Certificate\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-selfsigned-issuer\" Issuer\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-validating-webhook-configuration\" ValidatingWebhookConfiguration\n    uninstall.go:144: [debug] purge requested for kubeslice-controller\n    Error: INSTALLATION FAILED: release kubeslice-controller failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-controller failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1660712528000,"stop":1660712719739,"duration":191739}},{"uid":"8d36fe10ce346d08","status":"passed","time":{"start":1660636759000,"stop":1660636980211,"duration":221211}},{"uid":"7b47c8118230ae63","status":"passed","time":{"start":1660636482000,"stop":1660636676795,"duration":194795}},{"uid":"d85cce234caf74bc","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000686018>: {\n        Underlying: <*exec.ExitError | 0xc0004a6020>{\n            ProcessState: {\n                pid: 6853,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 3, Usec: 578512},\n                    Stime: {Sec: 0, Usec: 516010},\n                    Maxrss: 90716,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 6268,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14168,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 25139,\n                    Nivcsw: 6166,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.2.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD serviceexports.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD serviceimports.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD slice.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD slicegateways.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 44 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 44 resources with timeout of 2m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n               ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.2.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD serviceexports.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD serviceimports.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD slice.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD slicegateways.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 44 resource(s)\n    wait.go:48: [debug] beginning wait for 44 resources with timeout of 2m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:310: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:310: [debug] Starting delete for \"jaeger\" Service\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-svc\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:310: [debug] Starting delete for \"jaeger\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook\" Deployment\n    client.go:310: [debug] Starting delete for \"prefix-service\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsm-kernel-forwarder\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"networkservices.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkserviceendpoints.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservicemanagers.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-cfg\" MutatingWebhookConfiguration\n    uninstall.go:144: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1660636748000,"stop":1660636969410,"duration":221410}},{"uid":"30df92fc1ebe2e32","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0000be5b8>: {\n        Underlying: <*exec.ExitError | 0xc0003d4200>{\n            ProcessState: {\n                pid: 6063,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 1, Usec: 414905},\n                    Stime: {Sec: 0, Usec: 195721},\n                    Maxrss: 77604,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 9770,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12840,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 16511,\n                    Nivcsw: 5117,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-0.2.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 26 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 26 resources with timeout of 2m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-0.2.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 26 resource(s)\n    wait.go:48: [debug] beginning wait for 26 resources with timeout of 2m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-controller\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-manager-metrics-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-controller-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-editor-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"workerslicegateways.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"clusters.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"projects.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"serviceexportconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"sliceconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workerserviceimports.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workersliceconfigs.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-serving-cert\" Certificate\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-selfsigned-issuer\" Issuer\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-validating-webhook-configuration\" ValidatingWebhookConfiguration\n    uninstall.go:144: [debug] purge requested for kubeslice-controller\n    Error: INSTALLATION FAILED: release kubeslice-controller failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-controller failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1660636362000,"stop":1660636569141,"duration":207141}},{"uid":"a0f6d7244c39d2e1","status":"passed","time":{"start":1660632006000,"stop":1660632201826,"duration":195826}},{"uid":"593dedd314cc0bb","status":"passed","time":{"start":1660624273000,"stop":1660624463275,"duration":190275}}]},"Worker Suite:Worker Suite#[It] Slice Config Negative Scenario tests Slice Config Negative Scenario Update Should fail while updating sliceconfig with bandwithguaranteedkbps more than bandwidthceilingkbps in qosprofiledetailsnvalid ":{"statistic":{"failed":0,"broken":0,"skipped":4,"passed":35,"unknown":0,"total":39},"items":[{"uid":"12168e5c90449c47","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"508565ea32d74e03","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"b4e9dc4b51d6d55e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"a95a975cb1069401","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"fdb8860c9119d6c6","status":"passed","time":{"start":1660113628000,"stop":1660113628216,"duration":216}},{"uid":"7f217d9fae26e48d","status":"passed","time":{"start":1660104898000,"stop":1660104898230,"duration":230}},{"uid":"baddd58a1b48f1e3","status":"passed","time":{"start":1660067259000,"stop":1660067259302,"duration":302}},{"uid":"db127599586e5ed9","status":"passed","time":{"start":1660047551000,"stop":1660047551612,"duration":612}},{"uid":"31970597fea6bd34","status":"passed","time":{"start":1659982549000,"stop":1659982549844,"duration":844}},{"uid":"285ed495ec1158df","status":"passed","time":{"start":1659970182000,"stop":1659970182378,"duration":378}},{"uid":"9c3f0b47caa5fe25","status":"passed","time":{"start":1659944495000,"stop":1659944495549,"duration":549}},{"uid":"47091ca595e1de2d","status":"passed","time":{"start":1659874977000,"stop":1659874977406,"duration":406}},{"uid":"1289aea424ef4089","status":"passed","time":{"start":1659496050000,"stop":1659496050404,"duration":404}},{"uid":"33d7db9f8743e785","status":"passed","time":{"start":1659496083000,"stop":1659496083533,"duration":533}},{"uid":"cbdb89252fe0a4e6","status":"passed","time":{"start":1659451501000,"stop":1659451501800,"duration":800}},{"uid":"959934a4b45a4b1d","status":"passed","time":{"start":1659450916000,"stop":1659450916436,"duration":436}},{"uid":"23dc11f4e6270956","status":"passed","time":{"start":1659451524000,"stop":1659451524395,"duration":395}},{"uid":"f1e5ab2dfdbe7365","status":"passed","time":{"start":1659447298000,"stop":1659447298312,"duration":312}},{"uid":"b914e019a1e1d757","status":"passed","time":{"start":1659447116000,"stop":1659447116262,"duration":262}},{"uid":"d2a3480049a451e9","status":"passed","time":{"start":1659448205000,"stop":1659448205478,"duration":478}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Update Should update successfully when Project Applied while removing a service account name in Write users":{"statistic":{"failed":7,"broken":0,"skipped":0,"passed":114,"unknown":0,"total":121},"items":[{"uid":"dfb1ca5b96f2c90a","status":"passed","time":{"start":1660828517000,"stop":1660828522045,"duration":5045}},{"uid":"3e706cecada5a169","status":"passed","time":{"start":1660807774000,"stop":1660807779571,"duration":5571}},{"uid":"a596d281265c9210","status":"passed","time":{"start":1660803998000,"stop":1660804002832,"duration":4832}},{"uid":"b132f512e97a31a0","status":"passed","time":{"start":1660805737000,"stop":1660805742459,"duration":5459}},{"uid":"29df03a8f1204d37","status":"passed","time":{"start":1660803696000,"stop":1660803700409,"duration":4409}},{"uid":"156a0d8a32269fac","status":"passed","time":{"start":1660798468000,"stop":1660798473678,"duration":5678}},{"uid":"df988f508410a4e0","status":"passed","time":{"start":1660748009000,"stop":1660748014459,"duration":5459}},{"uid":"9d45ef2d9fef1406","status":"passed","time":{"start":1660747865000,"stop":1660747870174,"duration":5174}},{"uid":"2bb5346cc17c0784","status":"passed","time":{"start":1660747735000,"stop":1660747740814,"duration":5814}},{"uid":"3e2462af748f427d","status":"passed","time":{"start":1660747654000,"stop":1660747658426,"duration":4426}},{"uid":"a060eb20142ae7f2","status":"passed","time":{"start":1660722683000,"stop":1660722687906,"duration":4906}},{"uid":"eda3d7e6fc7ce738","status":"passed","time":{"start":1660635916000,"stop":1660635920904,"duration":4904}},{"uid":"915ec98d23912b3a","status":"passed","time":{"start":1660635666000,"stop":1660635670849,"duration":4849}},{"uid":"783b93315bbbabd7","status":"passed","time":{"start":1660635860000,"stop":1660635864980,"duration":4980}},{"uid":"1d9c1be839b23abd","status":"passed","time":{"start":1660631164000,"stop":1660631168433,"duration":4433}},{"uid":"4b03f569b2f01536","status":"passed","time":{"start":1660623463000,"stop":1660623468812,"duration":5812}},{"uid":"648f07b43ac5fe33","status":"passed","time":{"start":1660623384000,"stop":1660623389323,"duration":5323}},{"uid":"952acf3adbbe7ad1","status":"passed","time":{"start":1660623360000,"stop":1660623364325,"duration":4325}},{"uid":"c7f34056c83e6e6b","status":"passed","time":{"start":1660452377000,"stop":1660452381729,"duration":4729}},{"uid":"84847a2233018b9d","status":"passed","time":{"start":1660451870000,"stop":1660451875558,"duration":5558}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Update Should Update successfullyy when Project is Applied with valid service account name in Read users":{"statistic":{"failed":7,"broken":0,"skipped":0,"passed":114,"unknown":0,"total":121},"items":[{"uid":"c9de88e9db1593aa","status":"passed","time":{"start":1660828517000,"stop":1660828525328,"duration":8328}},{"uid":"be96ca22f998aa17","status":"passed","time":{"start":1660807774000,"stop":1660807782840,"duration":8840}},{"uid":"7b3847a44607fddf","status":"passed","time":{"start":1660803998000,"stop":1660804006384,"duration":8384}},{"uid":"665e7b5a9921a90d","status":"passed","time":{"start":1660805737000,"stop":1660805746018,"duration":9018}},{"uid":"cc5a59f293e45fd9","status":"passed","time":{"start":1660803696000,"stop":1660803711026,"duration":15026}},{"uid":"7ae5e5c3303c0261","status":"passed","time":{"start":1660798468000,"stop":1660798476191,"duration":8191}},{"uid":"e39e78b1bb5ab6a0","status":"passed","time":{"start":1660748009000,"stop":1660748017862,"duration":8862}},{"uid":"1357af62c828571","status":"passed","time":{"start":1660747865000,"stop":1660747880678,"duration":15678}},{"uid":"833bf7f5c20c5a89","status":"passed","time":{"start":1660747735000,"stop":1660747744044,"duration":9044}},{"uid":"5782b5f25502ee21","status":"passed","time":{"start":1660747654000,"stop":1660747662737,"duration":8737}},{"uid":"d60f7b322e93d7bd","status":"passed","time":{"start":1660722683000,"stop":1660722697394,"duration":14394}},{"uid":"2a5adfba7b1bbfc7","status":"passed","time":{"start":1660635916000,"stop":1660635925384,"duration":9384}},{"uid":"7d5a7c0762eaf9b7","status":"passed","time":{"start":1660635666000,"stop":1660635674188,"duration":8188}},{"uid":"bd0e3942c0b5bf8d","status":"passed","time":{"start":1660635860000,"stop":1660635869382,"duration":9382}},{"uid":"2aa8bb2a5a6da90c","status":"passed","time":{"start":1660631164000,"stop":1660631172721,"duration":8721}},{"uid":"85146d3c8469b4a4","status":"passed","time":{"start":1660623463000,"stop":1660623472125,"duration":9125}},{"uid":"80d00ffac9476cf7","status":"passed","time":{"start":1660623384000,"stop":1660623392633,"duration":8633}},{"uid":"f0a5cd0d8c46a6fa","status":"passed","time":{"start":1660623360000,"stop":1660623368609,"duration":8609}},{"uid":"de6bdd1ec3025a46","status":"passed","time":{"start":1660452377000,"stop":1660452385244,"duration":8244}},{"uid":"b6e637f911537c5e","status":"passed","time":{"start":1660451870000,"stop":1660451887024,"duration":17024}}]},"Worker Suite:Worker Suite#[It] Attach/Deattach Scenario Attach cluster from slice  when Deattach cluster from slice  should have gateway pod running on attached cluster":{"statistic":{"failed":0,"broken":0,"skipped":27,"passed":3,"unknown":0,"total":30},"items":[{"uid":"f88d39f93ca0fa8f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"d3425fc9e6ffe8d5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"1e84f89bc917f3cf","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"b81c8c769888e8ec","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"555e31240e34c50e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"790d9b3d5f899d50","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"e235180fb4a2ffa7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"363616fddfb55827","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}},{"uid":"96d1b0dd0d5935bf","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659982549000,"stop":1659982549000,"duration":0}},{"uid":"e87263cf898fc369","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659970182000,"stop":1659970182000,"duration":0}},{"uid":"f482b73fcd6ee238","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659944495000,"stop":1659944495000,"duration":0}},{"uid":"549668cbae6f4d84","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659874977000,"stop":1659874977000,"duration":0}},{"uid":"e8d5cbdf96430536","status":"passed","time":{"start":1659496050000,"stop":1659496050092,"duration":92}},{"uid":"1a476fae25139fb","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496083000,"stop":1659496083000,"duration":0}},{"uid":"83c07334069f6936","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"7cfe380a4f59c443","status":"passed","time":{"start":1659450916000,"stop":1659450916111,"duration":111}},{"uid":"24a5b0e24eea21e6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"b431ad78de59c910","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447298000,"stop":1659447298000,"duration":0}},{"uid":"b50c2a230cd2654a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447116000,"stop":1659447116000,"duration":0}},{"uid":"b89ec0b725c3a4fe","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448205000,"stop":1659448205000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Intracluster slice tests Iperf connectivity should onboard deployment in application Namespaces without slice annotations":{"statistic":{"failed":33,"broken":0,"skipped":0,"passed":6,"unknown":0,"total":39},"items":[{"uid":"35807fefa231a726","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000496d20>: {\n        Underlying: <*exec.ExitError | 0xc000930400>{\n            ProcessState: {\n                pid: 7044,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 184573},\n                    Stime: {Sec: 0, Usec: 53833},\n                    Maxrss: 82224,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 7897,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 289,\n                    Nivcsw: 449,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660808797000,"stop":1660808797491,"duration":491}},{"uid":"107e5f38bf8d71f5","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00066d4b8>: {\n        Underlying: <*exec.ExitError | 0xc000917600>{\n            ProcessState: {\n                pid: 7222,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 136603},\n                    Stime: {Sec: 0, Usec: 54641},\n                    Maxrss: 93836,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 10623,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 327,\n                    Nivcsw: 527,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660299182000,"stop":1660299182381,"duration":381}},{"uid":"3ea76e28d6be1481","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0000ce3c0>: {\n        Underlying: <*exec.ExitError | 0xc0008b8540>{\n            ProcessState: {\n                pid: 7200,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 155972},\n                    Stime: {Sec: 0, Usec: 14179},\n                    Maxrss: 79248,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3736,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 279,\n                    Nivcsw: 219,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660295740000,"stop":1660295740341,"duration":341}},{"uid":"a4a174dadea5e72e","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00084c948>: {\n        Underlying: <*exec.ExitError | 0xc0008a19c0>{\n            ProcessState: {\n                pid: 7233,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 178679},\n                    Stime: {Sec: 0, Usec: 45708},\n                    Maxrss: 86816,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 5422,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 527,\n                    Nivcsw: 554,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660293657000,"stop":1660293657468,"duration":468}},{"uid":"eab693cd63959094","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 1\nto equal\n    <int>: 2","time":{"start":1660113628000,"stop":1660113817401,"duration":189401}},{"uid":"60d82e8838da507","status":"passed","time":{"start":1660104898000,"stop":1660104931357,"duration":33357}},{"uid":"a9fe513d1ac9c176","status":"failed","statusDetails":"Unexpected error:\n    <*errors.StatusError | 0xc0008fdcc0>: {\n        ErrStatus: {\n            TypeMeta: {Kind: \"\", APIVersion: \"\"},\n            ListMeta: {\n                SelfLink: \"\",\n                ResourceVersion: \"\",\n                Continue: \"\",\n                RemainingItemCount: nil,\n            },\n            Status: \"Failure\",\n            Message: \"namespaces \\\"appns1\\\" already exists\",\n            Reason: \"AlreadyExists\",\n            Details: {Name: \"appns1\", Group: \"\", Kind: \"namespaces\", UID: \"\", Causes: nil, RetryAfterSeconds: 0},\n            Code: 409,\n        },\n    }\n    namespaces \"appns1\" already exists\noccurred","time":{"start":1660067259000,"stop":1660067265261,"duration":6261}},{"uid":"6510f61f87f6472e","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 1\nto equal\n    <int>: 2","time":{"start":1660047551000,"stop":1660047741110,"duration":190110}},{"uid":"e2af5ba248631f5f","status":"failed","statusDetails":"Unexpected error:\n    <*errors.StatusError | 0xc0009460a0>: {\n        ErrStatus: {\n            TypeMeta: {Kind: \"\", APIVersion: \"\"},\n            ListMeta: {\n                SelfLink: \"\",\n                ResourceVersion: \"\",\n                Continue: \"\",\n                RemainingItemCount: nil,\n            },\n            Status: \"Failure\",\n            Message: \"namespaces \\\"appns1\\\" already exists\",\n            Reason: \"AlreadyExists\",\n            Details: {Name: \"appns1\", Group: \"\", Kind: \"namespaces\", UID: \"\", Causes: nil, RetryAfterSeconds: 0},\n            Code: 409,\n        },\n    }\n    namespaces \"appns1\" already exists\noccurred","time":{"start":1659982549000,"stop":1659982554162,"duration":5162}},{"uid":"76ae164694a735da","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 1\nto equal\n    <int>: 2","time":{"start":1659970182000,"stop":1659970371591,"duration":189591}},{"uid":"46e1425f925dab7","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 1\nto equal\n    <int>: 2","time":{"start":1659944495000,"stop":1659944685253,"duration":190253}},{"uid":"5f16298ecf0441a0","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 1\nto equal\n    <int>: 2","time":{"start":1659874977000,"stop":1659875166054,"duration":189054}},{"uid":"120f76f5576d0cb1","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 1\nto equal\n    <int>: 2","time":{"start":1659496050000,"stop":1659496239207,"duration":189207}},{"uid":"95cf27caac449ab3","status":"failed","statusDetails":"Unexpected error:\n    <*errors.StatusError | 0xc0000b1400>: {\n        ErrStatus: {\n            TypeMeta: {Kind: \"\", APIVersion: \"\"},\n            ListMeta: {\n                SelfLink: \"\",\n                ResourceVersion: \"\",\n                Continue: \"\",\n                RemainingItemCount: nil,\n            },\n            Status: \"Failure\",\n            Message: \"namespaces \\\"appns1\\\" already exists\",\n            Reason: \"AlreadyExists\",\n            Details: {Name: \"appns1\", Group: \"\", Kind: \"namespaces\", UID: \"\", Causes: nil, RetryAfterSeconds: 0},\n            Code: 409,\n        },\n    }\n    namespaces \"appns1\" already exists\noccurred","time":{"start":1659496083000,"stop":1659496089212,"duration":6212}},{"uid":"bbca4b55e0b9f6a0","status":"failed","statusDetails":"Unexpected error:\n    <*errors.StatusError | 0xc000624960>: {\n        ErrStatus: {\n            TypeMeta: {Kind: \"\", APIVersion: \"\"},\n            ListMeta: {\n                SelfLink: \"\",\n                ResourceVersion: \"\",\n                Continue: \"\",\n                RemainingItemCount: nil,\n            },\n            Status: \"Failure\",\n            Message: \"namespaces \\\"appns1\\\" already exists\",\n            Reason: \"AlreadyExists\",\n            Details: {Name: \"appns1\", Group: \"\", Kind: \"namespaces\", UID: \"\", Causes: nil, RetryAfterSeconds: 0},\n            Code: 409,\n        },\n    }\n    namespaces \"appns1\" already exists\noccurred","time":{"start":1659451501000,"stop":1659451507073,"duration":6073}},{"uid":"fd3780e463895c04","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 1\nto equal\n    <int>: 2","time":{"start":1659450916000,"stop":1659451105827,"duration":189827}},{"uid":"f28351a98a3fc559","status":"failed","statusDetails":"Unexpected error:\n    <*errors.StatusError | 0xc0006e2be0>: {\n        ErrStatus: {\n            TypeMeta: {Kind: \"\", APIVersion: \"\"},\n            ListMeta: {\n                SelfLink: \"\",\n                ResourceVersion: \"\",\n                Continue: \"\",\n                RemainingItemCount: nil,\n            },\n            Status: \"Failure\",\n            Message: \"namespaces \\\"appns1\\\" already exists\",\n            Reason: \"AlreadyExists\",\n            Details: {Name: \"appns1\", Group: \"\", Kind: \"namespaces\", UID: \"\", Causes: nil, RetryAfterSeconds: 0},\n            Code: 409,\n        },\n    }\n    namespaces \"appns1\" already exists\noccurred","time":{"start":1659451524000,"stop":1659451530205,"duration":6205}},{"uid":"5fda1d8097581a77","status":"passed","time":{"start":1659447298000,"stop":1659447320509,"duration":22509}},{"uid":"1656176ee08de5a0","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 2","time":{"start":1659447116000,"stop":1659447304906,"duration":188906}},{"uid":"4b15356ae35084d6","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 1\nto equal\n    <int>: 2","time":{"start":1659448205000,"stop":1659448394827,"duration":189827}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting vl3 pod Should restart vl3 pod":{"statistic":{"failed":0,"broken":0,"skipped":39,"passed":0,"unknown":0,"total":39},"items":[{"uid":"12f4fd71e8305024","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"6e9da996a03df39f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"4704c9aba44cacc0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"a0f6d0c2dddea2bb","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"6d8449b7c8a56722","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"6e56ad5d7fd6f3fe","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"ae0e2b6adc6d0521","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"bec01fdc3a6c357a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}},{"uid":"ea7650c4c0ded86","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659982549000,"stop":1659982549000,"duration":0}},{"uid":"349df464973f84cd","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659970182000,"stop":1659970182000,"duration":0}},{"uid":"7aaa82744beeecee","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659944495000,"stop":1659944495000,"duration":0}},{"uid":"b2138379fac28fb","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659874977000,"stop":1659874977000,"duration":0}},{"uid":"250fcb183522d5e3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496050000,"stop":1659496050000,"duration":0}},{"uid":"28b6b3dd4f0cab5e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496083000,"stop":1659496083000,"duration":0}},{"uid":"65eb609cab4f34ed","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"1bcee717a2b13273","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659450916000,"stop":1659450916000,"duration":0}},{"uid":"b77c86cf7d1efae6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"318665a126beea06","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447298000,"stop":1659447298000,"duration":0}},{"uid":"44200cf437485fb0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447116000,"stop":1659447116000,"duration":0}},{"uid":"366748bc86b49eb6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448205000,"stop":1659448205000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice Config Negative Scenario tests Slice Config Negative Scenario Creation Should fail while creating sliceconfig with bandwithguaranteedkbps more than bandwidthceilingkbps in qosprofiledetailsnvalid ":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":39,"unknown":0,"total":39},"items":[{"uid":"1e0fafdd28899c4a","status":"passed","time":{"start":1660808797000,"stop":1660808797001,"duration":1}},{"uid":"8d2bd0b889d12b51","status":"passed","time":{"start":1660299182000,"stop":1660299182001,"duration":1}},{"uid":"4d77327659a45351","status":"passed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"a90b9b894ea2363b","status":"passed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"90a0e6b8d695258b","status":"passed","time":{"start":1660113628000,"stop":1660113628205,"duration":205}},{"uid":"55c390b09b40186d","status":"passed","time":{"start":1660104898000,"stop":1660104898200,"duration":200}},{"uid":"679987ed71045ea4","status":"passed","time":{"start":1660067259000,"stop":1660067259155,"duration":155}},{"uid":"7ed2d94e39a6cea1","status":"passed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}},{"uid":"4b770c5616a8c89a","status":"passed","time":{"start":1659982549000,"stop":1659982549613,"duration":613}},{"uid":"a25d7ba68bb69b85","status":"passed","time":{"start":1659970182000,"stop":1659970182000,"duration":0}},{"uid":"c814df820a004a88","status":"passed","time":{"start":1659944495000,"stop":1659944495000,"duration":0}},{"uid":"77a217487bebb8f8","status":"passed","time":{"start":1659874977000,"stop":1659874977265,"duration":265}},{"uid":"8ace5e607526ac43","status":"passed","time":{"start":1659496050000,"stop":1659496050000,"duration":0}},{"uid":"64b069d63825b8a2","status":"passed","time":{"start":1659496083000,"stop":1659496083000,"duration":0}},{"uid":"31e1bdde97820f9d","status":"passed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"9a6ab991f7ca653a","status":"passed","time":{"start":1659450916000,"stop":1659450916000,"duration":0}},{"uid":"fa9c9380ecc65db","status":"passed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"cf825e5c1afd3c31","status":"passed","time":{"start":1659447298000,"stop":1659447298000,"duration":0}},{"uid":"d9bc8a0fe4ed4154","status":"passed","time":{"start":1659447116000,"stop":1659447116000,"duration":0}},{"uid":"1ff28ec297cd02e4","status":"passed","time":{"start":1659448205000,"stop":1659448205258,"duration":258}}]},"Hub Suite:Hub Suite#[It] Cluster CR tests Cluster CR validation Register worker cluster":{"statistic":{"failed":29,"broken":0,"skipped":0,"passed":95,"unknown":0,"total":124},"items":[{"uid":"c320efe1eefa09bd","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00035a060>: {\n        Underlying: <*exec.ExitError | 0xc000494000>{\n            ProcessState: {\n                pid: 6515,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 3, Usec: 184404},\n                    Stime: {Sec: 0, Usec: 555114},\n                    Maxrss: 87460,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 6284,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14168,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 24074,\n                    Nivcsw: 5836,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.2.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD serviceexports.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD serviceimports.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD slice.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD slicegateways.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 44 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 44 resources with timeout of 2m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.2.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD serviceexports.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD serviceimports.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD slice.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD slicegateways.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 44 resource(s)\n    wait.go:48: [debug] beginning wait for 44 resources with timeout of 2m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:310: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-svc\" Service\n    client.go:310: [debug] Starting delete for \"jaeger\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:310: [debug] Starting delete for \"jaeger\" Deployment\n    client.go:310: [debug] Starting delete for \"prefix-service\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsm-kernel-forwarder\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"networkservices.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkserviceendpoints.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservicemanagers.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-cfg\" MutatingWebhookConfiguration\n    uninstall.go:144: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1660828517000,"stop":1660828649712,"duration":132712}},{"uid":"591dcd2afc2b594","status":"passed","time":{"start":1660807774000,"stop":1660807809206,"duration":35206}},{"uid":"d5da3b64d05b91dc","status":"passed","time":{"start":1660803998000,"stop":1660804055440,"duration":57440}},{"uid":"fe14c9bfab021612","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000190030>: {\n        Underlying: <*exec.ExitError | 0xc00046c000>{\n            ProcessState: {\n                pid: 6600,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 2, Usec: 771806},\n                    Stime: {Sec: 0, Usec: 332785},\n                    Maxrss: 82488,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 10024,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14232,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 22747,\n                    Nivcsw: 5467,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.2.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:165: [debug] Clearing discovery cache\",\n                        \"wait.go:48: [debug] beginning wait for 4 resources with timeout of 1m0s\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 44 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 44 resources with timeout of 2m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.2.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:165: [debug] Clearing discovery cache\n    wait.go:48: [debug] beginning wait for 4 resources with timeout of 1m0s\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 44 resource(s)\n    wait.go:48: [debug] beginning wait for 44 resources with timeout of 2m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:310: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-svc\" Service\n    client.go:310: [debug] Starting delete for \"jaeger\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:310: [debug] Starting delete for \"jaeger\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook\" Deployment\n    client.go:310: [debug] Starting delete for \"prefix-service\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsm-kernel-forwarder\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"networkservices.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkserviceendpoints.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservicemanagers.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-cfg\" MutatingWebhookConfiguration\n    uninstall.go:144: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1660805737000,"stop":1660805869651,"duration":132651}},{"uid":"3ebae4bbc1043e9f","status":"passed","time":{"start":1660803696000,"stop":1660803730375,"duration":34375}},{"uid":"4f3a01562e65f07d","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00086a018>: {\n        Underlying: <*exec.ExitError | 0xc000472000>{\n            ProcessState: {\n                pid: 6135,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 2, Usec: 795506},\n                    Stime: {Sec: 0, Usec: 449851},\n                    Maxrss: 82848,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 16212,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14232,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 24522,\n                    Nivcsw: 6527,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.2.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:165: [debug] Clearing discovery cache\",\n                        \"wait.go:48: [debug] beginning wait for 4 resources with timeout of 1m0s\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 44 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 44 resources with timeout of 2m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operat...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.2.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:165: [debug] Clearing discovery cache\n    wait.go:48: [debug] beginning wait for 4 resources with timeout of 1m0s\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 44 resource(s)\n    wait.go:48: [debug] beginning wait for 44 resources with timeout of 2m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:310: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:310: [debug] Starting delete for \"jaeger\" Service\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-svc\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:310: [debug] Starting delete for \"jaeger\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook\" Deployment\n    client.go:310: [debug] Starting delete for \"prefix-service\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsm-kernel-forwarder\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:310: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"networkservices.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkserviceendpoints.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservicemanagers.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-cfg\" MutatingWebhookConfiguration\n    uninstall.go:144: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1660798468000,"stop":1660798601153,"duration":133153}},{"uid":"96e3abd6c7a68056","status":"passed","time":{"start":1660748009000,"stop":1660748057149,"duration":48149}},{"uid":"f4727769a0e7063f","status":"passed","time":{"start":1660747865000,"stop":1660747922575,"duration":57575}},{"uid":"79e68f9b4c18a08e","status":"passed","time":{"start":1660747735000,"stop":1660747779988,"duration":44988}},{"uid":"651904b83cacbf9b","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000340030>: {\n        Underlying: <*exec.ExitError | 0xc0006a6000>{\n            ProcessState: {\n                pid: 6250,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 2, Usec: 285844},\n                    Stime: {Sec: 0, Usec: 210143},\n                    Maxrss: 82496,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 8604,\n                    Majflt: 7,\n                    Nswap: 0,\n                    Inblock: 408,\n                    Oublock: 14168,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 20722,\n                    Nivcsw: 4328,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.2.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD serviceexports.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD serviceimports.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD slice.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD slicegateways.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 44 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 44 resources with timeout of 2m0s\",\n                        \"ready.go:291: [debug] DaemonSet is not ready: kubeslice-system/nsm-kernel-forwarder. 0 out of 1 expected pods have been scheduled\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:2...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.2.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD serviceexports.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD serviceimports.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD slice.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD slicegateways.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 44 resource(s)\n    wait.go:48: [debug] beginning wait for 44 resources with timeout of 2m0s\n    ready.go:291: [debug] DaemonSet is not ready: kubeslice-system/nsm-kernel-forwarder. 0 out of 1 expected pods have been scheduled\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:310: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-svc\" Service\n    client.go:310: [debug] Starting delete for \"jaeger\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:310: [debug] Starting delete for \"jaeger\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook\" Deployment\n    client.go:310: [debug] Starting delete for \"prefix-service\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsm-kernel-forwarder\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"networkservices.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkserviceendpoints.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservicemanagers.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-cfg\" MutatingWebhookConfiguration\n    uninstall.go:144: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1660747654000,"stop":1660747784837,"duration":130837}},{"uid":"12143aaf7b7b230d","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000618018>: {\n        Underlying: <*exec.ExitError | 0xc00087c000>{\n            ProcessState: {\n                pid: 6764,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 2, Usec: 753945},\n                    Stime: {Sec: 0, Usec: 381195},\n                    Maxrss: 88268,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3230,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14168,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 23468,\n                    Nivcsw: 5841,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.2.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD serviceexports.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD serviceimports.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD slice.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD slicegateways.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 44 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 44 resources with timeout of 2m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.2.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD serviceexports.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD serviceimports.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD slice.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD slicegateways.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 44 resource(s)\n    wait.go:48: [debug] beginning wait for 44 resources with timeout of 2m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:310: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-svc\" Service\n    client.go:310: [debug] Starting delete for \"jaeger\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:310: [debug] Starting delete for \"jaeger\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook\" Deployment\n    client.go:310: [debug] Starting delete for \"prefix-service\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsm-kernel-forwarder\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"networkserviceendpoints.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservices.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservicemanagers.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-cfg\" MutatingWebhookConfiguration\n    uninstall.go:144: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1660722683000,"stop":1660722814925,"duration":131925}},{"uid":"f934239500edf349","status":"passed","time":{"start":1660635916000,"stop":1660635971111,"duration":55111}},{"uid":"a4daf5503b3c6548","status":"passed","time":{"start":1660635666000,"stop":1660635701000,"duration":35000}},{"uid":"9b0ccd3f92b677c5","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000624018>: {\n        Underlying: <*exec.ExitError | 0xc0006b0000>{\n            ProcessState: {\n                pid: 6109,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 2, Usec: 939806},\n                    Stime: {Sec: 0, Usec: 599538},\n                    Maxrss: 89004,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 15122,\n                    Majflt: 4,\n                    Nswap: 0,\n                    Inblock: 744,\n                    Oublock: 13768,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 24190,\n                    Nivcsw: 6613,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.2.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:165: [debug] Clearing discovery cache\",\n                        \"wait.go:48: [debug] beginning wait for 4 resources with timeout of 1m0s\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 44 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 44 resources with timeout of 2m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-sys...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.2.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:165: [debug] Clearing discovery cache\n    wait.go:48: [debug] beginning wait for 4 resources with timeout of 1m0s\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 44 resource(s)\n    wait.go:48: [debug] beginning wait for 44 resources with timeout of 2m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:310: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-svc\" Service\n    client.go:310: [debug] Starting delete for \"jaeger\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook\" Deployment\n    client.go:310: [debug] Starting delete for \"jaeger\" Deployment\n    client.go:310: [debug] Starting delete for \"prefix-service\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsm-kernel-forwarder\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:310: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"networkserviceendpoints.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservicemanagers.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservices.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-cfg\" MutatingWebhookConfiguration\n    uninstall.go:144: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1660635860000,"stop":1660635993492,"duration":133492}},{"uid":"b55e38bd5f85f59a","status":"passed","time":{"start":1660631164000,"stop":1660631217545,"duration":53545}},{"uid":"b0ea4adba97db970","status":"passed","time":{"start":1660623463000,"stop":1660623499063,"duration":36063}},{"uid":"af91b190b6263fe8","status":"passed","time":{"start":1660623384000,"stop":1660623430351,"duration":46351}},{"uid":"b94d41cdcf619ad","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000130048>: {\n        Underlying: <*exec.ExitError | 0xc00079c000>{\n            ProcessState: {\n                pid: 6555,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 2, Usec: 121406},\n                    Stime: {Sec: 0, Usec: 224213},\n                    Maxrss: 90540,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4288,\n                    Majflt: 2,\n                    Nswap: 0,\n                    Inblock: 312,\n                    Oublock: 14168,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 20995,\n                    Nivcsw: 4496,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.2.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD serviceexports.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD serviceimports.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD slice.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD slicegateways.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 44 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 44 resources with timeout of 2m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [deb...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.2.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD serviceexports.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD serviceimports.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD slice.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD slicegateways.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 44 resource(s)\n    wait.go:48: [debug] beginning wait for 44 resources with timeout of 2m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:310: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"jaeger\" Service\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-svc\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook\" Deployment\n    client.go:310: [debug] Starting delete for \"jaeger\" Deployment\n    client.go:310: [debug] Starting delete for \"prefix-service\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-kernel-forwarder\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"networkservices.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkserviceendpoints.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservicemanagers.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-cfg\" MutatingWebhookConfiguration\n    uninstall.go:144: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1660623360000,"stop":1660623490662,"duration":130662}},{"uid":"2e51e7839fb673b0","status":"passed","time":{"start":1660452377000,"stop":1660452412568,"duration":35568}},{"uid":"6f43180ffe03165","status":"passed","time":{"start":1660451870000,"stop":1660451903989,"duration":33989}}]},"Worker Suite:Worker Suite#[It] Slice Config Negative Scenario tests Slice Config Negative Scenario Creation Should fail while creating sliceconfig with invalid values on slice name":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":39,"unknown":0,"total":39},"items":[{"uid":"6003aa082d9a15e2","status":"passed","time":{"start":1660808797000,"stop":1660808797416,"duration":416}},{"uid":"a9c35014c0d5286a","status":"passed","time":{"start":1660299182000,"stop":1660299182377,"duration":377}},{"uid":"52382223a86a0c0","status":"passed","time":{"start":1660295740000,"stop":1660295740342,"duration":342}},{"uid":"7b683e9b2dddfecb","status":"passed","time":{"start":1660293657000,"stop":1660293657456,"duration":456}},{"uid":"264aa8647beb9e93","status":"passed","time":{"start":1660113628000,"stop":1660113628773,"duration":773}},{"uid":"6e4fe1fbc6893916","status":"passed","time":{"start":1660104898000,"stop":1660104898923,"duration":923}},{"uid":"2a08273bcd85497f","status":"passed","time":{"start":1660067259000,"stop":1660067260202,"duration":1202}},{"uid":"f85ca57d0077a3c9","status":"passed","time":{"start":1660047551000,"stop":1660047551518,"duration":518}},{"uid":"3b64ba1098de671e","status":"passed","time":{"start":1659982549000,"stop":1659982549954,"duration":954}},{"uid":"3ed56ddde224430f","status":"passed","time":{"start":1659970182000,"stop":1659970183255,"duration":1255}},{"uid":"a93884af4c759219","status":"passed","time":{"start":1659944495000,"stop":1659944495527,"duration":527}},{"uid":"a154922038bc1f89","status":"passed","time":{"start":1659874977000,"stop":1659874977593,"duration":593}},{"uid":"8560c8faeae5056f","status":"passed","time":{"start":1659496050000,"stop":1659496050331,"duration":331}},{"uid":"e49752667e00b771","status":"passed","time":{"start":1659496083000,"stop":1659496083650,"duration":650}},{"uid":"42d43a09981e0141","status":"passed","time":{"start":1659451501000,"stop":1659451501930,"duration":930}},{"uid":"9d9ef9e3c0821f0a","status":"passed","time":{"start":1659450916000,"stop":1659450916846,"duration":846}},{"uid":"f7288fb1dfb03e1d","status":"passed","time":{"start":1659451524000,"stop":1659451525131,"duration":1131}},{"uid":"1858bdfdd3b593cc","status":"passed","time":{"start":1659447298000,"stop":1659447298785,"duration":785}},{"uid":"f0886a930a479a86","status":"passed","time":{"start":1659447116000,"stop":1659447117263,"duration":1263}},{"uid":"ecb322719b80293b","status":"passed","time":{"start":1659448205000,"stop":1659448205477,"duration":477}}]}}