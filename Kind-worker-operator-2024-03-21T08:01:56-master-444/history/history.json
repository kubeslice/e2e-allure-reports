{"Worker Suite:Worker Suite#[It] Slice Config Negative Scenario tests Slice Config Negative Scenario Update Should fail while updating sliceconfig with invalid clusters":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"979e5ea97b49289c","status":"passed","time":{"start":1709719453000,"stop":1709719453583,"duration":583}}]},"Controller Suite:Controller Suite#[It] Slice resource quota Tests ResourceQuota creation tests Create ResourceQuota fail test - negative pod cound":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":1,"unknown":0,"total":2},"items":[{"uid":"216f8708df157df7","status":"passed","time":{"start":1693899867000,"stop":1693899870199,"duration":3199}},{"uid":"93fae744327fffa8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting worker-operator pod Check ping between iperf-server and iperf-client after worker-operator pod restart":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":0,"unknown":0,"total":1},"items":[{"uid":"a4a4dcaccb3a370e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1709719453000,"stop":1709719453000,"duration":0}}]},"Controller Suite:Controller Suite#[It] Validate License Tests License Validate tests should fail CRUD in case someone tampered license.key":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":1,"unknown":0,"total":2},"items":[{"uid":"75ceeca2827c6406","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693899867000,"stop":1693899867000,"duration":0}},{"uid":"377f8a0b70fb5617","status":"passed","time":{"start":1693828311000,"stop":1693828315242,"duration":4242}}]},"Controller Suite:Controller Suite#[It] Slice RBAC Tests SliceRoleTemplate Tests Create SliceRoleTempalate successful test: Creating SliceRoleTempalate read-only-role":{"statistic":{"failed":1,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":2},"items":[{"uid":"c769c51fecb70afd","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000017b90>: {\n        Underlying: <*exec.ExitError | 0xc00072f6e0>{\n            ProcessState: {\n                pid: 6892,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 438725},\n                    Stime: {Sec: 0, Usec: 298954},\n                    Maxrss: 112868,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 6980,\n                    Majflt: 19,\n                    Nswap: 0,\n                    Inblock: 3280,\n                    Oublock: 20624,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 1653,\n                    Nivcsw: 3826,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Slice%20RBAC%20Tests%20SliceRoleTemplate%20Tests%20Create%20SliceRoleTempalate%20successful%20test:%20Creating%20SliceRoleTempalate%20read-only-role3754015361\\\": Internal error occurred: failed calling webhook \\\"msliceconfig.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-sliceconfig?timeout=10s\\\": dial tcp 10.96.230.189:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Slice%20RBAC%20Tests%20SliceRoleTemplate%20Tests%20Create%20SliceRoleTempalate%20successful%20test:%20Creating%20SliceRoleTempalate%20read-only-role3754015361\\\": Internal error occurred: failed calling webhook \\\"msliceconfig.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-sliceconfig?timeout=10s\\\": dial tcp 10.96.230.189:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Slice%20RBAC%20Tests%20SliceRoleTemplate%20Tests%20Create%20SliceRoleTempalate%20successful%20test:%20Creating%20SliceRoleTempalate%20read-only-role3754015361\\\": Internal error occurred: failed calling webhook \\\"msliceconfig.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-sliceconfig?timeout=10s\\\": dial tcp 10.96.230.189:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Slice%20RBAC%20Tests%20SliceRoleTemplate%20Tests%20Create%20SliceRoleTempalate%20successful%20test:%20Creating%20SliceRoleTempalate%20read-only-role3754015361\\\": Internal error occurred: failed calling webhook \\\"msliceconfig.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-sliceconfig?timeout=10s\\\": dial tcp 10.96.230.189:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Slice%20RBAC%20Tests%20SliceRoleTemplate%20Tests%20Create%20SliceRoleTempalate%20successful%20test:%20Creating%20SliceRoleTempalate%20read-only-role3754015361\": Internal error occurred: failed calling webhook \"msliceconfig.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-sliceconfig?timeout=10s\": dial tcp 10.96.230.189:443: connect: connection refused\noccurred","time":{"start":1693899867000,"stop":1693899877332,"duration":10332}},{"uid":"3c9f0dd45accf71e","status":"passed","time":{"start":1693828311000,"stop":1693828336247,"duration":25247}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting vl3 pod Check ping between iperf-server and iperf-client after vl3 pod restart":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":0,"unknown":0,"total":1},"items":[{"uid":"40da546e0cfc7e50","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1709719453000,"stop":1709719453000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice Config Events Test Events get recorded for service export creation and deletion Events get recorded for service export deletion Validating events for worker service imports deletion":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"217c0b3969c8c039","status":"passed","time":{"start":1709719453000,"stop":1709719454860,"duration":1860}}]},"Intracluster Suite:Intracluster Suite#[It] Intracluster slice tests Iperf connectivity should have vl3 router running":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"b63200323e7576b3","status":"passed","time":{"start":1688025155000,"stop":1688025155006,"duration":6}}]},"Worker Suite:Worker Suite#[It] Hub netpol scenarios: Slice Configs Netpol scenarios Should create slice for valid namespace and valid clusters in applicationNamespaces of sliceconfigs manifest":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"ee95cd4f85b5ef1a","status":"passed","time":{"start":1709719453000,"stop":1709719455456,"duration":2456}}]},"SliceHealth Suite:SliceHealth Suite#[It] Testing Slice Health feature >  Verify various properties of SliceHealth object in worker slice config spec >  Should have slice egress unavailable when egress is unavailable":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"8f78dd72aeba50fa","status":"passed","time":{"start":1709723127000,"stop":1709723246511,"duration":119511}}]},"Controller Suite:Controller Suite#[It] Slice resource quota Tests ResourceQuota creation tests Create ResourceQuota fail test - namespace.DefaultRequestPerContainer.CPU > namespace.Limit.CPU":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":1,"unknown":0,"total":2},"items":[{"uid":"4d18ddfc9fd281ee","status":"passed","time":{"start":1693899867000,"stop":1693899870836,"duration":3836}},{"uid":"65f306f3c0addf23","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice Config Negative Scenario tests Slice Config Negative Scenario Creation Should fail while creating sliceconfig with invalid clusters":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"9757a7eb841e8754","status":"passed","time":{"start":1709719453000,"stop":1709719453261,"duration":261}}]},"Controller Suite:Controller Suite#[It] Slice RBAC Tests RBAC Common Tests WorkerSliceRoleBinding generation test: WorkerSliceRoleBinding with kubernetes role should be generated automatically when forcefully deleted":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":0,"unknown":0,"total":2},"items":[{"uid":"78d89d2f2e80e759","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693899867000,"stop":1693899867000,"duration":0}},{"uid":"e8b402d760792414","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Intracluster Suite:Intracluster Suite#[BeforeSuite]":{"statistic":{"failed":3,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":4},"items":[{"uid":"ecf0173f28527b90","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000e858>: {\n        Underlying: <*exec.ExitError | 0xc000078e20>{\n            ProcessState: {\n                pid: 6426,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 107212},\n                    Stime: {Sec: 0, Usec: 28864},\n                    Maxrss: 50372,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2141,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 176,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 652,\n                    Nivcsw: 389,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:200: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:217: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-1.1.2.tgz\",\n                        \"\",\n                        \"Error: INSTALLATION FAILED: cannot re-use a name that is still in use\",\n                        \"helm.go:84: [debug] cannot re-use a name that is still in use\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).availableName\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:517\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:227\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:287\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:145\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.6.1/command.go:916\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.6.1/command.go:1044\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.6.1/command.go:968\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:250\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1598\",\n                        \"INSTALLATION FAILED\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:147\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.6.1/command.go:916\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.6.1/command.go:1044\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.6.1/command.go:968\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:250\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1598\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is in...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:200: [debug] Original chart version: \"\"\n    install.go:217: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-1.1.2.tgz\n    \n    Error: INSTALLATION FAILED: cannot re-use a name that is still in use\n    helm.go:84: [debug] cannot re-use a name that is still in use\n    helm.sh/helm/v3/pkg/action.(*Install).availableName\n    \thelm.sh/helm/v3/pkg/action/install.go:517\n    helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\n    \thelm.sh/helm/v3/pkg/action/install.go:227\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:287\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:145\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:916\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:1044\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:968\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1598\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:147\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:916\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:1044\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:968\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1598\noccurred","time":{"start":1695806741000,"stop":1695806741653,"duration":653}},{"uid":"c229893beaa509e8","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004b1350>: {\n        Underlying: <*exec.ExitError | 0xc00013a2a0>{\n            ProcessState: {\n                pid: 6358,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 97224},\n                    Stime: {Sec: 0, Usec: 28357},\n                    Maxrss: 50484,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2337,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 168,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 541,\n                    Nivcsw: 344,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:200: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:217: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-1.2.0.tgz\",\n                        \"\",\n                        \"Error: INSTALLATION FAILED: cannot re-use a name that is still in use\",\n                        \"helm.go:84: [debug] cannot re-use a name that is still in use\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).availableName\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:512\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:222\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:286\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:145\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.6.1/command.go:916\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.6.1/command.go:1044\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.6.1/command.go:968\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:250\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1598\",\n                        \"INSTALLATION FAILED\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:147\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.6.1/command.go:916\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.6.1/command.go:1044\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.6.1/command.go:968\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:250\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1598\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is ins...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:200: [debug] Original chart version: \"\"\n    install.go:217: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-1.2.0.tgz\n    \n    Error: INSTALLATION FAILED: cannot re-use a name that is still in use\n    helm.go:84: [debug] cannot re-use a name that is still in use\n    helm.sh/helm/v3/pkg/action.(*Install).availableName\n    \thelm.sh/helm/v3/pkg/action/install.go:512\n    helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\n    \thelm.sh/helm/v3/pkg/action/install.go:222\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:286\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:145\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:916\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:1044\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:968\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1598\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:147\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:916\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:1044\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:968\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1598\noccurred","time":{"start":1689751062000,"stop":1689751062403,"duration":403}},{"uid":"8cd686f2467a5451","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000f098>: {\n        Underlying: <*exec.ExitError | 0xc00049e880>{\n            ProcessState: {\n                pid: 6336,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 103164},\n                    Stime: {Sec: 0, Usec: 19104},\n                    Maxrss: 48060,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2379,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 168,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 609,\n                    Nivcsw: 335,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:200: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:217: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-1.2.0.tgz\",\n                        \"\",\n                        \"Error: INSTALLATION FAILED: cannot re-use a name that is still in use\",\n                        \"helm.go:84: [debug] cannot re-use a name that is still in use\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).availableName\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:512\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:222\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:286\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:145\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.6.1/command.go:916\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.6.1/command.go:1044\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.6.1/command.go:968\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:250\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1598\",\n                        \"INSTALLATION FAILED\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:147\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.6.1/command.go:916\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.6.1/command.go:1044\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.6.1/command.go:968\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:250\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1598\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is in...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:200: [debug] Original chart version: \"\"\n    install.go:217: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-1.2.0.tgz\n    \n    Error: INSTALLATION FAILED: cannot re-use a name that is still in use\n    helm.go:84: [debug] cannot re-use a name that is still in use\n    helm.sh/helm/v3/pkg/action.(*Install).availableName\n    \thelm.sh/helm/v3/pkg/action/install.go:512\n    helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\n    \thelm.sh/helm/v3/pkg/action/install.go:222\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:286\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:145\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:916\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:1044\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:968\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1598\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:147\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:916\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:1044\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:968\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1598\noccurred","time":{"start":1688627854000,"stop":1688627854455,"duration":455}},{"uid":"7e4c5bfa62c44c00","status":"passed","time":{"start":1688025155000,"stop":1688025328626,"duration":173626}}]},"Controller Suite:Controller Suite#[It] Events Test Events get recorded for slice resource quota deletion Validating events for worker slice resource quotas deletion":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":0,"unknown":0,"total":2},"items":[{"uid":"df7ceb896904c81d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693899867000,"stop":1693899867000,"duration":0}},{"uid":"3984e10358cc890a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting forwarder-kernel pod Should restart forwarder-kernel pod":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":0,"unknown":0,"total":1},"items":[{"uid":"4078bcfad2e81d1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1709719453000,"stop":1709719453000,"duration":0}}]},"Controller Suite:Controller Suite#[It] Slice RBAC Tests RBAC Common Tests WorkerSliceRoleBinding generation test: Creation of SliceRoleBinding with custom roles should generate WorkerSliceRoleBinding":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":0,"unknown":0,"total":2},"items":[{"uid":"b4abde88371f68eb","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693899867000,"stop":1693899867000,"duration":0}},{"uid":"55e085e80e88904c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests Iperf App connectivity Verify Connectivity between iperf-server and iperf-client":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":0,"unknown":0,"total":1},"items":[{"uid":"6f7b9835edffa6bf","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1709719453000,"stop":1709719453000,"duration":0}}]},"Controller Suite:Controller Suite#[It] E2E tests for license job > when license config is tampered with should eventually record appropiate event":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":1,"unknown":0,"total":2},"items":[{"uid":"476a6f98234b25f9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693899867000,"stop":1693899867000,"duration":0}},{"uid":"45a80bec720e7d75","status":"passed","time":{"start":1693828311000,"stop":1693828324174,"duration":13174}}]},"Controller Suite:Controller Suite#[It] Validate License Tests License Validate tests should update the license to expired but grace period not expired - should not fail CRUD":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":1,"unknown":0,"total":2},"items":[{"uid":"7eea43739e2a6575","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693899867000,"stop":1693899867000,"duration":0}},{"uid":"70ff515933d500b5","status":"passed","time":{"start":1693828311000,"stop":1693828318439,"duration":7439}}]},"Controller Suite:Controller Suite#[It] Events Test Events get recorded for slice resource quota deletion Validating events for slice resource quota recreation after forceful deletion":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":0,"unknown":0,"total":2},"items":[{"uid":"eab793f1a42df61a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693899867000,"stop":1693899867000,"duration":0}},{"uid":"3e796b363568bd48","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice Config Events Test Events get recorded for slice config events on worker Events get recorded for slice config update on worker Validating events for slice configs updates":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"56ef483f08491f8a","status":"passed","time":{"start":1709719453000,"stop":1709719455715,"duration":2715}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests Iperf App connectivity should have gateway pod running":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"dbc5c649791e6d7f","status":"passed","time":{"start":1709719453000,"stop":1709719499177,"duration":46177}}]},"VPNKeyRotation Suite:VPNKeyRotation Suite#[AfterSuite]":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":7,"unknown":0,"total":7},"items":[{"uid":"161311a3b30af757","status":"passed","time":{"start":1711007882000,"stop":1711007882916,"duration":916}},{"uid":"15b77ea5f125fd75","status":"passed","time":{"start":1710850777000,"stop":1710850778028,"duration":1028}},{"uid":"7498d328d0453482","status":"passed","time":{"start":1710825517000,"stop":1710825517395,"duration":395}},{"uid":"998c576951593139","status":"passed","time":{"start":1710224686000,"stop":1710225935974,"duration":1249974}},{"uid":"cb599bbf2a58c28c","status":"passed","time":{"start":1709883716000,"stop":1709884967758,"duration":1251758}},{"uid":"e82f766275c1cf2a","status":"passed","time":{"start":1709717686000,"stop":1709718935790,"duration":1249790}},{"uid":"c91b0023299f314","status":"passed","time":{"start":1695807344000,"stop":1695807644811,"duration":300811}}]},"VPNKeyRotation Suite:VPNKeyRotation Suite#[BeforeSuite]":{"statistic":{"failed":4,"broken":0,"skipped":0,"passed":3,"unknown":0,"total":7},"items":[{"uid":"78594aa8b88d2f7d","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000e4e0>: {\n        Underlying: <*exec.ExitError | 0xc0005181e0>{\n            ProcessState: {\n                pid: 7822,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 124009},\n                    Stime: {Sec: 0, Usec: 15501},\n                    Maxrss: 47524,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2252,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 176,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 636,\n                    Nivcsw: 254,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:214: [debug] Original chart version: \\\"\\\"\",\n                        \"Error: INSTALLATION FAILED: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/nexus/kubeslice-controller-1.2.1.tgz : 404 Not Found\",\n                        \"helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/nexus/kubeslice-controller-1.2.1.tgz : 404 Not Found\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\",\n                        \"helm.sh/helm/v3/pkg/downloader.(*ChartDownloader).DownloadTo\",\n                        \"\\thelm.sh/helm/v3/pkg/downloader/chart_downloader.go:100\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:805\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:226\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:152\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.8.0/command.go:983\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.8.0/command.go:1115\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.8.0/command.go:1039\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:267\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1650\",\n                        \"INSTALLATION FAILED\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:154\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.8.0/command.go:983\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.8.0/command.go:1115\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.8.0/command.go:1039\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"ru...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:214: [debug] Original chart version: \"\"\n    Error: INSTALLATION FAILED: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/nexus/kubeslice-controller-1.2.1.tgz : 404 Not Found\n    helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/nexus/kubeslice-controller-1.2.1.tgz : 404 Not Found\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\n    helm.sh/helm/v3/pkg/downloader.(*ChartDownloader).DownloadTo\n    \thelm.sh/helm/v3/pkg/downloader/chart_downloader.go:100\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:805\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:226\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:152\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:983\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:1115\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:1039\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:267\n    runtime.goexit\n    \truntime/asm_amd64.s:1650\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:154\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:983\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:1115\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:1039\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:267\n    runtime.goexit\n    \truntime/asm_amd64.s:1650\noccurred","time":{"start":1711007882000,"stop":1711007882378,"duration":378}},{"uid":"9e72947f760d5613","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000e0c0>: {\n        Underlying: <*exec.ExitError | 0xc000440040>{\n            ProcessState: {\n                pid: 7858,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 100005},\n                    Stime: {Sec: 0, Usec: 19231},\n                    Maxrss: 47256,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2007,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 176,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 657,\n                    Nivcsw: 449,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:214: [debug] Original chart version: \\\"\\\"\",\n                        \"Error: INSTALLATION FAILED: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/nexus/kubeslice-controller-1.2.1.tgz : 404 Not Found\",\n                        \"helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/nexus/kubeslice-controller-1.2.1.tgz : 404 Not Found\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\",\n                        \"helm.sh/helm/v3/pkg/downloader.(*ChartDownloader).DownloadTo\",\n                        \"\\thelm.sh/helm/v3/pkg/downloader/chart_downloader.go:100\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:805\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:226\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:152\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.8.0/command.go:983\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.8.0/command.go:1115\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.8.0/command.go:1039\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:267\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1650\",\n                        \"INSTALLATION FAILED\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:154\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.8.0/command.go:983\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.8.0/command.go:1115\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.8.0/command.go:1039\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"ru...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:214: [debug] Original chart version: \"\"\n    Error: INSTALLATION FAILED: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/nexus/kubeslice-controller-1.2.1.tgz : 404 Not Found\n    helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/nexus/kubeslice-controller-1.2.1.tgz : 404 Not Found\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\n    helm.sh/helm/v3/pkg/downloader.(*ChartDownloader).DownloadTo\n    \thelm.sh/helm/v3/pkg/downloader/chart_downloader.go:100\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:805\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:226\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:152\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:983\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:1115\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:1039\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:267\n    runtime.goexit\n    \truntime/asm_amd64.s:1650\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:154\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:983\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:1115\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:1039\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:267\n    runtime.goexit\n    \truntime/asm_amd64.s:1650\noccurred","time":{"start":1710850777000,"stop":1710850777456,"duration":456}},{"uid":"9fc8ee6488bacbaf","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000128ff0>: {\n        Underlying: <*exec.ExitError | 0xc00014ffa0>{\n            ProcessState: {\n                pid: 7794,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 73756},\n                    Stime: {Sec: 0, Usec: 14048},\n                    Maxrss: 46256,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 1739,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 176,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 521,\n                    Nivcsw: 121,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:214: [debug] Original chart version: \\\"\\\"\",\n                        \"Error: INSTALLATION FAILED: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/nexus/kubeslice-controller-1.2.1.tgz : 404 Not Found\",\n                        \"helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/nexus/kubeslice-controller-1.2.1.tgz : 404 Not Found\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\",\n                        \"helm.sh/helm/v3/pkg/downloader.(*ChartDownloader).DownloadTo\",\n                        \"\\thelm.sh/helm/v3/pkg/downloader/chart_downloader.go:100\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:805\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:226\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:152\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.8.0/command.go:983\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.8.0/command.go:1115\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.8.0/command.go:1039\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:267\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1650\",\n                        \"INSTALLATION FAILED\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:154\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.8.0/command.go:983\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.8.0/command.go:1115\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.8.0/command.go:1039\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"run...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:214: [debug] Original chart version: \"\"\n    Error: INSTALLATION FAILED: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/nexus/kubeslice-controller-1.2.1.tgz : 404 Not Found\n    helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/nexus/kubeslice-controller-1.2.1.tgz : 404 Not Found\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\n    helm.sh/helm/v3/pkg/downloader.(*ChartDownloader).DownloadTo\n    \thelm.sh/helm/v3/pkg/downloader/chart_downloader.go:100\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:805\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:226\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:152\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:983\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:1115\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:1039\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:267\n    runtime.goexit\n    \truntime/asm_amd64.s:1650\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:154\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:983\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:1115\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:1039\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:267\n    runtime.goexit\n    \truntime/asm_amd64.s:1650\noccurred","time":{"start":1710825517000,"stop":1710825517137,"duration":137}},{"uid":"151c5871a74a531a","status":"passed","time":{"start":1710224686000,"stop":1710224987245,"duration":301245}},{"uid":"36a61658b999a893","status":"passed","time":{"start":1709883716000,"stop":1709883932044,"duration":216044}},{"uid":"6a8aa070dc1b4a6a","status":"passed","time":{"start":1709717686000,"stop":1709718008642,"duration":322642}},{"uid":"9b41f17e3ba2b1de","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000e240>: {\n        Underlying: <*exec.ExitError | 0xc0004a0200>{\n            ProcessState: {\n                pid: 6528,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 121102},\n                    Stime: {Sec: 0, Usec: 20879},\n                    Maxrss: 48372,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2143,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 176,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 550,\n                    Nivcsw: 355,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:200: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:217: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-1.1.2.tgz\",\n                        \"\",\n                        \"Error: INSTALLATION FAILED: cannot re-use a name that is still in use\",\n                        \"helm.go:84: [debug] cannot re-use a name that is still in use\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).availableName\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:517\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:227\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:287\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:145\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.6.1/command.go:916\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.6.1/command.go:1044\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.6.1/command.go:968\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:250\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1598\",\n                        \"INSTALLATION FAILED\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:147\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.6.1/command.go:916\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.6.1/command.go:1044\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.6.1/command.go:968\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:250\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1598\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is in...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:200: [debug] Original chart version: \"\"\n    install.go:217: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-1.1.2.tgz\n    \n    Error: INSTALLATION FAILED: cannot re-use a name that is still in use\n    helm.go:84: [debug] cannot re-use a name that is still in use\n    helm.sh/helm/v3/pkg/action.(*Install).availableName\n    \thelm.sh/helm/v3/pkg/action/install.go:517\n    helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\n    \thelm.sh/helm/v3/pkg/action/install.go:227\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:287\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:145\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:916\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:1044\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:968\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1598\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:147\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:916\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:1044\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:968\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1598\noccurred","time":{"start":1695807344000,"stop":1695807344661,"duration":661}}]},"Worker Suite:Worker Suite#[It] Slice Config Events Test Events get recorded for slice config events on worker Events get recorded for slice config delete on worker Validating events for worker slice configs deletion":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"1ce3a4ecc3e96580","status":"passed","time":{"start":1709719453000,"stop":1709719478435,"duration":25435}}]},"Worker Suite:Worker Suite#[It] mutation webhook tests Testing mutation webhook functionality after namespace onboard Should mutate deployments with labels and annotations with nsm sidecars injected":{"statistic":{"failed":1,"broken":0,"skipped":0,"passed":0,"unknown":0,"total":1},"items":[{"uid":"bbd1a9c8225692ed","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000128528>: {\n        Underlying: <*exec.ExitError | 0xc000c8e040>{\n            ProcessState: {\n                pid: 10858,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 264528},\n                    Stime: {Sec: 0, Usec: 56112},\n                    Maxrss: 115176,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 18013,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 19200,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 695,\n                    Nivcsw: 1766,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/e2e/assets/webhook/deployment.yaml\\\": Internal error occurred: failed calling webhook \\\"webhook.kubeslice.io\\\": failed to call webhook: Post \\\"https://kubeslice-webhook-service.kubeslice-system.svc:443/mutate-webhook?timeout=10s\\\": EOF\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/e2e/assets/webhook/deployment.yaml\\\": Internal error occurred: failed calling webhook \\\"webhook.kubeslice.io\\\": failed to call webhook: Post \\\"https://kubeslice-webhook-service.kubeslice-system.svc:443/mutate-webhook?timeout=10s\\\": EOF\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/e2e/assets/webhook/deployment.yaml\\\": Internal error occurred: failed calling webhook \\\"webhook.kubeslice.io\\\": failed to call webhook: Post \\\"https://kubeslice-webhook-service.kubeslice-system.svc:443/mutate-webhook?timeout=10s\\\": EOF\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/e2e/assets/webhook/deployment.yaml\\\": Internal error occurred: failed calling webhook \\\"webhook.kubeslice.io\\\": failed to call webhook: Post \\\"https://kubeslice-webhook-service.kubeslice-system.svc:443/mutate-webhook?timeout=10s\\\": EOF\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/e2e/assets/webhook/deployment.yaml\": Internal error occurred: failed calling webhook \"webhook.kubeslice.io\": failed to call webhook: Post \"https://kubeslice-webhook-service.kubeslice-system.svc:443/mutate-webhook?timeout=10s\": EOF\noccurred","time":{"start":1709719453000,"stop":1709719491226,"duration":38226}}]},"Worker Suite:Worker Suite#[It] Slice Config Negative Scenario tests Slice Config Negative Scenario Creation Should fail while creating sliceconfig with with invalid DSCP class in qosprofiledetails":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"de3236319e0e6e4e","status":"passed","time":{"start":1709719453000,"stop":1709719453197,"duration":197}}]},"SliceHealth Suite:SliceHealth Suite#[It] Slice Health Check tests Test Slice Health Check Should have slice health status Normal in controller slice CR":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":0,"unknown":0,"total":1},"items":[{"uid":"75fbb8615c4d5c73","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1688026084000,"stop":1688026084000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice creation scenarios: when slice red is installed install slice blue workers should have vl3 routers from both slices":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"156f52ac53d5db1a","status":"passed","time":{"start":1709719453000,"stop":1709719455089,"duration":2089}}]},"Controller Suite:Controller Suite#[It] E2E tests for license job > when controller is installed with default options should record appropiate event":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":2,"unknown":0,"total":2},"items":[{"uid":"2aa8be32e1eb42be","status":"passed","time":{"start":1693899867000,"stop":1693899871445,"duration":4445}},{"uid":"4cd58a812330d28d","status":"passed","time":{"start":1693828311000,"stop":1693828320963,"duration":9963}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Update Should fail when Project is applied with service account name as blank in Write users":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"7e12b059c021b5b0","status":"passed","time":{"start":1688022039000,"stop":1688022043894,"duration":4894}}]},"Controller Suite:Controller Suite#[It] Events Test Events get recorded for slice role binding creation and deletion Events get recorded for slice role binding deletion Validating events for worker slice role bindings deletion":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":0,"unknown":0,"total":2},"items":[{"uid":"601bb2f3280b8d9a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693899867000,"stop":1693899867000,"duration":0}},{"uid":"ab4ef2e67acf0d36","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Deletion Should fail when deleting a project that does not exist":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"c9838ac3f5a19546","status":"passed","time":{"start":1688022039000,"stop":1688022046764,"duration":7764}}]},"Controller Suite:Controller Suite#[It] Events Test Events get recorded for slice node affinity creation and deletion Events get recorded for slice node affinity creation Validating events for worker slice node affinities creation":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":0,"unknown":0,"total":2},"items":[{"uid":"122e6fdbc84669bf","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693899867000,"stop":1693899867000,"duration":0}},{"uid":"a909bb2ef158f6ec","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Hub netpol scenarios: Slice Configs Netpol scenarios Should successfully deploy slice for valid namespace and valid clusters in allowedNamespaces of sliceconfigs manifest":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"79e964e32ac000d3","status":"passed","time":{"start":1709719453000,"stop":1709719454228,"duration":1228}}]},"Controller Suite:Controller Suite#[It] Validate License Tests License Validate tests Should have all the fields populated in secret":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":2,"unknown":0,"total":2},"items":[{"uid":"ce500dc129340c1d","status":"passed","time":{"start":1693899867000,"stop":1693899868159,"duration":1159}},{"uid":"ba9fe0c6c4c2cca3","status":"passed","time":{"start":1693828311000,"stop":1693828311275,"duration":275}}]},"Controller Suite:Controller Suite#[It] Slice resource quota Tests ResourceQuota creation tests Create ResourceQuota fail test - namespace.DefaultRequestPerContainer.Mem > namespace.Limit.Mem":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":1,"unknown":0,"total":2},"items":[{"uid":"e4b2681190ebe6b","status":"passed","time":{"start":1693899867000,"stop":1693899869473,"duration":2473}},{"uid":"8a6e77ad43b29cdd","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Update Should fail when Project is Applied with service account name as combination of special characters in Write users":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"1861f732ec019a8c","status":"passed","time":{"start":1688022039000,"stop":1688022044879,"duration":5879}}]},"Controller Suite:Controller Suite#[It] Slice resource quota Tests ResourceQuota creation tests Create ResourceQuota fail test - namespace.DefaultRequestPerContainer.EphemeralStorage > namespace.Limit.EphemeralStorage":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":1,"unknown":0,"total":2},"items":[{"uid":"36ef786088a874d4","status":"passed","time":{"start":1693899867000,"stop":1693899870714,"duration":3714}},{"uid":"62ef8e944a89e774","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Controller Suite:Controller Suite#[It] Slice node affinity Tests SliceNodeAffinity tests - includes creation and deletion testcase to check node selector labels for expanded namespaces":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":0,"unknown":0,"total":2},"items":[{"uid":"cf490714c9366945","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693899867000,"stop":1693899867000,"duration":0}},{"uid":"fd9e47d44777c74b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice Config Negative Scenario tests Slice Config Negative Scenario Update Should update while deploying sliceconfig with existing slice name":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"65e93c3a3d1d4764","status":"passed","time":{"start":1709719453000,"stop":1709719453743,"duration":743}}]},"Controller Suite:Controller Suite#[It] Slice RBAC Tests SliceRoleBinding Tests Create SliceRoleBinding failure test: Creating SliceRoleBinding slice-red with empty namespace":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":0,"unknown":0,"total":2},"items":[{"uid":"5cbcac0a154769db","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693899867000,"stop":1693899867000,"duration":0}},{"uid":"da45641e1b545e81","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests Iperf App connectivity Should install iperf-server on server cluster":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":0,"unknown":0,"total":1},"items":[{"uid":"8957421313ccbea","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1709719453000,"stop":1709719453000,"duration":0}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests slice with netpol should remove network policy resource from application namespace once the isolationEnabled is set to false":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"266171982ca9d1c2","status":"passed","time":{"start":1709719453000,"stop":1709719468621,"duration":15621}}]},"Controller Suite:Controller Suite#[It] Slice node affinity Tests SliceNodeAffinity tests - includes creation and deletion slice node affinity Passing - deletion of sliceconfig deletes the slicenodeaffinity":{"statistic":{"failed":1,"broken":0,"skipped":1,"passed":0,"unknown":0,"total":2},"items":[{"uid":"9c6525c7ab4300a9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693899867000,"stop":1693899867000,"duration":0}},{"uid":"d8d18b8893975dab","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0009073f8>: {\n        Underlying: <*exec.ExitError | 0xc000623be0>{\n            ProcessState: {\n                pid: 6536,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 300735},\n                    Stime: {Sec: 0, Usec: 138496},\n                    Maxrss: 103640,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 6539,\n                    Majflt: 679,\n                    Nswap: 0,\n                    Inblock: 28600,\n                    Oublock: 20592,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 1896,\n                    Nivcsw: 1554,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/e2e/assets/controller/nodeaffinity/slicenodeaffinity_red_success.yaml\\\": Internal error occurred: failed calling webhook \\\"vslicenodeaffinity.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/validate-controller-kubeslice-io-v1alpha1-slicenodeaffinity?timeout=10s\\\": dial tcp 10.96.60.138:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/e2e/assets/controller/nodeaffinity/slicenodeaffinity_red_success.yaml\\\": Internal error occurred: failed calling webhook \\\"vslicenodeaffinity.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/validate-controller-kubeslice-io-v1alpha1-slicenodeaffinity?timeout=10s\\\": dial tcp 10.96.60.138:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/e2e/assets/controller/nodeaffinity/slicenodeaffinity_red_success.yaml\\\": Internal error occurred: failed calling webhook \\\"vslicenodeaffinity.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/validate-controller-kubeslice-io-v1alpha1-slicenodeaffinity?timeout=10s\\\": dial tcp 10.96.60.138:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/e2e/assets/controller/nodeaffinity/slicenodeaffinity_red_success.yaml\\\": Internal error occurred: failed calling webhook \\\"vslicenodeaffinity.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/validate-controller-kubeslice-io-v1alpha1-slicenodeaffinity?timeout=10s\\\": dial tcp 10.96.60.138:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/e2e/assets/controller/nodeaffinity/slicenodeaffinity_red_success.yaml\": Internal error occurred: failed calling webhook \"vslicenodeaffinity.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/validate-controller-kubeslice-io-v1alpha1-slicenodeaffinity?timeout=10s\": dial tcp 10.96.60.138:443: connect: connection refused\noccurred","time":{"start":1693828311000,"stop":1693828387472,"duration":76472}}]},"Controller Suite:Controller Suite#[It] Slice resource quota Tests ResourceQuota creation tests Create ResourceQuota fail test - namespace.DefaultLimitPerContainer.Memory is negative":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":1,"unknown":0,"total":2},"items":[{"uid":"bd3b81d38ae70496","status":"passed","time":{"start":1693899867000,"stop":1693899876081,"duration":9081}},{"uid":"161ab71dbc12c99","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Worker Suite:Worker Suite#[It] NodeRestartSlice Testing node restart Should have application namespaces created":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"4fdac55ab6dc5148","status":"passed","time":{"start":1709719453000,"stop":1709719453226,"duration":226}}]},"Controller Suite:Controller Suite#[It] Slice RBAC Tests RBAC Common Tests WorkerSliceRoleBinding data propagation test: Checking successful propagation of ApplyTo":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":0,"unknown":0,"total":2},"items":[{"uid":"385b5ef1a49765df","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693899867000,"stop":1693899867000,"duration":0}},{"uid":"7f3b76d1fcf6c61b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting forwarder-kernel pod Check ping between iperf-server and iperf-client after forwarder-kernel pod restart":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":0,"unknown":0,"total":1},"items":[{"uid":"b37a1e94bc98c3f3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1709719453000,"stop":1709719453000,"duration":0}}]},"Controller Suite:Controller Suite#[It] Slice RBAC Tests RBAC Common Tests SliceRoleBinding status test: SliceRoleBinding should have error status for invalid namespace for k8s roles":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":0,"unknown":0,"total":2},"items":[{"uid":"b98f16e67c65497f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693899867000,"stop":1693899867000,"duration":0}},{"uid":"8d993fceb2ed05c0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Controller Suite:Controller Suite#[It] Slice resource quota Tests ResourceQuota creation tests Create ResourceQuota fail test- Request.Mem > Limit.Mem at slice level":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":1,"unknown":0,"total":2},"items":[{"uid":"34c4b21a8c23ad17","status":"passed","time":{"start":1693899867000,"stop":1693899881434,"duration":14434}},{"uid":"969d825ac35cacd8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Controller Suite:Controller Suite#[It] Slice RBAC Tests SliceRoleTemplate Tests Create SliceRoleTempalate failure test: Creating SliceRoleTempalate read-only-role, missing resources field":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":1,"unknown":0,"total":2},"items":[{"uid":"eb12920f6530bf51","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693899867000,"stop":1693899867000,"duration":0}},{"uid":"ea657b0e989e14d2","status":"passed","time":{"start":1693828311000,"stop":1693828316024,"duration":5024}}]},"Hub Suite:Hub Suite#[It] Cluster negative tests Worker Cluster Registration with Wrong endpoint":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"4b5e4d54fe849d4d","status":"passed","time":{"start":1688022039000,"stop":1688022184601,"duration":145601}}]},"Worker Suite:Worker Suite#[It] GW redundacy tests Test gw redundancy checking if gw pods are balanced after adding the label to the node":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":0,"unknown":0,"total":1},"items":[{"uid":"7b2df816c133d3e3","status":"skipped","statusDetails":"skipped - Spec skipped because Skip() was called in BeforeAll","time":{"start":1709719453000,"stop":1709719453000,"duration":0}}]},"Worker Suite:Worker Suite#[It] GW redundacy tests Test gw redundancy Should have vl3 pod running":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":0,"unknown":0,"total":1},"items":[{"uid":"285ae722e59c2b69","status":"skipped","statusDetails":"skipped - Gw Node Count required = 2","time":{"start":1709719453000,"stop":1709719453024,"duration":24}}]},"VPNKeyRotation Suite:VPNKeyRotation Suite#[It] Verify vpn key rotation functionality: all FSM's should get over":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":3,"unknown":0,"total":3},"items":[{"uid":"3085acde51ced9cb","status":"passed","time":{"start":1710224686000,"stop":1710224736481,"duration":50481}},{"uid":"aa779a54de92909d","status":"passed","time":{"start":1709883716000,"stop":1709883766418,"duration":50418}},{"uid":"841d0c26f5a6ca6e","status":"passed","time":{"start":1709717686000,"stop":1709717735892,"duration":49892}}]},"Worker Suite:Worker Suite#[It] Hub netpol scenarios: Slice Configs Netpol scenarios Should successfully deploy for valid namespace creating clusters with * in both applicationNamespace and allowedNamespace":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"f8f6a963b675075d","status":"passed","time":{"start":1709719453000,"stop":1709719453768,"duration":768}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Creation Should update Project while applying valid manifest with existing Project name":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"1133a4bc9de0e0a4","status":"passed","time":{"start":1688022039000,"stop":1688022043764,"duration":4764}}]},"Worker Suite:Worker Suite#[BeforeSuite]":{"statistic":{"failed":6,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":7},"items":[{"uid":"1ee681dfbf8136a3","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004ee018>: {\n        Underlying: <*exec.ExitError | 0xc0004d8040>{\n            ProcessState: {\n                pid: 7752,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 121470},\n                    Stime: {Sec: 0, Usec: 18979},\n                    Maxrss: 48920,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 1791,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 176,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 669,\n                    Nivcsw: 416,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:214: [debug] Original chart version: \\\"\\\"\",\n                        \"Error: INSTALLATION FAILED: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/nexus/kubeslice-controller-1.2.1.tgz : 404 Not Found\",\n                        \"helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/nexus/kubeslice-controller-1.2.1.tgz : 404 Not Found\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\",\n                        \"helm.sh/helm/v3/pkg/downloader.(*ChartDownloader).DownloadTo\",\n                        \"\\thelm.sh/helm/v3/pkg/downloader/chart_downloader.go:100\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:805\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:226\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:152\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.8.0/command.go:983\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.8.0/command.go:1115\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.8.0/command.go:1039\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:267\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1650\",\n                        \"INSTALLATION FAILED\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:154\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.8.0/command.go:983\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.8.0/command.go:1115\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.8.0/command.go:1039\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"ru...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:214: [debug] Original chart version: \"\"\n    Error: INSTALLATION FAILED: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/nexus/kubeslice-controller-1.2.1.tgz : 404 Not Found\n    helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/nexus/kubeslice-controller-1.2.1.tgz : 404 Not Found\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\n    helm.sh/helm/v3/pkg/downloader.(*ChartDownloader).DownloadTo\n    \thelm.sh/helm/v3/pkg/downloader/chart_downloader.go:100\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:805\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:226\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:152\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:983\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:1115\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:1039\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:267\n    runtime.goexit\n    \truntime/asm_amd64.s:1650\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:154\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:983\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:1115\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:1039\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:267\n    runtime.goexit\n    \truntime/asm_amd64.s:1650\noccurred","time":{"start":1711007881000,"stop":1711007881483,"duration":483}},{"uid":"dd82c208fd19a967","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00019c030>: {\n        Underlying: <*exec.ExitError | 0xc000490040>{\n            ProcessState: {\n                pid: 7921,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 95328},\n                    Stime: {Sec: 0, Usec: 12999},\n                    Maxrss: 47108,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 1632,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 176,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 442,\n                    Nivcsw: 346,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:214: [debug] Original chart version: \\\"\\\"\",\n                        \"Error: INSTALLATION FAILED: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/nexus/kubeslice-controller-1.2.1.tgz : 404 Not Found\",\n                        \"helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/nexus/kubeslice-controller-1.2.1.tgz : 404 Not Found\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\",\n                        \"helm.sh/helm/v3/pkg/downloader.(*ChartDownloader).DownloadTo\",\n                        \"\\thelm.sh/helm/v3/pkg/downloader/chart_downloader.go:100\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:805\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:226\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:152\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.8.0/command.go:983\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.8.0/command.go:1115\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.8.0/command.go:1039\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:267\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1650\",\n                        \"INSTALLATION FAILED\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:154\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.8.0/command.go:983\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.8.0/command.go:1115\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.8.0/command.go:1039\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"run...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:214: [debug] Original chart version: \"\"\n    Error: INSTALLATION FAILED: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/nexus/kubeslice-controller-1.2.1.tgz : 404 Not Found\n    helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/nexus/kubeslice-controller-1.2.1.tgz : 404 Not Found\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\n    helm.sh/helm/v3/pkg/downloader.(*ChartDownloader).DownloadTo\n    \thelm.sh/helm/v3/pkg/downloader/chart_downloader.go:100\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:805\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:226\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:152\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:983\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:1115\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:1039\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:267\n    runtime.goexit\n    \truntime/asm_amd64.s:1650\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:154\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:983\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:1115\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:1039\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:267\n    runtime.goexit\n    \truntime/asm_amd64.s:1650\noccurred","time":{"start":1710850779000,"stop":1710850779192,"duration":192}},{"uid":"7731f6b868294709","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00036fbc0>: {\n        Underlying: <*exec.ExitError | 0xc000046800>{\n            ProcessState: {\n                pid: 7661,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 137900},\n                    Stime: {Sec: 0, Usec: 18635},\n                    Maxrss: 47196,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2382,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 176,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 570,\n                    Nivcsw: 627,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:214: [debug] Original chart version: \\\"\\\"\",\n                        \"Error: INSTALLATION FAILED: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/nexus/kubeslice-controller-1.2.1.tgz : 404 Not Found\",\n                        \"helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/nexus/kubeslice-controller-1.2.1.tgz : 404 Not Found\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\",\n                        \"helm.sh/helm/v3/pkg/downloader.(*ChartDownloader).DownloadTo\",\n                        \"\\thelm.sh/helm/v3/pkg/downloader/chart_downloader.go:100\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:805\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:226\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:152\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.8.0/command.go:983\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.8.0/command.go:1115\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.8.0/command.go:1039\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:267\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1650\",\n                        \"INSTALLATION FAILED\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:154\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.8.0/command.go:983\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.8.0/command.go:1115\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.8.0/command.go:1039\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"ru...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:214: [debug] Original chart version: \"\"\n    Error: INSTALLATION FAILED: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/nexus/kubeslice-controller-1.2.1.tgz : 404 Not Found\n    helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/nexus/kubeslice-controller-1.2.1.tgz : 404 Not Found\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\n    helm.sh/helm/v3/pkg/downloader.(*ChartDownloader).DownloadTo\n    \thelm.sh/helm/v3/pkg/downloader/chart_downloader.go:100\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:805\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:226\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:152\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:983\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:1115\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:1039\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:267\n    runtime.goexit\n    \truntime/asm_amd64.s:1650\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:154\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:983\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:1115\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:1039\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:267\n    runtime.goexit\n    \truntime/asm_amd64.s:1650\noccurred","time":{"start":1710825513000,"stop":1710825513695,"duration":695}},{"uid":"cbfbd04242ea3926","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00099a018>: {\n        Underlying: <*exec.ExitError | 0xc00098c000>{\n            ProcessState: {\n                pid: 9639,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 8, Usec: 93415},\n                    Stime: {Sec: 0, Usec: 874734},\n                    Maxrss: 121372,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 37704,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 21592,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 56701,\n                    Nivcsw: 10513,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:214: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:231: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-1.2.1.tgz\",\n                        \"\",\n                        \"client.go:142: [debug] creating 1 resource(s)\",\n                        \"install.go:168: [debug] CRD serviceexports.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:142: [debug] creating 1 resource(s)\",\n                        \"install.go:168: [debug] CRD serviceimports.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:142: [debug] creating 1 resource(s)\",\n                        \"install.go:168: [debug] CRD slicegateways.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:142: [debug] creating 1 resource(s)\",\n                        \"install.go:168: [debug] CRD slices.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:142: [debug] creating 1 resource(s)\",\n                        \"install.go:168: [debug] CRD networkservices.networkservicemesh.io is already present. Skipping.\",\n                        \"client.go:142: [debug] creating 1 resource(s)\",\n                        \"install.go:168: [debug] CRD networkserviceendpoints.networkservicemesh.io is already present. Skipping.\",\n                        \"client.go:142: [debug] creating 1 resource(s)\",\n                        \"install.go:168: [debug] CRD clusterfederatedtrustdomains.spire.spiffe.io is already present. Skipping.\",\n                        \"client.go:142: [debug] creating 1 resource(s)\",\n                        \"install.go:168: [debug] CRD clusterspiffeids.spire.spiffe.io is already present. Skipping.\",\n                        \"client.go:142: [debug] creating 1 resource(s)\",\n                        \"client.go:486: [debug] Starting delete for \\\"kubeslice-install-crds\\\" ServiceAccount\",\n                        \"client.go:490: [debug] Ignoring delete failure for \\\"kubeslice-install-crds\\\" /v1, Kind=ServiceAccount: serviceaccounts \\\"kubeslice-install-crds\\\" not found\",\n                        \"wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\",\n                        \"client.go:142: [debug] creating 1 resource(s)\",\n                        \"client.go:486: [debug] Starting delete for \\\"kubeslice-install-crds\\\" ConfigMap\",\n                        \"client.go:490: [debug] Ignoring delete failure for \\\"kubeslice-install-crds\\\" /v1, Kind=ConfigMap: configmaps \\\"kubeslice-install-crds\\\" not found\",\n                        \"wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:214: [debug] Original chart version: \"\"\n    install.go:231: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-1.2.1.tgz\n    \n    client.go:142: [debug] creating 1 resource(s)\n    install.go:168: [debug] CRD serviceexports.networking.kubeslice.io is already present. Skipping.\n    client.go:142: [debug] creating 1 resource(s)\n    install.go:168: [debug] CRD serviceimports.networking.kubeslice.io is already present. Skipping.\n    client.go:142: [debug] creating 1 resource(s)\n    install.go:168: [debug] CRD slicegateways.networking.kubeslice.io is already present. Skipping.\n    client.go:142: [debug] creating 1 resource(s)\n    install.go:168: [debug] CRD slices.networking.kubeslice.io is already present. Skipping.\n    client.go:142: [debug] creating 1 resource(s)\n    install.go:168: [debug] CRD networkservices.networkservicemesh.io is already present. Skipping.\n    client.go:142: [debug] creating 1 resource(s)\n    install.go:168: [debug] CRD networkserviceendpoints.networkservicemesh.io is already present. Skipping.\n    client.go:142: [debug] creating 1 resource(s)\n    install.go:168: [debug] CRD clusterfederatedtrustdomains.spire.spiffe.io is already present. Skipping.\n    client.go:142: [debug] creating 1 resource(s)\n    install.go:168: [debug] CRD clusterspiffeids.spire.spiffe.io is already present. Skipping.\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"kubeslice-install-crds\" ServiceAccount\n    client.go:490: [debug] Ignoring delete failure for \"kubeslice-install-crds\" /v1, Kind=ServiceAccount: serviceaccounts \"kubeslice-install-crds\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"kubeslice-install-crds\" ConfigMap\n    client.go:490: [debug] Ignoring delete failure for \"kubeslice-install-crds\" /v1, Kind=ConfigMap: configmaps \"kubeslice-install-crds\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"kubeslice-install-crds\" ClusterRole\n    client.go:490: [debug] Ignoring delete failure for \"kubeslice-install-crds\" rbac.authorization.k8s.io/v1, Kind=ClusterRole: clusterroles.rbac.authorization.k8s.io \"kubeslice-install-crds\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"kubeslice-install-crds\" ClusterRoleBinding\n    client.go:490: [debug] Ignoring delete failure for \"kubeslice-install-crds\" rbac.authorization.k8s.io/v1, Kind=ClusterRoleBinding: clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-install-crds\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:490: [debug] Ignoring delete failure for \"nsm-config\" /v1, Kind=ConfigMap: configmaps \"nsm-config\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"nsm-crd-install\" ConfigMap\n    client.go:490: [debug] Ignoring delete failure for \"nsm-crd-install\" /v1, Kind=ConfigMap: configmaps \"nsm-crd-install\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"nsm-install-crds\" ServiceAccount\n    client.go:490: [debug] Ignoring delete failure for \"nsm-install-crds\" /v1, Kind=ServiceAccount: serviceaccounts \"nsm-install-crds\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"nsm-install-crds\" ClusterRole\n    client.go:490: [debug] Ignoring delete failure for \"nsm-install-crds\" rbac.authorization.k8s.io/v1, Kind=ClusterRole: clusterroles.rbac.authorization.k8s.io \"nsm-install-crds\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"nsm-install-crds\" ClusterRoleBinding\n    client.go:490: [debug] Ignoring delete failure for \"nsm-install-crds\" rbac.authorization.k8s.io/v1, Kind=ClusterRoleBinding: clusterrolebindings.rbac.authorization.k8s.io \"nsm-install-crds\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"spire-clusterid-cr-install\" ConfigMap\n    client.go:490: [debug] Ignoring delete failure for \"spire-clusterid-cr-install\" /v1, Kind=ConfigMap: configmaps \"spire-clusterid-cr-install\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"spire-crd-install\" ConfigMap\n    client.go:490: [debug] Ignoring delete failure for \"spire-crd-install\" /v1, Kind=ConfigMap: configmaps \"spire-crd-install\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"spire-install-clusterid-cr\" ServiceAccount\n    client.go:490: [debug] Ignoring delete failure for \"spire-install-clusterid-cr\" /v1, Kind=ServiceAccount: serviceaccounts \"spire-install-clusterid-cr\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"spire-install-clusterid-cr\" ClusterRole\n    client.go:490: [debug] Ignoring delete failure for \"spire-install-clusterid-cr\" rbac.authorization.k8s.io/v1, Kind=ClusterRole: clusterroles.rbac.authorization.k8s.io \"spire-install-clusterid-cr\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"spire-install-clusterid-cr\" ClusterRoleBinding\n    client.go:490: [debug] Ignoring delete failure for \"spire-install-clusterid-cr\" rbac.authorization.k8s.io/v1, Kind=ClusterRoleBinding: clusterrolebindings.rbac.authorization.k8s.io \"spire-install-clusterid-cr\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"spire-install-crds\" ServiceAccount\n    client.go:490: [debug] Ignoring delete failure for \"spire-install-crds\" /v1, Kind=ServiceAccount: serviceaccounts \"spire-install-crds\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"spire-install-crds\" ClusterRole\n    client.go:490: [debug] Ignoring delete failure for \"spire-install-crds\" rbac.authorization.k8s.io/v1, Kind=ClusterRole: clusterroles.rbac.authorization.k8s.io \"spire-install-crds\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"spire-install-crds\" ClusterRoleBinding\n    client.go:490: [debug] Ignoring delete failure for \"spire-install-crds\" rbac.authorization.k8s.io/v1, Kind=ClusterRoleBinding: clusterrolebindings.rbac.authorization.k8s.io \"spire-install-crds\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"kubeslice-install-crds\" Job\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:712: [debug] Watching for changes to Job kubeslice-install-crds with timeout of 10m0s\n    client.go:740: [debug] Add/Modify event for kubeslice-install-crds: ADDED\n    client.go:779: [debug] kubeslice-install-crds: Jobs active: 0, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for kubeslice-install-crds: MODIFIED\n    client.go:779: [debug] kubeslice-install-crds: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for kubeslice-install-crds: MODIFIED\n    client.go:486: [debug] Starting delete for \"nsm-install-crds\" Job\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:712: [debug] Watching for changes to Job nsm-install-crds with timeout of 10m0s\n    client.go:740: [debug] Add/Modify event for nsm-install-crds: ADDED\n    client.go:779: [debug] nsm-install-crds: Jobs active: 0, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for nsm-install-crds: MODIFIED\n    client.go:779: [debug] nsm-install-crds: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for nsm-install-crds: MODIFIED\n    client.go:486: [debug] Starting delete for \"spire-install-crds\" Job\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:712: [debug] Watching for changes to Job spire-install-crds with timeout of 10m0s\n    client.go:740: [debug] Add/Modify event for spire-install-crds: ADDED\n    client.go:779: [debug] spire-install-crds: Jobs active: 0, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for spire-install-crds: MODIFIED\n    client.go:779: [debug] spire-install-crds: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for spire-install-crds: MODIFIED\n    client.go:779: [debug] spire-install-crds: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for spire-install-crds: MODIFIED\n    client.go:779: [debug] spire-install-crds: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for spire-install-crds: MODIFIED\n    client.go:486: [debug] Starting delete for \"spire-install-clusterid-cr\" Job\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:712: [debug] Watching for changes to Job spire-install-clusterid-cr with timeout of 10m0s\n    client.go:740: [debug] Add/Modify event for spire-install-clusterid-cr: ADDED\n    client.go:779: [debug] spire-install-clusterid-cr: Jobs active: 0, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for spire-install-clusterid-cr: MODIFIED\n    client.go:779: [debug] spire-install-clusterid-cr: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for spire-install-clusterid-cr: MODIFIED\n    client.go:486: [debug] Starting delete for \"kubeslice-install-crds\" ServiceAccount\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"kubeslice-install-crds\" ConfigMap\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"kubeslice-install-crds\" ClusterRole\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"kubeslice-install-crds\" ClusterRoleBinding\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"nsm-crd-install\" ConfigMap\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"nsm-install-crds\" ServiceAccount\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"nsm-install-crds\" ClusterRole\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"nsm-install-crds\" ClusterRoleBinding\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"spire-clusterid-cr-install\" ConfigMap\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"spire-crd-install\" ConfigMap\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"spire-install-clusterid-cr\" ServiceAccount\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"spire-install-clusterid-cr\" ClusterRole\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"spire-install-clusterid-cr\" ClusterRoleBinding\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"spire-install-crds\" ServiceAccount\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"spire-install-crds\" ClusterRole\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"spire-install-crds\" ClusterRoleBinding\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 74 resource(s)\n    wait.go:48: [debug] beginning wait for 74 resources with timeout of 10m0s\n    ready.go:312: [debug] DaemonSet is not ready: spire/spiffe-csi-driver. observedGeneration (0) does not match spec generation (1).\n    ready.go:336: [debug] DaemonSet is not ready: spire/spiffe-csi-driver. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spiffe-csi-driver. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spiffe-csi-driver. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spiffe-csi-driver. 1 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    install.go:488: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:102: [debug] uninstall: Deleting kubeslice-worker\n    client.go:486: [debug] Starting delete for \"nsm-delete-webhooks\" ClusterRole\n    client.go:490: [debug] Ignoring delete failure for \"nsm-delete-webhooks\" rbac.authorization.k8s.io/v1, Kind=ClusterRole: clusterroles.rbac.authorization.k8s.io \"nsm-delete-webhooks\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"nsm-delete-webhooks\" ClusterRoleBinding\n    client.go:490: [debug] Ignoring delete failure for \"nsm-delete-webhooks\" rbac.authorization.k8s.io/v1, Kind=ClusterRoleBinding: clusterrolebindings.rbac.authorization.k8s.io \"nsm-delete-webhooks\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"kubeslice-cleanup\" ServiceAccount\n    client.go:490: [debug] Ignoring delete failure for \"kubeslice-cleanup\" /v1, Kind=ServiceAccount: serviceaccounts \"kubeslice-cleanup\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"kubeslice-cleanup\" ClusterRole\n    client.go:490: [debug] Ignoring delete failure for \"kubeslice-cleanup\" rbac.authorization.k8s.io/v1, Kind=ClusterRole: clusterroles.rbac.authorization.k8s.io \"kubeslice-cleanup\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"kubeslice-cleanup\" ClusterRoleBinding\n    client.go:490: [debug] Ignoring delete failure for \"kubeslice-cleanup\" rbac.authorization.k8s.io/v1, Kind=ClusterRoleBinding: clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-cleanup\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"nsm-delete-webhooks\" ServiceAccount\n    client.go:490: [debug] Ignoring delete failure for \"nsm-delete-webhooks\" /v1, Kind=ServiceAccount: serviceaccounts \"nsm-delete-webhooks\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"nsm-delete-webhooks\" ConfigMap\n    client.go:490: [debug] Ignoring delete failure for \"nsm-delete-webhooks\" /v1, Kind=ConfigMap: configmaps \"nsm-delete-webhooks\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"kubeslice-cleanup\" Job\n    client.go:490: [debug] Ignoring delete failure for \"kubeslice-cleanup\" batch/v1, Kind=Job: jobs.batch \"kubeslice-cleanup\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:712: [debug] Watching for changes to Job kubeslice-cleanup with timeout of 10m0s\n    client.go:740: [debug] Add/Modify event for kubeslice-cleanup: ADDED\n    client.go:779: [debug] kubeslice-cleanup: Jobs active: 0, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for kubeslice-cleanup: MODIFIED\n    client.go:779: [debug] kubeslice-cleanup: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for kubeslice-cleanup: MODIFIED\n    client.go:486: [debug] Starting delete for \"nsm-delete-webhooks\" Job\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:712: [debug] Watching for changes to Job nsm-delete-webhooks with timeout of 10m0s\n    client.go:740: [debug] Add/Modify event for nsm-delete-webhooks: ADDED\n    client.go:779: [debug] nsm-delete-webhooks: Jobs active: 0, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for nsm-delete-webhooks: MODIFIED\n    client.go:779: [debug] nsm-delete-webhooks: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for nsm-delete-webhooks: MODIFIED\n    client.go:486: [debug] Starting delete for \"nsm-delete-webhooks\" ClusterRole\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"nsm-delete-webhooks\" ClusterRoleBinding\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"kubeslice-cleanup\" ServiceAccount\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"kubeslice-cleanup\" ClusterRole\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"kubeslice-cleanup\" ClusterRoleBinding\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"nsm-delete-webhooks\" ServiceAccount\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"nsm-delete-webhooks\" ConfigMap\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"kubeslice-cleanup\" Job\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    uninstall.go:248: [debug] uninstall: given cascade value: , defaulting to delete propagation background\n    client.go:486: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:486: [debug] Starting delete for \"spire-controller-manager-webhook-service\" Service\n    client.go:486: [debug] Starting delete for \"nsmgr\" Service\n    client.go:486: [debug] Starting delete for \"registry\" Service\n    client.go:486: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:486: [debug] Starting delete for \"spire-server\" Service\n    client.go:486: [debug] Starting delete for \"admission-webhook-svc\" Service\n    client.go:486: [debug] Starting delete for \"spire-server\" StatefulSet\n    client.go:486: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:486: [debug] Starting delete for \"nsm-admission-webhook-k8s\" Deployment\n    client.go:486: [debug] Starting delete for \"registry-k8s\" Deployment\n    client.go:486: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:486: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:486: [debug] Starting delete for \"spiffe-csi-driver\" DaemonSet\n    client.go:486: [debug] Starting delete for \"spire-agent\" DaemonSet\n    client.go:486: [debug] Starting delete for \"forwarder-kernel\" DaemonSet\n    client.go:486: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:486: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:486: [debug] Starting delete for \"leader-election-rolebinding\" RoleBinding\n    client.go:486: [debug] Starting delete for \"spire-server-role-binding\" RoleBinding\n    client.go:486: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:486: [debug] Starting delete for \"leader-election-role\" Role\n    client.go:486: [debug] Starting delete for \"spire-server-role\" Role\n    client.go:486: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:486: [debug] Starting delete for \"admission-webhook-binding\" ClusterRoleBinding\n    client.go:486: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:486: [debug] Starting delete for \"manager-rolebinding\" ClusterRoleBinding\n    client.go:486: [debug] Starting delete for \"spire-server-trust-role-binding\" ClusterRoleBinding\n    client.go:486: [debug] Starting delete for \"spire-agent-cluster-role-binding\" ClusterRoleBinding\n    client.go:486: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRoleBinding\n    client.go:486: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:486: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:486: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:486: [debug] Starting delete for \"admission-webhook-role\" ClusterRole\n    client.go:486: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:486: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:486: [debug] Starting delete for \"manager-role\" ClusterRole\n    client.go:486: [debug] Starting delete for \"spire-server-trust-role\" ClusterRole\n    client.go:486: [debug] Starting delete for \"spire-agent-cluster-role\" ClusterRole\n    client.go:486: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRole\n    client.go:486: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:486: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:486: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:486: [debug] Starting delete for \"kubeslice-worker-event-schema-conf\" ConfigMap\n    client.go:486: [debug] Starting delete for \"spire-agent\" ConfigMap\n    client.go:486: [debug] Starting delete for \"spire-server\" ConfigMap\n    client.go:486: [debug] Starting delete for \"spire-bundle\" ConfigMap\n    client.go:486: [debug] Starting delete for \"spire-controller-manager-config\" ConfigMap\n    client.go:486: [debug] Starting delete for \"nsmgr-cm\" ConfigMap\n    client.go:486: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:486: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:486: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:486: [debug] Starting delete for \"kubeslice-kubernetes-dashboard-creds\" Secret\n    client.go:486: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ServiceAccount\n    client.go:486: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:486: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:486: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:486: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:486: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:486: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:486: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:486: [debug] Starting delete for \"admission-webhook-sa\" ServiceAccount\n    client.go:486: [debug] Starting delete for \"kubeslice-gateway-edge\" ServiceAccount\n    client.go:486: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:486: [debug] Starting delete for \"spiffe-csi-driver\" ServiceAccount\n    client.go:486: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:486: [debug] Starting delete for \"spire-server\" ServiceAccount\n    client.go:486: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:486: [debug] Starting delete for \"spire-agent\" ServiceAccount\n    client.go:486: [debug] Starting delete for \"spire\" Namespace\n    client.go:486: [debug] Starting delete for \"nsm-webhook-high-priority\" PriorityClass\n    client.go:486: [debug] Starting delete for \"csi.spiffe.io\" CSIDriver\n    client.go:486: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:486: [debug] Starting delete for \"spire-controller-manager-webhook\" ValidatingWebhookConfiguration\n    client.go:486: [debug] Starting delete for \"kubeslice-postdelete-job\" ClusterRole\n    client.go:490: [debug] Ignoring delete failure for \"kubeslice-postdelete-job\" rbac.authorization.k8s.io/v1, Kind=ClusterRole: clusterroles.rbac.authorization.k8s.io \"kubeslice-postdelete-job\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"kubeslice-postdelete-job\" ClusterRoleBinding\n    client.go:490: [debug] Ignoring delete failure for \"kubeslice-postdelete-job\" rbac.authorization.k8s.io/v1, Kind=ClusterRoleBinding: clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-postdelete-job\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"kubeslice-postdelete-job\" ServiceAccount\n    client.go:490: [debug] Ignoring delete failure for \"kubeslice-postdelete-job\" /v1, Kind=ServiceAccount: serviceaccounts \"kubeslice-postdelete-job\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"kubeslice-postdelete-job\" ConfigMap\n    client.go:490: [debug] Ignoring delete failure for \"kubeslice-postdelete-job\" /v1, Kind=ConfigMap: configmaps \"kubeslice-postdelete-job\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"kubeslice-postdelete-job\" Job\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:712: [debug] Watching for changes to Job kubeslice-postdelete-job with timeout of 10m0s\n    client.go:740: [debug] Add/Modify event for kubeslice-postdelete-job: ADDED\n    client.go:779: [debug] kubeslice-postdelete-job: Jobs active: 0, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for kubeslice-postdelete-job: MODIFIED\n    client.go:779: [debug] kubeslice-postdelete-job: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for kubeslice-postdelete-job: MODIFIED\n    client.go:486: [debug] Starting delete for \"kubeslice-postdelete-job\" ClusterRole\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"kubeslice-postdelete-job\" ClusterRoleBinding\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"kubeslice-postdelete-job\" ServiceAccount\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"kubeslice-postdelete-job\" ConfigMap\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    uninstall.go:155: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: client rate limiter Wait returned an error: context deadline exceeded\n    helm.go:84: [debug] client rate limiter Wait returned an error: context deadline exceeded\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:496\n    helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\n    \thelm.sh/helm/v3/pkg/action/install.go:394\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:306\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:152\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:983\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:1115\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:1039\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:267\n    runtime.goexit\n    \truntime/asm_amd64.s:1650\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:154\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:983\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:1115\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:1039\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:267\n    runtime.goexit\n    \truntime/asm_amd64.s:1650\noccurred","time":{"start":1710227819000,"stop":1710228516947,"duration":697947}},{"uid":"bb5a0a1396f72e8c","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00044e048>: {\n        Underlying: <*exec.ExitError | 0xc0006b4000>{\n            ProcessState: {\n                pid: 9437,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 8, Usec: 201221},\n                    Stime: {Sec: 0, Usec: 881759},\n                    Maxrss: 114508,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 41221,\n                    Majflt: 1,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 21592,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 57058,\n                    Nivcsw: 10447,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:214: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:231: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-1.2.1.tgz\",\n                        \"\",\n                        \"client.go:142: [debug] creating 1 resource(s)\",\n                        \"install.go:168: [debug] CRD serviceexports.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:142: [debug] creating 1 resource(s)\",\n                        \"install.go:168: [debug] CRD serviceimports.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:142: [debug] creating 1 resource(s)\",\n                        \"install.go:168: [debug] CRD slicegateways.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:142: [debug] creating 1 resource(s)\",\n                        \"install.go:168: [debug] CRD slices.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:142: [debug] creating 1 resource(s)\",\n                        \"install.go:168: [debug] CRD networkservices.networkservicemesh.io is already present. Skipping.\",\n                        \"client.go:142: [debug] creating 1 resource(s)\",\n                        \"install.go:168: [debug] CRD networkserviceendpoints.networkservicemesh.io is already present. Skipping.\",\n                        \"client.go:142: [debug] creating 1 resource(s)\",\n                        \"install.go:168: [debug] CRD clusterfederatedtrustdomains.spire.spiffe.io is already present. Skipping.\",\n                        \"client.go:142: [debug] creating 1 resource(s)\",\n                        \"install.go:168: [debug] CRD clusterspiffeids.spire.spiffe.io is already present. Skipping.\",\n                        \"client.go:142: [debug] creating 1 resource(s)\",\n                        \"client.go:486: [debug] Starting delete for \\\"kubeslice-install-crds\\\" ServiceAccount\",\n                        \"client.go:490: [debug] Ignoring delete failure for \\\"kubeslice-install-crds\\\" /v1, Kind=ServiceAccount: serviceaccounts \\\"kubeslice-install-crds\\\" not found\",\n                        \"wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\",\n                        \"client.go:142: [debug] creating 1 resource(s)\",\n                        \"client.go:486: [debug] Starting delete for \\\"kubeslice-install-crds\\\" ConfigMap\",\n                        \"client.go:490: [debug] Ignoring delete failure for \\\"kubeslice-install-crds\\\" /v1, Kind=ConfigMap: configmaps \\\"kubeslice-install-crds\\\" not found\",\n                        \"wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:214: [debug] Original chart version: \"\"\n    install.go:231: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-1.2.1.tgz\n    \n    client.go:142: [debug] creating 1 resource(s)\n    install.go:168: [debug] CRD serviceexports.networking.kubeslice.io is already present. Skipping.\n    client.go:142: [debug] creating 1 resource(s)\n    install.go:168: [debug] CRD serviceimports.networking.kubeslice.io is already present. Skipping.\n    client.go:142: [debug] creating 1 resource(s)\n    install.go:168: [debug] CRD slicegateways.networking.kubeslice.io is already present. Skipping.\n    client.go:142: [debug] creating 1 resource(s)\n    install.go:168: [debug] CRD slices.networking.kubeslice.io is already present. Skipping.\n    client.go:142: [debug] creating 1 resource(s)\n    install.go:168: [debug] CRD networkservices.networkservicemesh.io is already present. Skipping.\n    client.go:142: [debug] creating 1 resource(s)\n    install.go:168: [debug] CRD networkserviceendpoints.networkservicemesh.io is already present. Skipping.\n    client.go:142: [debug] creating 1 resource(s)\n    install.go:168: [debug] CRD clusterfederatedtrustdomains.spire.spiffe.io is already present. Skipping.\n    client.go:142: [debug] creating 1 resource(s)\n    install.go:168: [debug] CRD clusterspiffeids.spire.spiffe.io is already present. Skipping.\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"kubeslice-install-crds\" ServiceAccount\n    client.go:490: [debug] Ignoring delete failure for \"kubeslice-install-crds\" /v1, Kind=ServiceAccount: serviceaccounts \"kubeslice-install-crds\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"kubeslice-install-crds\" ConfigMap\n    client.go:490: [debug] Ignoring delete failure for \"kubeslice-install-crds\" /v1, Kind=ConfigMap: configmaps \"kubeslice-install-crds\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"kubeslice-install-crds\" ClusterRole\n    client.go:490: [debug] Ignoring delete failure for \"kubeslice-install-crds\" rbac.authorization.k8s.io/v1, Kind=ClusterRole: clusterroles.rbac.authorization.k8s.io \"kubeslice-install-crds\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"kubeslice-install-crds\" ClusterRoleBinding\n    client.go:490: [debug] Ignoring delete failure for \"kubeslice-install-crds\" rbac.authorization.k8s.io/v1, Kind=ClusterRoleBinding: clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-install-crds\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:490: [debug] Ignoring delete failure for \"nsm-config\" /v1, Kind=ConfigMap: configmaps \"nsm-config\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"nsm-crd-install\" ConfigMap\n    client.go:490: [debug] Ignoring delete failure for \"nsm-crd-install\" /v1, Kind=ConfigMap: configmaps \"nsm-crd-install\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"nsm-install-crds\" ServiceAccount\n    client.go:490: [debug] Ignoring delete failure for \"nsm-install-crds\" /v1, Kind=ServiceAccount: serviceaccounts \"nsm-install-crds\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"nsm-install-crds\" ClusterRole\n    client.go:490: [debug] Ignoring delete failure for \"nsm-install-crds\" rbac.authorization.k8s.io/v1, Kind=ClusterRole: clusterroles.rbac.authorization.k8s.io \"nsm-install-crds\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"nsm-install-crds\" ClusterRoleBinding\n    client.go:490: [debug] Ignoring delete failure for \"nsm-install-crds\" rbac.authorization.k8s.io/v1, Kind=ClusterRoleBinding: clusterrolebindings.rbac.authorization.k8s.io \"nsm-install-crds\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"spire-clusterid-cr-install\" ConfigMap\n    client.go:490: [debug] Ignoring delete failure for \"spire-clusterid-cr-install\" /v1, Kind=ConfigMap: configmaps \"spire-clusterid-cr-install\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"spire-crd-install\" ConfigMap\n    client.go:490: [debug] Ignoring delete failure for \"spire-crd-install\" /v1, Kind=ConfigMap: configmaps \"spire-crd-install\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"spire-install-clusterid-cr\" ServiceAccount\n    client.go:490: [debug] Ignoring delete failure for \"spire-install-clusterid-cr\" /v1, Kind=ServiceAccount: serviceaccounts \"spire-install-clusterid-cr\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"spire-install-clusterid-cr\" ClusterRole\n    client.go:490: [debug] Ignoring delete failure for \"spire-install-clusterid-cr\" rbac.authorization.k8s.io/v1, Kind=ClusterRole: clusterroles.rbac.authorization.k8s.io \"spire-install-clusterid-cr\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"spire-install-clusterid-cr\" ClusterRoleBinding\n    client.go:490: [debug] Ignoring delete failure for \"spire-install-clusterid-cr\" rbac.authorization.k8s.io/v1, Kind=ClusterRoleBinding: clusterrolebindings.rbac.authorization.k8s.io \"spire-install-clusterid-cr\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"spire-install-crds\" ServiceAccount\n    client.go:490: [debug] Ignoring delete failure for \"spire-install-crds\" /v1, Kind=ServiceAccount: serviceaccounts \"spire-install-crds\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"spire-install-crds\" ClusterRole\n    client.go:490: [debug] Ignoring delete failure for \"spire-install-crds\" rbac.authorization.k8s.io/v1, Kind=ClusterRole: clusterroles.rbac.authorization.k8s.io \"spire-install-crds\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"spire-install-crds\" ClusterRoleBinding\n    client.go:490: [debug] Ignoring delete failure for \"spire-install-crds\" rbac.authorization.k8s.io/v1, Kind=ClusterRoleBinding: clusterrolebindings.rbac.authorization.k8s.io \"spire-install-crds\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"kubeslice-install-crds\" Job\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:712: [debug] Watching for changes to Job kubeslice-install-crds with timeout of 10m0s\n    client.go:740: [debug] Add/Modify event for kubeslice-install-crds: ADDED\n    client.go:779: [debug] kubeslice-install-crds: Jobs active: 0, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for kubeslice-install-crds: MODIFIED\n    client.go:779: [debug] kubeslice-install-crds: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for kubeslice-install-crds: MODIFIED\n    client.go:779: [debug] kubeslice-install-crds: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for kubeslice-install-crds: MODIFIED\n    client.go:779: [debug] kubeslice-install-crds: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for kubeslice-install-crds: MODIFIED\n    client.go:486: [debug] Starting delete for \"nsm-install-crds\" Job\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:712: [debug] Watching for changes to Job nsm-install-crds with timeout of 10m0s\n    client.go:740: [debug] Add/Modify event for nsm-install-crds: ADDED\n    client.go:779: [debug] nsm-install-crds: Jobs active: 0, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for nsm-install-crds: MODIFIED\n    client.go:779: [debug] nsm-install-crds: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for nsm-install-crds: MODIFIED\n    client.go:779: [debug] nsm-install-crds: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for nsm-install-crds: MODIFIED\n    client.go:779: [debug] nsm-install-crds: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for nsm-install-crds: MODIFIED\n    client.go:486: [debug] Starting delete for \"spire-install-crds\" Job\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:712: [debug] Watching for changes to Job spire-install-crds with timeout of 10m0s\n    client.go:740: [debug] Add/Modify event for spire-install-crds: ADDED\n    client.go:779: [debug] spire-install-crds: Jobs active: 0, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for spire-install-crds: MODIFIED\n    client.go:779: [debug] spire-install-crds: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for spire-install-crds: MODIFIED\n    client.go:486: [debug] Starting delete for \"spire-install-clusterid-cr\" Job\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:712: [debug] Watching for changes to Job spire-install-clusterid-cr with timeout of 10m0s\n    client.go:740: [debug] Add/Modify event for spire-install-clusterid-cr: ADDED\n    client.go:779: [debug] spire-install-clusterid-cr: Jobs active: 0, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for spire-install-clusterid-cr: MODIFIED\n    client.go:779: [debug] spire-install-clusterid-cr: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for spire-install-clusterid-cr: MODIFIED\n    client.go:779: [debug] spire-install-clusterid-cr: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for spire-install-clusterid-cr: MODIFIED\n    client.go:779: [debug] spire-install-clusterid-cr: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for spire-install-clusterid-cr: MODIFIED\n    client.go:486: [debug] Starting delete for \"kubeslice-install-crds\" ServiceAccount\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"kubeslice-install-crds\" ConfigMap\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"kubeslice-install-crds\" ClusterRole\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"kubeslice-install-crds\" ClusterRoleBinding\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"nsm-crd-install\" ConfigMap\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"nsm-install-crds\" ServiceAccount\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"nsm-install-crds\" ClusterRole\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"nsm-install-crds\" ClusterRoleBinding\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"spire-clusterid-cr-install\" ConfigMap\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"spire-crd-install\" ConfigMap\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"spire-install-clusterid-cr\" ServiceAccount\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"spire-install-clusterid-cr\" ClusterRole\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"spire-install-clusterid-cr\" ClusterRoleBinding\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"spire-install-crds\" ServiceAccount\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"spire-install-crds\" ClusterRole\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"spire-install-crds\" ClusterRoleBinding\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 74 resource(s)\n    wait.go:48: [debug] beginning wait for 74 resources with timeout of 10m0s\n    ready.go:336: [debug] DaemonSet is not ready: spire/spiffe-csi-driver. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spiffe-csi-driver. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spiffe-csi-driver. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    install.go:488: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:102: [debug] uninstall: Deleting kubeslice-worker\n    client.go:486: [debug] Starting delete for \"nsm-delete-webhooks\" ClusterRole\n    client.go:490: [debug] Ignoring delete failure for \"nsm-delete-webhooks\" rbac.authorization.k8s.io/v1, Kind=ClusterRole: clusterroles.rbac.authorization.k8s.io \"nsm-delete-webhooks\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"nsm-delete-webhooks\" ClusterRoleBinding\n    client.go:490: [debug] Ignoring delete failure for \"nsm-delete-webhooks\" rbac.authorization.k8s.io/v1, Kind=ClusterRoleBinding: clusterrolebindings.rbac.authorization.k8s.io \"nsm-delete-webhooks\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"kubeslice-cleanup\" ServiceAccount\n    client.go:490: [debug] Ignoring delete failure for \"kubeslice-cleanup\" /v1, Kind=ServiceAccount: serviceaccounts \"kubeslice-cleanup\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"kubeslice-cleanup\" ClusterRole\n    client.go:490: [debug] Ignoring delete failure for \"kubeslice-cleanup\" rbac.authorization.k8s.io/v1, Kind=ClusterRole: clusterroles.rbac.authorization.k8s.io \"kubeslice-cleanup\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"kubeslice-cleanup\" ClusterRoleBinding\n    client.go:490: [debug] Ignoring delete failure for \"kubeslice-cleanup\" rbac.authorization.k8s.io/v1, Kind=ClusterRoleBinding: clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-cleanup\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"nsm-delete-webhooks\" ServiceAccount\n    client.go:490: [debug] Ignoring delete failure for \"nsm-delete-webhooks\" /v1, Kind=ServiceAccount: serviceaccounts \"nsm-delete-webhooks\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"nsm-delete-webhooks\" ConfigMap\n    client.go:490: [debug] Ignoring delete failure for \"nsm-delete-webhooks\" /v1, Kind=ConfigMap: configmaps \"nsm-delete-webhooks\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"kubeslice-cleanup\" Job\n    client.go:490: [debug] Ignoring delete failure for \"kubeslice-cleanup\" batch/v1, Kind=Job: jobs.batch \"kubeslice-cleanup\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:712: [debug] Watching for changes to Job kubeslice-cleanup with timeout of 10m0s\n    client.go:740: [debug] Add/Modify event for kubeslice-cleanup: ADDED\n    client.go:779: [debug] kubeslice-cleanup: Jobs active: 0, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for kubeslice-cleanup: MODIFIED\n    client.go:779: [debug] kubeslice-cleanup: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for kubeslice-cleanup: MODIFIED\n    client.go:486: [debug] Starting delete for \"nsm-delete-webhooks\" Job\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:712: [debug] Watching for changes to Job nsm-delete-webhooks with timeout of 10m0s\n    client.go:740: [debug] Add/Modify event for nsm-delete-webhooks: ADDED\n    client.go:779: [debug] nsm-delete-webhooks: Jobs active: 0, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for nsm-delete-webhooks: MODIFIED\n    client.go:779: [debug] nsm-delete-webhooks: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for nsm-delete-webhooks: MODIFIED\n    client.go:486: [debug] Starting delete for \"nsm-delete-webhooks\" ClusterRole\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"nsm-delete-webhooks\" ClusterRoleBinding\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"kubeslice-cleanup\" ServiceAccount\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"kubeslice-cleanup\" ClusterRole\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"kubeslice-cleanup\" ClusterRoleBinding\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"nsm-delete-webhooks\" ServiceAccount\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"nsm-delete-webhooks\" ConfigMap\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"kubeslice-cleanup\" Job\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    uninstall.go:248: [debug] uninstall: given cascade value: , defaulting to delete propagation background\n    client.go:486: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:486: [debug] Starting delete for \"spire-controller-manager-webhook-service\" Service\n    client.go:486: [debug] Starting delete for \"admission-webhook-svc\" Service\n    client.go:486: [debug] Starting delete for \"spire-server\" Service\n    client.go:486: [debug] Starting delete for \"registry\" Service\n    client.go:486: [debug] Starting delete for \"nsmgr\" Service\n    client.go:486: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:486: [debug] Starting delete for \"spire-server\" StatefulSet\n    client.go:486: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:486: [debug] Starting delete for \"nsm-admission-webhook-k8s\" Deployment\n    client.go:486: [debug] Starting delete for \"registry-k8s\" Deployment\n    client.go:486: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:486: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:486: [debug] Starting delete for \"spiffe-csi-driver\" DaemonSet\n    client.go:486: [debug] Starting delete for \"spire-agent\" DaemonSet\n    client.go:486: [debug] Starting delete for \"forwarder-kernel\" DaemonSet\n    client.go:486: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:486: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:486: [debug] Starting delete for \"leader-election-rolebinding\" RoleBinding\n    client.go:486: [debug] Starting delete for \"spire-server-role-binding\" RoleBinding\n    client.go:486: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:486: [debug] Starting delete for \"leader-election-role\" Role\n    client.go:486: [debug] Starting delete for \"spire-server-role\" Role\n    client.go:486: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:486: [debug] Starting delete for \"admission-webhook-binding\" ClusterRoleBinding\n    client.go:486: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:486: [debug] Starting delete for \"manager-rolebinding\" ClusterRoleBinding\n    client.go:486: [debug] Starting delete for \"spire-server-trust-role-binding\" ClusterRoleBinding\n    client.go:486: [debug] Starting delete for \"spire-agent-cluster-role-binding\" ClusterRoleBinding\n    client.go:486: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRoleBinding\n    client.go:486: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:486: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:486: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:486: [debug] Starting delete for \"admission-webhook-role\" ClusterRole\n    client.go:486: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:486: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:486: [debug] Starting delete for \"manager-role\" ClusterRole\n    client.go:486: [debug] Starting delete for \"spire-server-trust-role\" ClusterRole\n    client.go:486: [debug] Starting delete for \"spire-agent-cluster-role\" ClusterRole\n    client.go:486: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRole\n    client.go:486: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:486: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:486: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:486: [debug] Starting delete for \"kubeslice-worker-event-schema-conf\" ConfigMap\n    client.go:486: [debug] Starting delete for \"spire-bundle\" ConfigMap\n    client.go:486: [debug] Starting delete for \"spire-controller-manager-config\" ConfigMap\n    client.go:486: [debug] Starting delete for \"spire-server\" ConfigMap\n    client.go:486: [debug] Starting delete for \"spire-agent\" ConfigMap\n    client.go:486: [debug] Starting delete for \"nsmgr-cm\" ConfigMap\n    client.go:486: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:486: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:486: [debug] Starting delete for \"kubeslice-kubernetes-dashboard-creds\" Secret\n    client.go:486: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:486: [debug] Starting delete for \"spire-agent\" ServiceAccount\n    client.go:486: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ServiceAccount\n    client.go:486: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:486: [debug] Starting delete for \"kubeslice-gateway-edge\" ServiceAccount\n    client.go:486: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:486: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:486: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:486: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:486: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:486: [debug] Starting delete for \"admission-webhook-sa\" ServiceAccount\n    client.go:486: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:486: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:486: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:486: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:486: [debug] Starting delete for \"spiffe-csi-driver\" ServiceAccount\n    client.go:486: [debug] Starting delete for \"spire-server\" ServiceAccount\n    client.go:486: [debug] Starting delete for \"spire\" Namespace\n    client.go:486: [debug] Starting delete for \"nsm-webhook-high-priority\" PriorityClass\n    client.go:486: [debug] Starting delete for \"csi.spiffe.io\" CSIDriver\n    client.go:486: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:486: [debug] Starting delete for \"spire-controller-manager-webhook\" ValidatingWebhookConfiguration\n    client.go:486: [debug] Starting delete for \"kubeslice-postdelete-job\" ClusterRole\n    client.go:490: [debug] Ignoring delete failure for \"kubeslice-postdelete-job\" rbac.authorization.k8s.io/v1, Kind=ClusterRole: clusterroles.rbac.authorization.k8s.io \"kubeslice-postdelete-job\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"kubeslice-postdelete-job\" ClusterRoleBinding\n    client.go:490: [debug] Ignoring delete failure for \"kubeslice-postdelete-job\" rbac.authorization.k8s.io/v1, Kind=ClusterRoleBinding: clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-postdelete-job\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"kubeslice-postdelete-job\" ServiceAccount\n    client.go:490: [debug] Ignoring delete failure for \"kubeslice-postdelete-job\" /v1, Kind=ServiceAccount: serviceaccounts \"kubeslice-postdelete-job\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"kubeslice-postdelete-job\" ConfigMap\n    client.go:490: [debug] Ignoring delete failure for \"kubeslice-postdelete-job\" /v1, Kind=ConfigMap: configmaps \"kubeslice-postdelete-job\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"kubeslice-postdelete-job\" Job\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:712: [debug] Watching for changes to Job kubeslice-postdelete-job with timeout of 10m0s\n    client.go:740: [debug] Add/Modify event for kubeslice-postdelete-job: ADDED\n    client.go:779: [debug] kubeslice-postdelete-job: Jobs active: 0, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for kubeslice-postdelete-job: MODIFIED\n    client.go:779: [debug] kubeslice-postdelete-job: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for kubeslice-postdelete-job: MODIFIED\n    client.go:486: [debug] Starting delete for \"kubeslice-postdelete-job\" ClusterRole\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"kubeslice-postdelete-job\" ClusterRoleBinding\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"kubeslice-postdelete-job\" ServiceAccount\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"kubeslice-postdelete-job\" ConfigMap\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    uninstall.go:155: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: client rate limiter Wait returned an error: context deadline exceeded\n    helm.go:84: [debug] client rate limiter Wait returned an error: context deadline exceeded\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:496\n    helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\n    \thelm.sh/helm/v3/pkg/action/install.go:394\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:306\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:152\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:983\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:1115\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:1039\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:267\n    runtime.goexit\n    \truntime/asm_amd64.s:1650\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:154\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:983\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:1115\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:1039\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:267\n    runtime.goexit\n    \truntime/asm_amd64.s:1650\noccurred","time":{"start":1709886756000,"stop":1709887453763,"duration":697763}},{"uid":"5a9538a3cdc74c0f","status":"passed","time":{"start":1709719453000,"stop":1709719602015,"duration":149015}},{"uid":"3fcd4274e0a85d49","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0001f1e60>: {\n        Underlying: <*exec.ExitError | 0xc000462b00>{\n            ProcessState: {\n                pid: 6392,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 42165},\n                    Stime: {Sec: 0, Usec: 16866},\n                    Maxrss: 43264,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 1507,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 189,\n                    Nivcsw: 39,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: error executing jsonpath \\\"\\\\\\\"{.clusters[?(@.name==\\\\\\\"kind-controller\\\\\\\")].cluster.server}\\\\\\\"\\\": Error executing template: <nil> is not array or slice and cannot be filtered. Printing more information for debugging the template:\",\n                        \"\\ttemplate was:\",\n                        \"\\t\\t\\\"{.clusters[?(@.name==\\\"kind-controller\\\")].cluster.server}\\\"\",\n                        \"\\tobject given to jsonpath engine was:\",\n                        \"\\t\\tmap[string]interface {}{\\\"apiVersion\\\":\\\"v1\\\", \\\"clusters\\\":interface {}(nil), \\\"contexts\\\":interface {}(nil), \\\"current-context\\\":\\\"\\\", \\\"kind\\\":\\\"Config\\\", \\\"preferences\\\":map[string]interface {}{}, \\\"users\\\":interface {}(nil)}\",\n                        \"\",\n                        \"\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: error executing jsonpath \\\"\\\\\\\"{.clusters[?(@.name==\\\\\\\"kind-controller\\\\\\\")].cluster.server}\\\\\\\"\\\": Error executing template: <nil> is not array or slice and cannot be filtered. Printing more information for debugging the template:\",\n                    \"\\ttemplate was:\",\n                    \"\\t\\t\\\"{.clusters[?(@.name==\\\"kind-controller\\\")].cluster.server}\\\"\",\n                    \"\\tobject given to jsonpath engine was:\",\n                    \"\\t\\tmap[string]interface {}{\\\"apiVersion\\\":\\\"v1\\\", \\\"clusters\\\":interface {}(nil), \\\"contexts\\\":interface {}(nil), \\\"current-context\\\":\\\"\\\", \\\"kind\\\":\\\"Config\\\", \\\"preferences\\\":map[string]interface {}{}, \\\"users\\\":interface {}(nil)}\",\n                    \"\",\n                    \"\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: error executing jsonpath \\\"\\\\\\\"{.clusters[?(@.name==\\\\\\\"kind-controller\\\\\\\")].cluster.server}\\\\\\\"\\\": Error executing template: <nil> is not array or slice and cannot be filtered. Printing more information for debugging the template:\",\n                        \"\\ttemplate was:\",\n                        \"\\t\\t\\\"{.clusters[?(@.name==\\\"kind-controller\\\")].cluster.server}\\\"\",\n                        \"\\tobject given to jsonpath engine was:\",\n                        \"\\t\\tmap[string]interface {}{\\\"apiVersion\\\":\\\"v1\\\", \\\"clusters\\\":interface {}(nil), \\\"contexts\\\":interface {}(nil), \\\"current-context\\\":\\\"\\\", \\\"kind\\\":\\\"Config\\\", \\\"preferences\\\":map[string]interface {}{}, \\\"users\\\":interface {}(nil)}\",\n                        \"\",\n                        \"\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: error executing jsonpath \\\"\\\\\\\"{.clusters[?(@.name==\\\\\\\"kind-controller\\\\\\\")].cluster.server}\\\\\\\"\\\": Error executing template: <nil> is not array or slice and cannot be filtered. Printing more information for debugging the template:\",\n                    \"\\ttemplate was:\",\n                    \"\\t\\t\\\"{.clusters[?(@...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; error: error executing jsonpath \"\\\"{.clusters[?(@.name==\\\"kind-controller\\\")].cluster.server}\\\"\": Error executing template: <nil> is not array or slice and cannot be filtered. Printing more information for debugging the template:\n    \ttemplate was:\n    \t\t\"{.clusters[?(@.name==\"kind-controller\")].cluster.server}\"\n    \tobject given to jsonpath engine was:\n    \t\tmap[string]interface {}{\"apiVersion\":\"v1\", \"clusters\":interface {}(nil), \"contexts\":interface {}(nil), \"current-context\":\"\", \"kind\":\"Config\", \"preferences\":map[string]interface {}{}, \"users\":interface {}(nil)}\n    \n    \noccurred","time":{"start":1688018439000,"stop":1688018439058,"duration":58}}]},"Worker Suite:Worker Suite#[It] GW redundacy tests Test gw redundancy should able to remove the label from gw pod":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":0,"unknown":0,"total":1},"items":[{"uid":"4818bd0ea6a4ff06","status":"skipped","statusDetails":"skipped - Spec skipped because Skip() was called in BeforeAll","time":{"start":1709719453000,"stop":1709719453000,"duration":0}}]},"Controller Suite:Controller Suite#[It] Slice resource quota Tests ResourceQuota creation tests Create ResourceQuota fail test - sum(cluster.request.CPU) > slice.CPU":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":1,"unknown":0,"total":2},"items":[{"uid":"907c04c38d34120c","status":"passed","time":{"start":1693899867000,"stop":1693899875245,"duration":8245}},{"uid":"d26d139fecff45b9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Namespace reconcile test scenarios: when Namespaces are labeled with kubeslice label Onboard cluster objs should have app ns & attached slice entry":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"6ef240dbad5289a2","status":"passed","time":{"start":1709719453000,"stop":1709719453602,"duration":602}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting iperf-server pod Check ping between iperf-server and iperf-client after iperf-server pod restart":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":0,"unknown":0,"total":1},"items":[{"uid":"326948f731de7539","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1709719453000,"stop":1709719453000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice creation scenarios: when slice red is installed workers should have gateway pod running":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"3bd5c5494d69c3da","status":"passed","time":{"start":1709719453000,"stop":1709719479119,"duration":26119}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Update Should update successfully when Project Applied while removing a service account name in Read users":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"dcf349d50124e020","status":"passed","time":{"start":1688022039000,"stop":1688022043750,"duration":4750}}]},"Worker Suite:Worker Suite#[It] mutation webhook tests Testing mutation webhook functionality after namespace onboard Should mutate pods with labels and annotations with nsm sidecars injected":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":0,"unknown":0,"total":1},"items":[{"uid":"44db2492b7fbfb9c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1709719453000,"stop":1709719453000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Hub netpol scenarios: Slice Configs Netpol scenarios Should fail while Deleting Slice without removing the namespaceIsolationProfile part in slice configs":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"5af96250b251e209","status":"passed","time":{"start":1709719453000,"stop":1709719453081,"duration":81}}]},"Controller Suite:Controller Suite#[It] Slice resource quota Tests ResourceQuota creation tests Create ResourceQuota fail test - slice.DefaultRequestPerContainer.mem > slice.Request.mem":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":1,"unknown":0,"total":2},"items":[{"uid":"fa0eb679537d578d","status":"passed","time":{"start":1693899867000,"stop":1693899874667,"duration":7667}},{"uid":"5e45ec706ccb733d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Controller Suite:Controller Suite#[It] Slice resource quota Tests ResourceQuota creation tests Create ResourceQuota fail test - sum(namespace.limit.mem) > slice.mem":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":1,"unknown":0,"total":2},"items":[{"uid":"5ed2a0e0cbfac8bb","status":"passed","time":{"start":1693899867000,"stop":1693899872786,"duration":5786}},{"uid":"844e88152bb40417","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice deletion tests: Slice is installed & validated Slice object is deleted from hub should remove vl3 router from spoke":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"7e4bffdaef047e7b","status":"passed","time":{"start":1709719453000,"stop":1709719510050,"duration":57050}}]},"Worker Suite:Worker Suite#[It] Namespace reconcile test scenarios: when Namespaces are labeled with kubeslice label Onboard label app ns on workers":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"8bfa513a5a2d59c0","status":"passed","time":{"start":1709719453000,"stop":1709719454609,"duration":1609}}]},"Worker Suite:Worker Suite#[It] mutation webhook tests Testing mutation webhook functionality after namespace onboard Should verify iperf connectivity":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":0,"unknown":0,"total":1},"items":[{"uid":"dd1661f4937215c9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1709719453000,"stop":1709719453000,"duration":0}}]},"SliceHealth Suite:SliceHealth Suite#[It] Verify gw redundancy wrt health check when atleast one gateway is down should have tunnel status in warning state":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":1,"unknown":0,"total":2},"items":[{"uid":"2947b00c249ade23","status":"passed","time":{"start":1709723127000,"stop":1709723127136,"duration":136}},{"uid":"7f6ac5d60b0552b7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1688026084000,"stop":1688026084000,"duration":0}}]},"SliceHealth Suite:SliceHealth Suite#[It] Testing Slice Health feature >  Verify various properties of SliceHealth object in worker slice config spec >  Should have slicegateway in Error state when both slicegateways are unavailable":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"7d0da5cb1f2ba9b2","status":"passed","time":{"start":1709723127000,"stop":1709723367469,"duration":240469}}]},"Worker Suite:Worker Suite#[It]  QosProfile tests QosProfile Create validation Create qos profile- validation of BandwidthCeilingKbps > BandwidthGuaranteedKbps":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"f1e155dc89bc24a5","status":"passed","time":{"start":1709719453000,"stop":1709719453335,"duration":335}}]},"Controller Suite:Controller Suite#[It] Slice resource quota Tests ResourceQuota creation tests Delete resource quota should recreate the resource quota test":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":0,"unknown":0,"total":2},"items":[{"uid":"ae78146cff4a5a66","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693899867000,"stop":1693899867000,"duration":0}},{"uid":"f6af246d20d45bb0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Controller Suite:Controller Suite#[It] Slice RBAC Tests RBAC Common Tests WorkerSliceRoleBinding data propagation test: Checking successful propagation of rules with namespace as asterisk":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":0,"unknown":0,"total":2},"items":[{"uid":"bbf785060c497426","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693899867000,"stop":1693899867000,"duration":0}},{"uid":"b520a27f24f971af","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Controller Suite:Controller Suite#[BeforeSuite]":{"statistic":{"failed":4,"broken":0,"skipped":0,"passed":2,"unknown":0,"total":6},"items":[{"uid":"7acf1faefd75583d","status":"passed","time":{"start":1693899867000,"stop":1693900480650,"duration":613650}},{"uid":"22b93747f889e367","status":"passed","time":{"start":1693828311000,"stop":1693829071086,"duration":760086}},{"uid":"1e7336d0b014177e","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000618018>: {\n        Underlying: <*exec.ExitError | 0xc000476000>{\n            ProcessState: {\n                pid: 5863,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 8, Usec: 446202},\n                    Stime: {Sec: 1, Usec: 281797},\n                    Maxrss: 115848,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 26522,\n                    Majflt: 470,\n                    Nswap: 0,\n                    Inblock: 66896,\n                    Oublock: 23312,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 81699,\n                    Nivcsw: 37723,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: [\n                    \"Release \\\"kubeslice-controller\\\" does not exist. Installing it now.\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"history.go:56: [debug] getting history for release kubeslice-controller\",\n                        \"Release \\\"kubeslice-controller\\\" does not exist. Installing it now.\",\n                        \"install.go:200: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:217: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-1.3.4.tgz\",\n                        \"\",\n                        \"client.go:134: [debug] creating 1 resource(s)\",\n                        \"client.go:134: [debug] creating 47 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 47 resources with timeout of 5m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n            ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; history.go:56: [debug] getting history for release kubeslice-controller\n    install.go:200: [debug] Original chart version: \"\"\n    install.go:217: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-1.3.4.tgz\n    \n    client.go:134: [debug] creating 1 resource(s)\n    client.go:134: [debug] creating 47 resource(s)\n    wait.go:48: [debug] beginning wait for 47 resources with timeout of 5m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    install.go:475: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:97: [debug] uninstall: Deleting kubeslice-controller\n    client.go:134: [debug] creating 1 resource(s)\n    client.go:706: [debug] Watching for changes to Job kubeslice-controller-cleanup with timeout of 5m0s\n    client.go:734: [debug] Add/Modify event for kubeslice-controller-cleanup: ADDED\n    client.go:773: [debug] kubeslice-controller-cleanup: Jobs active: 0, jobs failed: 0, jobs succeeded: 0\n    client.go:734: [debug] Add/Modify event for kubeslice-controller-cleanup: MODIFIED\n    client.go:773: [debug] kubeslice-controller-cleanup: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:734: [debug] Add/Modify event for kubeslice-controller-cleanup: MODIFIED\n    client.go:478: [debug] Starting delete for \"kubeslice-controller-cleanup\" Job\n    uninstall.go:243: [debug] uninstall: given cascade value: , defaulting to delete propagation background\n    client.go:478: [debug] Starting delete for \"kubeslice-controller-prometheus-service\" Service\n    client.go:478: [debug] Starting delete for \"kubeslice-controller-controller-manager-metrics-service\" Service\n    client.go:478: [debug] Starting delete for \"kubeslice-controller-webhook-service\" Service\n    client.go:478: [debug] Starting delete for \"kubeslice-controller-prometheus\" Deployment\n    client.go:478: [debug] Starting delete for \"kubeslice-controller-manager\" Deployment\n    client.go:478: [debug] Starting delete for \"kubeslice-controller-leader-election-rolebinding\" RoleBinding\n    client.go:478: [debug] Starting delete for \"kubeslice-controller-leader-election-role\" Role\n    client.go:478: [debug] Starting delete for \"kubeslice-controller-kube-state-metrics\" ClusterRoleBinding\n    client.go:478: [debug] Starting delete for \"kubeslice-controller-manager-rolebinding\" ClusterRoleBinding\n    client.go:478: [debug] Starting delete for \"kubeslice-controller-ovpn-controller-rolebinding\" ClusterRoleBinding\n    client.go:478: [debug] Starting delete for \"kubeslice-controller-proxy-rolebinding\" ClusterRoleBinding\n    client.go:478: [debug] Starting delete for \"kubeslice-controller-license-job-rolebinding\" ClusterRoleBinding\n    client.go:478: [debug] Starting delete for \"kubeslice-controller-prometheus\" ClusterRoleBinding\n    client.go:478: [debug] Starting delete for \"kubeslice-controller-prometheus\" ClusterRole\n    client.go:478: [debug] Starting delete for \"kubeslice-controller-metrics-reader\" ClusterRole\n    client.go:478: [debug] Starting delete for \"kubeslice-controller-controller-role\" ClusterRole\n    client.go:478: [debug] Starting delete for \"kubeslice-controller-kube-state-metrics\" ClusterRole\n    client.go:478: [debug] Starting delete for \"kubeslice-controller-proxy-role\" ClusterRole\n    client.go:478: [debug] Starting delete for \"kubeslice-controller-ovpn-editor-role\" ClusterRole\n    client.go:478: [debug] Starting delete for \"kubeslice-controller-license-job-role\" ClusterRole\n    client.go:478: [debug] Starting delete for \"slicenodeaffinities.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:478: [debug] Starting delete for \"projects.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:478: [debug] Starting delete for \"serviceexportconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:478: [debug] Starting delete for \"sliceconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:478: [debug] Starting delete for \"clusters.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:478: [debug] Starting delete for \"workersliceconfigs.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:478: [debug] Starting delete for \"workerslicegateways.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:478: [debug] Starting delete for \"workerslicegwrecyclers.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:478: [debug] Starting delete for \"sliceqosconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:478: [debug] Starting delete for \"sliceresourcequotaconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:478: [debug] Starting delete for \"slicerolebindings.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:478: [debug] Starting delete for \"sliceroletemplates.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:478: [debug] Starting delete for \"workerserviceimports.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:478: [debug] Starting delete for \"workersliceresourcequotas.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:478: [debug] Starting delete for \"workerslicenodeaffinities.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:478: [debug] Starting delete for \"workerslicerolebindings.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:478: [debug] Starting delete for \"kubeslice-controller-event-schema-conf\" ConfigMap\n    client.go:478: [debug] Starting delete for \"kubeslice-controller-manager-config\" ConfigMap\n    client.go:478: [debug] Starting delete for \"kubeslice-controller-prometheus-server-conf\" ConfigMap\n    client.go:478: [debug] Starting delete for \"kubeslice-controller-license-config\" ConfigMap\n    client.go:478: [debug] Starting delete for \"kubeslice-image-pull-secret\" Secret\n    client.go:478: [debug] Starting delete for \"webhook-server-cert-secret\" Secret\n    client.go:478: [debug] Starting delete for \"kubeslice-controller-license-job-manager\" ServiceAccount\n    client.go:478: [debug] Starting delete for \"kubeslice-controller-controller-manager\" ServiceAccount\n    client.go:478: [debug] Starting delete for \"kubeslice-controller-ovpn-manager\" ServiceAccount\n    client.go:478: [debug] Starting delete for \"kubeslice-controller-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:478: [debug] Starting delete for \"kubeslice-controller-validating-webhook-configuration\" ValidatingWebhookConfiguration\n    uninstall.go:150: [debug] purge requested for kubeslice-controller\n    Error: release kubeslice-controller failed, and has been uninstalled due to atomic being set: context deadline exceeded\n    helm.go:84: [debug] context deadline exceeded\n    release kubeslice-controller failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:483\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:467\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:423\n    runtime.goexit\n    \truntime/asm_amd64.s:1598\noccurred","time":{"start":1690377721000,"stop":1690378045280,"duration":324280}},{"uid":"384eb503b78e6fe6","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0003b8078>: {\n        Underlying: <*exec.ExitError | 0xc000472000>{\n            ProcessState: {\n                pid: 5872,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 7, Usec: 71892},\n                    Stime: {Sec: 0, Usec: 925219},\n                    Maxrss: 111732,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 11068,\n                    Majflt: 124,\n                    Nswap: 0,\n                    Inblock: 18976,\n                    Oublock: 23312,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 73922,\n                    Nivcsw: 32035,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: [\n                    \"Release \\\"kubeslice-controller\\\" does not exist. Installing it now.\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"history.go:56: [debug] getting history for release kubeslice-controller\",\n                        \"Release \\\"kubeslice-controller\\\" does not exist. Installing it now.\",\n                        \"install.go:200: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:217: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-1.3.4.tgz\",\n                        \"\",\n                        \"client.go:134: [debug] creating 1 resource(s)\",\n                        \"client.go:134: [debug] creating 47 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 47 resources with timeout of 5m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n             ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; history.go:56: [debug] getting history for release kubeslice-controller\n    install.go:200: [debug] Original chart version: \"\"\n    install.go:217: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-1.3.4.tgz\n    \n    client.go:134: [debug] creating 1 resource(s)\n    client.go:134: [debug] creating 47 resource(s)\n    wait.go:48: [debug] beginning wait for 47 resources with timeout of 5m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    install.go:475: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:97: [debug] uninstall: Deleting kubeslice-controller\n    client.go:134: [debug] creating 1 resource(s)\n    client.go:706: [debug] Watching for changes to Job kubeslice-controller-cleanup with timeout of 5m0s\n    client.go:734: [debug] Add/Modify event for kubeslice-controller-cleanup: ADDED\n    client.go:773: [debug] kubeslice-controller-cleanup: Jobs active: 0, jobs failed: 0, jobs succeeded: 0\n    client.go:734: [debug] Add/Modify event for kubeslice-controller-cleanup: MODIFIED\n    client.go:773: [debug] kubeslice-controller-cleanup: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:734: [debug] Add/Modify event for kubeslice-controller-cleanup: MODIFIED\n    client.go:478: [debug] Starting delete for \"kubeslice-controller-cleanup\" Job\n    uninstall.go:243: [debug] uninstall: given cascade value: , defaulting to delete propagation background\n    client.go:478: [debug] Starting delete for \"kubeslice-controller-prometheus-service\" Service\n    client.go:478: [debug] Starting delete for \"kubeslice-controller-controller-manager-metrics-service\" Service\n    client.go:478: [debug] Starting delete for \"kubeslice-controller-webhook-service\" Service\n    client.go:478: [debug] Starting delete for \"kubeslice-controller-prometheus\" Deployment\n    client.go:478: [debug] Starting delete for \"kubeslice-controller-manager\" Deployment\n    client.go:478: [debug] Starting delete for \"kubeslice-controller-leader-election-rolebinding\" RoleBinding\n    client.go:478: [debug] Starting delete for \"kubeslice-controller-leader-election-role\" Role\n    client.go:478: [debug] Starting delete for \"kubeslice-controller-kube-state-metrics\" ClusterRoleBinding\n    client.go:478: [debug] Starting delete for \"kubeslice-controller-manager-rolebinding\" ClusterRoleBinding\n    client.go:478: [debug] Starting delete for \"kubeslice-controller-ovpn-controller-rolebinding\" ClusterRoleBinding\n    client.go:478: [debug] Starting delete for \"kubeslice-controller-proxy-rolebinding\" ClusterRoleBinding\n    client.go:478: [debug] Starting delete for \"kubeslice-controller-license-job-rolebinding\" ClusterRoleBinding\n    client.go:478: [debug] Starting delete for \"kubeslice-controller-prometheus\" ClusterRoleBinding\n    client.go:478: [debug] Starting delete for \"kubeslice-controller-prometheus\" ClusterRole\n    client.go:478: [debug] Starting delete for \"kubeslice-controller-controller-role\" ClusterRole\n    client.go:478: [debug] Starting delete for \"kubeslice-controller-kube-state-metrics\" ClusterRole\n    client.go:478: [debug] Starting delete for \"kubeslice-controller-metrics-reader\" ClusterRole\n    client.go:478: [debug] Starting delete for \"kubeslice-controller-ovpn-editor-role\" ClusterRole\n    client.go:478: [debug] Starting delete for \"kubeslice-controller-proxy-role\" ClusterRole\n    client.go:478: [debug] Starting delete for \"kubeslice-controller-license-job-role\" ClusterRole\n    client.go:478: [debug] Starting delete for \"clusters.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:478: [debug] Starting delete for \"projects.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:478: [debug] Starting delete for \"serviceexportconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:478: [debug] Starting delete for \"sliceconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:478: [debug] Starting delete for \"slicenodeaffinities.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:478: [debug] Starting delete for \"sliceqosconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:478: [debug] Starting delete for \"sliceresourcequotaconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:478: [debug] Starting delete for \"slicerolebindings.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:478: [debug] Starting delete for \"sliceroletemplates.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:478: [debug] Starting delete for \"workerserviceimports.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:478: [debug] Starting delete for \"workersliceconfigs.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:478: [debug] Starting delete for \"workerslicegateways.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:478: [debug] Starting delete for \"workerslicegwrecyclers.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:478: [debug] Starting delete for \"workerslicenodeaffinities.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:478: [debug] Starting delete for \"workersliceresourcequotas.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:478: [debug] Starting delete for \"workerslicerolebindings.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:478: [debug] Starting delete for \"kubeslice-controller-prometheus-server-conf\" ConfigMap\n    client.go:478: [debug] Starting delete for \"kubeslice-controller-event-schema-conf\" ConfigMap\n    client.go:478: [debug] Starting delete for \"kubeslice-controller-manager-config\" ConfigMap\n    client.go:478: [debug] Starting delete for \"kubeslice-controller-license-config\" ConfigMap\n    client.go:478: [debug] Starting delete for \"kubeslice-image-pull-secret\" Secret\n    client.go:478: [debug] Starting delete for \"webhook-server-cert-secret\" Secret\n    client.go:478: [debug] Starting delete for \"kubeslice-controller-license-job-manager\" ServiceAccount\n    client.go:478: [debug] Starting delete for \"kubeslice-controller-controller-manager\" ServiceAccount\n    client.go:478: [debug] Starting delete for \"kubeslice-controller-ovpn-manager\" ServiceAccount\n    client.go:478: [debug] Starting delete for \"kubeslice-controller-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:478: [debug] Starting delete for \"kubeslice-controller-validating-webhook-configuration\" ValidatingWebhookConfiguration\n    uninstall.go:150: [debug] purge requested for kubeslice-controller\n    Error: release kubeslice-controller failed, and has been uninstalled due to atomic being set: context deadline exceeded\n    helm.go:84: [debug] context deadline exceeded\n    release kubeslice-controller failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:483\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:467\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:423\n    runtime.goexit\n    \truntime/asm_amd64.s:1598\noccurred","time":{"start":1690371125000,"stop":1690371443749,"duration":318749}},{"uid":"85e12f821cb61827","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000496168>: {\n        Underlying: <*exec.ExitError | 0xc0004ac120>{\n            ProcessState: {\n                pid: 5957,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 54926},\n                    Stime: {Sec: 0, Usec: 11769},\n                    Maxrss: 42128,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 1518,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 251,\n                    Nivcsw: 200,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: error executing jsonpath \\\"\\\\\\\"{.clusters[?(@.name==\\\\\\\"kind-controller\\\\\\\")].cluster.server}\\\\\\\"\\\": Error executing template: <nil> is not array or slice and cannot be filtered. Printing more information for debugging the template:\",\n                        \"\\ttemplate was:\",\n                        \"\\t\\t\\\"{.clusters[?(@.name==\\\"kind-controller\\\")].cluster.server}\\\"\",\n                        \"\\tobject given to jsonpath engine was:\",\n                        \"\\t\\tmap[string]interface {}{\\\"apiVersion\\\":\\\"v1\\\", \\\"clusters\\\":interface {}(nil), \\\"contexts\\\":interface {}(nil), \\\"current-context\\\":\\\"\\\", \\\"kind\\\":\\\"Config\\\", \\\"preferences\\\":map[string]interface {}{}, \\\"users\\\":interface {}(nil)}\",\n                        \"\",\n                        \"\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: error executing jsonpath \\\"\\\\\\\"{.clusters[?(@.name==\\\\\\\"kind-controller\\\\\\\")].cluster.server}\\\\\\\"\\\": Error executing template: <nil> is not array or slice and cannot be filtered. Printing more information for debugging the template:\",\n                    \"\\ttemplate was:\",\n                    \"\\t\\t\\\"{.clusters[?(@.name==\\\"kind-controller\\\")].cluster.server}\\\"\",\n                    \"\\tobject given to jsonpath engine was:\",\n                    \"\\t\\tmap[string]interface {}{\\\"apiVersion\\\":\\\"v1\\\", \\\"clusters\\\":interface {}(nil), \\\"contexts\\\":interface {}(nil), \\\"current-context\\\":\\\"\\\", \\\"kind\\\":\\\"Config\\\", \\\"preferences\\\":map[string]interface {}{}, \\\"users\\\":interface {}(nil)}\",\n                    \"\",\n                    \"\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: error executing jsonpath \\\"\\\\\\\"{.clusters[?(@.name==\\\\\\\"kind-controller\\\\\\\")].cluster.server}\\\\\\\"\\\": Error executing template: <nil> is not array or slice and cannot be filtered. Printing more information for debugging the template:\",\n                        \"\\ttemplate was:\",\n                        \"\\t\\t\\\"{.clusters[?(@.name==\\\"kind-controller\\\")].cluster.server}\\\"\",\n                        \"\\tobject given to jsonpath engine was:\",\n                        \"\\t\\tmap[string]interface {}{\\\"apiVersion\\\":\\\"v1\\\", \\\"clusters\\\":interface {}(nil), \\\"contexts\\\":interface {}(nil), \\\"current-context\\\":\\\"\\\", \\\"kind\\\":\\\"Config\\\", \\\"preferences\\\":map[string]interface {}{}, \\\"users\\\":interface {}(nil)}\",\n                        \"\",\n                        \"\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: error executing jsonpath \\\"\\\\\\\"{.clusters[?(@.name==\\\\\\\"kind-controller\\\\\\\")].cluster.server}\\\\\\\"\\\": Error executing template: <nil> is not array or slice and cannot be filtered. Printing more information for debugging the template:\",\n                    \"\\ttemplate was:\",\n                    \"\\t\\t\\\"{.clusters[?(...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; error: error executing jsonpath \"\\\"{.clusters[?(@.name==\\\"kind-controller\\\")].cluster.server}\\\"\": Error executing template: <nil> is not array or slice and cannot be filtered. Printing more information for debugging the template:\n    \ttemplate was:\n    \t\t\"{.clusters[?(@.name==\"kind-controller\")].cluster.server}\"\n    \tobject given to jsonpath engine was:\n    \t\tmap[string]interface {}{\"apiVersion\":\"v1\", \"clusters\":interface {}(nil), \"contexts\":interface {}(nil), \"current-context\":\"\", \"kind\":\"Config\", \"preferences\":map[string]interface {}{}, \"users\":interface {}(nil)}\n    \n    \noccurred","time":{"start":1689758771000,"stop":1689758771075,"duration":75}},{"uid":"14893ac9a70379c8","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000463368>: {\n        Underlying: <*exec.ExitError | 0xc0003db720>{\n            ProcessState: {\n                pid: 5944,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 39703},\n                    Stime: {Sec: 0, Usec: 11911},\n                    Maxrss: 42048,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2006,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 222,\n                    Nivcsw: 64,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: error executing jsonpath \\\"\\\\\\\"{.clusters[?(@.name==\\\\\\\"kind-controller\\\\\\\")].cluster.server}\\\\\\\"\\\": Error executing template: <nil> is not array or slice and cannot be filtered. Printing more information for debugging the template:\",\n                        \"\\ttemplate was:\",\n                        \"\\t\\t\\\"{.clusters[?(@.name==\\\"kind-controller\\\")].cluster.server}\\\"\",\n                        \"\\tobject given to jsonpath engine was:\",\n                        \"\\t\\tmap[string]interface {}{\\\"apiVersion\\\":\\\"v1\\\", \\\"clusters\\\":interface {}(nil), \\\"contexts\\\":interface {}(nil), \\\"current-context\\\":\\\"\\\", \\\"kind\\\":\\\"Config\\\", \\\"preferences\\\":map[string]interface {}{}, \\\"users\\\":interface {}(nil)}\",\n                        \"\",\n                        \"\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: error executing jsonpath \\\"\\\\\\\"{.clusters[?(@.name==\\\\\\\"kind-controller\\\\\\\")].cluster.server}\\\\\\\"\\\": Error executing template: <nil> is not array or slice and cannot be filtered. Printing more information for debugging the template:\",\n                    \"\\ttemplate was:\",\n                    \"\\t\\t\\\"{.clusters[?(@.name==\\\"kind-controller\\\")].cluster.server}\\\"\",\n                    \"\\tobject given to jsonpath engine was:\",\n                    \"\\t\\tmap[string]interface {}{\\\"apiVersion\\\":\\\"v1\\\", \\\"clusters\\\":interface {}(nil), \\\"contexts\\\":interface {}(nil), \\\"current-context\\\":\\\"\\\", \\\"kind\\\":\\\"Config\\\", \\\"preferences\\\":map[string]interface {}{}, \\\"users\\\":interface {}(nil)}\",\n                    \"\",\n                    \"\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: error executing jsonpath \\\"\\\\\\\"{.clusters[?(@.name==\\\\\\\"kind-controller\\\\\\\")].cluster.server}\\\\\\\"\\\": Error executing template: <nil> is not array or slice and cannot be filtered. Printing more information for debugging the template:\",\n                        \"\\ttemplate was:\",\n                        \"\\t\\t\\\"{.clusters[?(@.name==\\\"kind-controller\\\")].cluster.server}\\\"\",\n                        \"\\tobject given to jsonpath engine was:\",\n                        \"\\t\\tmap[string]interface {}{\\\"apiVersion\\\":\\\"v1\\\", \\\"clusters\\\":interface {}(nil), \\\"contexts\\\":interface {}(nil), \\\"current-context\\\":\\\"\\\", \\\"kind\\\":\\\"Config\\\", \\\"preferences\\\":map[string]interface {}{}, \\\"users\\\":interface {}(nil)}\",\n                        \"\",\n                        \"\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: error executing jsonpath \\\"\\\\\\\"{.clusters[?(@.name==\\\\\\\"kind-controller\\\\\\\")].cluster.server}\\\\\\\"\\\": Error executing template: <nil> is not array or slice and cannot be filtered. Printing more information for debugging the template:\",\n                    \"\\ttemplate was:\",\n                    \"\\t\\t\\\"{.clusters[?(@...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; error: error executing jsonpath \"\\\"{.clusters[?(@.name==\\\"kind-controller\\\")].cluster.server}\\\"\": Error executing template: <nil> is not array or slice and cannot be filtered. Printing more information for debugging the template:\n    \ttemplate was:\n    \t\t\"{.clusters[?(@.name==\"kind-controller\")].cluster.server}\"\n    \tobject given to jsonpath engine was:\n    \t\tmap[string]interface {}{\"apiVersion\":\"v1\", \"clusters\":interface {}(nil), \"contexts\":interface {}(nil), \"current-context\":\"\", \"kind\":\"Config\", \"preferences\":map[string]interface {}{}, \"users\":interface {}(nil)}\n    \n    \noccurred","time":{"start":1689755137000,"stop":1689755137061,"duration":61}}]},"Worker Suite:Worker Suite#[It] Hub netpol scenarios: Slice Configs Netpol scenarios Should fail when deploying with invalid namespace in both applicationNamespace and allowedNamespace":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"fe01cddfd70d254e","status":"passed","time":{"start":1709719453000,"stop":1709719453749,"duration":749}}]},"Hub Suite:Hub Suite#[AfterSuite]":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":4,"unknown":0,"total":4},"items":[{"uid":"b4564a8c4c7a0766","status":"passed","time":{"start":1695805835000,"stop":1695806135984,"duration":300984}},{"uid":"11d49aa167c5eda3","status":"passed","time":{"start":1689750156000,"stop":1689750457095,"duration":301095}},{"uid":"a882604afc922415","status":"passed","time":{"start":1688626949000,"stop":1688627250054,"duration":301054}},{"uid":"bceb9263bdb76971","status":"passed","time":{"start":1688022039000,"stop":1688022041714,"duration":2714}}]},"Worker Suite:Worker Suite#[It] Edit NodeIP tests for cluster CR,  Edit NodeIP validation,  Test nodeip validity >  Verify Node IP address in cluster status should match with openvpn server endpoint ip":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"f3a3b14a6d01ec2c","status":"passed","time":{"start":1709719453000,"stop":1709719502194,"duration":49194}}]},"Controller Suite:Controller Suite#[It] Slice resource quota Tests ResourceQuota creation tests Create ResourceQuota fail test - namespace.DefaultLimitPerContainer.Mem > namespace.Limit.Mem":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":1,"unknown":0,"total":2},"items":[{"uid":"61e2660a1ead675","status":"passed","time":{"start":1693899867000,"stop":1693899874231,"duration":7231}},{"uid":"fe44ac7d5cfe6ae3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Hub Suite:Hub Suite#[It] Cluster auto deregister validation tests Cluster auto deregister Creates cluster secrets":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"ea0e3430c423df0d","status":"passed","time":{"start":1688022039000,"stop":1688022165816,"duration":126816}}]},"Hub Suite:Hub Suite#[It] Cluster CR tests Cluster CR validation Creates cluster secrets":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"69cbafba9535dfd1","status":"passed","time":{"start":1688022039000,"stop":1688022042600,"duration":3600}}]},"Worker Suite:Worker Suite#[It] Test Slice Gateway Service config >  gatewayServiceType: LB >  verify slice gw edge creation":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":0,"unknown":0,"total":1},"items":[{"uid":"28d08cbd693264e5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1709719453000,"stop":1709719453000,"duration":0}}]},"Controller Suite:Controller Suite#[It] Slice RBAC Tests RBAC Common Tests SliceRoleBinding status test: SliceRoleBinding should have empty status for successful WorkerSliceRoleBinding generation":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":0,"unknown":0,"total":2},"items":[{"uid":"c6c5dd6339e01266","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693899867000,"stop":1693899867000,"duration":0}},{"uid":"ef9765649b54f383","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Iperf connectivity should onboard deployment in application Namespaces without slice annotations":{"statistic":{"failed":1,"broken":0,"skipped":0,"passed":0,"unknown":0,"total":1},"items":[{"uid":"f4a60fcc1eb59ab7","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00087b9e0>: {\n        Underlying: <*exec.ExitError | 0xc0009d42e0>{\n            ProcessState: {\n                pid: 9378,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 179306},\n                    Stime: {Sec: 0, Usec: 62367},\n                    Maxrss: 119776,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 12477,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 19200,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 575,\n                    Nivcsw: 497,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/e2e/assets/application/iperf/iperf-client.yaml\\\": Internal error occurred: failed calling webhook \\\"webhook.kubeslice.io\\\": failed to call webhook: Post \\\"https://kubeslice-webhook-service.kubeslice-system.svc:443/mutate-webhook?timeout=10s\\\": EOF\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/e2e/assets/application/iperf/iperf-client.yaml\\\": Internal error occurred: failed calling webhook \\\"webhook.kubeslice.io\\\": failed to call webhook: Post \\\"https://kubeslice-webhook-service.kubeslice-system.svc:443/mutate-webhook?timeout=10s\\\": EOF\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/e2e/assets/application/iperf/iperf-client.yaml\\\": Internal error occurred: failed calling webhook \\\"webhook.kubeslice.io\\\": failed to call webhook: Post \\\"https://kubeslice-webhook-service.kubeslice-system.svc:443/mutate-webhook?timeout=10s\\\": EOF\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/e2e/assets/application/iperf/iperf-client.yaml\\\": Internal error occurred: failed calling webhook \\\"webhook.kubeslice.io\\\": failed to call webhook: Post \\\"https://kubeslice-webhook-service.kubeslice-system.svc:443/mutate-webhook?timeout=10s\\\": EOF\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/e2e/assets/application/iperf/iperf-client.yaml\": Internal error occurred: failed calling webhook \"webhook.kubeslice.io\": failed to call webhook: Post \"https://kubeslice-webhook-service.kubeslice-system.svc:443/mutate-webhook?timeout=10s\": EOF\noccurred","time":{"start":1709719453000,"stop":1709719463067,"duration":10067}}]},"VPNKeyRotation Suite:VPNKeyRotation Suite#[It] Verify vpn key rotation functionality: should have created vpnkeyrotation config":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":3,"unknown":0,"total":3},"items":[{"uid":"dbc0e85fde6d3006","status":"passed","time":{"start":1710224686000,"stop":1710224722689,"duration":36689}},{"uid":"e84b3814098917d0","status":"passed","time":{"start":1709883716000,"stop":1709883743533,"duration":27533}},{"uid":"4ebcda65e4ba87ce","status":"passed","time":{"start":1709717686000,"stop":1709717722567,"duration":36567}}]},"Worker Suite:Worker Suite#[It] Test Slice Gateway Service config >  gatewayServiceType: LB >  should create workerSliceGw cr with gw svc type=LB & protocol=TCP":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":0,"unknown":0,"total":1},"items":[{"uid":"b20c8e95f9773954","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1709719453000,"stop":1709719453000,"duration":0}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Creation Should fail while creating Project with  project name as blank":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"2a9831abbaf03a03","status":"passed","time":{"start":1688022039000,"stop":1688022039236,"duration":236}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Update Should Update successfully when Project is Applied with valid service account name in Read users":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"16b1ec889e7da2cd","status":"passed","time":{"start":1688022039000,"stop":1688022047393,"duration":8393}}]},"Controller Suite:Controller Suite#[It] Slice resource quota Tests ResourceQuota creation tests Create ResourceQuota fail test - slice.limit.podcount > sum(namespace.limit.podcount)":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":1,"unknown":0,"total":2},"items":[{"uid":"5ebd14e75409894e","status":"passed","time":{"start":1693899867000,"stop":1693899887329,"duration":20329}},{"uid":"91252f787e7d04a9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests Iperf App connectivity Verify Iperf traffic":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":0,"unknown":0,"total":1},"items":[{"uid":"a8e3e3b1dee89a99","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1709719453000,"stop":1709719453000,"duration":0}}]},"Controller Suite:Controller Suite#[It] E2E tests for license job > when Controller is upgraded license mode = auto apply helm upgrade":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":1,"unknown":0,"total":2},"items":[{"uid":"c15eaa65d99ce499","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693899867000,"stop":1693899867000,"duration":0}},{"uid":"b45d5d6515e69def","status":"passed","time":{"start":1693828311000,"stop":1693828528773,"duration":217773}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Iperf connectivity Verify Connectivity between iperf-server and iperf-client":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":0,"unknown":0,"total":1},"items":[{"uid":"983693eb2a811bcf","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1709719453000,"stop":1709719453000,"duration":0}}]},"Controller Suite:Controller Suite#[It] Slice resource quota Tests ResourceQuota creation tests Create ResourceQuota fail test- Request.EphemeralStorage > Limit.EphemeralStorage at namespace level":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":1,"unknown":0,"total":2},"items":[{"uid":"a87d1c75cf943761","status":"passed","time":{"start":1693899867000,"stop":1693899907476,"duration":40476}},{"uid":"948508453739b119","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Hub Suite:Hub Suite#[It] Project and Cluster Events Test Events get recorded for project creation and deletion Events get recorded for project creation Validating events for role binding creation":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"e4e20991e629a9eb","status":"passed","time":{"start":1688022039000,"stop":1688022045250,"duration":6250}}]},"Controller Suite:Controller Suite#[It] Slice resource quota Tests ResourceQuota creation tests Create ResourceQuota fail test-cluster.limit.cpu negative":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":1,"unknown":0,"total":2},"items":[{"uid":"cab61c5a8e494c13","status":"passed","time":{"start":1693899867000,"stop":1693899871451,"duration":4451}},{"uid":"e8c20c79fa3f996f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Controller Suite:Controller Suite#[It] Slice resource quota Tests ResourceQuota creation tests Create ResourceQuota fail test - namespace.DefaultRequestPerContainer.Mem > namespace.Request.Mem":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":1,"unknown":0,"total":2},"items":[{"uid":"9dac86b4143cb9be","status":"passed","time":{"start":1693899867000,"stop":1693899882053,"duration":15053}},{"uid":"384427dd993b0daa","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Worker Suite:Worker Suite#[It]  QosProfile tests QosProfile delete validation should remove vl3 router from spoke":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"effab718b2fc4cee","status":"passed","time":{"start":1709719453000,"stop":1709719456023,"duration":3023}}]},"Hub Suite:Hub Suite#[It] Cluster negative tests Worker Cluster Registration with Wrong token":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"46630b4f5c462de0","status":"passed","time":{"start":1688022039000,"stop":1688022040359,"duration":1359}}]},"Worker Suite:Worker Suite#[It] Slice Creation scenarios Slice with MEDIUM QOS should have vl3 router running":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"b0287f9dde37baa7","status":"passed","time":{"start":1709719453000,"stop":1709719458953,"duration":5953}}]},"Controller Suite:Controller Suite#[It] Slice RBAC Tests SliceRoleBinding Tests Create SliceRoleBinding failure test: Creating SliceRoleBinding slice-red with invalid subject kind":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":0,"unknown":0,"total":2},"items":[{"uid":"30f5cbbe4ac6af4d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693899867000,"stop":1693899867000,"duration":0}},{"uid":"270b283585ca99f7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Hub Suite:Hub Suite#[It] Project and Cluster Events Test Events get recorded for cluster creation and deletion Events get recorded for cluster creation Validating events for cluster service account secret creation":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"ec16e01047a2bf56","status":"passed","time":{"start":1688022039000,"stop":1688022039077,"duration":77}}]},"Controller Suite:Controller Suite#[It] E2E tests for license job > when controller is installed with default options Kubeslice license configuration should exist":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":2,"unknown":0,"total":2},"items":[{"uid":"ebd225270c995f4f","status":"passed","time":{"start":1693899867000,"stop":1693899868224,"duration":1224}},{"uid":"6128cca6e333c3d7","status":"passed","time":{"start":1693828311000,"stop":1693828315233,"duration":4233}}]},"Hub Suite:Hub Suite#[It] Cluster CR tests Cluster CR validation Verify NodeIPs are updated in cluster CR after worker installation":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"2fbf708ee1369e53","status":"passed","time":{"start":1688022039000,"stop":1688022683620,"duration":644620}}]},"Worker Suite:Worker Suite#[It] Slice creation scenarios: when slice red is installed workers should have vl3 router pods running":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"95a177fbf2033d3f","status":"passed","time":{"start":1709719453000,"stop":1709719455075,"duration":2075}}]},"Worker Suite:Worker Suite#[It] Slice Config Events Test Events get recorded for slice config creation and deletion Events get recorded for slice config deletion Validating events for slice config deletion":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"c8f9c0330f4f766d","status":"passed","time":{"start":1709719453000,"stop":1709719477784,"duration":24784}}]},"Controller Suite:Controller Suite#[It] Slice RBAC Tests SliceRoleBinding Tests Create SliceRoleBinding failure test: Creating SliceRoleBinding slice-red with empty subject name":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":0,"unknown":0,"total":2},"items":[{"uid":"dd4938ac5ee68153","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693899867000,"stop":1693899867000,"duration":0}},{"uid":"f0bce53aea0e8534","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Hub Suite:Hub Suite#[It] Project and Cluster Events Test Events get recorded for cluster creation and deletion Events get recorded for cluster deletion Validating events for cluster role binding deletion":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"722939c78fbaeb3","status":"passed","time":{"start":1688022039000,"stop":1688022043255,"duration":4255}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests Iperf App connectivity Application namespaces should be isolated":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":0,"unknown":0,"total":1},"items":[{"uid":"3c26342113a71dc0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1709719453000,"stop":1709719453000,"duration":0}}]},"Controller Suite:Controller Suite#[It] Slice resource quota Tests ResourceQuota creation tests Create ResourceQuota fail test - namespace.DefaultLimitPerContainer.CPU is negative":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":1,"unknown":0,"total":2},"items":[{"uid":"867f27aa5b21624f","status":"passed","time":{"start":1693899867000,"stop":1693899871423,"duration":4423}},{"uid":"84fbd4a98af9a42f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Controller Suite:Controller Suite#[It] Slice RBAC Tests RBAC Common Tests WorkerSliceRoleBinding generation test: Creation of SliceRoleBinding with custom roles and k8s roles should generate two WorkerSliceRoleBindings":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":0,"unknown":0,"total":2},"items":[{"uid":"310eab3538bdb013","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693899867000,"stop":1693899867000,"duration":0}},{"uid":"f454019a45eb7eb7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"SliceHealth Suite:SliceHealth Suite#[It] Slice Health Check tests Test Slice Health Check Should have slice egress unavailable when egress is unavailable":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":0,"unknown":0,"total":1},"items":[{"uid":"d7f680e6614d9a1d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1688026084000,"stop":1688026084000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Test Slice Gateway Service config >  default sliceGatewaySvcType >  should create slice gateway cr with default gw svc type & protocol":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"17979e9fe7e6f41b","status":"passed","time":{"start":1709719453000,"stop":1709719453162,"duration":162}}]},"Controller Suite:Controller Suite#[It] Slice resource quota Tests ResourceQuota creation tests Create ResourceQuota fail test - sum(namespace.limit.cpu) > cluster.cpu":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":1,"unknown":0,"total":2},"items":[{"uid":"496cb6546369bbc6","status":"passed","time":{"start":1693899867000,"stop":1693899910223,"duration":43223}},{"uid":"5247ef14714c43d8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Controller Suite:Controller Suite#[It] Slice resource quota Tests ResourceQuota creation tests Create ResourceQuota fail test - namespace.DefaultRequestPerContainer.Memory is negative":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":1,"unknown":0,"total":2},"items":[{"uid":"f59c068cf3d5df8b","status":"passed","time":{"start":1693899867000,"stop":1693899870298,"duration":3298}},{"uid":"a107f6e0e282f2fc","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice Config Events Test Events get recorded for service export creation and deletion Events get recorded for service export deletion Validating events for service export deletion":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"6062754113e63dfc","status":"passed","time":{"start":1709719453000,"stop":1709719455684,"duration":2684}}]},"Controller Suite:Controller Suite#[It] Slice resource quota Tests ResourceQuota creation tests Create ResourceQuota fail test - No provision of defaultRequestPerContainer for the slice":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":1,"unknown":0,"total":2},"items":[{"uid":"2274263f68664c0b","status":"passed","time":{"start":1693899867000,"stop":1693899873546,"duration":6546}},{"uid":"343e3df51932c7ca","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Hub Suite:Hub Suite#[It] Cluster negative tests Worker Cluster Registration with Wrong ca.cert":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"d2046d516febc28d","status":"passed","time":{"start":1688022039000,"stop":1688022040200,"duration":1200}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting nsm-manager pod Check ping between iperf-server and iperf-client after nsm-manager pod restart":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":0,"unknown":0,"total":1},"items":[{"uid":"a9d97ed9e870c80f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1709719453000,"stop":1709719453000,"duration":0}}]},"Controller Suite:Controller Suite#[It] E2E tests for license job > when Controller is upgraded license mode = auto Should eventually create new controller pod":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":1,"unknown":0,"total":2},"items":[{"uid":"3ce18aab3acdf106","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693899867000,"stop":1693899867000,"duration":0}},{"uid":"eebe107f9a7bc363","status":"passed","time":{"start":1693828311000,"stop":1693828311799,"duration":799}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Creation Should fail while creating Project with project name as combination of special characters":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"96b3318a8936baad","status":"passed","time":{"start":1688022039000,"stop":1688022039250,"duration":250}}]},"VPNKeyRotation Suite:VPNKeyRotation Suite#[It] Verify vpn key rotation functionality: Should update new certificate and expiry TS":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":3,"unknown":0,"total":3},"items":[{"uid":"c5b2fce53784827f","status":"passed","time":{"start":1710224686000,"stop":1710224717035,"duration":31035}},{"uid":"85a086a7a64981d","status":"passed","time":{"start":1709883716000,"stop":1709883747067,"duration":31067}},{"uid":"23b56784ee13e314","status":"passed","time":{"start":1709717686000,"stop":1709717717076,"duration":31076}}]},"Worker Suite:Worker Suite#[It] Hub netpol scenarios: Slice Configs Netpol scenarios Should fail while creating namespaces with same name in both allowedNamespace and applicationNamespace":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"c0bd0bbee366c122","status":"passed","time":{"start":1709719453000,"stop":1709719453216,"duration":216}}]},"Worker Suite:Worker Suite#[It] Test Slice Gateway Service config >  default sliceGatewaySvcType >  should create workerSliceConfig cr with default gw svc type & protocol":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"8732993fd38a1c39","status":"passed","time":{"start":1709719453000,"stop":1709719453095,"duration":95}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting iperf-server pod Should restart iperf-server pod":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":0,"unknown":0,"total":1},"items":[{"uid":"de4fb1e5869c893e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1709719453000,"stop":1709719453000,"duration":0}}]},"Controller Suite:Controller Suite#[It] Slice RBAC Tests RBAC Common Tests WorkerSliceRoleBinding data propagation test: Checking successful creation of labels":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":0,"unknown":0,"total":2},"items":[{"uid":"e7d8a35339ebb0a2","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693899867000,"stop":1693899867000,"duration":0}},{"uid":"55c1166e32b7d9e1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting worker-operator pod Should restart worker-operator pod":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":0,"unknown":0,"total":1},"items":[{"uid":"b6d5a2bd9204faf9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1709719453000,"stop":1709719453000,"duration":0}}]},"Controller Suite:Controller Suite#[It] Validate License Tests License Validate tests should update the license to expired and grace period also expired and fail CRUD":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":1,"unknown":0,"total":2},"items":[{"uid":"7f744bba182b256c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693899867000,"stop":1693899867000,"duration":0}},{"uid":"8bff13c3d03b4e78","status":"passed","time":{"start":1693828311000,"stop":1693828314657,"duration":3657}}]},"Worker Suite:Worker Suite#[It] Slice Creation scenarios Slice with LOW QOS should have vl3 router running":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"7a3a35f0bfc81c6a","status":"passed","time":{"start":1709719453000,"stop":1709719456271,"duration":3271}}]},"Worker Suite:Worker Suite#[It] Attach/Deattach Scenario Attach cluster to a slice  when Deattach cluster from slice  should have gateway pod deleted from deattached cluster":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"b2b7d2f5814b0e64","status":"passed","time":{"start":1709719453000,"stop":1709719473065,"duration":20065}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests slice with netpol should install slice on each worker cluster with correct namespaceisolationprofile":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"c460bac920cbdf9d","status":"passed","time":{"start":1709719453000,"stop":1709719453121,"duration":121}}]},"VPNKeyRotation Suite:VPNKeyRotation Suite#[It] Verify vpn key rotation functionality: all workers should be in IN_PROGRESS":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":3,"unknown":0,"total":3},"items":[{"uid":"66874cfe2302dfb0","status":"passed","time":{"start":1710224686000,"stop":1710224687409,"duration":1409}},{"uid":"7f5e5094ae20d41d","status":"passed","time":{"start":1709883716000,"stop":1709883717330,"duration":1330}},{"uid":"8af7a81de8b8b1f5","status":"passed","time":{"start":1709717686000,"stop":1709717687269,"duration":1269}}]},"Controller Suite:Controller Suite#[It] E2E tests for license job > when Controller is upgraded license mode = auto should launch a `job` for fetching license data":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":1,"unknown":0,"total":2},"items":[{"uid":"6e1d3c6154cd81ec","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693899867000,"stop":1693899867000,"duration":0}},{"uid":"5c3152c3886ce9f7","status":"passed","time":{"start":1693828311000,"stop":1693828426761,"duration":115761}}]},"Controller Suite:Controller Suite#[It] Slice resource quota Tests ResourceQuota creation tests Create ResourceQuota fail test- Request.CPU > Limit.CPU at slice level":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":1,"unknown":0,"total":2},"items":[{"uid":"5e6d2e55d2958648","status":"passed","time":{"start":1693899867000,"stop":1693899888872,"duration":21872}},{"uid":"f61c72dc3f9f9540","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"VPNKeyRotation Suite:VPNKeyRotation Suite#[It] Verify vpn key rotation functionality: worker status should be back to COMPLETE state":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":3,"unknown":0,"total":3},"items":[{"uid":"50d490f88db369d6","status":"passed","time":{"start":1710224686000,"stop":1710224755919,"duration":69919}},{"uid":"104a7aa48824c3f9","status":"passed","time":{"start":1709883716000,"stop":1709883785697,"duration":69697}},{"uid":"9e72b576beee1ee3","status":"passed","time":{"start":1709717686000,"stop":1709717756957,"duration":70957}}]},"SliceHealth Suite:SliceHealth Suite#[It] Testing Slice Health feature >  Verify various properties of SliceHealth object in worker slice config spec >  Should have individual component status Normal":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"d9ccd6b58fadee19","status":"passed","time":{"start":1709723127000,"stop":1709723136827,"duration":9827}}]},"Controller Suite:Controller Suite#[It] Slice node affinity Tests SliceNodeAffinity tests - includes creation and deletion slice node affinity creation -failed case if clusters are repeated":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":1,"unknown":0,"total":2},"items":[{"uid":"1e17a3a7ee793365","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693899867000,"stop":1693899867000,"duration":0}},{"uid":"713efc1f3b412e70","status":"passed","time":{"start":1693828311000,"stop":1693828321416,"duration":10416}}]},"VPNKeyRotation Suite:VPNKeyRotation Suite#[It] Verify vpn key rotation functionality: should have FSM's running simultaneously":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":3,"unknown":0,"total":3},"items":[{"uid":"9a958d143e89192f","status":"passed","time":{"start":1710224686000,"stop":1710224686133,"duration":133}},{"uid":"d7b74adc090804a9","status":"passed","time":{"start":1709883716000,"stop":1709883716154,"duration":154}},{"uid":"29114e9a5112ba11","status":"passed","time":{"start":1709717686000,"stop":1709717686190,"duration":190}}]},"Worker Suite:Worker Suite#[It] Slice creation scenarios: when slice red is installed install slice blue workers should get attached to slice blue":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"e193a83a6cc5c71b","status":"passed","time":{"start":1709719453000,"stop":1709719454833,"duration":1833}}]},"Worker Suite:Worker Suite#[It] Attach/Deattach Scenario Attach cluster to a slice  when Deattach cluster from slice  should have gateway pod running on attached cluster":{"statistic":{"failed":1,"broken":0,"skipped":0,"passed":0,"unknown":0,"total":1},"items":[{"uid":"ded233f288b1fbe8","status":"failed","statusDetails":"Timed out after 500.002s.\nExpected\n    <int>: 0\nto equal\n    <int>: 2","time":{"start":1709719453000,"stop":1709719953178,"duration":500178}}]},"Intracluster Suite:Intracluster Suite#[AfterSuite]":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":4,"unknown":0,"total":4},"items":[{"uid":"a1c77aca1405871a","status":"passed","time":{"start":1695806741000,"stop":1695807041672,"duration":300672}},{"uid":"2cb9a9f4c124fa69","status":"passed","time":{"start":1689751062000,"stop":1689751362625,"duration":300625}},{"uid":"3e6772abec027ef8","status":"passed","time":{"start":1688627854000,"stop":1688628154532,"duration":300532}},{"uid":"98e58dbfa523cd34","status":"passed","time":{"start":1688025155000,"stop":1688025787097,"duration":632097}}]},"Hub Suite:Hub Suite#[It] Project and Cluster Events Test Events get recorded for cluster creation and deletion Events get recorded for cluster deletion Validating events for cluster service account deletion":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"55a0a54c49bfec58","status":"passed","time":{"start":1688022039000,"stop":1688022039065,"duration":65}}]},"Controller Suite:Controller Suite#[It] Slice node affinity Tests SliceNodeAffinity tests - includes creation and deletion slice node affinity creation -failed case if namespace is not among the application namespaces for the respective cluster":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":1,"unknown":0,"total":2},"items":[{"uid":"2bf24e23a47675a0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693899867000,"stop":1693899867000,"duration":0}},{"uid":"a1159f9e16531b9a","status":"passed","time":{"start":1693828311000,"stop":1693828315186,"duration":4186}}]},"Controller Suite:Controller Suite#[It] Slice RBAC Tests SliceRoleTemplate Tests Delete SliceRoleTemplate failure test: Deleting SliceRoleTempalate read-only-role participating in a SliceRoleBinding":{"statistic":{"failed":1,"broken":0,"skipped":1,"passed":0,"unknown":0,"total":2},"items":[{"uid":"91632ce205578fe2","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693899867000,"stop":1693899867000,"duration":0}},{"uid":"b07869b300245a50","status":"failed","statusDetails":"Timed out after 30.304s.\nUnexpected error:\n    <*json.SyntaxError | 0xc0009067c8>: {\n        msg: \"invalid character 'E' looking for beginning of value\",\n        Offset: 1,\n    }\n    invalid character 'E' looking for beginning of value\noccurred","time":{"start":1693828311000,"stop":1693828410017,"duration":99017}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Update Should fail when Project is Applied with service account name as blank in Read users":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"9fcdc3beeff34617","status":"passed","time":{"start":1688022039000,"stop":1688022043868,"duration":4868}}]},"Controller Suite:Controller Suite#[It] Slice resource quota Tests ResourceQuota creation tests Create ResourceQuota fail test - namespace.DefaultLimitPerContainer.CPU > namespace.Request.CPU":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":1,"unknown":0,"total":2},"items":[{"uid":"9544a78cb96f9a56","status":"passed","time":{"start":1693899867000,"stop":1693899871851,"duration":4851}},{"uid":"5ce076d0b9b2a220","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Test Slice Gateway Service config >  gatewayServiceType: LB >  verify slice healthy":{"statistic":{"failed":1,"broken":0,"skipped":0,"passed":0,"unknown":0,"total":1},"items":[{"uid":"c21648220c015059","status":"failed","statusDetails":"Timed out after 600.000s.\nExpected\n    <bool>: false\nto be true","time":{"start":1709719453000,"stop":1709720190910,"duration":737910}}]},"VPNKeyRotation Suite:VPNKeyRotation Suite#[It] Verify vpn key rotation functionality: should have status updated from all the worker clusters to COMPLETE":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":3,"unknown":0,"total":3},"items":[{"uid":"f5998ca99ab76497","status":"passed","time":{"start":1710224686000,"stop":1710224686089,"duration":89}},{"uid":"838036ce8aeba50a","status":"passed","time":{"start":1709883716000,"stop":1709883716077,"duration":77}},{"uid":"2fcfe87b9e1d6799","status":"passed","time":{"start":1709717686000,"stop":1709717686078,"duration":78}}]},"Worker Suite:Worker Suite#[It] Attach/Deattach Scenario Attach cluster to a slice  when Deattach cluster from slice  should have vl3 router deleted from deattach cluster":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"d89020f6242e7e07","status":"passed","time":{"start":1709719453000,"stop":1709719475126,"duration":22126}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting nsm-manager pod Should restart nsm-manager pod":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":0,"unknown":0,"total":1},"items":[{"uid":"29d9aad79253dc5a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1709719453000,"stop":1709719453000,"duration":0}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests Iperf App connectivity Should have application namespaces created":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"f1ddb51a70a4a823","status":"passed","time":{"start":1709719453000,"stop":1709719455990,"duration":2990}}]},"Controller Suite:Controller Suite#[It] Slice resource quota Tests ResourceQuota creation tests Create ResourceQuota fail test - sum(cluster.request.mem) > slice.mem":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":1,"unknown":0,"total":2},"items":[{"uid":"ab9dca8a55c71d72","status":"passed","time":{"start":1693899867000,"stop":1693899878268,"duration":11268}},{"uid":"84669de7fd92638a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Intracluster Suite:Intracluster Suite#[It] Intracluster slice tests Slice with netpol should remove network policy resource from application namespace once the isolationEnabled is set to false":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"7bdf9383408e6153","status":"passed","time":{"start":1688025155000,"stop":1688025164148,"duration":9148}}]},"Worker Suite:Worker Suite#[It] Test Slice Gateway Service config >  gatewayServiceType: LB >  verify l3 traffic":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":0,"unknown":0,"total":1},"items":[{"uid":"83e4838d5912b11d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1709719453000,"stop":1709719453000,"duration":0}}]},"Controller Suite:Controller Suite#[It] Slice resource quota Tests ResourceQuota creation tests Create ResourceQuota fail test - sum(namespace.request.cpu) > cluster.cpu":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":1,"unknown":0,"total":2},"items":[{"uid":"cfc568e2b7a7a50","status":"passed","time":{"start":1693899867000,"stop":1693899874597,"duration":7597}},{"uid":"c4cb384b78d771b7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Controller Suite:Controller Suite#[It] E2E tests for license job > when controller is installed with default options should evntually create license secret":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":2,"unknown":0,"total":2},"items":[{"uid":"94704cc97cfe9d9b","status":"passed","time":{"start":1693899867000,"stop":1693899868922,"duration":1922}},{"uid":"375a1e7c8756bb19","status":"passed","time":{"start":1693828311000,"stop":1693828315873,"duration":4873}}]},"Controller Suite:Controller Suite#[It] Slice RBAC Tests SliceRoleBinding Tests Create SliceRoleBinding failure test: Creating SliceRoleBinding slice-red with unsupported K8s Role":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":0,"unknown":0,"total":2},"items":[{"uid":"ee57fea66d8ece5f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693899867000,"stop":1693899867000,"duration":0}},{"uid":"cf0bdf0583af2a5c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Controller Suite:Controller Suite#[It] Slice RBAC Tests RBAC Common Tests SliceRoleBinding status test: SliceRoleBinding should have error status for role-ref not found":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":0,"unknown":0,"total":2},"items":[{"uid":"2275ce77cb89ad08","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693899867000,"stop":1693899867000,"duration":0}},{"uid":"4a98a7840f9accb3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Intracluster Suite:Intracluster Suite#[It] Intracluster slice tests Slice with netpol should reconcile netpol config":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"961fc174052b2654","status":"passed","time":{"start":1688025155000,"stop":1688025155205,"duration":205}}]},"Controller Suite:Controller Suite#[It] Events Test Events get recorded for slice role binding creation and deletion Events get recorded for slice role binding creation Validating events for worker slice role bindings creation":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":0,"unknown":0,"total":2},"items":[{"uid":"9b3f2820fe51466b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693899867000,"stop":1693899867000,"duration":0}},{"uid":"89808ecac04e1293","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Controller Suite:Controller Suite#[It] Events Test Events get recorded for slice config creation Validating events for slice resource quota config creation":{"statistic":{"failed":2,"broken":0,"skipped":0,"passed":0,"unknown":0,"total":2},"items":[{"uid":"ea0cbd2270b04e12","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0006de438>: {\n        Underlying: <*exec.ExitError | 0xc00063aa20>{\n            ProcessState: {\n                pid: 6812,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 380690},\n                    Stime: {Sec: 0, Usec: 155233},\n                    Maxrss: 102832,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 9687,\n                    Majflt: 9,\n                    Nswap: 0,\n                    Inblock: 12824,\n                    Oublock: 20624,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 1142,\n                    Nivcsw: 2835,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/e2e/assets/controller/slice/slice_event.yaml\\\": Internal error occurred: failed calling webhook \\\"msliceconfig.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-sliceconfig?timeout=10s\\\": dial tcp 10.96.230.189:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/e2e/assets/controller/slice/slice_event.yaml\\\": Internal error occurred: failed calling webhook \\\"msliceconfig.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-sliceconfig?timeout=10s\\\": dial tcp 10.96.230.189:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/e2e/assets/controller/slice/slice_event.yaml\\\": Internal error occurred: failed calling webhook \\\"msliceconfig.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-sliceconfig?timeout=10s\\\": dial tcp 10.96.230.189:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/e2e/assets/controller/slice/slice_event.yaml\\\": Internal error occurred: failed calling webhook \\\"msliceconfig.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-sliceconfig?timeout=10s\\\": dial tcp 10.96.230.189:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/e2e/assets/controller/slice/slice_event.yaml\": Internal error occurred: failed calling webhook \"msliceconfig.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-sliceconfig?timeout=10s\": dial tcp 10.96.230.189:443: connect: connection refused\noccurred","time":{"start":1693899867000,"stop":1693899875164,"duration":8164}},{"uid":"ea71192651769640","status":"failed","statusDetails":"Timed out after 41.604s.\nExpected\n    <bool>: false\nto be true","time":{"start":1693828311000,"stop":1693828443533,"duration":132533}}]},"Controller Suite:Controller Suite#[It] Slice node affinity Tests SliceNodeAffinity tests - includes creation and deletion slice node affinity creation -failed case if wild card * for application namespaces is repeated":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":1,"unknown":0,"total":2},"items":[{"uid":"dcf2b28bb649857c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693899867000,"stop":1693899867000,"duration":0}},{"uid":"32348490f79b6c87","status":"passed","time":{"start":1693828311000,"stop":1693828321433,"duration":10433}}]},"Worker Suite:Worker Suite#[It] Slice Creation scenarios Slice with LOW QOS should have gateway pod running":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"7b53e2b0573f273a","status":"passed","time":{"start":1709719453000,"stop":1709719468041,"duration":15041}}]},"Worker Suite:Worker Suite#[It] mutation webhook tests Testing mutation webhook functionality after namespace onboard Should mutate daemonsets with labels and annotations with nsm sidecars injected":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":0,"unknown":0,"total":1},"items":[{"uid":"5a9b9e0de5336997","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1709719453000,"stop":1709719453000,"duration":0}}]},"Controller Suite:Controller Suite#[It] Events Test Events get recorded for slice role template deletion Events get recorded for slice role template Validating events for slice role template deletion":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":0,"unknown":0,"total":2},"items":[{"uid":"1da564a390764ced","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693899867000,"stop":1693899867000,"duration":0}},{"uid":"2c055676217f7227","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Hub Suite:Hub Suite#[It] Cluster health check tests Cluster health check - without istio should contain node IP & cni subnet status information":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":0,"unknown":0,"total":1},"items":[{"uid":"a9b7672b02dc4b87","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1688022039000,"stop":1688022039000,"duration":0}}]},"Controller Suite:Controller Suite#[It] Slice resource quota Tests ResourceQuota creation tests Create ResourceQuota fail test- Request.EphemeralStorage > Limit.EphemeralStorage at cluster level":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":1,"unknown":0,"total":2},"items":[{"uid":"bf8041f7594e8d2c","status":"passed","time":{"start":1693899867000,"stop":1693899911328,"duration":44328}},{"uid":"70e7e0b54b42741f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Creation Should create Project while using valid manifest":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"2d64d088b0950f1f","status":"passed","time":{"start":1688022039000,"stop":1688022047590,"duration":8590}}]},"SliceHealth Suite:SliceHealth Suite#[It] Slice Health Check tests Test Slice Health Check Should have slicegateway in Error state when both slicegateways are unavailable":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":0,"unknown":0,"total":1},"items":[{"uid":"e1872fd168cbec5a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1688026084000,"stop":1688026084000,"duration":0}}]},"Controller Suite:Controller Suite#[It] Events Test Events get recorded for slice config creation Validating events for setting slice config as owner of slice resource quota":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":0,"unknown":0,"total":2},"items":[{"uid":"17e8357d9df6b6e2","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693899867000,"stop":1693899867000,"duration":0}},{"uid":"4f4a2f342873ddf0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Controller Suite:Controller Suite#[It] Slice node affinity Tests SliceNodeAffinity tests - includes creation and deletion slice node affinity creation -failed case if cluster is not participating in given slice":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":1,"unknown":0,"total":2},"items":[{"uid":"b37eb391bd39b767","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693899867000,"stop":1693899867000,"duration":0}},{"uid":"e1e1c28f0a292643","status":"passed","time":{"start":1693828311000,"stop":1693828320451,"duration":9451}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting iperf-client pod Should restart iperf-client pod":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":0,"unknown":0,"total":1},"items":[{"uid":"85b6d9648a5a074e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1709719453000,"stop":1709719453000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Namespace reconcile test scenarios: when Namespaces are labeled with kubeslice label Delete namespace deleted app ns entry should get removed from cluster objs":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"9aab9119fdd1ad0d","status":"passed","time":{"start":1709719453000,"stop":1709719464973,"duration":11973}}]},"Controller Suite:Controller Suite#[It] Slice resource quota Tests ResourceQuota creation tests ResourceQuota should be created once SliceConfig gets created":{"statistic":{"failed":1,"broken":0,"skipped":1,"passed":0,"unknown":0,"total":2},"items":[{"uid":"814790d867baceff","status":"failed","statusDetails":"Timed out after 31.040s.\nUnexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0006de288>: {\n        Underlying: <*exec.ExitError | 0xc00063a480>{\n            ProcessState: {\n                pid: 6786,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 145725},\n                    Stime: {Sec: 0, Usec: 53314},\n                    Maxrss: 43260,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2319,\n                    Majflt: 45,\n                    Nswap: 0,\n                    Inblock: 2704,\n                    Oublock: 1064,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 1533,\n                    Nivcsw: 1330,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"E0905 08:09:17.446251    6786 memcache.go:287] couldn't get resource list for projectcalico.org/v3: the server is currently unable to handle the request\",\n                        \"E0905 08:09:25.641849    6786 memcache.go:121] couldn't get resource list for projectcalico.org/v3: the server is currently unable to handle the request\",\n                        \"E0905 08:09:26.965977    6786 memcache.go:121] couldn't get resource list for projectcalico.org/v3: the server is currently unable to handle the request\",\n                        \"E0905 08:09:28.008372    6786 memcache.go:121] couldn't get resource list for projectcalico.org/v3: the server is currently unable to handle the request\",\n                        \"Error from server (NotFound): sliceresourcequotaconfigs.controller.kubeslice.io \\\"slice-blue\\\" not found\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"E0905 08:09:17.446251    6786 memcache.go:287] couldn't get resource list for projectcalico.org/v3: the server is currently unable to handle the request\",\n                    \"E0905 08:09:25.641849    6786 memcache.go:121] couldn't get resource list for projectcalico.org/v3: the server is currently unable to handle the request\",\n                    \"E0905 08:09:26.965977    6786 memcache.go:121] couldn't get resource list for projectcalico.org/v3: the server is currently unable to handle the request\",\n                    \"E0905 08:09:28.008372    6786 memcache.go:121] couldn't get resource list for projectcalico.org/v3: the server is currently unable to handle the request\",\n                    \"Error from server (NotFound): sliceresourcequotaconfigs.controller.kubeslice.io \\\"slice-blue\\\" not found\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"E0905 08:09:17.446251    6786 memcache.go:287] couldn't get resource list for projectcalico.org/v3: the server is currently unable to handle the request\",\n                        \"E0905 08:09:25.641849    6786 memcache.go:121] couldn't get resource list for projectcalico.org/v3: the server is currently unable to handle the request\",\n                        \"E0905 08:09:26.965977    6786 memcache.go:121] couldn't get resource list for projectcalico.org/v3: the server is currently unable to handle the request\",\n                        \"E0905 08:09:28.008372    6786 memcache.go:121] couldn't get resource list for projectcalico.org/v3: the server is currently unable to handle the request\",\n                        \"Error from server (NotFound): sliceresourcequotaconfigs.controller.kubeslice.io \\\"slice-blue\\\" not found\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"E0905 08:09:17.446251    6786 memcache.go:287] couldn't get resource list for projectcalico.org/v3: the serv...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; E0905 08:09:17.446251    6786 memcache.go:287] couldn't get resource list for projectcalico.org/v3: the server is currently unable to handle the request\n    E0905 08:09:25.641849    6786 memcache.go:121] couldn't get resource list for projectcalico.org/v3: the server is currently unable to handle the request\n    E0905 08:09:26.965977    6786 memcache.go:121] couldn't get resource list for projectcalico.org/v3: the server is currently unable to handle the request\n    E0905 08:09:28.008372    6786 memcache.go:121] couldn't get resource list for projectcalico.org/v3: the server is currently unable to handle the request\n    Error from server (NotFound): sliceresourcequotaconfigs.controller.kubeslice.io \"slice-blue\" not found\noccurred","time":{"start":1693899867000,"stop":1693899929330,"duration":62330}},{"uid":"14cf0243475ed3f1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Worker Suite:Worker Suite#[It] GW redundacy tests Test gw redundancy Should have GW pod running":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":0,"unknown":0,"total":1},"items":[{"uid":"43c1da442a761202","status":"skipped","statusDetails":"skipped - Spec skipped because Skip() was called in BeforeAll","time":{"start":1709719453000,"stop":1709719453000,"duration":0}}]},"Controller Suite:Controller Suite#[It] Slice node affinity Tests SliceNodeAffinity tests - includes creation and deletion slice node affinity creation -failed case if application namespaces are repeated":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":1,"unknown":0,"total":2},"items":[{"uid":"da07a5a68a8af576","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693899867000,"stop":1693899867000,"duration":0}},{"uid":"6dd744cfb286dbc2","status":"passed","time":{"start":1693828311000,"stop":1693828323228,"duration":12228}}]},"Controller Suite:Controller Suite#[It] Slice resource quota Tests ResourceQuota creation tests Create ResourceQuota fail test-namespace.limit.cpu negative":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":1,"unknown":0,"total":2},"items":[{"uid":"bda81451c04c13da","status":"passed","time":{"start":1693899867000,"stop":1693899870844,"duration":3844}},{"uid":"14fa6d7b137763fa","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice creation scenarios: when slice red is installed install slice blue workers should have gateway pods from both slices":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"d8b1fda23799fa32","status":"passed","time":{"start":1709719453000,"stop":1709719599177,"duration":146177}}]},"Worker Suite:Worker Suite#[It] Namespace reconcile test scenarios: when Namespaces are labeled with kubeslice label Offboard slice get detached from app ns in cluster objects":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"275b81f88a9d9aa4","status":"passed","time":{"start":1709719453000,"stop":1709719453385,"duration":385}}]},"VPNKeyRotation Suite:VPNKeyRotation Suite#[It] Verify vpn key rotation functionality: update sliceconfig and set renewBefore":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":3,"unknown":0,"total":3},"items":[{"uid":"869828c4d9a8df17","status":"passed","time":{"start":1710224686000,"stop":1710224686268,"duration":268}},{"uid":"ab78fd3e34521344","status":"passed","time":{"start":1709883716000,"stop":1709883716285,"duration":285}},{"uid":"8bea8bdd81762213","status":"passed","time":{"start":1709717686000,"stop":1709717686305,"duration":305}}]},"Controller Suite:Controller Suite#[It] Slice resource quota Tests ResourceQuota creation tests Create ResourceQuota fail test - namespace.DefaultLimitPerContainer.Mem > namespace.Request.Mem":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":1,"unknown":0,"total":2},"items":[{"uid":"ffd749d6e6ffcaae","status":"passed","time":{"start":1693899867000,"stop":1693899869608,"duration":2608}},{"uid":"1d3f5f8c06a3dff","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Attach/Deattach Scenario Attach cluster to a slice  when Deattach cluster from slice  should have vl3 router running on attached cluster":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"82b05e74766296c7","status":"passed","time":{"start":1709719453000,"stop":1709719453013,"duration":13}}]},"Controller Suite:Controller Suite#[It] Slice RBAC Tests RBAC Common Tests SliceRoleBinding status test: SliceRoleBinding should have error status for invalid namespace and invalid roles":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":0,"unknown":0,"total":2},"items":[{"uid":"e5e6897fe27b861a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693899867000,"stop":1693899867000,"duration":0}},{"uid":"ca9f2c898ac37111","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Controller Suite:Controller Suite#[It] Slice resource quota Tests ResourceQuota creation tests Create ResourceQuota fail test - slice.limit.CPU negative  ":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":1,"unknown":0,"total":2},"items":[{"uid":"74d21409cd7a5d8e","status":"passed","time":{"start":1693899867000,"stop":1693899876091,"duration":9091}},{"uid":"526dd8c43ebd28be","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Controller Suite:Controller Suite#[It] Slice resource quota Tests ResourceQuota creation tests Create ResourceQuota fail test - sum(namespace.limit.cpu) > slice.cpu":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":1,"unknown":0,"total":2},"items":[{"uid":"bbe4ce357cfabd38","status":"passed","time":{"start":1693899867000,"stop":1693899897067,"duration":30067}},{"uid":"1643f1e5b1cdbeda","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting vl3 pod Should restart vl3 pod":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":0,"unknown":0,"total":1},"items":[{"uid":"a3213bd63b8c07cd","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1709719453000,"stop":1709719453000,"duration":0}}]},"Hub Suite:Hub Suite#[It] Project and Cluster Events Test Events get recorded for project creation and deletion Events get recorded for project deletion Validating events for project deletion":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"ff2582d0c39b16eb","status":"passed","time":{"start":1688022039000,"stop":1688022043472,"duration":4472}}]},"Worker Suite:Worker Suite#[It] Slice Config Negative Scenario tests Slice Config Negative Scenario Creation Should fail while creating sliceconfig with invalid values on slice name":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"d688a141064dfa6","status":"passed","time":{"start":1709719453000,"stop":1709719453303,"duration":303}}]},"SliceHealth Suite:SliceHealth Suite#[It] Verify gw redundancy wrt health check when both gateways are down should have gw in error state":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":1,"unknown":0,"total":2},"items":[{"uid":"821c7760c6b23b77","status":"passed","time":{"start":1709723127000,"stop":1709723246749,"duration":119749}},{"uid":"6fc4174623ac6d9c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1688026084000,"stop":1688026084000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice Config Events Test Events get recorded for slice config creation and deletion Events get recorded for slice config deletion Validating events for worker slice gateways deletion":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"c234ffa6e9884ac5","status":"passed","time":{"start":1709719453000,"stop":1709719453163,"duration":163}}]},"Hub Suite:Hub Suite#[It] Project and Cluster Events Test Events get recorded for cluster creation and deletion Events get recorded for cluster creation Validating events for cluster role binding creation":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"7e6fbb774b4090f5","status":"passed","time":{"start":1688022039000,"stop":1688022043461,"duration":4461}}]},"SliceHealth Suite:SliceHealth Suite#[It] Testing Slice Health feature >  Verify various properties of SliceHealth object in worker slice config spec >  Should have overall slice health status Normal in workersliceconfig CR":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"8f9b72ecb9d9c566","status":"passed","time":{"start":1709723127000,"stop":1709723127235,"duration":235}}]},"Controller Suite:Controller Suite#[It] Slice node affinity Tests SliceNodeAffinity tests - includes creation and deletion slice node affinity creation -failed case if node selctor label is not among - In, NotIn, Exists, DoesNotExist, Gt and Lt.":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":1,"unknown":0,"total":2},"items":[{"uid":"5488f7ac79dd43b8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693899867000,"stop":1693899867000,"duration":0}},{"uid":"417522b485a8e668","status":"passed","time":{"start":1693828311000,"stop":1693828322276,"duration":11276}}]},"Controller Suite:Controller Suite#[It] E2E tests for license job > when controller is installed with default options should trigger a job":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":2,"unknown":0,"total":2},"items":[{"uid":"a5715bbbf59552c3","status":"passed","time":{"start":1693899867000,"stop":1693899868807,"duration":1807}},{"uid":"e6ce67fcdd04f62b","status":"passed","time":{"start":1693828311000,"stop":1693828317455,"duration":6455}}]},"Worker Suite:Worker Suite#[It]  QosProfile tests QosProfile Create validation Creates qosprofile":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"a3f95973681d8061","status":"passed","time":{"start":1709719453000,"stop":1709719454002,"duration":1002}}]},"Controller Suite:Controller Suite#[It] Slice resource quota Tests ResourceQuota creation tests Create ResourceQuota fail test - sum(cluster.limit.podcount) > slice.podcount":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":1,"unknown":0,"total":2},"items":[{"uid":"e6082a79009f985a","status":"passed","time":{"start":1693899867000,"stop":1693899881430,"duration":14430}},{"uid":"f8e89b9f87d0f951","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Hub Suite:Hub Suite#[It] Project and Cluster Events Test Events get recorded for project creation and deletion Events get recorded for project creation Validating events for namespace creation":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"2e9abc5384f69bc7","status":"passed","time":{"start":1688022039000,"stop":1688022040287,"duration":1287}}]},"Worker Suite:Worker Suite#[It] Test Slice Gateway Service config >  gatewayServiceType: LB >  should create workerSliceConfig cr with gw svc type=LB & protocol=TCP":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":0,"unknown":0,"total":1},"items":[{"uid":"e0965f7ae73a6cd3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1709719453000,"stop":1709719453000,"duration":0}}]},"Controller Suite:Controller Suite#[It] Validate License Tests License Validate tests Should revert back the fields in secret,in case someone tampered":{"statistic":{"failed":1,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":2},"items":[{"uid":"f377fd06cab27a27","status":"failed","statusDetails":"Expected\n    <*shell.ErrWithCmdOutput | 0xc0000172f0>: {\n        Underlying: <*exec.ExitError | 0xc00040f740>{\n            ProcessState: {\n                pid: 6902,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 399528},\n                    Stime: {Sec: 0, Usec: 150079},\n                    Maxrss: 107940,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 7767,\n                    Majflt: 35,\n                    Nswap: 0,\n                    Inblock: 16680,\n                    Oublock: 20624,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 2387,\n                    Nivcsw: 2485,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Warning: resource secrets/kubeslice-license-file is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically.\",\n                        \"Error from server (Conflict): error when applying patch:\",\n                        \"{\\\"data\\\":{\\\"license-created\\\":\\\"MjAyMy0wOS0wNVQwNzo0NTowNC45NTFadGFtcGVyaW5nX2l0\\\",\\\"license-type\\\":\\\"a3ViZXNsaWNlLXRyaWFsLWxpY2Vuc2U=\\\"},\\\"metadata\\\":{\\\"annotations\\\":{\\\"kubectl.kubernetes.io/last-applied-configuration\\\":\\\"{\\\\\\\"apiVersion\\\\\\\":\\\\\\\"v1\\\\\\\",\\\\\\\"data\\\\\\\":{\\\\\\\"customer-name\\\\\\\":\\\\\\\"aG90IGhhd2s=\\\\\\\",\\\\\\\"grace-period\\\\\\\":\\\\\\\"NQ==\\\\\\\",\\\\\\\"license-created\\\\\\\":\\\\\\\"MjAyMy0wOS0wNVQwNzo0NTowNC45NTFadGFtcGVyaW5nX2l0\\\\\\\",\\\\\\\"license-expiration\\\\\\\":\\\\\\\"MjAyMy0wOS0xMlQwNzo0NTowNC45NTJa\\\\\\\",\\\\\\\"license-id\\\\\\\":\\\\\\\"OGFjMWM5OWQtNmUzOS00NGE5LTk2ZGEtYzM5MWJhN2Y4NmY4\\\\\\\",\\\\\\\"license-type\\\\\\\":\\\\\\\"a3ViZXNsaWNlLXRyaWFsLWxpY2Vuc2U=\\\\\\\",\\\\\\\"license-updated\\\\\\\":\\\\\\\"MjAyMy0wOS0wNVQwNzo0NTowNS4wODha\\\\\\\",\\\\\\\"license.key\\\\\\\":\\\\\\\"a2V5L2V5SmhZMk52ZFc1MElqcDdJbWxrSWpvaU1HSTJaRFk0WVdZdE9EazROaTAwTjJJd0xUbGpPVEV0WW1aaE1HRTFNMlJoWTJNekluMHNJbkJ5YjJSMVkzUWlPbnNpYVdRaU9pSm1OVE16TkdOalpDMWxaRGszTFRSaE1qUXRPRE16Wmkxak56bGxPR0ZtTWpRMllUWWlmU3dpY0c5c2FXTjVJanA3SW1sa0lqb2laVEZtTnpSak9ERXRZV1V5WXkwMFlqQTNMVGcxTnpndE16RTFPVFU1T0RZeVpqTTVJaXdpWkhWeVlYUnBiMjRpT2pZd05EZ3dNSDBzSW5WelpYSWlPbTUxYkd3c0lteHBZMlZ1YzJVaU9uc2lhV1FpT2lJNFlXTXhZems1WkMwMlpUTTVMVFEwWVRrdE9UWmtZUzFqTXpreFltRTNaamcyWmpnaUxDSmpjbVZoZEdWa0lqb2lNakF5TXkwd09TMHdOVlF3TnpvME5Ub3dOQzQ1TlRGYUlpd2laWGh3YVhKNUlqb2lNakF5TXkwd09TMHhNbFF3TnpvME5Ub3dOQzQ1TlRKYUluMTkuNmxqSVJ2dTFWbXNoNGU2Z1YzR25WNUN5VWg1QWhBWnpLMzJZYnpNRGstZFlOT1c2WDluVlU2SEdxNkVOaVpWR1RBLWhMR2VOREZ6VnYzU1pjSGtFQ1E9PQ==\\\\\\\",\\\\\\\"machine.file\\\\\\\":\\\\\\\"LS0tLS1CRUdJTiBNQUNISU5FIEZJTEUtLS0tLQpleUpsYm1NaU9pSlhhbEZVY0N0aFJrWTBSVFZHWTI4NVRXdzBaVE5FZVVwWlRIUXpaM1IzTXpkbWVGRXIKUVU1TlpXWkVTMjAzZDFwaE1HcFlaM1ZUTjFSb1lrbHZhbUZwZW1oWlZVUkZaRmwxYWl0R1ZGcG5NVFJsCmFHSkZLMEpKYjJOaVRubzFRVWhIYnpOaWEyOVZOMFZ3U0V0bGRFRXpORW8wWTIxMlVVUjNOVlpGWjFSVgpTM0JVWkhjclNIcFZUMWhHVFVVMmRtNXFWMHRyV1d0SFpWazNZbkp0TW01cVprTjNVVWRXUWxSdlNreDQKYkZoWE9IVmhhMHhaV0ZweVREWm1iV1JPZW5seVZqQjVXVTl3TUZKcVF6Um1VM0paYnpsaVFtWnBWVU5FClNIQTNURFpZWW0wcmRXWmpTMEUwU0hkd2RIQlZVaTlFWlVvelRGSXdTRXN3UTNWNU56SnlVMDFSZFdSTQpUbWxHUVVwbGVHZFpjR1pKZWxNMlIxUlpSM05TZDNkcU1qTkxVVUozVG5KU09XeHhRbXRRY0hrM1pFa3YKYldoeVkxaHZiRFpCVEU5a1pUZENVelpTVGpCMU1YZElXV1prUW1GNWVIVkljR1ZPTkhBd1YwSktTa3RPClZtdHJSbXd3YWxaVVoyTkhPV0ZWT0Uxb2VsUXJiWE0xTlhCMmJrMDFLMWhGZFRsNGQzUnRVa0Z2ZVV4UgpjRUo1VW5CNlRXZzVhWG8xYUM5UWFscE1lR3g2TVhkRGRtNHlXV053VWpSSlZFMURjWGd4YUN0Nk0yWmoKV0RsUlZrRlVXR2g1Y1RKNVkxUlNNbWx5U2tWbGFGUXhiSG8xZDBaRVkwSnFWRlU0V1dkRlNESnBNSFZ1CmJ6azRhekZDVVdrNWMwSnlRbWhoYzNSck56QjZaMjV5TjBRNVUzVmxPVVZoVVhsQlMyRTJUamRzYUhKeApjMHQ1WjB0eGVrWkxPVzh6V0ZCNmRGVjNVbTl1YjJWdWFGQldOMFl4UVVOUU1FaE1hRzV4Y1ZsMlkyRnEKVT...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\nto be nil","time":{"start":1693899867000,"stop":1693899908198,"duration":41198}},{"uid":"66a2018d4e8853bf","status":"passed","time":{"start":1693828311000,"stop":1693828323265,"duration":12265}}]},"SliceHealth Suite:SliceHealth Suite#[It] Verify gw redundancy wrt health check when slice is installed should eventually have slicehealth status normal":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":2,"unknown":0,"total":2},"items":[{"uid":"262ca8951854a9f3","status":"passed","time":{"start":1709723127000,"stop":1709723249414,"duration":122414}},{"uid":"e2272610a02e3389","status":"passed","time":{"start":1688026084000,"stop":1688026206536,"duration":122536}}]},"Controller Suite:Controller Suite#[It] Events Test Events get recorded for slice node affinity creation and deletion Events get recorded for slice node affinity deletion Validating events for worker slice node affinities deletion":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":0,"unknown":0,"total":2},"items":[{"uid":"931a95e010872f68","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693899867000,"stop":1693899867000,"duration":0}},{"uid":"7257dcce86dfcd46","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Hub Suite:Hub Suite#[It] Project and Cluster Events Test Events get recorded for project creation and deletion Events get recorded for project creation Validating events for cluster role creation":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"f776d2c86c0d0f74","status":"passed","time":{"start":1688022039000,"stop":1688022039075,"duration":75}}]},"Controller Suite:Controller Suite#[It] Events Test Events get recorded for slice role binding creation and deletion Events get recorded for slice role binding creation Validating events for setting slice config as owner of slice role binding":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":0,"unknown":0,"total":2},"items":[{"uid":"a9ca912ba0070e06","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693899867000,"stop":1693899867000,"duration":0}},{"uid":"96845eb732ce62ad","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Controller Suite:Controller Suite#[It] Slice RBAC Tests RBAC Common Tests WorkerSliceRoleBinding generation test: WorkerSliceRoleBinding with custom role should be generated automatically when forcefully deleted":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":0,"unknown":0,"total":2},"items":[{"uid":"90ee812f0d34f8f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693899867000,"stop":1693899867000,"duration":0}},{"uid":"141e6bb6c75c7461","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Controller Suite:Controller Suite#[It] Slice resource quota Tests ResourceQuota creation tests Create ResourceQuota fail test - sum(namespace.limit.mem) > cluster.mem":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":1,"unknown":0,"total":2},"items":[{"uid":"a6bf7578899fcd10","status":"passed","time":{"start":1693899867000,"stop":1693899875872,"duration":8872}},{"uid":"5e60796835100ede","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice Config Events Test Events get recorded for slice config creation and deletion Events get recorded for slice config creation Validating events for worker slice gateways creation":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"bafd08110f6e1976","status":"passed","time":{"start":1709719453000,"stop":1709719453350,"duration":350}}]},"Controller Suite:Controller Suite#[It] Slice RBAC Tests SliceRoleBinding Tests Create SliceRoleBinding failure test: Creating SliceRoleBinding slice-red with empty RoleRef Name":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":0,"unknown":0,"total":2},"items":[{"uid":"9def1368aba90524","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693899867000,"stop":1693899867000,"duration":0}},{"uid":"984a69b51aff008f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Controller Suite:Controller Suite#[It] Slice RBAC Tests SliceRoleBinding Tests Create SliceRoleBinding failure test: Creating SliceRoleBinding slice-red with empty subject":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":0,"unknown":0,"total":2},"items":[{"uid":"5e7f3a3efb91b54a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693899867000,"stop":1693899867000,"duration":0}},{"uid":"de95dee3e865ffa3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Hub Suite:Hub Suite#[It] Project and Cluster Events Test Events get recorded for project creation and deletion Events get recorded for project deletion Validating events for namespace deletion":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"f5edc60a22f99f6d","status":"passed","time":{"start":1688022039000,"stop":1688022039066,"duration":66}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests slice with netpol Should contain application namespaces":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"5744d2340f345abd","status":"passed","time":{"start":1709719453000,"stop":1709719453334,"duration":334}}]},"Intracluster Suite:Intracluster Suite#[It] Intracluster slice tests Iperf connectivity Verify Iperf traffic":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"7f4c01e3b8611ef1","status":"passed","time":{"start":1688025155000,"stop":1688025223538,"duration":68538}}]},"Controller Suite:Controller Suite#[It] Slice resource quota Tests ResourceQuota creation tests Create ResourceQuota fail test-cluster.limit.mem negative":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":1,"unknown":0,"total":2},"items":[{"uid":"405527d95309003e","status":"passed","time":{"start":1693899867000,"stop":1693899870199,"duration":3199}},{"uid":"61f31b3e095ee79e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice Config Negative Scenario tests Slice Config Negative Scenario Update Should fail while updating sliceconfig with invalid values on slice subnet":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"b49f0b6d2e275e4f","status":"passed","time":{"start":1709719453000,"stop":1709719453667,"duration":667}}]},"Controller Suite:Controller Suite#[It] Slice resource quota Tests ResourceQuota creation tests Create ResourceQuota fail test - cluster.limit.podcount > sum(namespace.limit.podcount)":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":1,"unknown":0,"total":2},"items":[{"uid":"93d8267089dc188f","status":"passed","time":{"start":1693899867000,"stop":1693899913258,"duration":46258}},{"uid":"25395bea57958267","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Hub Suite:Hub Suite#[It] Project and Cluster Events Test Events get recorded for project creation and deletion Events get recorded for project creation Validating events for role creation":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"edeac29fbdf55e7f","status":"passed","time":{"start":1688022039000,"stop":1688022039128,"duration":128}}]},"Worker Suite:Worker Suite#[It] Edit NodeIP tests for cluster CR,  Edit NodeIP validation,  Edit node IP in cluster spec":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"bcb0d44a35bd3bae","status":"passed","time":{"start":1709719453000,"stop":1709719505630,"duration":52630}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests Iperf App connectivity Should install iperf-sleep on client cluster":{"statistic":{"failed":1,"broken":0,"skipped":0,"passed":0,"unknown":0,"total":1},"items":[{"uid":"20b7057343b29bb5","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000b34030>: {\n        Underlying: <*exec.ExitError | 0xc0000b5a60>{\n            ProcessState: {\n                pid: 11146,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 197383},\n                    Stime: {Sec: 0, Usec: 51319},\n                    Maxrss: 111868,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 18582,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 19200,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 494,\n                    Nivcsw: 419,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/e2e/assets/application/netpol/iperf-client.yaml\\\": Internal error occurred: failed calling webhook \\\"webhook.kubeslice.io\\\": failed to call webhook: Post \\\"https://kubeslice-webhook-service.kubeslice-system.svc:443/mutate-webhook?timeout=10s\\\": EOF\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/e2e/assets/application/netpol/iperf-client.yaml\\\": Internal error occurred: failed calling webhook \\\"webhook.kubeslice.io\\\": failed to call webhook: Post \\\"https://kubeslice-webhook-service.kubeslice-system.svc:443/mutate-webhook?timeout=10s\\\": EOF\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/e2e/assets/application/netpol/iperf-client.yaml\\\": Internal error occurred: failed calling webhook \\\"webhook.kubeslice.io\\\": failed to call webhook: Post \\\"https://kubeslice-webhook-service.kubeslice-system.svc:443/mutate-webhook?timeout=10s\\\": EOF\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/e2e/assets/application/netpol/iperf-client.yaml\\\": Internal error occurred: failed calling webhook \\\"webhook.kubeslice.io\\\": failed to call webhook: Post \\\"https://kubeslice-webhook-service.kubeslice-system.svc:443/mutate-webhook?timeout=10s\\\": EOF\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/e2e/assets/application/netpol/iperf-client.yaml\": Internal error occurred: failed calling webhook \"webhook.kubeslice.io\": failed to call webhook: Post \"https://kubeslice-webhook-service.kubeslice-system.svc:443/mutate-webhook?timeout=10s\": EOF\noccurred","time":{"start":1709719453000,"stop":1709719466201,"duration":13201}}]},"Worker Suite:Worker Suite#[It] Test Slice Gateway Service config >  default sliceGatewaySvcType >  verify l3 traffic is not working when gw svc type=NodePort":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"d1103e633c9aeaae","status":"passed","time":{"start":1709719453000,"stop":1709719453714,"duration":714}}]},"Controller Suite:Controller Suite#[It] Events Test Events get recorded for slice node affinity creation and deletion Events get recorded for slice node affinity deletion Validating events for slice node affinity deletion":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":0,"unknown":0,"total":2},"items":[{"uid":"45fbe7a0ead1f23f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693899867000,"stop":1693899867000,"duration":0}},{"uid":"36c25b0df47ec152","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Test Slice Gateway Service config >  default sliceGatewaySvcType >  should create slice cr with default gw svc type & protocol":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"e83b18074afda9a4","status":"passed","time":{"start":1709719453000,"stop":1709719453116,"duration":116}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests slice with netpol should contain the allowed namespaces":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"30474f871f6ff68","status":"passed","time":{"start":1709719453000,"stop":1709719453180,"duration":180}}]},"Controller Suite:Controller Suite#[It] Slice RBAC Tests SliceRoleBinding Tests Create SliceRoleBinding successful test: Creating SliceRoleBinding slice-red with Kubernetes Roles":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":0,"unknown":0,"total":2},"items":[{"uid":"162ccc833dfdcddd","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693899867000,"stop":1693899867000,"duration":0}},{"uid":"2cca47ea298a2683","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Controller Suite:Controller Suite#[It] Slice RBAC Tests SliceRoleBinding Tests Create SliceRoleBinding failure test: Creating SliceRoleBinding slice-red with invalid ApiVersion in Role":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":0,"unknown":0,"total":2},"items":[{"uid":"90435048dfbdc1f1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693899867000,"stop":1693899867000,"duration":0}},{"uid":"d1f1f2eed57beef6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Should have application namespaces created":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"c59555ac96644506","status":"passed","time":{"start":1709719453000,"stop":1709719455624,"duration":2624}}]},"Controller Suite:Controller Suite#[It] Slice resource quota Tests ResourceQuota creation tests Create ResourceQuota fail test":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":1,"unknown":0,"total":2},"items":[{"uid":"3e964d6041e177b8","status":"passed","time":{"start":1693899867000,"stop":1693899917230,"duration":50230}},{"uid":"ee0b519660c1149c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Creation Should create Multiple Projects in controller using valid manifest":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"d5ab998bd8be6bac","status":"passed","time":{"start":1688022039000,"stop":1688022046883,"duration":7883}}]},"Worker Suite:Worker Suite#[It] Slice Config Negative Scenario tests Slice Config Negative Scenario Creation Should fail while creating sliceconfig with invalid values on slice subnet":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"8b92aab6c46926cc","status":"passed","time":{"start":1709719453000,"stop":1709719453172,"duration":172}}]},"Controller Suite:Controller Suite#[It] Slice resource quota Tests ResourceQuota creation tests Create ResourceQuota fail test - sum(namespace.request.EphemeralStorage) > slice.request.EphemeralStorage":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":1,"unknown":0,"total":2},"items":[{"uid":"6e86997a39edcdd4","status":"passed","time":{"start":1693899867000,"stop":1693899870417,"duration":3417}},{"uid":"5632863f520105fa","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"SliceHealth Suite:SliceHealth Suite#[It] Testing Slice Health feature >  Verify various properties of SliceHealth object in worker slice config spec >  Should not have slice ingress status when ingress is unavailable":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"5d68115ebf0bc9e8","status":"passed","time":{"start":1709723127000,"stop":1709723244757,"duration":117757}}]},"Worker Suite:Worker Suite#[It] Namespace reconcile test scenarios: when Namespaces are labeled with kubeslice label Offboard remove label from app ns":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"cecbc8edb2c70b55","status":"passed","time":{"start":1709719453000,"stop":1709719453327,"duration":327}}]},"Controller Suite:Controller Suite#[It] E2E tests for license job > when Controller is upgraded license mode = manual should not launch a new license job":{"statistic":{"failed":1,"broken":0,"skipped":1,"passed":0,"unknown":0,"total":2},"items":[{"uid":"437500ede89e434c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693899867000,"stop":1693899867000,"duration":0}},{"uid":"4fc1e94988e259b0","status":"failed","statusDetails":"Timed out after 200.002s.\nExpected\n    <int>: 1\nto be zero-valued","time":{"start":1693828311000,"stop":1693828669599,"duration":358599}}]},"Controller Suite:Controller Suite#[It] Slice node affinity Tests SliceNodeAffinity tests - includes creation and deletion slice node affinity creation -failed case if owner sliceconfig is not present":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":1,"unknown":0,"total":2},"items":[{"uid":"78cf324b5bfd3fad","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693899867000,"stop":1693899867000,"duration":0}},{"uid":"4cbf97ff1ade5a62","status":"passed","time":{"start":1693828311000,"stop":1693828314605,"duration":3605}}]},"Intracluster Suite:Intracluster Suite#[It] Intracluster slice tests Slice with netpol should reconcile netpol in app NS":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"67fdae79f6020848","status":"passed","time":{"start":1688025155000,"stop":1688025155138,"duration":138}}]},"Worker Suite:Worker Suite#[It] Slice Config Events Test Events get recorded for service export creation and deletion Events get recorded for service export creation Validating events for worker service imports creation":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"94b14928e26f31d8","status":"passed","time":{"start":1709719453000,"stop":1709719456927,"duration":3927}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Deletion Should Delete an existing project successfully":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"c185478d50b7a6cd","status":"passed","time":{"start":1688022039000,"stop":1688022043737,"duration":4737}}]},"Controller Suite:Controller Suite#[It] Slice RBAC Tests RBAC Common Tests WorkerSliceRoleBinding generation test: Creation of SliceRoleBinding with k8s roles should generate WorkerSliceRoleBinding":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":0,"unknown":0,"total":2},"items":[{"uid":"3678f4d42793964f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693899867000,"stop":1693899867000,"duration":0}},{"uid":"297160d379ae3c9b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Worker Suite:Worker Suite#[AfterSuite]":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":7,"unknown":0,"total":7},"items":[{"uid":"1d7f9f675ce6c0d6","status":"passed","time":{"start":1711007881000,"stop":1711007882150,"duration":1150}},{"uid":"baacf6fa4e1a278d","status":"passed","time":{"start":1710850779000,"stop":1710850779416,"duration":416}},{"uid":"27b23c3afe890d69","status":"passed","time":{"start":1710825513000,"stop":1710825513943,"duration":943}},{"uid":"a35b4888a22ba69b","status":"passed","time":{"start":1710227819000,"stop":1710228454400,"duration":635400}},{"uid":"eece6058281c4781","status":"passed","time":{"start":1709886756000,"stop":1709887391639,"duration":635639}},{"uid":"ffd2dcf0ad920860","status":"passed","time":{"start":1709719453000,"stop":1709720699263,"duration":1246263}},{"uid":"504dbb660704473e","status":"passed","time":{"start":1688018439000,"stop":1688018439339,"duration":339}}]},"Worker Suite:Worker Suite#[It]  QosProfile tests QosProfile delete validation Delete qos profile - negative scenario":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"e04bcc4481cb27b2","status":"passed","time":{"start":1709719453000,"stop":1709719454958,"duration":1958}}]},"Controller Suite:Controller Suite#[It] Slice resource quota Tests ResourceQuota creation tests Create ResourceQuota fail test - namespace.DefaultRequestPerContainer.ES is negative":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":1,"unknown":0,"total":2},"items":[{"uid":"96fe860438aa43be","status":"passed","time":{"start":1693899867000,"stop":1693899870170,"duration":3170}},{"uid":"5a420b21dda49689","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"SliceHealth Suite:SliceHealth Suite#[It] Verify gw redundancy wrt health check when both gateways are down should have tunnel status in error state":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":1,"unknown":0,"total":2},"items":[{"uid":"4ac11a46d46ca46","status":"passed","time":{"start":1709723127000,"stop":1709723127149,"duration":149}},{"uid":"b0b095a62705926c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1688026084000,"stop":1688026084000,"duration":0}}]},"Controller Suite:Controller Suite#[It] Slice resource quota Tests ResourceQuota creation tests Create ResourceQuota fail test - sum(cluster.limit.CPU) > slice.CPU":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":1,"unknown":0,"total":2},"items":[{"uid":"351daa24c612ad7b","status":"passed","time":{"start":1693899867000,"stop":1693899878996,"duration":11996}},{"uid":"23bab09d1e2799a4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"SliceHealth Suite:SliceHealth Suite#[It] Verify gw redundancy wrt health check when atleast one gateway is down should have gw health in warning state":{"statistic":{"failed":1,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":2},"items":[{"uid":"104d9f9001a2a175","status":"passed","time":{"start":1709723127000,"stop":1709723246887,"duration":119887}},{"uid":"2bfefdbdf57455f6","status":"failed","statusDetails":"Timed out after 720.005s.\nExpected\n    <bool>: false\nto be true","time":{"start":1688026084000,"stop":1688026807757,"duration":723757}}]},"Controller Suite:Controller Suite#[It] Slice resource quota Tests ResourceQuota creation tests Create ResourceQuota fail test - namespace.DefaultLimitPerContainer.EphemeralStorage > namespace.Limit.EphemeralStorage":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":1,"unknown":0,"total":2},"items":[{"uid":"4a8ce0a36a2d275b","status":"passed","time":{"start":1693899867000,"stop":1693899873953,"duration":6953}},{"uid":"6a9d56b0616e55b8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"SliceHealth Suite:SliceHealth Suite#[It] Verify gw redundancy wrt health check when both gateways are up should have gw in normal state":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":1,"unknown":0,"total":2},"items":[{"uid":"21577a29c5eeafc6","status":"passed","time":{"start":1709723127000,"stop":1709723247059,"duration":120059}},{"uid":"17abb4d4e3be499e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1688026084000,"stop":1688026084000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice creation scenarios: when slice red is installed workers should get attached to slice":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"802a328e5bdee4a9","status":"passed","time":{"start":1709719453000,"stop":1709719456368,"duration":3368}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests slice with netpol Should have application namespaces created":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"487910c79b31cf3","status":"passed","time":{"start":1709719453000,"stop":1709719455835,"duration":2835}}]},"SliceHealth Suite:SliceHealth Suite#[It] Verify gw redundancy wrt health check when both gateways are up should have tunnel status in normal state":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":1,"unknown":0,"total":2},"items":[{"uid":"9cefc0e6d7207033","status":"passed","time":{"start":1709723127000,"stop":1709723127914,"duration":914}},{"uid":"ec999425d007f8f1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1688026084000,"stop":1688026084000,"duration":0}}]},"Controller Suite:Controller Suite#[It] Slice resource quota Tests ResourceQuota creation tests Create ResourceQuota fail test- Request.Mem > Limit.Mem at cluster level":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":1,"unknown":0,"total":2},"items":[{"uid":"c662fb78de0cc36b","status":"passed","time":{"start":1693899867000,"stop":1693899911250,"duration":44250}},{"uid":"562f2a3b10b1c9c9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"VPNKeyRotation Suite:VPNKeyRotation Suite#[It] Verify vpn key rotation functionality: should run iperf":{"statistic":{"failed":1,"broken":0,"skipped":0,"passed":2,"unknown":0,"total":3},"items":[{"uid":"859c44d2d739c532","status":"passed","time":{"start":1710224686000,"stop":1710224779817,"duration":93817}},{"uid":"4e3ea22a35cef4cd","status":"passed","time":{"start":1709883716000,"stop":1709883810186,"duration":94186}},{"uid":"d115142ec17eb4a1","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000b68240>: {\n        Underlying: <*exec.ExitError | 0xc000046380>{\n            ProcessState: {\n                pid: 9040,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 180682},\n                    Stime: {Sec: 0, Usec: 43363},\n                    Maxrss: 111744,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 9623,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 1024,\n                    Oublock: 19552,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 654,\n                    Nivcsw: 531,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: [\n                    \"serviceexport.networking.kubeslice.io/iperf-server created\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"serviceexport.networking.kubeslice.io/iperf-server created\",\n                        \"Error from server (InternalError): error when creating \\\"/e2e/assets/applications/iperf-server-vpn.yaml\\\": Internal error occurred: failed calling webhook \\\"webhook.kubeslice.io\\\": failed to call webhook: Post \\\"https://kubeslice-webhook-service.kubeslice-system.svc:443/mutate-webhook?timeout=10s\\\": EOF\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/e2e/assets/applications/iperf-server-vpn.yaml\\\": Internal error occurred: failed calling webhook \\\"webhook.kubeslice.io\\\": failed to call webhook: Post \\\"https://kubeslice-webhook-service.kubeslice-system.svc:443/mutate-webhook?timeout=10s\\\": EOF\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"serviceexport.networking.kubeslice.io/iperf-server created\",\n                        \"Error from server (InternalError): error when creating \\\"/e2e/assets/applications/iperf-server-vpn.yaml\\\": Internal error occurred: failed calling webhook \\\"webhook.kubeslice.io\\\": failed to call webhook: Post \\\"https://kubeslice-webhook-service.kubeslice-system.svc:443/mutate-webhook?timeout=10s\\\": EOF\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"serviceexport.networking.kubeslice.io/iperf-server created\",\n                    \"Error from server (InternalError): error when creating \\\"/e2e/assets/applications/iperf-server-vpn.yaml\\\": Internal error occurred: failed calling webhook \\\"webhook.kubeslice.io\\\": failed to call webhook: Post \\\"https://kubeslice-webhook-service.kubeslice-system.svc:443/mutate-webhook?timeout=10s\\\": EOF\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/e2e/assets/applications/iperf-server-vpn.yaml\": Internal error occurred: failed calling webhook \"webhook.kubeslice.io\": failed to call webhook: Post \"https://kubeslice-webhook-service.kubeslice-system.svc:443/mutate-webhook?timeout=10s\": EOF\noccurred","time":{"start":1709717686000,"stop":1709717688766,"duration":2766}}]},"Controller Suite:Controller Suite#[It] Slice RBAC Tests SliceRoleBinding Tests Create SliceRoleBinding successful test: Checking Owner Reference set to SliceConfig":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":0,"unknown":0,"total":2},"items":[{"uid":"13e3f229a7851e60","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693899867000,"stop":1693899867000,"duration":0}},{"uid":"6a355641d87da0a8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Hub Suite:Hub Suite#[It] Project and Cluster Events Test Events get recorded for cluster creation and deletion Events get recorded for cluster deletion Validating events for cluster deletion":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"1d961e876d295d6a","status":"passed","time":{"start":1688022039000,"stop":1688022040711,"duration":1711}}]},"Controller Suite:Controller Suite#[It] Slice resource quota Tests ResourceQuota creation tests Create ResourceQuota fail test - sum(namespace.request.mem) > slice.mem":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":1,"unknown":0,"total":2},"items":[{"uid":"dfbdeb18d7b16e43","status":"passed","time":{"start":1693899867000,"stop":1693899873235,"duration":6235}},{"uid":"19e9d49cf848d37c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Controller Suite:Controller Suite#[It] Slice RBAC Tests SliceRoleTemplate Tests Create SliceRoleTempalate failure test: Creating SliceRoleTempalate read-only-role, missing apiGroups field":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":1,"unknown":0,"total":2},"items":[{"uid":"dc4a4c0b44fc0112","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693899867000,"stop":1693899867000,"duration":0}},{"uid":"c63181013188cf8f","status":"passed","time":{"start":1693828311000,"stop":1693828317889,"duration":6889}}]},"Controller Suite:Controller Suite#[It] Slice RBAC Tests SliceRoleBinding Tests Create SliceRoleBinding failure test: Creating SliceRoleBinding slice-red with invalid RoleRef Name":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":0,"unknown":0,"total":2},"items":[{"uid":"32d168ceb82a7d32","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693899867000,"stop":1693899867000,"duration":0}},{"uid":"9800d3e812341212","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice Creation scenarios Slice with LOW QOS should reconcile low qos slice":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"cef17095fc0e194f","status":"passed","time":{"start":1709719453000,"stop":1709719453323,"duration":323}}]},"Controller Suite:Controller Suite#[It] Slice resource quota Tests ResourceQuota creation tests Create ResourceQuota fail test - sum(namespace.request.mem) > cluster.mem":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":1,"unknown":0,"total":2},"items":[{"uid":"5835724baf8d598b","status":"passed","time":{"start":1693899867000,"stop":1693899881399,"duration":14399}},{"uid":"b091501ac9525a0d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Controller Suite:Controller Suite#[It] Slice resource quota Tests ResourceQuota creation tests Create ResourceQuota fail test - namespace.DefaultRequestPerContainer.CPU > namespace.Request.CPU":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":1,"unknown":0,"total":2},"items":[{"uid":"b8889674ff6c893e","status":"passed","time":{"start":1693899867000,"stop":1693899876715,"duration":9715}},{"uid":"16ff371d3a8d2406","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Controller Suite:Controller Suite#[It] E2E tests for license job > when Controller is upgraded license mode = manual apply helm upgrade":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":1,"unknown":0,"total":2},"items":[{"uid":"221e3e40917722ac","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693899867000,"stop":1693899867000,"duration":0}},{"uid":"3073219b362bf503","status":"passed","time":{"start":1693828311000,"stop":1693828427732,"duration":116732}}]},"Hub Suite:Hub Suite#[It] Project and Cluster Events Test Events get recorded for cluster creation and deletion Events get recorded for cluster creation Validating events for cluster service account creation":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"d078a813a0ab6580","status":"passed","time":{"start":1688022039000,"stop":1688022040519,"duration":1519}}]},"Worker Suite:Worker Suite#[It] Test Slice Gateway Service config >  gatewayServiceType: LB >  Verify load balancer creation":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":0,"unknown":0,"total":1},"items":[{"uid":"296f169cdf5ccd99","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1709719453000,"stop":1709719453000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Edit NodeIP tests for cluster CR,  Edit NodeIP validation,  Test nodeip validity >  Verify Events - should have ClusterNodeIpAutoDetected event in auto-detection":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"a948f36974ab7423","status":"passed","time":{"start":1709719453000,"stop":1709719453718,"duration":718}}]},"Worker Suite:Worker Suite#[It] NodeRestartSlice Testing node restart should re-establish connection on node restart":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":0,"unknown":0,"total":1},"items":[{"uid":"ad2c1e2df32b3207","status":"skipped","statusDetails":"pending","time":{"start":1709719453000,"stop":1709719453000,"duration":0}}]},"Worker Suite:Worker Suite#[It]  QosProfile tests QosProfile delete validation should remove gateway pod from spoke":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"a607c974b38393de","status":"passed","time":{"start":1709719453000,"stop":1709719453004,"duration":4}}]},"SliceHealth Suite:SliceHealth Suite#[It] Testing Slice Health feature >  Verify various properties of SliceHealth object in worker slice config spec >  Should have slice slicerouter status Error when slicerouter is unavailable":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"f10da73a75a18385","status":"passed","time":{"start":1709723127000,"stop":1709723252810,"duration":125810}}]},"Worker Suite:Worker Suite#[It] Slice Config Events Test Events get recorded for slice qos config deletion Validating events for slice qos config deletion":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"cb7cff6bd5c7be83","status":"passed","time":{"start":1709719453000,"stop":1709719453762,"duration":762}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests slice with netpol should have gateway pod running":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"e04ae473545fc866","status":"passed","time":{"start":1709719453000,"stop":1709719621540,"duration":168540}}]},"Intracluster Suite:Intracluster Suite#[It] Intracluster slice tests Iperf connectivity Verify Connectivity between iperf-server and iperf-client":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"2fe1a9b41bfc6310","status":"passed","time":{"start":1688025155000,"stop":1688025161670,"duration":6670}}]},"VPNKeyRotation Suite:VPNKeyRotation Suite#[It] Verify vpn key rotation functionality: should update certificate creation and expiry TS once all jobs are over":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":3,"unknown":0,"total":3},"items":[{"uid":"6e77166744f40990","status":"passed","time":{"start":1710224686000,"stop":1710224686102,"duration":102}},{"uid":"4f506e509955dcb5","status":"passed","time":{"start":1709883716000,"stop":1709883719349,"duration":3349}},{"uid":"10a1b0336470d64b","status":"passed","time":{"start":1709717686000,"stop":1709717686096,"duration":96}}]},"Worker Suite:Worker Suite#[It] Slice Config Negative Scenario tests Slice Config Negative Scenario Update Should fail while updating sliceconfig with with invalid DSCP class in qosprofiledetails":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"eeb175d9e71a57ce","status":"passed","time":{"start":1709719453000,"stop":1709719453748,"duration":748}}]},"Worker Suite:Worker Suite#[It] mutation webhook tests Testing mutation webhook functionality after namespace deboard Should remove labels and annotaions from pod spec which will remove sidecars from statefulsets":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":0,"unknown":0,"total":1},"items":[{"uid":"e39702aea9654671","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1709719453000,"stop":1709719453000,"duration":0}}]},"SliceHealth Suite:SliceHealth Suite#[AfterSuite]":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":11,"unknown":0,"total":11},"items":[{"uid":"41ed4989a7fc1396","status":"passed","time":{"start":1711007884000,"stop":1711007884371,"duration":371}},{"uid":"56948e63c8ca6e42","status":"passed","time":{"start":1710850775000,"stop":1710850775925,"duration":925}},{"uid":"f947a74de2fcd3ab","status":"passed","time":{"start":1710825518000,"stop":1710825518439,"duration":439}},{"uid":"4d56b171e1f61e71","status":"passed","time":{"start":1710226523000,"stop":1710227158065,"duration":635065}},{"uid":"4604491fa78ccdcb","status":"passed","time":{"start":1709885462000,"stop":1709886096477,"duration":634477}},{"uid":"487f67ff256f3f2","status":"passed","time":{"start":1709723127000,"stop":1709724408503,"duration":1281503}},{"uid":"282cbebb69eb56cc","status":"passed","time":{"start":1695807042000,"stop":1695807342770,"duration":300770}},{"uid":"e2c5662393ec82b2","status":"passed","time":{"start":1689751363000,"stop":1689751663695,"duration":300695}},{"uid":"1b5884fae2692ec5","status":"passed","time":{"start":1688628155000,"stop":1688628455676,"duration":300676}},{"uid":"11093e3bbac876a3","status":"passed","time":{"start":1688026084000,"stop":1688027359364,"duration":1275364}},{"uid":"64d1f57a0920b48d","status":"passed","time":{"start":1688018435000,"stop":1688018435448,"duration":448}}]},"Worker Suite:Worker Suite#[It] mutation webhook tests Testing mutation webhook functionality after namespace deboard Should remove labels and annotaions from pod spec which will remove sidecars from daemonsets":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":0,"unknown":0,"total":1},"items":[{"uid":"83f13b0f1cadd538","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1709719453000,"stop":1709719453000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Hub netpol scenarios: Slice Configs Netpol scenarios Should fail when deploying with invalid clusters in both applicationNamespace and allowedNamespace":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"e1689e6c9a760417","status":"passed","time":{"start":1709719453000,"stop":1709719453867,"duration":867}}]},"Hub Suite:Hub Suite#[It] Project and Cluster Events Test Events get recorded for project creation and deletion Events get recorded for project creation Validating events for service account creation":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"84eceefdc700e89d","status":"passed","time":{"start":1688022039000,"stop":1688022039075,"duration":75}}]},"Controller Suite:Controller Suite#[It] Slice RBAC Tests SliceRoleBinding Tests Create SliceRoleBinding failure test: Creating SliceRoleBinding slice-red with invalid subject name":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":0,"unknown":0,"total":2},"items":[{"uid":"7dc715fc76ba614","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693899867000,"stop":1693899867000,"duration":0}},{"uid":"1ca999ae3f621575","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Controller Suite:Controller Suite#[It] Slice resource quota Tests ResourceQuota creation tests Create ResourceQuota fail test - sum(cluster.limit.mem) > slice.mem":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":1,"unknown":0,"total":2},"items":[{"uid":"67476be4be3fe504","status":"passed","time":{"start":1693899867000,"stop":1693899872172,"duration":5172}},{"uid":"9b4dab4627e1cd7d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice Config Events Test Events get recorded for slice config creation and deletion Events get recorded for slice config deletion Validating events for secrets deletion":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"16ad59b5d9a6428b","status":"passed","time":{"start":1709719453000,"stop":1709719453211,"duration":211}}]},"Worker Suite:Worker Suite#[It] Namespace reconcile test scenarios: when Namespaces are labeled with kubeslice label Delete namespace deboarded app ns gets deleted":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"36cdd00e62e51674","status":"passed","time":{"start":1709719453000,"stop":1709719463916,"duration":10916}}]},"Worker Suite:Worker Suite#[It] Test Slice Gateway Service config >  default sliceGatewaySvcType >  verify slice healthy":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"ae7c4503b0d44d2","status":"passed","time":{"start":1709719453000,"stop":1709719575325,"duration":122325}}]},"Worker Suite:Worker Suite#[It] Slice Config Events Test Events get recorded for slice config creation and deletion Events get recorded for slice config creation Validating events for worker slice configs creation":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"6ac5c223e6b40b30","status":"passed","time":{"start":1709719453000,"stop":1709719454148,"duration":1148}}]},"Worker Suite:Worker Suite#[It] Hub netpol scenarios: Slice Configs Netpol scenarios Should remove successfully while Deleting Slice after removing the applicationNamespace in namespaceIsolationProfile part in slice configs":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"9dafa548a463d435","status":"passed","time":{"start":1709719453000,"stop":1709719454694,"duration":1694}}]},"Controller Suite:Controller Suite#[It] Slice resource quota Tests ResourceQuota creation tests Create ResourceQuota successful test":{"statistic":{"failed":1,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":2},"items":[{"uid":"a5aa40b941cb5058","status":"passed","time":{"start":1693899867000,"stop":1693899880229,"duration":13229}},{"uid":"ec22768c66be932f","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000906018>: {\n        Underlying: <*exec.ExitError | 0xc000622040>{\n            ProcessState: {\n                pid: 6553,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 419542},\n                    Stime: {Sec: 0, Usec: 183073},\n                    Maxrss: 118684,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 6549,\n                    Majflt: 172,\n                    Nswap: 0,\n                    Inblock: 30728,\n                    Oublock: 20624,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 1893,\n                    Nivcsw: 2810,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Slice%20resource%20quota%20Tests%20ResourceQuota%20creation%20tests%20Create%20ResourceQuota%20successful%20test877885790\\\": Internal error occurred: failed calling webhook \\\"msliceconfig.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-sliceconfig?timeout=10s\\\": dial tcp 10.96.60.138:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Slice%20resource%20quota%20Tests%20ResourceQuota%20creation%20tests%20Create%20ResourceQuota%20successful%20test877885790\\\": Internal error occurred: failed calling webhook \\\"msliceconfig.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-sliceconfig?timeout=10s\\\": dial tcp 10.96.60.138:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Slice%20resource%20quota%20Tests%20ResourceQuota%20creation%20tests%20Create%20ResourceQuota%20successful%20test877885790\\\": Internal error occurred: failed calling webhook \\\"msliceconfig.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-sliceconfig?timeout=10s\\\": dial tcp 10.96.60.138:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Slice%20resource%20quota%20Tests%20ResourceQuota%20creation%20tests%20Create%20ResourceQuota%20successful%20test877885790\\\": Internal error occurred: failed calling webhook \\\"msliceconfig.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-sliceconfig?timeout=10s\\\": dial tcp 10.96.60.138:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Slice%20resource%20quota%20Tests%20ResourceQuota%20creation%20tests%20Create%20ResourceQuota%20successful%20test877885790\": Internal error occurred: failed calling webhook \"msliceconfig.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-sliceconfig?timeout=10s\": dial tcp 10.96.60.138:443: connect: connection refused\noccurred","time":{"start":1693828311000,"stop":1693828321536,"duration":10536}}]},"Controller Suite:Controller Suite#[It] Slice resource quota Tests ResourceQuota creation tests Create ResourceQuota fail test - namespace.DefaultLimitPerContainer.ES is negative":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":1,"unknown":0,"total":2},"items":[{"uid":"c2ddce2a4f9fcb01","status":"passed","time":{"start":1693899867000,"stop":1693899881580,"duration":14580}},{"uid":"4e83ad1d80fa79ae","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Controller Suite:Controller Suite#[It] Events Test Events get recorded for slice resource quota deletion Validating events for worker slice resource quota creation":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":0,"unknown":0,"total":2},"items":[{"uid":"91db1f52009f2bde","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693899867000,"stop":1693899867000,"duration":0}},{"uid":"83a566b0227f29ee","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Controller Suite:Controller Suite#[It] Events Test Events get recorded for slice resource quota deletion Validating events for setting slice config as owner of slice resource quota":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":0,"unknown":0,"total":2},"items":[{"uid":"189c3734b6d9d1e2","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693899867000,"stop":1693899867000,"duration":0}},{"uid":"3a20d0800047d05c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Test Slice Gateway Service config >  default sliceGatewaySvcType >  should create workerSliceGw cr with default gw svc type & protocol":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"fcc0bcbf700ea0da","status":"passed","time":{"start":1709719453000,"stop":1709719453085,"duration":85}}]},"Controller Suite:Controller Suite#[It] Validate License Tests License Validate tests Should have license secret installed":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":2,"unknown":0,"total":2},"items":[{"uid":"70a1ef4b69875359","status":"passed","time":{"start":1693899867000,"stop":1693899870214,"duration":3214}},{"uid":"4e75dc0af4f8be4e","status":"passed","time":{"start":1693828311000,"stop":1693828311318,"duration":318}}]},"Controller Suite:Controller Suite#[It] Slice resource quota Tests ResourceQuota creation tests Create ResourceQuota fail test - slice.DefaultRequestPerContainer.EphemeralStorage > slice.Request.EphemeralStorage":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":1,"unknown":0,"total":2},"items":[{"uid":"1587bed25ea45b3e","status":"passed","time":{"start":1693899867000,"stop":1693899878340,"duration":11340}},{"uid":"6d249dc5ac7fbcd3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Worker Suite:Worker Suite#[It] mutation webhook tests Testing mutation webhook functionality after namespace deboard Should remove labels and annotations from pod spec which will remove sidecars from deployments":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":0,"unknown":0,"total":1},"items":[{"uid":"efa08eaa7c53cc9e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1709719453000,"stop":1709719453000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice Creation scenarios Slice with MEDIUM QOS should have gateway pod running":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"1f0f3688902797c","status":"passed","time":{"start":1709719453000,"stop":1709719473059,"duration":20059}}]},"Worker Suite:Worker Suite#[It] GW redundacy tests Test gw redundancy there should be two pods in one node (not balanced)":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":0,"unknown":0,"total":1},"items":[{"uid":"9c118b0a1a81bfdb","status":"skipped","statusDetails":"skipped - Spec skipped because Skip() was called in BeforeAll","time":{"start":1709719453000,"stop":1709719453000,"duration":0}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests slice with netpol Should label all application namespaces with kubeslice namespace":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"34bd9933f480ccbe","status":"passed","time":{"start":1709719453000,"stop":1709719453195,"duration":195}}]},"Worker Suite:Worker Suite#[It] NodeRestartSlice Testing node restart should have vl3 router running":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"88b1e8920000c4e0","status":"passed","time":{"start":1709719453000,"stop":1709719456081,"duration":3081}}]},"Controller Suite:Controller Suite#[It] Slice RBAC Tests RBAC Common Tests WorkerSliceRoleBinding data propagation test: Checking successful propagation of rules":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":0,"unknown":0,"total":2},"items":[{"uid":"2cbc897142e151c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693899867000,"stop":1693899867000,"duration":0}},{"uid":"b6fce01440b2b894","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Controller Suite:Controller Suite#[It] Slice RBAC Tests SliceRoleBinding Tests Create SliceRoleBinding failure test: Creating SliceRoleBinding slice-red with duplicate SliceRoleTemplate":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":0,"unknown":0,"total":2},"items":[{"uid":"946d65ba987c9a81","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693899867000,"stop":1693899867000,"duration":0}},{"uid":"1ce18ea7cd9734c1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice Creation scenarios Slice with MEDIUM QOS should reconcile medium qos slice":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"e1772e0b65cc5d9e","status":"passed","time":{"start":1709719453000,"stop":1709719453293,"duration":293}}]},"Controller Suite:Controller Suite#[It] Slice resource quota Tests ResourceQuota creation tests Create ResourceQuota fail test - namespace.DefaultLimitPerContainer.CPU > namespace.Limit.CPU":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":1,"unknown":0,"total":2},"items":[{"uid":"889820760757d364","status":"passed","time":{"start":1693899867000,"stop":1693899882290,"duration":15290}},{"uid":"702b280213db5a3f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Controller Suite:Controller Suite#[It] Slice resource quota Tests ResourceQuota creation tests Create ResourceQuota fail test- Request.CPU > Limit.CPU at cluster level":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":1,"unknown":0,"total":2},"items":[{"uid":"8d1384ff8ef30703","status":"passed","time":{"start":1693899867000,"stop":1693899879038,"duration":12038}},{"uid":"289d41c731b78ba","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Intracluster Suite:Intracluster Suite#[It] Intracluster slice tests Slice with netpol should have vl3 router running":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"5abd7a6e040f698d","status":"passed","time":{"start":1688025155000,"stop":1688025160301,"duration":5301}}]},"Worker Suite:Worker Suite#[It] NodeRestartSlice Testing node restart iperf connectivity across multi cluster":{"statistic":{"failed":1,"broken":0,"skipped":0,"passed":0,"unknown":0,"total":1},"items":[{"uid":"6580d87a7de7556","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00077e108>: {\n        Underlying: <*exec.ExitError | 0xc0009bf840>{\n            ProcessState: {\n                pid: 9328,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 213712},\n                    Stime: {Sec: 0, Usec: 58955},\n                    Maxrss: 116164,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 20553,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 19200,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 598,\n                    Nivcsw: 1051,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: [\n                    \"serviceexport.networking.kubeslice.io/iperf-server created\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"serviceexport.networking.kubeslice.io/iperf-server created\",\n                        \"Error from server (InternalError): error when creating \\\"/e2e/assets/applications/iperf-server.yaml\\\": Internal error occurred: failed calling webhook \\\"webhook.kubeslice.io\\\": failed to call webhook: Post \\\"https://kubeslice-webhook-service.kubeslice-system.svc:443/mutate-webhook?timeout=10s\\\": EOF\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/e2e/assets/applications/iperf-server.yaml\\\": Internal error occurred: failed calling webhook \\\"webhook.kubeslice.io\\\": failed to call webhook: Post \\\"https://kubeslice-webhook-service.kubeslice-system.svc:443/mutate-webhook?timeout=10s\\\": EOF\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"serviceexport.networking.kubeslice.io/iperf-server created\",\n                        \"Error from server (InternalError): error when creating \\\"/e2e/assets/applications/iperf-server.yaml\\\": Internal error occurred: failed calling webhook \\\"webhook.kubeslice.io\\\": failed to call webhook: Post \\\"https://kubeslice-webhook-service.kubeslice-system.svc:443/mutate-webhook?timeout=10s\\\": EOF\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"serviceexport.networking.kubeslice.io/iperf-server created\",\n                    \"Error from server (InternalError): error when creating \\\"/e2e/assets/applications/iperf-server.yaml\\\": Internal error occurred: failed calling webhook \\\"webhook.kubeslice.io\\\": failed to call webhook: Post \\\"https://kubeslice-webhook-service.kubeslice-system.svc:443/mutate-webhook?timeout=10s\\\": EOF\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/e2e/assets/applications/iperf-server.yaml\": Internal error occurred: failed calling webhook \"webhook.kubeslice.io\": failed to call webhook: Post \"https://kubeslice-webhook-service.kubeslice-system.svc:443/mutate-webhook?timeout=10s\": EOF\noccurred","time":{"start":1709719453000,"stop":1709719473322,"duration":20322}}]},"Controller Suite:Controller Suite#[It] E2E tests for license job > when Controller is upgraded license mode = manual Should not create a new secret":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":0,"unknown":0,"total":2},"items":[{"uid":"29c30d43b3314b3e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693899867000,"stop":1693899867000,"duration":0}},{"uid":"3a44bac29ea7243","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Iperf connectivity Verify Iperf traffic":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":0,"unknown":0,"total":1},"items":[{"uid":"ab4cebf8317df054","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1709719453000,"stop":1709719453000,"duration":0}}]},"SliceHealth Suite:SliceHealth Suite#[It] Slice Health Check tests Test Slice Health Check Should not have slice ingress status when ingress is unavailable":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":0,"unknown":0,"total":1},"items":[{"uid":"56d2f5707dae7974","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1688026084000,"stop":1688026084000,"duration":0}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests Iperf App connectivity Check bandwidth ceiling":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":0,"unknown":0,"total":1},"items":[{"uid":"faec31e20f87c6a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1709719453000,"stop":1709719453000,"duration":0}}]},"Hub Suite:Hub Suite#[BeforeSuite]":{"statistic":{"failed":3,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":4},"items":[{"uid":"37df25b1f7062373","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00049ee58>: {\n        Underlying: <*exec.ExitError | 0xc0004aa000>{\n            ProcessState: {\n                pid: 6177,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 4, Usec: 195614},\n                    Stime: {Sec: 0, Usec: 544183},\n                    Maxrss: 88088,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 14514,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 1360,\n                    Oublock: 17392,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 44099,\n                    Nivcsw: 14687,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:200: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:217: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-1.1.2.tgz\",\n                        \"\",\n                        \"client.go:134: [debug] creating 1 resource(s)\",\n                        \"client.go:134: [debug] creating 29 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 29 resources with timeout of 5m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                   ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:200: [debug] Original chart version: \"\"\n    install.go:217: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-1.1.2.tgz\n    \n    client.go:134: [debug] creating 1 resource(s)\n    client.go:134: [debug] creating 29 resource(s)\n    wait.go:48: [debug] beginning wait for 29 resources with timeout of 5m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    install.go:475: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:97: [debug] uninstall: Deleting kubeslice-controller\n    client.go:134: [debug] creating 1 resource(s)\n    client.go:706: [debug] Watching for changes to Job kubeslice-controller-cleanup with timeout of 5m0s\n    client.go:734: [debug] Add/Modify event for kubeslice-controller-cleanup: ADDED\n    client.go:773: [debug] kubeslice-controller-cleanup: Jobs active: 0, jobs failed: 0, jobs succeeded: 0\n    client.go:734: [debug] Add/Modify event for kubeslice-controller-cleanup: MODIFIED\n    client.go:773: [debug] kubeslice-controller-cleanup: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:478: [debug] Starting delete for \"kubeslice-controller-cleanup\" Job\n    Error: INSTALLATION FAILED: an error occurred while uninstalling the release. original install error: context deadline exceeded: 1 error occurred:\n    \t* timed out waiting for the condition\n    \n    \n    helm.go:84: [debug] 1 error occurred:\n    \t* timed out waiting for the condition\n    \n    \n    an error occurred while uninstalling the release. original install error: context deadline exceeded\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:481\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:467\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:423\n    runtime.goexit\n    \truntime/asm_amd64.s:1598\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:147\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:916\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:1044\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:968\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1598\noccurred","time":{"start":1695805835000,"stop":1695806440136,"duration":605136}},{"uid":"e772e0f7ef6c311","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000498528>: {\n        Underlying: <*exec.ExitError | 0xc000116000>{\n            ProcessState: {\n                pid: 6165,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 3, Usec: 965017},\n                    Stime: {Sec: 0, Usec: 692834},\n                    Maxrss: 93440,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 8942,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 2248,\n                    Oublock: 17384,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 43008,\n                    Nivcsw: 13396,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:200: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:217: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-1.2.0.tgz\",\n                        \"\",\n                        \"client.go:134: [debug] creating 1 resource(s)\",\n                        \"client.go:134: [debug] creating 29 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 29 resources with timeout of 5m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                    ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:200: [debug] Original chart version: \"\"\n    install.go:217: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-1.2.0.tgz\n    \n    client.go:134: [debug] creating 1 resource(s)\n    client.go:134: [debug] creating 29 resource(s)\n    wait.go:48: [debug] beginning wait for 29 resources with timeout of 5m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    install.go:470: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:97: [debug] uninstall: Deleting kubeslice-controller\n    client.go:134: [debug] creating 1 resource(s)\n    client.go:706: [debug] Watching for changes to Job kubeslice-controller-cleanup with timeout of 5m0s\n    client.go:734: [debug] Add/Modify event for kubeslice-controller-cleanup: ADDED\n    client.go:773: [debug] kubeslice-controller-cleanup: Jobs active: 0, jobs failed: 0, jobs succeeded: 0\n    client.go:734: [debug] Add/Modify event for kubeslice-controller-cleanup: MODIFIED\n    client.go:773: [debug] kubeslice-controller-cleanup: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:478: [debug] Starting delete for \"kubeslice-controller-cleanup\" Job\n    Error: INSTALLATION FAILED: an error occurred while uninstalling the release. original install error: context deadline exceeded: 1 error occurred:\n    \t* timed out waiting for the condition\n    \n    \n    helm.go:84: [debug] 1 error occurred:\n    \t* timed out waiting for the condition\n    \n    \n    an error occurred while uninstalling the release. original install error: context deadline exceeded\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:476\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:462\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:418\n    runtime.goexit\n    \truntime/asm_amd64.s:1598\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:147\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:916\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:1044\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:968\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1598\noccurred","time":{"start":1689750156000,"stop":1689750760905,"duration":604905}},{"uid":"e6c71bdd9c8771fa","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004a4060>: {\n        Underlying: <*exec.ExitError | 0xc000078020>{\n            ProcessState: {\n                pid: 6147,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 3, Usec: 485829},\n                    Stime: {Sec: 0, Usec: 501499},\n                    Maxrss: 82224,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 5270,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 1088,\n                    Oublock: 11584,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 41386,\n                    Nivcsw: 11774,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:200: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:217: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-1.2.0.tgz\",\n                        \"\",\n                        \"client.go:134: [debug] creating 1 resource(s)\",\n                        \"client.go:134: [debug] creating 29 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 29 resources with timeout of 5m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                    ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:200: [debug] Original chart version: \"\"\n    install.go:217: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-1.2.0.tgz\n    \n    client.go:134: [debug] creating 1 resource(s)\n    client.go:134: [debug] creating 29 resource(s)\n    wait.go:48: [debug] beginning wait for 29 resources with timeout of 5m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    install.go:470: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:97: [debug] uninstall: Deleting kubeslice-controller\n    client.go:134: [debug] creating 1 resource(s)\n    client.go:706: [debug] Watching for changes to Job kubeslice-controller-cleanup with timeout of 5m0s\n    client.go:734: [debug] Add/Modify event for kubeslice-controller-cleanup: ADDED\n    client.go:773: [debug] kubeslice-controller-cleanup: Jobs active: 0, jobs failed: 0, jobs succeeded: 0\n    client.go:734: [debug] Add/Modify event for kubeslice-controller-cleanup: MODIFIED\n    client.go:773: [debug] kubeslice-controller-cleanup: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:478: [debug] Starting delete for \"kubeslice-controller-cleanup\" Job\n    Error: INSTALLATION FAILED: an error occurred while uninstalling the release. original install error: context deadline exceeded: 1 error occurred:\n    \t* timed out waiting for the condition\n    \n    \n    helm.go:84: [debug] 1 error occurred:\n    \t* timed out waiting for the condition\n    \n    \n    an error occurred while uninstalling the release. original install error: context deadline exceeded\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:476\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:462\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:418\n    runtime.goexit\n    \truntime/asm_amd64.s:1598\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:147\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:916\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:1044\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:968\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1598\noccurred","time":{"start":1688626949000,"stop":1688627552804,"duration":603804}},{"uid":"5948718af3daf0e9","status":"passed","time":{"start":1688022039000,"stop":1688022053084,"duration":14084}}]},"Worker Suite:Worker Suite#[It] Edit NodeIP tests for cluster CR,  Edit NodeIP validation,  Node IP Auto-detection should work":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"3b528d7295ce2af3","status":"passed","time":{"start":1709719453000,"stop":1709719453735,"duration":735}}]},"Controller Suite:Controller Suite#[It] Slice resource quota Tests ResourceQuota creation tests Create ResourceQuota fail test - not applied in project namespace  ":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":1,"unknown":0,"total":2},"items":[{"uid":"243e498be087771c","status":"passed","time":{"start":1693899867000,"stop":1693899877872,"duration":10872}},{"uid":"579a6f075e86498b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Controller Suite:Controller Suite#[It] Slice resource quota Tests ResourceQuota creation tests Create ResourceQuota fail test - slice.limit.mem negative  ":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":1,"unknown":0,"total":2},"items":[{"uid":"ff3e03d4c32d5bee","status":"passed","time":{"start":1693899867000,"stop":1693899873426,"duration":6426}},{"uid":"52ee4d5ba1e8facd","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Controller Suite:Controller Suite#[It] Events Test Events get recorded for slice node affinity creation and deletion Events get recorded for slice node affinity creation Validating events for setting slice config as owner of slice node affinity":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":0,"unknown":0,"total":2},"items":[{"uid":"1e22b1e8e35a910","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693899867000,"stop":1693899867000,"duration":0}},{"uid":"d670052518a9215","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Hub Suite:Hub Suite#[It] Cluster health check tests Cluster health check - without istio Has health status normal":{"statistic":{"failed":1,"broken":0,"skipped":0,"passed":0,"unknown":0,"total":1},"items":[{"uid":"b0c3eea5e4f82850","status":"failed","statusDetails":"Timed out after 120.000s.\nExpected\n    <bool>: false\nto be true","time":{"start":1688022039000,"stop":1688022812843,"duration":773843}}]},"Controller Suite:Controller Suite#[It] Slice RBAC Tests SliceRoleBinding Tests Create SliceRoleBinding failure test: Creating SliceRoleBinding slice-red with duplicate Kubernetes Roles":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":0,"unknown":0,"total":2},"items":[{"uid":"2745bc96a88ddcd5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693899867000,"stop":1693899867000,"duration":0}},{"uid":"f49a4063943b0c22","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Worker Suite:Worker Suite#[It] GW redundacy tests Test gw redundancy run iperf":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":0,"unknown":0,"total":1},"items":[{"uid":"690e8e9ebb23fa0c","status":"skipped","statusDetails":"skipped - Spec skipped because Skip() was called in BeforeAll","time":{"start":1709719453000,"stop":1709719453000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice deletion tests: Slice is installed & validated Slice object is deleted from hub should remove gateway pod from spoke":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"1457353dca1eccf2","status":"passed","time":{"start":1709719453000,"stop":1709719471068,"duration":18068}}]},"Controller Suite:Controller Suite#[It] Slice resource quota Tests ResourceQuota creation tests Create ResourceQuota fail test - namespace.DefaultRequestPerContainer.EphemeralStorage > namespace.Request.EphemeralStorage":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":1,"unknown":0,"total":2},"items":[{"uid":"65f1b270055a230b","status":"passed","time":{"start":1693899867000,"stop":1693899874066,"duration":7066}},{"uid":"5199fb2a2ebcffb","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Controller Suite:Controller Suite#[It] Slice RBAC Tests SliceRoleBinding Tests Create SliceRoleBinding successful test: Creating SliceRoleBinding slice-red with both SliceRoleTemplate and Kubernetes Roles":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":0,"unknown":0,"total":2},"items":[{"uid":"e276399205b31d2d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693899867000,"stop":1693899867000,"duration":0}},{"uid":"3ee796991b364bdb","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Controller Suite:Controller Suite#[It] Validate License Tests License Validate tests should fail CRUD in case someone tampered machine.file":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":1,"unknown":0,"total":2},"items":[{"uid":"ceefc7733b98808a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693899867000,"stop":1693899867000,"duration":0}},{"uid":"16187bd7ead03b8e","status":"passed","time":{"start":1693828311000,"stop":1693828316052,"duration":5052}}]},"Controller Suite:Controller Suite#[It] Slice resource quota Tests ResourceQuota creation tests Create ResourceQuota fail test - sum(namespace.request.cpu) > slice.cpu":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":1,"unknown":0,"total":2},"items":[{"uid":"2a4a73f10d1e67c4","status":"passed","time":{"start":1693899867000,"stop":1693899876081,"duration":9081}},{"uid":"6aa87da4d3bb010f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Creation Should fail while creating service accounts as combination of special characters":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"15ac33d822fe389e","status":"passed","time":{"start":1688022039000,"stop":1688022039271,"duration":271}}]},"Intracluster Suite:Intracluster Suite#[It] Intracluster slice tests Slice with netpol should create & label app ns with kubeslice label":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"407908df22066657","status":"passed","time":{"start":1688025155000,"stop":1688025155111,"duration":111}}]},"Controller Suite:Controller Suite#[It] Events Test Events get recorded for slice config creation Validating events for worker slice resource quota creation":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":0,"unknown":0,"total":2},"items":[{"uid":"f8e10c065c771023","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693899867000,"stop":1693899867000,"duration":0}},{"uid":"6386a34105998cf0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Controller Suite:Controller Suite#[It] Slice resource quota Tests ResourceQuota creation tests Create ResourceQuota fail test - namespace.DefaultRequestPerContainer.CPU is negative":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":1,"unknown":0,"total":2},"items":[{"uid":"52c41f5ccaef1142","status":"passed","time":{"start":1693899867000,"stop":1693899872757,"duration":5757}},{"uid":"e29f277c4a9804f9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Controller Suite:Controller Suite#[It] Slice resource quota Tests ResourceQuota creation tests Create ResourceQuota fail test - sum(cluster.request.EphemeralStorage) > slice.request.EphemeralStorage":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":1,"unknown":0,"total":2},"items":[{"uid":"73a450b81e0e6891","status":"passed","time":{"start":1693899867000,"stop":1693899871556,"duration":4556}},{"uid":"e30a101f9d0e201","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice Config Events Test Events get recorded for slice config creation and deletion Events get recorded for slice config deletion Validating events for worker slice configs deletion":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"7fdd4d16247b5847","status":"passed","time":{"start":1709719453000,"stop":1709719453297,"duration":297}}]},"SliceHealth Suite:SliceHealth Suite#[It] Slice Health Check tests Test Slice Health Check Should have slice slicerouter status Error when slicerouter is unavailable":{"statistic":{"failed":1,"broken":0,"skipped":0,"passed":0,"unknown":0,"total":1},"items":[{"uid":"6e010f9729d521a","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00080c018>: {\n        Underlying: <*exec.ExitError | 0xc0003d4000>{\n            ProcessState: {\n                pid: 8860,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 1, Usec: 328430},\n                    Stime: {Sec: 0, Usec: 227981},\n                    Maxrss: 92140,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 22839,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 16368,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 11478,\n                    Nivcsw: 4670,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:200: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:217: [debug] CHART PATH: /root/.cache/helm/repository/istio-discovery-1.16.0.tgz\",\n                        \"\",\n                        \"client.go:134: [debug] creating 1 resource(s)\",\n                        \"client.go:134: [debug] creating 24 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 24 resources with timeout of 5m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:200: [debug] Original chart version: \"\"\n    install.go:217: [debug] CHART PATH: /root/.cache/helm/repository/istio-discovery-1.16.0.tgz\n    \n    client.go:134: [debug] creating 1 resource(s)\n    client.go:134: [debug] creating 24 resource(s)\n    wait.go:48: [debug] beginning wait for 24 resources with timeout of 5m0s\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    install.go:470: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:97: [debug] uninstall: Deleting istiod\n    uninstall.go:243: [debug] uninstall: given cascade value: , defaulting to delete propagation background\n    client.go:478: [debug] Starting delete for \"istiod\" Service\n    client.go:478: [debug] Starting delete for \"istiod\" HorizontalPodAutoscaler\n    client.go:478: [debug] Starting delete for \"istiod\" Deployment\n    client.go:478: [debug] Starting delete for \"istiod\" RoleBinding\n    client.go:478: [debug] Starting delete for \"istiod\" Role\n    client.go:478: [debug] Starting delete for \"istio-reader-clusterrole-istio-system\" ClusterRoleBinding\n    client.go:478: [debug] Starting delete for \"istiod-clusterrole-istio-system\" ClusterRoleBinding\n    client.go:478: [debug] Starting delete for \"istiod-gateway-controller-istio-system\" ClusterRoleBinding\n    client.go:478: [debug] Starting delete for \"istio-reader-clusterrole-istio-system\" ClusterRole\n    client.go:478: [debug] Starting delete for \"istiod-clusterrole-istio-system\" ClusterRole\n    client.go:478: [debug] Starting delete for \"istiod-gateway-controller-istio-system\" ClusterRole\n    client.go:478: [debug] Starting delete for \"istio-sidecar-injector\" ConfigMap\n    client.go:478: [debug] Starting delete for \"istio\" ConfigMap\n    client.go:478: [debug] Starting delete for \"istiod\" ServiceAccount\n    client.go:478: [debug] Starting delete for \"istiod\" PodDisruptionBudget\n    client.go:478: [debug] Starting delete for \"tcp-stats-filter-1.16\" EnvoyFilter\n    client.go:478: [debug] Starting delete for \"stats-filter-1.13\" EnvoyFilter\n    client.go:478: [debug] Starting delete for \"tcp-stats-filter-1.13\" EnvoyFilter\n    client.go:478: [debug] Starting delete for \"stats-filter-1.14\" EnvoyFilter\n    client.go:478: [debug] Starting delete for \"tcp-stats-filter-1.14\" EnvoyFilter\n    client.go:478: [debug] Starting delete for \"stats-filter-1.15\" EnvoyFilter\n    client.go:478: [debug] Starting delete for \"tcp-stats-filter-1.15\" EnvoyFilter\n    client.go:478: [debug] Starting delete for \"stats-filter-1.16\" EnvoyFilter\n    client.go:478: [debug] Starting delete for \"istio-sidecar-injector\" MutatingWebhookConfiguration\n    uninstall.go:150: [debug] purge requested for istiod\n    Error: INSTALLATION FAILED: release istiod failed, and has been uninstalled due to atomic being set: context deadline exceeded\n    helm.go:84: [debug] context deadline exceeded\n    release istiod failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:478\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:462\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:418\n    runtime.goexit\n    \truntime/asm_amd64.s:1598\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:147\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:916\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:1044\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:968\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1598\noccurred","time":{"start":1688026084000,"stop":1688026400126,"duration":316126}}]},"Hub Suite:Hub Suite#[It] Project and Cluster Events Test Events get recorded for project creation and deletion Events get recorded for project creation Validating events for service account secret creation":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"1bf1fdfe76c65847","status":"passed","time":{"start":1688022039000,"stop":1688022039085,"duration":85}}]},"Worker Suite:Worker Suite#[It]  QosProfile tests QosProfile delete validation Delete qosProfile positive scenario":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"93ca335b1f095493","status":"passed","time":{"start":1709719453000,"stop":1709719453426,"duration":426}}]},"Hub Suite:Hub Suite#[It] Cluster CR tests Cluster CR validation Register worker cluster (with minimal values in cluster CR)":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"2d26f60d8b34d04a","status":"passed","time":{"start":1688022039000,"stop":1688022682877,"duration":643877}}]},"Controller Suite:Controller Suite#[It] Slice RBAC Tests SliceRoleTemplate Tests Create SliceRoleTempalate failure test: Creating SliceRoleTempalate read-only-role, missing verbs field":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":1,"unknown":0,"total":2},"items":[{"uid":"690ab6d117151217","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693899867000,"stop":1693899867000,"duration":0}},{"uid":"f462cb88ba35fcd3","status":"passed","time":{"start":1693828311000,"stop":1693828314265,"duration":3265}}]},"Controller Suite:Controller Suite#[It] Slice RBAC Tests SliceRoleBinding Tests Delete SliceRoleBinding success test: Deletion of SliceConfig should delete SliceRoleBinding":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":0,"unknown":0,"total":2},"items":[{"uid":"be15664e241b6b87","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693899867000,"stop":1693899867000,"duration":0}},{"uid":"6db2ee6c8af18e05","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"SliceHealth Suite:SliceHealth Suite#[It] Slice Health Check tests Test Slice Health Check Should have component status Normal when services are available":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":0,"unknown":0,"total":1},"items":[{"uid":"d497d0c2ec0af0f9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1688026084000,"stop":1688026084000,"duration":0}}]},"Worker Suite:Worker Suite#[It] GW redundacy tests Test gw redundancy Should verify cluster CR status field":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":0,"unknown":0,"total":1},"items":[{"uid":"a1abf38207ad0179","status":"skipped","statusDetails":"skipped - Spec skipped because Skip() was called in BeforeAll","time":{"start":1709719453000,"stop":1709719453000,"duration":0}}]},"Intracluster Suite:Intracluster Suite#[It] Intracluster slice tests Iperf connectivity should create all application namespaces":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"c92d3a7459ec60a9","status":"passed","time":{"start":1688025155000,"stop":1688025157434,"duration":2434}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting mesh-dns pod Check ping between iperf-server and iperf-client after mesh-dns pod restart":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":0,"unknown":0,"total":1},"items":[{"uid":"3660683f3a0b3592","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1709719453000,"stop":1709719453000,"duration":0}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests slice with netpol should have vl3 router running":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"44a337f52f5fc8dc","status":"passed","time":{"start":1709719453000,"stop":1709719457069,"duration":4069}}]},"Worker Suite:Worker Suite#[It] mutation webhook tests Testing mutation webhook functionality after namespace onboard Should mutate statefulsets with labels and annotations with nsm sidecars injected":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":0,"unknown":0,"total":1},"items":[{"uid":"987e81b83d9ea797","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1709719453000,"stop":1709719453000,"duration":0}}]},"Controller Suite:Controller Suite#[It] Events Test Events get recorded for project creation Validating events for project creation":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":2,"unknown":0,"total":2},"items":[{"uid":"8db52857ffaa9d68","status":"passed","time":{"start":1693899867000,"stop":1693899876387,"duration":9387}},{"uid":"7169737bae886fa9","status":"passed","time":{"start":1693828311000,"stop":1693828311544,"duration":544}}]},"Worker Suite:Worker Suite#[It] Edit NodeIP tests for cluster CR,  Edit NodeIP validation,  Test nodeip validity >  Support more than two node ips in auto detection":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"5de4df036196c72a","status":"passed","time":{"start":1709719453000,"stop":1709719456233,"duration":3233}}]},"Controller Suite:Controller Suite#[It] E2E tests for license job > when license config is tampered with Apply tampered cm":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":2,"unknown":0,"total":2},"items":[{"uid":"cd7baf9c38ddaf75","status":"passed","time":{"start":1693899867000,"stop":1693899875458,"duration":8458}},{"uid":"fffcb38d860e7a60","status":"passed","time":{"start":1693828311000,"stop":1693828325209,"duration":14209}}]},"Controller Suite:Controller Suite#[It] Slice node affinity Tests SliceNodeAffinity tests - includes creation and deletion slice node affinity creation -failed case if not applied in project namespace":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":1,"unknown":0,"total":2},"items":[{"uid":"df691af815b09468","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693899867000,"stop":1693899867000,"duration":0}},{"uid":"abcfe6b8e6ac76f3","status":"passed","time":{"start":1693828311000,"stop":1693828314705,"duration":3705}}]},"Hub Suite:Hub Suite#[It] Cluster negative tests Worker Cluster Registration with Wrong clustername":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"42655503d36c6d8e","status":"passed","time":{"start":1688022039000,"stop":1688022046816,"duration":7816}}]},"Controller Suite:Controller Suite#[It] Slice RBAC Tests SliceRoleBinding Tests Create SliceRoleBinding failure test: Creating SliceRoleBinding slice-green with name not same as Slice slice-red":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":0,"unknown":0,"total":2},"items":[{"uid":"cfca5c52b8bd850e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693899867000,"stop":1693899867000,"duration":0}},{"uid":"3339c17984382abc","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"SliceHealth Suite:SliceHealth Suite#[BeforeSuite]":{"statistic":{"failed":9,"broken":0,"skipped":0,"passed":2,"unknown":0,"total":11},"items":[{"uid":"2d861b4e589dd812","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0000c6d20>: {\n        Underlying: <*exec.ExitError | 0xc0000ea400>{\n            ProcessState: {\n                pid: 7888,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 74072},\n                    Stime: {Sec: 0, Usec: 13071},\n                    Maxrss: 48848,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2797,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 176,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 554,\n                    Nivcsw: 141,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:214: [debug] Original chart version: \\\"\\\"\",\n                        \"Error: INSTALLATION FAILED: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/nexus/kubeslice-controller-1.2.1.tgz : 404 Not Found\",\n                        \"helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/nexus/kubeslice-controller-1.2.1.tgz : 404 Not Found\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\",\n                        \"helm.sh/helm/v3/pkg/downloader.(*ChartDownloader).DownloadTo\",\n                        \"\\thelm.sh/helm/v3/pkg/downloader/chart_downloader.go:100\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:805\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:226\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:152\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.8.0/command.go:983\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.8.0/command.go:1115\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.8.0/command.go:1039\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:267\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1650\",\n                        \"INSTALLATION FAILED\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:154\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.8.0/command.go:983\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.8.0/command.go:1115\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.8.0/command.go:1039\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"run...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:214: [debug] Original chart version: \"\"\n    Error: INSTALLATION FAILED: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/nexus/kubeslice-controller-1.2.1.tgz : 404 Not Found\n    helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/nexus/kubeslice-controller-1.2.1.tgz : 404 Not Found\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\n    helm.sh/helm/v3/pkg/downloader.(*ChartDownloader).DownloadTo\n    \thelm.sh/helm/v3/pkg/downloader/chart_downloader.go:100\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:805\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:226\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:152\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:983\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:1115\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:1039\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:267\n    runtime.goexit\n    \truntime/asm_amd64.s:1650\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:154\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:983\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:1115\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:1039\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:267\n    runtime.goexit\n    \truntime/asm_amd64.s:1650\noccurred","time":{"start":1711007884000,"stop":1711007884175,"duration":175}},{"uid":"430b7c8d375602ec","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000ed50>: {\n        Underlying: <*exec.ExitError | 0xc0005982a0>{\n            ProcessState: {\n                pid: 7683,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 158579},\n                    Stime: {Sec: 0, Usec: 25815},\n                    Maxrss: 49252,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 1855,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 176,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 574,\n                    Nivcsw: 934,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:214: [debug] Original chart version: \\\"\\\"\",\n                        \"Error: INSTALLATION FAILED: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/nexus/kubeslice-controller-1.2.1.tgz : 404 Not Found\",\n                        \"helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/nexus/kubeslice-controller-1.2.1.tgz : 404 Not Found\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\",\n                        \"helm.sh/helm/v3/pkg/downloader.(*ChartDownloader).DownloadTo\",\n                        \"\\thelm.sh/helm/v3/pkg/downloader/chart_downloader.go:100\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:805\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:226\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:152\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.8.0/command.go:983\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.8.0/command.go:1115\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.8.0/command.go:1039\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:267\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1650\",\n                        \"INSTALLATION FAILED\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:154\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.8.0/command.go:983\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.8.0/command.go:1115\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.8.0/command.go:1039\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"ru...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:214: [debug] Original chart version: \"\"\n    Error: INSTALLATION FAILED: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/nexus/kubeslice-controller-1.2.1.tgz : 404 Not Found\n    helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/nexus/kubeslice-controller-1.2.1.tgz : 404 Not Found\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\n    helm.sh/helm/v3/pkg/downloader.(*ChartDownloader).DownloadTo\n    \thelm.sh/helm/v3/pkg/downloader/chart_downloader.go:100\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:805\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:226\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:152\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:983\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:1115\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:1039\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:267\n    runtime.goexit\n    \truntime/asm_amd64.s:1650\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:154\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:983\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:1115\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:1039\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:267\n    runtime.goexit\n    \truntime/asm_amd64.s:1650\noccurred","time":{"start":1710850775000,"stop":1710850775662,"duration":662}},{"uid":"599f02e45767393f","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00012a300>: {\n        Underlying: <*exec.ExitError | 0xc000046460>{\n            ProcessState: {\n                pid: 7875,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 78079},\n                    Stime: {Sec: 0, Usec: 0},\n                    Maxrss: 47068,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 1826,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 176,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 609,\n                    Nivcsw: 137,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:214: [debug] Original chart version: \\\"\\\"\",\n                        \"Error: INSTALLATION FAILED: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/nexus/kubeslice-controller-1.2.1.tgz : 404 Not Found\",\n                        \"helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/nexus/kubeslice-controller-1.2.1.tgz : 404 Not Found\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\",\n                        \"helm.sh/helm/v3/pkg/downloader.(*ChartDownloader).DownloadTo\",\n                        \"\\thelm.sh/helm/v3/pkg/downloader/chart_downloader.go:100\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:805\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:226\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:152\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.8.0/command.go:983\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.8.0/command.go:1115\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.8.0/command.go:1039\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:267\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1650\",\n                        \"INSTALLATION FAILED\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:154\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.8.0/command.go:983\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.8.0/command.go:1115\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.8.0/command.go:1039\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:214: [debug] Original chart version: \"\"\n    Error: INSTALLATION FAILED: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/nexus/kubeslice-controller-1.2.1.tgz : 404 Not Found\n    helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/nexus/kubeslice-controller-1.2.1.tgz : 404 Not Found\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\n    helm.sh/helm/v3/pkg/downloader.(*ChartDownloader).DownloadTo\n    \thelm.sh/helm/v3/pkg/downloader/chart_downloader.go:100\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:805\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:226\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:152\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:983\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:1115\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:1039\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:267\n    runtime.goexit\n    \truntime/asm_amd64.s:1650\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:154\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:983\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:1115\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:1039\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:267\n    runtime.goexit\n    \truntime/asm_amd64.s:1650\noccurred","time":{"start":1710825518000,"stop":1710825518117,"duration":117}},{"uid":"af36be1112d47f6a","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00082a018>: {\n        Underlying: <*exec.ExitError | 0xc0004fc000>{\n            ProcessState: {\n                pid: 9523,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 8, Usec: 259715},\n                    Stime: {Sec: 0, Usec: 737888},\n                    Maxrss: 124944,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 37064,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 776,\n                    Oublock: 21592,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 57565,\n                    Nivcsw: 11017,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:214: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:231: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-1.2.1.tgz\",\n                        \"\",\n                        \"client.go:142: [debug] creating 1 resource(s)\",\n                        \"install.go:168: [debug] CRD serviceexports.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:142: [debug] creating 1 resource(s)\",\n                        \"install.go:168: [debug] CRD serviceimports.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:142: [debug] creating 1 resource(s)\",\n                        \"install.go:168: [debug] CRD slicegateways.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:142: [debug] creating 1 resource(s)\",\n                        \"install.go:168: [debug] CRD slices.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:142: [debug] creating 1 resource(s)\",\n                        \"install.go:168: [debug] CRD clusterfederatedtrustdomains.spire.spiffe.io is already present. Skipping.\",\n                        \"client.go:142: [debug] creating 1 resource(s)\",\n                        \"install.go:168: [debug] CRD clusterspiffeids.spire.spiffe.io is already present. Skipping.\",\n                        \"client.go:142: [debug] creating 1 resource(s)\",\n                        \"install.go:168: [debug] CRD networkservices.networkservicemesh.io is already present. Skipping.\",\n                        \"client.go:142: [debug] creating 1 resource(s)\",\n                        \"install.go:168: [debug] CRD networkserviceendpoints.networkservicemesh.io is already present. Skipping.\",\n                        \"client.go:142: [debug] creating 1 resource(s)\",\n                        \"client.go:486: [debug] Starting delete for \\\"kubeslice-install-crds\\\" ServiceAccount\",\n                        \"client.go:490: [debug] Ignoring delete failure for \\\"kubeslice-install-crds\\\" /v1, Kind=ServiceAccount: serviceaccounts \\\"kubeslice-install-crds\\\" not found\",\n                        \"wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\",\n                        \"client.go:142: [debug] creating 1 resource(s)\",\n                        \"client.go:486: [debug] Starting delete for \\\"kubeslice-install-crds\\\" ConfigMap\",\n                        \"client.go:490: [debug] Ignoring delete failure for \\\"kubeslice-install-crds\\\" /v1, Kind=ConfigMap: configmaps \\\"kubeslice-install-crds\\\" not found\",\n                        \"wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeo...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:214: [debug] Original chart version: \"\"\n    install.go:231: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-1.2.1.tgz\n    \n    client.go:142: [debug] creating 1 resource(s)\n    install.go:168: [debug] CRD serviceexports.networking.kubeslice.io is already present. Skipping.\n    client.go:142: [debug] creating 1 resource(s)\n    install.go:168: [debug] CRD serviceimports.networking.kubeslice.io is already present. Skipping.\n    client.go:142: [debug] creating 1 resource(s)\n    install.go:168: [debug] CRD slicegateways.networking.kubeslice.io is already present. Skipping.\n    client.go:142: [debug] creating 1 resource(s)\n    install.go:168: [debug] CRD slices.networking.kubeslice.io is already present. Skipping.\n    client.go:142: [debug] creating 1 resource(s)\n    install.go:168: [debug] CRD clusterfederatedtrustdomains.spire.spiffe.io is already present. Skipping.\n    client.go:142: [debug] creating 1 resource(s)\n    install.go:168: [debug] CRD clusterspiffeids.spire.spiffe.io is already present. Skipping.\n    client.go:142: [debug] creating 1 resource(s)\n    install.go:168: [debug] CRD networkservices.networkservicemesh.io is already present. Skipping.\n    client.go:142: [debug] creating 1 resource(s)\n    install.go:168: [debug] CRD networkserviceendpoints.networkservicemesh.io is already present. Skipping.\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"kubeslice-install-crds\" ServiceAccount\n    client.go:490: [debug] Ignoring delete failure for \"kubeslice-install-crds\" /v1, Kind=ServiceAccount: serviceaccounts \"kubeslice-install-crds\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"kubeslice-install-crds\" ConfigMap\n    client.go:490: [debug] Ignoring delete failure for \"kubeslice-install-crds\" /v1, Kind=ConfigMap: configmaps \"kubeslice-install-crds\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"kubeslice-install-crds\" ClusterRole\n    client.go:490: [debug] Ignoring delete failure for \"kubeslice-install-crds\" rbac.authorization.k8s.io/v1, Kind=ClusterRole: clusterroles.rbac.authorization.k8s.io \"kubeslice-install-crds\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"kubeslice-install-crds\" ClusterRoleBinding\n    client.go:490: [debug] Ignoring delete failure for \"kubeslice-install-crds\" rbac.authorization.k8s.io/v1, Kind=ClusterRoleBinding: clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-install-crds\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:490: [debug] Ignoring delete failure for \"nsm-config\" /v1, Kind=ConfigMap: configmaps \"nsm-config\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"nsm-crd-install\" ConfigMap\n    client.go:490: [debug] Ignoring delete failure for \"nsm-crd-install\" /v1, Kind=ConfigMap: configmaps \"nsm-crd-install\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"nsm-install-crds\" ServiceAccount\n    client.go:490: [debug] Ignoring delete failure for \"nsm-install-crds\" /v1, Kind=ServiceAccount: serviceaccounts \"nsm-install-crds\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"nsm-install-crds\" ClusterRole\n    client.go:490: [debug] Ignoring delete failure for \"nsm-install-crds\" rbac.authorization.k8s.io/v1, Kind=ClusterRole: clusterroles.rbac.authorization.k8s.io \"nsm-install-crds\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"nsm-install-crds\" ClusterRoleBinding\n    client.go:490: [debug] Ignoring delete failure for \"nsm-install-crds\" rbac.authorization.k8s.io/v1, Kind=ClusterRoleBinding: clusterrolebindings.rbac.authorization.k8s.io \"nsm-install-crds\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"spire-clusterid-cr-install\" ConfigMap\n    client.go:490: [debug] Ignoring delete failure for \"spire-clusterid-cr-install\" /v1, Kind=ConfigMap: configmaps \"spire-clusterid-cr-install\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"spire-crd-install\" ConfigMap\n    client.go:490: [debug] Ignoring delete failure for \"spire-crd-install\" /v1, Kind=ConfigMap: configmaps \"spire-crd-install\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"spire-install-clusterid-cr\" ServiceAccount\n    client.go:490: [debug] Ignoring delete failure for \"spire-install-clusterid-cr\" /v1, Kind=ServiceAccount: serviceaccounts \"spire-install-clusterid-cr\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"spire-install-clusterid-cr\" ClusterRole\n    client.go:490: [debug] Ignoring delete failure for \"spire-install-clusterid-cr\" rbac.authorization.k8s.io/v1, Kind=ClusterRole: clusterroles.rbac.authorization.k8s.io \"spire-install-clusterid-cr\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"spire-install-clusterid-cr\" ClusterRoleBinding\n    client.go:490: [debug] Ignoring delete failure for \"spire-install-clusterid-cr\" rbac.authorization.k8s.io/v1, Kind=ClusterRoleBinding: clusterrolebindings.rbac.authorization.k8s.io \"spire-install-clusterid-cr\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"spire-install-crds\" ServiceAccount\n    client.go:490: [debug] Ignoring delete failure for \"spire-install-crds\" /v1, Kind=ServiceAccount: serviceaccounts \"spire-install-crds\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"spire-install-crds\" ClusterRole\n    client.go:490: [debug] Ignoring delete failure for \"spire-install-crds\" rbac.authorization.k8s.io/v1, Kind=ClusterRole: clusterroles.rbac.authorization.k8s.io \"spire-install-crds\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"spire-install-crds\" ClusterRoleBinding\n    client.go:490: [debug] Ignoring delete failure for \"spire-install-crds\" rbac.authorization.k8s.io/v1, Kind=ClusterRoleBinding: clusterrolebindings.rbac.authorization.k8s.io \"spire-install-crds\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"kubeslice-install-crds\" Job\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:712: [debug] Watching for changes to Job kubeslice-install-crds with timeout of 10m0s\n    client.go:740: [debug] Add/Modify event for kubeslice-install-crds: ADDED\n    client.go:779: [debug] kubeslice-install-crds: Jobs active: 0, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for kubeslice-install-crds: MODIFIED\n    client.go:779: [debug] kubeslice-install-crds: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for kubeslice-install-crds: MODIFIED\n    client.go:779: [debug] kubeslice-install-crds: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for kubeslice-install-crds: MODIFIED\n    client.go:779: [debug] kubeslice-install-crds: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for kubeslice-install-crds: MODIFIED\n    client.go:486: [debug] Starting delete for \"nsm-install-crds\" Job\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:712: [debug] Watching for changes to Job nsm-install-crds with timeout of 10m0s\n    client.go:740: [debug] Add/Modify event for nsm-install-crds: ADDED\n    client.go:779: [debug] nsm-install-crds: Jobs active: 0, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for nsm-install-crds: MODIFIED\n    client.go:779: [debug] nsm-install-crds: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for nsm-install-crds: MODIFIED\n    client.go:779: [debug] nsm-install-crds: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for nsm-install-crds: MODIFIED\n    client.go:779: [debug] nsm-install-crds: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for nsm-install-crds: MODIFIED\n    client.go:486: [debug] Starting delete for \"spire-install-crds\" Job\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:712: [debug] Watching for changes to Job spire-install-crds with timeout of 10m0s\n    client.go:740: [debug] Add/Modify event for spire-install-crds: ADDED\n    client.go:779: [debug] spire-install-crds: Jobs active: 0, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for spire-install-crds: MODIFIED\n    client.go:779: [debug] spire-install-crds: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for spire-install-crds: MODIFIED\n    client.go:779: [debug] spire-install-crds: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for spire-install-crds: MODIFIED\n    client.go:779: [debug] spire-install-crds: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for spire-install-crds: MODIFIED\n    client.go:486: [debug] Starting delete for \"spire-install-clusterid-cr\" Job\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:712: [debug] Watching for changes to Job spire-install-clusterid-cr with timeout of 10m0s\n    client.go:740: [debug] Add/Modify event for spire-install-clusterid-cr: ADDED\n    client.go:779: [debug] spire-install-clusterid-cr: Jobs active: 0, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for spire-install-clusterid-cr: MODIFIED\n    client.go:779: [debug] spire-install-clusterid-cr: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for spire-install-clusterid-cr: MODIFIED\n    client.go:779: [debug] spire-install-clusterid-cr: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for spire-install-clusterid-cr: MODIFIED\n    client.go:779: [debug] spire-install-clusterid-cr: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for spire-install-clusterid-cr: MODIFIED\n    client.go:486: [debug] Starting delete for \"kubeslice-install-crds\" ServiceAccount\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"kubeslice-install-crds\" ConfigMap\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"kubeslice-install-crds\" ClusterRole\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"kubeslice-install-crds\" ClusterRoleBinding\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"nsm-crd-install\" ConfigMap\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"nsm-install-crds\" ServiceAccount\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"nsm-install-crds\" ClusterRole\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"nsm-install-crds\" ClusterRoleBinding\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"spire-clusterid-cr-install\" ConfigMap\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"spire-crd-install\" ConfigMap\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"spire-install-clusterid-cr\" ServiceAccount\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"spire-install-clusterid-cr\" ClusterRole\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"spire-install-clusterid-cr\" ClusterRoleBinding\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"spire-install-crds\" ServiceAccount\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"spire-install-crds\" ClusterRole\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"spire-install-crds\" ClusterRoleBinding\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:393: [debug] checking 74 resources for changes\n    client.go:414: [debug] Created a new PriorityClass called \"nsm-webhook-high-priority\" in \n    \n    client.go:414: [debug] Created a new Namespace called \"spire\" in \n    \n    client.go:414: [debug] Created a new ServiceAccount called \"admission-webhook-sa\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new ServiceAccount called \"nse-acc\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new ServiceAccount called \"nsc-acc\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new ServiceAccount called \"nsmgr-acc\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new ServiceAccount called \"forward-plane-acc\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new ServiceAccount called \"spiffe-csi-driver\" in spire\n    \n    client.go:414: [debug] Created a new ServiceAccount called \"spire-server\" in spire\n    \n    client.go:414: [debug] Created a new ServiceAccount called \"spire-agent\" in spire\n    \n    client.go:414: [debug] Created a new ServiceAccount called \"kubeslice-kubernetes-dashboard\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new ServiceAccount called \"kubeslice-dns\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new ServiceAccount called \"kubeslice-gateway-edge\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new ServiceAccount called \"kubeslice-netop\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new ServiceAccount called \"vpn-gateway-server\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new ServiceAccount called \"vpn-gateway-client\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new ServiceAccount called \"slice-router\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new ServiceAccount called \"kubeslice-controller-manager\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new Secret called \"kubeslice-kubernetes-dashboard-creds\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new Secret called \"kubeslice-hub\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new Secret called \"kubeslice-admission-webhook-certs\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new ConfigMap called \"spire-bundle\" in spire\n    \n    client.go:414: [debug] Created a new ConfigMap called \"spire-controller-manager-config\" in spire\n    \n    client.go:414: [debug] Created a new ConfigMap called \"spire-server\" in spire\n    \n    client.go:414: [debug] Created a new ConfigMap called \"spire-agent\" in spire\n    \n    client.go:414: [debug] Created a new ConfigMap called \"nsmgr-cm\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new ConfigMap called \"kubeslice-manager-config\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new ConfigMap called \"kubeslice-worker-event-schema-conf\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new ClusterRole called \"admission-webhook-role\" in \n    \n    client.go:414: [debug] Created a new ClusterRole called \"nsm-role\" in \n    \n    client.go:414: [debug] Created a new ClusterRole called \"aggregate-network-services-view\" in \n    \n    client.go:414: [debug] Created a new ClusterRole called \"manager-role\" in \n    \n    client.go:414: [debug] Created a new ClusterRole called \"spire-server-trust-role\" in \n    \n    client.go:414: [debug] Created a new ClusterRole called \"spire-agent-cluster-role\" in \n    \n    client.go:414: [debug] Created a new ClusterRole called \"kubeslice-kubernetes-dashboard\" in \n    \n    client.go:414: [debug] Created a new ClusterRole called \"kubeslice-dns-role\" in \n    \n    client.go:414: [debug] Created a new ClusterRole called \"kubeslice-manager-role\" in \n    \n    client.go:414: [debug] Created a new ClusterRole called \"kubeslice-metrics-reader\" in \n    \n    client.go:414: [debug] Created a new ClusterRole called \"kubeslice-proxy-role\" in \n    \n    client.go:414: [debug] Created a new ClusterRoleBinding called \"admission-webhook-binding\" in \n    \n    client.go:414: [debug] Created a new ClusterRoleBinding called \"nsm-role-binding\" in \n    \n    client.go:414: [debug] Created a new ClusterRoleBinding called \"manager-rolebinding\" in \n    \n    client.go:414: [debug] Created a new ClusterRoleBinding called \"spire-server-trust-role-binding\" in \n    \n    client.go:414: [debug] Created a new ClusterRoleBinding called \"spire-agent-cluster-role-binding\" in \n    \n    client.go:414: [debug] Created a new ClusterRoleBinding called \"kubeslice-kubernetes-dashboard\" in \n    \n    client.go:414: [debug] Created a new ClusterRoleBinding called \"kubeslice-dns-rolebinding\" in \n    \n    client.go:414: [debug] Created a new ClusterRoleBinding called \"kubeslice-manager-rolebinding\" in \n    \n    client.go:414: [debug] Created a new ClusterRoleBinding called \"kubeslice-proxy-rolebinding\" in \n    \n    client.go:414: [debug] Created a new Role called \"leader-election-role\" in spire\n    \n    client.go:414: [debug] Created a new Role called \"spire-server-role\" in spire\n    \n    client.go:414: [debug] Created a new Role called \"kubeslice-leader-election-role\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new RoleBinding called \"leader-election-rolebinding\" in spire\n    \n    client.go:414: [debug] Created a new RoleBinding called \"spire-server-role-binding\" in spire\n    \n    client.go:414: [debug] Created a new RoleBinding called \"kubeslice-leader-election-rolebinding\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new Service called \"admission-webhook-svc\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new Service called \"spire-server\" in spire\n    \n    client.go:414: [debug] Created a new Service called \"spire-controller-manager-webhook-service\" in spire\n    \n    client.go:414: [debug] Created a new Service called \"nsmgr\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new Service called \"registry\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new Service called \"kubeslice-dns\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new Service called \"kubeslice-webhook-service\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new DaemonSet called \"spiffe-csi-driver\" in spire\n    \n    client.go:414: [debug] Created a new DaemonSet called \"spire-agent\" in spire\n    \n    client.go:414: [debug] Created a new DaemonSet called \"forwarder-kernel\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new DaemonSet called \"nsmgr\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new DaemonSet called \"kubeslice-netop\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new Deployment called \"nsm-admission-webhook-k8s\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new Deployment called \"registry-k8s\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new Deployment called \"kubeslice-dns\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new Deployment called \"kubeslice-operator\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new StatefulSet called \"spire-server\" in spire\n    \n    client.go:684: [debug] Looks like there are no changes for CSIDriver \"csi.spiffe.io\"\n    client.go:414: [debug] Created a new MutatingWebhookConfiguration called \"kubeslice-mutating-webhook-configuration\" in \n    \n    client.go:414: [debug] Created a new ValidatingWebhookConfiguration called \"spire-controller-manager-webhook\" in \n    \n    wait.go:48: [debug] beginning wait for 74 resources with timeout of 10m0s\n    ready.go:336: [debug] DaemonSet is not ready: spire/spiffe-csi-driver. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spiffe-csi-driver. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spiffe-csi-driver. 1 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    install.go:488: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:102: [debug] uninstall: Deleting kubeslice-worker\n    client.go:486: [debug] Starting delete for \"nsm-delete-webhooks\" ClusterRole\n    client.go:490: [debug] Ignoring delete failure for \"nsm-delete-webhooks\" rbac.authorization.k8s.io/v1, Kind=ClusterRole: clusterroles.rbac.authorization.k8s.io \"nsm-delete-webhooks\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"nsm-delete-webhooks\" ClusterRoleBinding\n    client.go:490: [debug] Ignoring delete failure for \"nsm-delete-webhooks\" rbac.authorization.k8s.io/v1, Kind=ClusterRoleBinding: clusterrolebindings.rbac.authorization.k8s.io \"nsm-delete-webhooks\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"kubeslice-cleanup\" ServiceAccount\n    client.go:490: [debug] Ignoring delete failure for \"kubeslice-cleanup\" /v1, Kind=ServiceAccount: serviceaccounts \"kubeslice-cleanup\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"kubeslice-cleanup\" ClusterRole\n    client.go:490: [debug] Ignoring delete failure for \"kubeslice-cleanup\" rbac.authorization.k8s.io/v1, Kind=ClusterRole: clusterroles.rbac.authorization.k8s.io \"kubeslice-cleanup\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"kubeslice-cleanup\" ClusterRoleBinding\n    client.go:490: [debug] Ignoring delete failure for \"kubeslice-cleanup\" rbac.authorization.k8s.io/v1, Kind=ClusterRoleBinding: clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-cleanup\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"nsm-delete-webhooks\" ServiceAccount\n    client.go:490: [debug] Ignoring delete failure for \"nsm-delete-webhooks\" /v1, Kind=ServiceAccount: serviceaccounts \"nsm-delete-webhooks\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"nsm-delete-webhooks\" ConfigMap\n    client.go:490: [debug] Ignoring delete failure for \"nsm-delete-webhooks\" /v1, Kind=ConfigMap: configmaps \"nsm-delete-webhooks\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"kubeslice-cleanup\" Job\n    client.go:490: [debug] Ignoring delete failure for \"kubeslice-cleanup\" batch/v1, Kind=Job: jobs.batch \"kubeslice-cleanup\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:712: [debug] Watching for changes to Job kubeslice-cleanup with timeout of 10m0s\n    client.go:740: [debug] Add/Modify event for kubeslice-cleanup: ADDED\n    client.go:779: [debug] kubeslice-cleanup: Jobs active: 0, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for kubeslice-cleanup: MODIFIED\n    client.go:779: [debug] kubeslice-cleanup: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for kubeslice-cleanup: MODIFIED\n    client.go:486: [debug] Starting delete for \"nsm-delete-webhooks\" Job\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:712: [debug] Watching for changes to Job nsm-delete-webhooks with timeout of 10m0s\n    client.go:740: [debug] Add/Modify event for nsm-delete-webhooks: ADDED\n    client.go:779: [debug] nsm-delete-webhooks: Jobs active: 0, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for nsm-delete-webhooks: MODIFIED\n    client.go:779: [debug] nsm-delete-webhooks: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for nsm-delete-webhooks: MODIFIED\n    client.go:486: [debug] Starting delete for \"nsm-delete-webhooks\" ClusterRole\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"nsm-delete-webhooks\" ClusterRoleBinding\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"kubeslice-cleanup\" ServiceAccount\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"kubeslice-cleanup\" ClusterRole\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"kubeslice-cleanup\" ClusterRoleBinding\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"nsm-delete-webhooks\" ServiceAccount\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"nsm-delete-webhooks\" ConfigMap\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"kubeslice-cleanup\" Job\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    uninstall.go:248: [debug] uninstall: given cascade value: , defaulting to delete propagation background\n    client.go:486: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:486: [debug] Starting delete for \"admission-webhook-svc\" Service\n    client.go:486: [debug] Starting delete for \"spire-server\" Service\n    client.go:486: [debug] Starting delete for \"spire-controller-manager-webhook-service\" Service\n    client.go:486: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:486: [debug] Starting delete for \"nsmgr\" Service\n    client.go:486: [debug] Starting delete for \"registry\" Service\n    client.go:486: [debug] Starting delete for \"spire-server\" StatefulSet\n    client.go:486: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:486: [debug] Starting delete for \"nsm-admission-webhook-k8s\" Deployment\n    client.go:486: [debug] Starting delete for \"registry-k8s\" Deployment\n    client.go:486: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:486: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:486: [debug] Starting delete for \"spiffe-csi-driver\" DaemonSet\n    client.go:486: [debug] Starting delete for \"spire-agent\" DaemonSet\n    client.go:486: [debug] Starting delete for \"forwarder-kernel\" DaemonSet\n    client.go:486: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:486: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:486: [debug] Starting delete for \"leader-election-rolebinding\" RoleBinding\n    client.go:486: [debug] Starting delete for \"spire-server-role-binding\" RoleBinding\n    client.go:486: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:486: [debug] Starting delete for \"leader-election-role\" Role\n    client.go:486: [debug] Starting delete for \"spire-server-role\" Role\n    client.go:486: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:486: [debug] Starting delete for \"admission-webhook-binding\" ClusterRoleBinding\n    client.go:486: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:486: [debug] Starting delete for \"manager-rolebinding\" ClusterRoleBinding\n    client.go:486: [debug] Starting delete for \"spire-server-trust-role-binding\" ClusterRoleBinding\n    client.go:486: [debug] Starting delete for \"spire-agent-cluster-role-binding\" ClusterRoleBinding\n    client.go:486: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRoleBinding\n    client.go:486: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:486: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:486: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:486: [debug] Starting delete for \"spire-server-trust-role\" ClusterRole\n    client.go:486: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:486: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:486: [debug] Starting delete for \"spire-agent-cluster-role\" ClusterRole\n    client.go:486: [debug] Starting delete for \"admission-webhook-role\" ClusterRole\n    client.go:486: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRole\n    client.go:486: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:486: [debug] Starting delete for \"manager-role\" ClusterRole\n    client.go:486: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:486: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:486: [debug] Starting delete for \"kubeslice-worker-event-schema-conf\" ConfigMap\n    client.go:486: [debug] Starting delete for \"spire-bundle\" ConfigMap\n    client.go:486: [debug] Starting delete for \"spire-controller-manager-config\" ConfigMap\n    client.go:486: [debug] Starting delete for \"spire-server\" ConfigMap\n    client.go:486: [debug] Starting delete for \"spire-agent\" ConfigMap\n    client.go:486: [debug] Starting delete for \"nsmgr-cm\" ConfigMap\n    client.go:486: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:486: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:486: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:486: [debug] Starting delete for \"kubeslice-kubernetes-dashboard-creds\" Secret\n    client.go:486: [debug] Starting delete for \"spire-agent\" ServiceAccount\n    client.go:486: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ServiceAccount\n    client.go:486: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:486: [debug] Starting delete for \"kubeslice-gateway-edge\" ServiceAccount\n    client.go:486: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:486: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:486: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:486: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:486: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:486: [debug] Starting delete for \"admission-webhook-sa\" ServiceAccount\n    client.go:486: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:486: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:486: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:486: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:486: [debug] Starting delete for \"spiffe-csi-driver\" ServiceAccount\n    client.go:486: [debug] Starting delete for \"spire-server\" ServiceAccount\n    client.go:486: [debug] Starting delete for \"spire\" Namespace\n    client.go:486: [debug] Starting delete for \"nsm-webhook-high-priority\" PriorityClass\n    client.go:486: [debug] Starting delete for \"csi.spiffe.io\" CSIDriver\n    client.go:486: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:486: [debug] Starting delete for \"spire-controller-manager-webhook\" ValidatingWebhookConfiguration\n    client.go:486: [debug] Starting delete for \"kubeslice-postdelete-job\" ClusterRole\n    client.go:490: [debug] Ignoring delete failure for \"kubeslice-postdelete-job\" rbac.authorization.k8s.io/v1, Kind=ClusterRole: clusterroles.rbac.authorization.k8s.io \"kubeslice-postdelete-job\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"kubeslice-postdelete-job\" ClusterRoleBinding\n    client.go:490: [debug] Ignoring delete failure for \"kubeslice-postdelete-job\" rbac.authorization.k8s.io/v1, Kind=ClusterRoleBinding: clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-postdelete-job\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"kubeslice-postdelete-job\" ServiceAccount\n    client.go:490: [debug] Ignoring delete failure for \"kubeslice-postdelete-job\" /v1, Kind=ServiceAccount: serviceaccounts \"kubeslice-postdelete-job\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"kubeslice-postdelete-job\" ConfigMap\n    client.go:490: [debug] Ignoring delete failure for \"kubeslice-postdelete-job\" /v1, Kind=ConfigMap: configmaps \"kubeslice-postdelete-job\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"kubeslice-postdelete-job\" Job\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:712: [debug] Watching for changes to Job kubeslice-postdelete-job with timeout of 10m0s\n    client.go:740: [debug] Add/Modify event for kubeslice-postdelete-job: ADDED\n    client.go:779: [debug] kubeslice-postdelete-job: Jobs active: 0, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for kubeslice-postdelete-job: MODIFIED\n    client.go:779: [debug] kubeslice-postdelete-job: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for kubeslice-postdelete-job: MODIFIED\n    client.go:779: [debug] kubeslice-postdelete-job: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for kubeslice-postdelete-job: MODIFIED\n    client.go:486: [debug] Starting delete for \"kubeslice-postdelete-job\" ClusterRole\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"kubeslice-postdelete-job\" ClusterRoleBinding\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"kubeslice-postdelete-job\" ServiceAccount\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"kubeslice-postdelete-job\" ConfigMap\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    uninstall.go:155: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: client rate limiter Wait returned an error: context deadline exceeded\n    helm.go:84: [debug] client rate limiter Wait returned an error: context deadline exceeded\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:496\n    helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\n    \thelm.sh/helm/v3/pkg/action/install.go:394\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:306\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:152\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:983\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:1115\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:1039\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:267\n    runtime.goexit\n    \truntime/asm_amd64.s:1650\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:154\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:983\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:1115\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:1039\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:267\n    runtime.goexit\n    \truntime/asm_amd64.s:1650\noccurred","time":{"start":1710226523000,"stop":1710227184086,"duration":661086}},{"uid":"6b543d6ebc369ddd","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0003cc0a8>: {\n        Underlying: <*exec.ExitError | 0xc0008ca000>{\n            ProcessState: {\n                pid: 9320,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 8, Usec: 344463},\n                    Stime: {Sec: 0, Usec: 757822},\n                    Maxrss: 121136,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 32632,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 776,\n                    Oublock: 21592,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 56341,\n                    Nivcsw: 11672,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:214: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:231: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-1.2.1.tgz\",\n                        \"\",\n                        \"client.go:142: [debug] creating 1 resource(s)\",\n                        \"install.go:168: [debug] CRD serviceexports.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:142: [debug] creating 1 resource(s)\",\n                        \"install.go:168: [debug] CRD serviceimports.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:142: [debug] creating 1 resource(s)\",\n                        \"install.go:168: [debug] CRD slicegateways.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:142: [debug] creating 1 resource(s)\",\n                        \"install.go:168: [debug] CRD slices.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:142: [debug] creating 1 resource(s)\",\n                        \"install.go:168: [debug] CRD networkservices.networkservicemesh.io is already present. Skipping.\",\n                        \"client.go:142: [debug] creating 1 resource(s)\",\n                        \"install.go:168: [debug] CRD networkserviceendpoints.networkservicemesh.io is already present. Skipping.\",\n                        \"client.go:142: [debug] creating 1 resource(s)\",\n                        \"install.go:168: [debug] CRD clusterfederatedtrustdomains.spire.spiffe.io is already present. Skipping.\",\n                        \"client.go:142: [debug] creating 1 resource(s)\",\n                        \"install.go:168: [debug] CRD clusterspiffeids.spire.spiffe.io is already present. Skipping.\",\n                        \"client.go:142: [debug] creating 1 resource(s)\",\n                        \"client.go:486: [debug] Starting delete for \\\"kubeslice-install-crds\\\" ServiceAccount\",\n                        \"client.go:490: [debug] Ignoring delete failure for \\\"kubeslice-install-crds\\\" /v1, Kind=ServiceAccount: serviceaccounts \\\"kubeslice-install-crds\\\" not found\",\n                        \"wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\",\n                        \"client.go:142: [debug] creating 1 resource(s)\",\n                        \"client.go:486: [debug] Starting delete for \\\"kubeslice-install-crds\\\" ConfigMap\",\n                        \"client.go:490: [debug] Ignoring delete failure for \\\"kubeslice-install-crds\\\" /v1, Kind=ConfigMap: configmaps \\\"kubeslice-install-crds\\\" not found\",\n                        \"wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeo...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:214: [debug] Original chart version: \"\"\n    install.go:231: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-1.2.1.tgz\n    \n    client.go:142: [debug] creating 1 resource(s)\n    install.go:168: [debug] CRD serviceexports.networking.kubeslice.io is already present. Skipping.\n    client.go:142: [debug] creating 1 resource(s)\n    install.go:168: [debug] CRD serviceimports.networking.kubeslice.io is already present. Skipping.\n    client.go:142: [debug] creating 1 resource(s)\n    install.go:168: [debug] CRD slicegateways.networking.kubeslice.io is already present. Skipping.\n    client.go:142: [debug] creating 1 resource(s)\n    install.go:168: [debug] CRD slices.networking.kubeslice.io is already present. Skipping.\n    client.go:142: [debug] creating 1 resource(s)\n    install.go:168: [debug] CRD networkservices.networkservicemesh.io is already present. Skipping.\n    client.go:142: [debug] creating 1 resource(s)\n    install.go:168: [debug] CRD networkserviceendpoints.networkservicemesh.io is already present. Skipping.\n    client.go:142: [debug] creating 1 resource(s)\n    install.go:168: [debug] CRD clusterfederatedtrustdomains.spire.spiffe.io is already present. Skipping.\n    client.go:142: [debug] creating 1 resource(s)\n    install.go:168: [debug] CRD clusterspiffeids.spire.spiffe.io is already present. Skipping.\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"kubeslice-install-crds\" ServiceAccount\n    client.go:490: [debug] Ignoring delete failure for \"kubeslice-install-crds\" /v1, Kind=ServiceAccount: serviceaccounts \"kubeslice-install-crds\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"kubeslice-install-crds\" ConfigMap\n    client.go:490: [debug] Ignoring delete failure for \"kubeslice-install-crds\" /v1, Kind=ConfigMap: configmaps \"kubeslice-install-crds\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"kubeslice-install-crds\" ClusterRole\n    client.go:490: [debug] Ignoring delete failure for \"kubeslice-install-crds\" rbac.authorization.k8s.io/v1, Kind=ClusterRole: clusterroles.rbac.authorization.k8s.io \"kubeslice-install-crds\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"kubeslice-install-crds\" ClusterRoleBinding\n    client.go:490: [debug] Ignoring delete failure for \"kubeslice-install-crds\" rbac.authorization.k8s.io/v1, Kind=ClusterRoleBinding: clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-install-crds\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:490: [debug] Ignoring delete failure for \"nsm-config\" /v1, Kind=ConfigMap: configmaps \"nsm-config\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"nsm-crd-install\" ConfigMap\n    client.go:490: [debug] Ignoring delete failure for \"nsm-crd-install\" /v1, Kind=ConfigMap: configmaps \"nsm-crd-install\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"nsm-install-crds\" ServiceAccount\n    client.go:490: [debug] Ignoring delete failure for \"nsm-install-crds\" /v1, Kind=ServiceAccount: serviceaccounts \"nsm-install-crds\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"nsm-install-crds\" ClusterRole\n    client.go:490: [debug] Ignoring delete failure for \"nsm-install-crds\" rbac.authorization.k8s.io/v1, Kind=ClusterRole: clusterroles.rbac.authorization.k8s.io \"nsm-install-crds\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"nsm-install-crds\" ClusterRoleBinding\n    client.go:490: [debug] Ignoring delete failure for \"nsm-install-crds\" rbac.authorization.k8s.io/v1, Kind=ClusterRoleBinding: clusterrolebindings.rbac.authorization.k8s.io \"nsm-install-crds\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"spire-clusterid-cr-install\" ConfigMap\n    client.go:490: [debug] Ignoring delete failure for \"spire-clusterid-cr-install\" /v1, Kind=ConfigMap: configmaps \"spire-clusterid-cr-install\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"spire-crd-install\" ConfigMap\n    client.go:490: [debug] Ignoring delete failure for \"spire-crd-install\" /v1, Kind=ConfigMap: configmaps \"spire-crd-install\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"spire-install-clusterid-cr\" ServiceAccount\n    client.go:490: [debug] Ignoring delete failure for \"spire-install-clusterid-cr\" /v1, Kind=ServiceAccount: serviceaccounts \"spire-install-clusterid-cr\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"spire-install-clusterid-cr\" ClusterRole\n    client.go:490: [debug] Ignoring delete failure for \"spire-install-clusterid-cr\" rbac.authorization.k8s.io/v1, Kind=ClusterRole: clusterroles.rbac.authorization.k8s.io \"spire-install-clusterid-cr\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"spire-install-clusterid-cr\" ClusterRoleBinding\n    client.go:490: [debug] Ignoring delete failure for \"spire-install-clusterid-cr\" rbac.authorization.k8s.io/v1, Kind=ClusterRoleBinding: clusterrolebindings.rbac.authorization.k8s.io \"spire-install-clusterid-cr\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"spire-install-crds\" ServiceAccount\n    client.go:490: [debug] Ignoring delete failure for \"spire-install-crds\" /v1, Kind=ServiceAccount: serviceaccounts \"spire-install-crds\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"spire-install-crds\" ClusterRole\n    client.go:490: [debug] Ignoring delete failure for \"spire-install-crds\" rbac.authorization.k8s.io/v1, Kind=ClusterRole: clusterroles.rbac.authorization.k8s.io \"spire-install-crds\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"spire-install-crds\" ClusterRoleBinding\n    client.go:490: [debug] Ignoring delete failure for \"spire-install-crds\" rbac.authorization.k8s.io/v1, Kind=ClusterRoleBinding: clusterrolebindings.rbac.authorization.k8s.io \"spire-install-crds\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"kubeslice-install-crds\" Job\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:712: [debug] Watching for changes to Job kubeslice-install-crds with timeout of 10m0s\n    client.go:740: [debug] Add/Modify event for kubeslice-install-crds: ADDED\n    client.go:779: [debug] kubeslice-install-crds: Jobs active: 0, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for kubeslice-install-crds: MODIFIED\n    client.go:779: [debug] kubeslice-install-crds: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for kubeslice-install-crds: MODIFIED\n    client.go:779: [debug] kubeslice-install-crds: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for kubeslice-install-crds: MODIFIED\n    client.go:779: [debug] kubeslice-install-crds: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for kubeslice-install-crds: MODIFIED\n    client.go:486: [debug] Starting delete for \"nsm-install-crds\" Job\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:712: [debug] Watching for changes to Job nsm-install-crds with timeout of 10m0s\n    client.go:740: [debug] Add/Modify event for nsm-install-crds: ADDED\n    client.go:779: [debug] nsm-install-crds: Jobs active: 0, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for nsm-install-crds: MODIFIED\n    client.go:779: [debug] nsm-install-crds: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for nsm-install-crds: MODIFIED\n    client.go:486: [debug] Starting delete for \"spire-install-crds\" Job\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:712: [debug] Watching for changes to Job spire-install-crds with timeout of 10m0s\n    client.go:740: [debug] Add/Modify event for spire-install-crds: ADDED\n    client.go:779: [debug] spire-install-crds: Jobs active: 0, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for spire-install-crds: MODIFIED\n    client.go:779: [debug] spire-install-crds: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for spire-install-crds: MODIFIED\n    client.go:779: [debug] spire-install-crds: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for spire-install-crds: MODIFIED\n    client.go:779: [debug] spire-install-crds: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for spire-install-crds: MODIFIED\n    client.go:486: [debug] Starting delete for \"spire-install-clusterid-cr\" Job\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:712: [debug] Watching for changes to Job spire-install-clusterid-cr with timeout of 10m0s\n    client.go:740: [debug] Add/Modify event for spire-install-clusterid-cr: ADDED\n    client.go:779: [debug] spire-install-clusterid-cr: Jobs active: 0, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for spire-install-clusterid-cr: MODIFIED\n    client.go:779: [debug] spire-install-clusterid-cr: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for spire-install-clusterid-cr: MODIFIED\n    client.go:779: [debug] spire-install-clusterid-cr: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for spire-install-clusterid-cr: MODIFIED\n    client.go:779: [debug] spire-install-clusterid-cr: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for spire-install-clusterid-cr: MODIFIED\n    client.go:486: [debug] Starting delete for \"kubeslice-install-crds\" ServiceAccount\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"kubeslice-install-crds\" ConfigMap\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"kubeslice-install-crds\" ClusterRole\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"kubeslice-install-crds\" ClusterRoleBinding\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"nsm-crd-install\" ConfigMap\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"nsm-install-crds\" ServiceAccount\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"nsm-install-crds\" ClusterRole\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"nsm-install-crds\" ClusterRoleBinding\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"spire-clusterid-cr-install\" ConfigMap\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"spire-crd-install\" ConfigMap\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"spire-install-clusterid-cr\" ServiceAccount\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"spire-install-clusterid-cr\" ClusterRole\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"spire-install-clusterid-cr\" ClusterRoleBinding\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"spire-install-crds\" ServiceAccount\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"spire-install-crds\" ClusterRole\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"spire-install-crds\" ClusterRoleBinding\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:393: [debug] checking 74 resources for changes\n    client.go:414: [debug] Created a new PriorityClass called \"nsm-webhook-high-priority\" in \n    \n    client.go:414: [debug] Created a new Namespace called \"spire\" in \n    \n    client.go:414: [debug] Created a new ServiceAccount called \"admission-webhook-sa\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new ServiceAccount called \"nse-acc\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new ServiceAccount called \"nsc-acc\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new ServiceAccount called \"nsmgr-acc\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new ServiceAccount called \"forward-plane-acc\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new ServiceAccount called \"spiffe-csi-driver\" in spire\n    \n    client.go:414: [debug] Created a new ServiceAccount called \"spire-server\" in spire\n    \n    client.go:414: [debug] Created a new ServiceAccount called \"spire-agent\" in spire\n    \n    client.go:414: [debug] Created a new ServiceAccount called \"kubeslice-kubernetes-dashboard\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new ServiceAccount called \"kubeslice-dns\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new ServiceAccount called \"kubeslice-gateway-edge\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new ServiceAccount called \"kubeslice-netop\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new ServiceAccount called \"vpn-gateway-server\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new ServiceAccount called \"vpn-gateway-client\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new ServiceAccount called \"slice-router\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new ServiceAccount called \"kubeslice-controller-manager\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new Secret called \"kubeslice-kubernetes-dashboard-creds\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new Secret called \"kubeslice-hub\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new Secret called \"kubeslice-admission-webhook-certs\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new ConfigMap called \"spire-bundle\" in spire\n    \n    client.go:414: [debug] Created a new ConfigMap called \"spire-controller-manager-config\" in spire\n    \n    client.go:414: [debug] Created a new ConfigMap called \"spire-server\" in spire\n    \n    client.go:414: [debug] Created a new ConfigMap called \"spire-agent\" in spire\n    \n    client.go:414: [debug] Created a new ConfigMap called \"nsmgr-cm\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new ConfigMap called \"kubeslice-manager-config\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new ConfigMap called \"kubeslice-worker-event-schema-conf\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new ClusterRole called \"admission-webhook-role\" in \n    \n    client.go:414: [debug] Created a new ClusterRole called \"nsm-role\" in \n    \n    client.go:414: [debug] Created a new ClusterRole called \"aggregate-network-services-view\" in \n    \n    client.go:414: [debug] Created a new ClusterRole called \"manager-role\" in \n    \n    client.go:414: [debug] Created a new ClusterRole called \"spire-server-trust-role\" in \n    \n    client.go:414: [debug] Created a new ClusterRole called \"spire-agent-cluster-role\" in \n    \n    client.go:414: [debug] Created a new ClusterRole called \"kubeslice-kubernetes-dashboard\" in \n    \n    client.go:414: [debug] Created a new ClusterRole called \"kubeslice-dns-role\" in \n    \n    client.go:414: [debug] Created a new ClusterRole called \"kubeslice-manager-role\" in \n    \n    client.go:414: [debug] Created a new ClusterRole called \"kubeslice-metrics-reader\" in \n    \n    client.go:414: [debug] Created a new ClusterRole called \"kubeslice-proxy-role\" in \n    \n    client.go:414: [debug] Created a new ClusterRoleBinding called \"admission-webhook-binding\" in \n    \n    client.go:414: [debug] Created a new ClusterRoleBinding called \"nsm-role-binding\" in \n    \n    client.go:414: [debug] Created a new ClusterRoleBinding called \"manager-rolebinding\" in \n    \n    client.go:414: [debug] Created a new ClusterRoleBinding called \"spire-server-trust-role-binding\" in \n    \n    client.go:414: [debug] Created a new ClusterRoleBinding called \"spire-agent-cluster-role-binding\" in \n    \n    client.go:414: [debug] Created a new ClusterRoleBinding called \"kubeslice-kubernetes-dashboard\" in \n    \n    client.go:414: [debug] Created a new ClusterRoleBinding called \"kubeslice-dns-rolebinding\" in \n    \n    client.go:414: [debug] Created a new ClusterRoleBinding called \"kubeslice-manager-rolebinding\" in \n    \n    client.go:414: [debug] Created a new ClusterRoleBinding called \"kubeslice-proxy-rolebinding\" in \n    \n    client.go:414: [debug] Created a new Role called \"leader-election-role\" in spire\n    \n    client.go:414: [debug] Created a new Role called \"spire-server-role\" in spire\n    \n    client.go:414: [debug] Created a new Role called \"kubeslice-leader-election-role\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new RoleBinding called \"leader-election-rolebinding\" in spire\n    \n    client.go:414: [debug] Created a new RoleBinding called \"spire-server-role-binding\" in spire\n    \n    client.go:414: [debug] Created a new RoleBinding called \"kubeslice-leader-election-rolebinding\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new Service called \"admission-webhook-svc\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new Service called \"spire-server\" in spire\n    \n    client.go:414: [debug] Created a new Service called \"spire-controller-manager-webhook-service\" in spire\n    \n    client.go:414: [debug] Created a new Service called \"nsmgr\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new Service called \"registry\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new Service called \"kubeslice-dns\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new Service called \"kubeslice-webhook-service\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new DaemonSet called \"spiffe-csi-driver\" in spire\n    \n    client.go:414: [debug] Created a new DaemonSet called \"spire-agent\" in spire\n    \n    client.go:414: [debug] Created a new DaemonSet called \"forwarder-kernel\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new DaemonSet called \"nsmgr\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new DaemonSet called \"kubeslice-netop\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new Deployment called \"nsm-admission-webhook-k8s\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new Deployment called \"registry-k8s\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new Deployment called \"kubeslice-dns\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new Deployment called \"kubeslice-operator\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new StatefulSet called \"spire-server\" in spire\n    \n    client.go:684: [debug] Looks like there are no changes for CSIDriver \"csi.spiffe.io\"\n    client.go:414: [debug] Created a new MutatingWebhookConfiguration called \"kubeslice-mutating-webhook-configuration\" in \n    \n    client.go:414: [debug] Created a new ValidatingWebhookConfiguration called \"spire-controller-manager-webhook\" in \n    \n    wait.go:48: [debug] beginning wait for 74 resources with timeout of 10m0s\n    ready.go:336: [debug] DaemonSet is not ready: spire/spiffe-csi-driver. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spiffe-csi-driver. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spiffe-csi-driver. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    install.go:488: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:102: [debug] uninstall: Deleting kubeslice-worker\n    client.go:486: [debug] Starting delete for \"nsm-delete-webhooks\" ClusterRole\n    client.go:490: [debug] Ignoring delete failure for \"nsm-delete-webhooks\" rbac.authorization.k8s.io/v1, Kind=ClusterRole: clusterroles.rbac.authorization.k8s.io \"nsm-delete-webhooks\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"nsm-delete-webhooks\" ClusterRoleBinding\n    client.go:490: [debug] Ignoring delete failure for \"nsm-delete-webhooks\" rbac.authorization.k8s.io/v1, Kind=ClusterRoleBinding: clusterrolebindings.rbac.authorization.k8s.io \"nsm-delete-webhooks\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"kubeslice-cleanup\" ServiceAccount\n    client.go:490: [debug] Ignoring delete failure for \"kubeslice-cleanup\" /v1, Kind=ServiceAccount: serviceaccounts \"kubeslice-cleanup\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"kubeslice-cleanup\" ClusterRole\n    client.go:490: [debug] Ignoring delete failure for \"kubeslice-cleanup\" rbac.authorization.k8s.io/v1, Kind=ClusterRole: clusterroles.rbac.authorization.k8s.io \"kubeslice-cleanup\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"kubeslice-cleanup\" ClusterRoleBinding\n    client.go:490: [debug] Ignoring delete failure for \"kubeslice-cleanup\" rbac.authorization.k8s.io/v1, Kind=ClusterRoleBinding: clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-cleanup\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"nsm-delete-webhooks\" ServiceAccount\n    client.go:490: [debug] Ignoring delete failure for \"nsm-delete-webhooks\" /v1, Kind=ServiceAccount: serviceaccounts \"nsm-delete-webhooks\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"nsm-delete-webhooks\" ConfigMap\n    client.go:490: [debug] Ignoring delete failure for \"nsm-delete-webhooks\" /v1, Kind=ConfigMap: configmaps \"nsm-delete-webhooks\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"kubeslice-cleanup\" Job\n    client.go:490: [debug] Ignoring delete failure for \"kubeslice-cleanup\" batch/v1, Kind=Job: jobs.batch \"kubeslice-cleanup\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:712: [debug] Watching for changes to Job kubeslice-cleanup with timeout of 10m0s\n    client.go:740: [debug] Add/Modify event for kubeslice-cleanup: ADDED\n    client.go:779: [debug] kubeslice-cleanup: Jobs active: 0, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for kubeslice-cleanup: MODIFIED\n    client.go:779: [debug] kubeslice-cleanup: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for kubeslice-cleanup: MODIFIED\n    client.go:486: [debug] Starting delete for \"nsm-delete-webhooks\" Job\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:712: [debug] Watching for changes to Job nsm-delete-webhooks with timeout of 10m0s\n    client.go:740: [debug] Add/Modify event for nsm-delete-webhooks: ADDED\n    client.go:779: [debug] nsm-delete-webhooks: Jobs active: 0, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for nsm-delete-webhooks: MODIFIED\n    client.go:779: [debug] nsm-delete-webhooks: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for nsm-delete-webhooks: MODIFIED\n    client.go:486: [debug] Starting delete for \"nsm-delete-webhooks\" ClusterRole\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"nsm-delete-webhooks\" ClusterRoleBinding\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"kubeslice-cleanup\" ServiceAccount\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"kubeslice-cleanup\" ClusterRole\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"kubeslice-cleanup\" ClusterRoleBinding\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"nsm-delete-webhooks\" ServiceAccount\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"nsm-delete-webhooks\" ConfigMap\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"kubeslice-cleanup\" Job\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    uninstall.go:248: [debug] uninstall: given cascade value: , defaulting to delete propagation background\n    client.go:486: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:486: [debug] Starting delete for \"admission-webhook-svc\" Service\n    client.go:486: [debug] Starting delete for \"registry\" Service\n    client.go:486: [debug] Starting delete for \"nsmgr\" Service\n    client.go:486: [debug] Starting delete for \"spire-controller-manager-webhook-service\" Service\n    client.go:486: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:486: [debug] Starting delete for \"spire-server\" Service\n    client.go:486: [debug] Starting delete for \"spire-server\" StatefulSet\n    client.go:486: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:486: [debug] Starting delete for \"nsm-admission-webhook-k8s\" Deployment\n    client.go:486: [debug] Starting delete for \"registry-k8s\" Deployment\n    client.go:486: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:486: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:486: [debug] Starting delete for \"spiffe-csi-driver\" DaemonSet\n    client.go:486: [debug] Starting delete for \"spire-agent\" DaemonSet\n    client.go:486: [debug] Starting delete for \"forwarder-kernel\" DaemonSet\n    client.go:486: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:486: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:486: [debug] Starting delete for \"leader-election-rolebinding\" RoleBinding\n    client.go:486: [debug] Starting delete for \"spire-server-role-binding\" RoleBinding\n    client.go:486: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:486: [debug] Starting delete for \"leader-election-role\" Role\n    client.go:486: [debug] Starting delete for \"spire-server-role\" Role\n    client.go:486: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:486: [debug] Starting delete for \"admission-webhook-binding\" ClusterRoleBinding\n    client.go:486: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:486: [debug] Starting delete for \"manager-rolebinding\" ClusterRoleBinding\n    client.go:486: [debug] Starting delete for \"spire-server-trust-role-binding\" ClusterRoleBinding\n    client.go:486: [debug] Starting delete for \"spire-agent-cluster-role-binding\" ClusterRoleBinding\n    client.go:486: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRoleBinding\n    client.go:486: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:486: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:486: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:486: [debug] Starting delete for \"admission-webhook-role\" ClusterRole\n    client.go:486: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:486: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:486: [debug] Starting delete for \"manager-role\" ClusterRole\n    client.go:486: [debug] Starting delete for \"spire-server-trust-role\" ClusterRole\n    client.go:486: [debug] Starting delete for \"spire-agent-cluster-role\" ClusterRole\n    client.go:486: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:486: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRole\n    client.go:486: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:486: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:486: [debug] Starting delete for \"kubeslice-worker-event-schema-conf\" ConfigMap\n    client.go:486: [debug] Starting delete for \"spire-bundle\" ConfigMap\n    client.go:486: [debug] Starting delete for \"spire-controller-manager-config\" ConfigMap\n    client.go:486: [debug] Starting delete for \"spire-server\" ConfigMap\n    client.go:486: [debug] Starting delete for \"nsmgr-cm\" ConfigMap\n    client.go:486: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:486: [debug] Starting delete for \"spire-agent\" ConfigMap\n    client.go:486: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:486: [debug] Starting delete for \"kubeslice-kubernetes-dashboard-creds\" Secret\n    client.go:486: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:486: [debug] Starting delete for \"spire-agent\" ServiceAccount\n    client.go:486: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ServiceAccount\n    client.go:486: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:486: [debug] Starting delete for \"kubeslice-gateway-edge\" ServiceAccount\n    client.go:486: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:486: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:486: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:486: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:486: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:486: [debug] Starting delete for \"admission-webhook-sa\" ServiceAccount\n    client.go:486: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:486: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:486: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:486: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:486: [debug] Starting delete for \"spiffe-csi-driver\" ServiceAccount\n    client.go:486: [debug] Starting delete for \"spire-server\" ServiceAccount\n    client.go:486: [debug] Starting delete for \"spire\" Namespace\n    client.go:486: [debug] Starting delete for \"nsm-webhook-high-priority\" PriorityClass\n    client.go:486: [debug] Starting delete for \"csi.spiffe.io\" CSIDriver\n    client.go:486: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:486: [debug] Starting delete for \"spire-controller-manager-webhook\" ValidatingWebhookConfiguration\n    client.go:486: [debug] Starting delete for \"kubeslice-postdelete-job\" ClusterRole\n    client.go:490: [debug] Ignoring delete failure for \"kubeslice-postdelete-job\" rbac.authorization.k8s.io/v1, Kind=ClusterRole: clusterroles.rbac.authorization.k8s.io \"kubeslice-postdelete-job\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"kubeslice-postdelete-job\" ClusterRoleBinding\n    client.go:490: [debug] Ignoring delete failure for \"kubeslice-postdelete-job\" rbac.authorization.k8s.io/v1, Kind=ClusterRoleBinding: clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-postdelete-job\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"kubeslice-postdelete-job\" ServiceAccount\n    client.go:490: [debug] Ignoring delete failure for \"kubeslice-postdelete-job\" /v1, Kind=ServiceAccount: serviceaccounts \"kubeslice-postdelete-job\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"kubeslice-postdelete-job\" ConfigMap\n    client.go:490: [debug] Ignoring delete failure for \"kubeslice-postdelete-job\" /v1, Kind=ConfigMap: configmaps \"kubeslice-postdelete-job\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"kubeslice-postdelete-job\" Job\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:712: [debug] Watching for changes to Job kubeslice-postdelete-job with timeout of 10m0s\n    client.go:740: [debug] Add/Modify event for kubeslice-postdelete-job: ADDED\n    client.go:779: [debug] kubeslice-postdelete-job: Jobs active: 0, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for kubeslice-postdelete-job: MODIFIED\n    client.go:779: [debug] kubeslice-postdelete-job: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for kubeslice-postdelete-job: MODIFIED\n    client.go:486: [debug] Starting delete for \"kubeslice-postdelete-job\" ClusterRole\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"kubeslice-postdelete-job\" ClusterRoleBinding\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"kubeslice-postdelete-job\" ServiceAccount\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"kubeslice-postdelete-job\" ConfigMap\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    uninstall.go:155: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: client rate limiter Wait returned an error: context deadline exceeded\n    helm.go:84: [debug] client rate limiter Wait returned an error: context deadline exceeded\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:496\n    helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\n    \thelm.sh/helm/v3/pkg/action/install.go:394\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:306\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:152\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:983\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:1115\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:1039\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:267\n    runtime.goexit\n    \truntime/asm_amd64.s:1650\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:154\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:983\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:1115\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:1039\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:267\n    runtime.goexit\n    \truntime/asm_amd64.s:1650\noccurred","time":{"start":1709885462000,"stop":1709886121435,"duration":659435}},{"uid":"b22f5254a6dcd7aa","status":"passed","time":{"start":1709723127000,"stop":1709723320067,"duration":193067}},{"uid":"4bb7ded7f53c95be","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000e480>: {\n        Underlying: <*exec.ExitError | 0xc0004f8240>{\n            ProcessState: {\n                pid: 6479,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 112083},\n                    Stime: {Sec: 0, Usec: 20378},\n                    Maxrss: 50008,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2469,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 176,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 652,\n                    Nivcsw: 448,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:200: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:217: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-1.1.2.tgz\",\n                        \"\",\n                        \"Error: INSTALLATION FAILED: cannot re-use a name that is still in use\",\n                        \"helm.go:84: [debug] cannot re-use a name that is still in use\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).availableName\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:517\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:227\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:287\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:145\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.6.1/command.go:916\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.6.1/command.go:1044\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.6.1/command.go:968\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:250\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1598\",\n                        \"INSTALLATION FAILED\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:147\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.6.1/command.go:916\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.6.1/command.go:1044\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.6.1/command.go:968\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:250\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1598\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is in...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:200: [debug] Original chart version: \"\"\n    install.go:217: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-1.1.2.tgz\n    \n    Error: INSTALLATION FAILED: cannot re-use a name that is still in use\n    helm.go:84: [debug] cannot re-use a name that is still in use\n    helm.sh/helm/v3/pkg/action.(*Install).availableName\n    \thelm.sh/helm/v3/pkg/action/install.go:517\n    helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\n    \thelm.sh/helm/v3/pkg/action/install.go:227\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:287\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:145\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:916\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:1044\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:968\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1598\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:147\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:916\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:1044\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:968\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1598\noccurred","time":{"start":1695807042000,"stop":1695807042621,"duration":621}},{"uid":"1f38bf34b0805f26","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000014e88>: {\n        Underlying: <*exec.ExitError | 0xc000476540>{\n            ProcessState: {\n                pid: 6405,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 89072},\n                    Stime: {Sec: 0, Usec: 32390},\n                    Maxrss: 48120,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2149,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 168,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 542,\n                    Nivcsw: 277,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:200: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:217: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-1.2.0.tgz\",\n                        \"\",\n                        \"Error: INSTALLATION FAILED: cannot re-use a name that is still in use\",\n                        \"helm.go:84: [debug] cannot re-use a name that is still in use\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).availableName\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:512\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:222\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:286\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:145\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.6.1/command.go:916\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.6.1/command.go:1044\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.6.1/command.go:968\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:250\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1598\",\n                        \"INSTALLATION FAILED\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:147\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.6.1/command.go:916\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.6.1/command.go:1044\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.6.1/command.go:968\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:250\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1598\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is ins...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:200: [debug] Original chart version: \"\"\n    install.go:217: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-1.2.0.tgz\n    \n    Error: INSTALLATION FAILED: cannot re-use a name that is still in use\n    helm.go:84: [debug] cannot re-use a name that is still in use\n    helm.sh/helm/v3/pkg/action.(*Install).availableName\n    \thelm.sh/helm/v3/pkg/action/install.go:512\n    helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\n    \thelm.sh/helm/v3/pkg/action/install.go:222\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:286\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:145\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:916\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:1044\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:968\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1598\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:147\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:916\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:1044\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:968\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1598\noccurred","time":{"start":1689751363000,"stop":1689751363398,"duration":398}},{"uid":"627308c2e51d2e47","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000128d50>: {\n        Underlying: <*exec.ExitError | 0xc0003fbf60>{\n            ProcessState: {\n                pid: 6379,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 103836},\n                    Stime: {Sec: 0, Usec: 23962},\n                    Maxrss: 48128,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2302,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 168,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 517,\n                    Nivcsw: 278,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:200: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:217: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-1.2.0.tgz\",\n                        \"\",\n                        \"Error: INSTALLATION FAILED: cannot re-use a name that is still in use\",\n                        \"helm.go:84: [debug] cannot re-use a name that is still in use\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).availableName\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:512\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:222\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:286\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:145\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.6.1/command.go:916\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.6.1/command.go:1044\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.6.1/command.go:968\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:250\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1598\",\n                        \"INSTALLATION FAILED\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:147\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.6.1/command.go:916\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.6.1/command.go:1044\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.6.1/command.go:968\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:250\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1598\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is in...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:200: [debug] Original chart version: \"\"\n    install.go:217: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-1.2.0.tgz\n    \n    Error: INSTALLATION FAILED: cannot re-use a name that is still in use\n    helm.go:84: [debug] cannot re-use a name that is still in use\n    helm.sh/helm/v3/pkg/action.(*Install).availableName\n    \thelm.sh/helm/v3/pkg/action/install.go:512\n    helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\n    \thelm.sh/helm/v3/pkg/action/install.go:222\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:286\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:145\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:916\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:1044\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:968\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1598\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:147\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:916\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:1044\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:968\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1598\noccurred","time":{"start":1688628155000,"stop":1688628155543,"duration":543}},{"uid":"ca2ade5904afb48e","status":"passed","time":{"start":1688026084000,"stop":1688026304701,"duration":220701}},{"uid":"61d09ccb032bdb72","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004a4558>: {\n        Underlying: <*exec.ExitError | 0xc0004b01a0>{\n            ProcessState: {\n                pid: 6319,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 69266},\n                    Stime: {Sec: 0, Usec: 4329},\n                    Maxrss: 43880,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2030,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 351,\n                    Nivcsw: 293,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: error executing jsonpath \\\"\\\\\\\"{.clusters[?(@.name==\\\\\\\"kind-controller\\\\\\\")].cluster.server}\\\\\\\"\\\": Error executing template: <nil> is not array or slice and cannot be filtered. Printing more information for debugging the template:\",\n                        \"\\ttemplate was:\",\n                        \"\\t\\t\\\"{.clusters[?(@.name==\\\"kind-controller\\\")].cluster.server}\\\"\",\n                        \"\\tobject given to jsonpath engine was:\",\n                        \"\\t\\tmap[string]interface {}{\\\"apiVersion\\\":\\\"v1\\\", \\\"clusters\\\":interface {}(nil), \\\"contexts\\\":interface {}(nil), \\\"current-context\\\":\\\"\\\", \\\"kind\\\":\\\"Config\\\", \\\"preferences\\\":map[string]interface {}{}, \\\"users\\\":interface {}(nil)}\",\n                        \"\",\n                        \"\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: error executing jsonpath \\\"\\\\\\\"{.clusters[?(@.name==\\\\\\\"kind-controller\\\\\\\")].cluster.server}\\\\\\\"\\\": Error executing template: <nil> is not array or slice and cannot be filtered. Printing more information for debugging the template:\",\n                    \"\\ttemplate was:\",\n                    \"\\t\\t\\\"{.clusters[?(@.name==\\\"kind-controller\\\")].cluster.server}\\\"\",\n                    \"\\tobject given to jsonpath engine was:\",\n                    \"\\t\\tmap[string]interface {}{\\\"apiVersion\\\":\\\"v1\\\", \\\"clusters\\\":interface {}(nil), \\\"contexts\\\":interface {}(nil), \\\"current-context\\\":\\\"\\\", \\\"kind\\\":\\\"Config\\\", \\\"preferences\\\":map[string]interface {}{}, \\\"users\\\":interface {}(nil)}\",\n                    \"\",\n                    \"\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: error executing jsonpath \\\"\\\\\\\"{.clusters[?(@.name==\\\\\\\"kind-controller\\\\\\\")].cluster.server}\\\\\\\"\\\": Error executing template: <nil> is not array or slice and cannot be filtered. Printing more information for debugging the template:\",\n                        \"\\ttemplate was:\",\n                        \"\\t\\t\\\"{.clusters[?(@.name==\\\"kind-controller\\\")].cluster.server}\\\"\",\n                        \"\\tobject given to jsonpath engine was:\",\n                        \"\\t\\tmap[string]interface {}{\\\"apiVersion\\\":\\\"v1\\\", \\\"clusters\\\":interface {}(nil), \\\"contexts\\\":interface {}(nil), \\\"current-context\\\":\\\"\\\", \\\"kind\\\":\\\"Config\\\", \\\"preferences\\\":map[string]interface {}{}, \\\"users\\\":interface {}(nil)}\",\n                        \"\",\n                        \"\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: error executing jsonpath \\\"\\\\\\\"{.clusters[?(@.name==\\\\\\\"kind-controller\\\\\\\")].cluster.server}\\\\\\\"\\\": Error executing template: <nil> is not array or slice and cannot be filtered. Printing more information for debugging the template:\",\n                    \"\\ttemplate was:\",\n                    \"\\t\\t\\\"{.clusters[?(@...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; error: error executing jsonpath \"\\\"{.clusters[?(@.name==\\\"kind-controller\\\")].cluster.server}\\\"\": Error executing template: <nil> is not array or slice and cannot be filtered. Printing more information for debugging the template:\n    \ttemplate was:\n    \t\t\"{.clusters[?(@.name==\"kind-controller\")].cluster.server}\"\n    \tobject given to jsonpath engine was:\n    \t\tmap[string]interface {}{\"apiVersion\":\"v1\", \"clusters\":interface {}(nil), \"contexts\":interface {}(nil), \"current-context\":\"\", \"kind\":\"Config\", \"preferences\":map[string]interface {}{}, \"users\":interface {}(nil)}\n    \n    \noccurred","time":{"start":1688018435000,"stop":1688018435086,"duration":86}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting mesh-dns pod Should restart mesh-dns pod":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":0,"unknown":0,"total":1},"items":[{"uid":"9b16d295916c435f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1709719453000,"stop":1709719453000,"duration":0}}]},"Controller Suite:Controller Suite#[It] Slice RBAC Tests RBAC Common Tests WorkerSliceRoleBinding removal test: If roles are removed from SliceRolebinding, WorkerSliceRoleBinding should be removed":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":0,"unknown":0,"total":2},"items":[{"uid":"63291d83bde35f24","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693899867000,"stop":1693899867000,"duration":0}},{"uid":"5ecf96e1cc024463","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Controller Suite:Controller Suite#[AfterSuite]":{"statistic":{"failed":6,"broken":0,"skipped":0,"passed":0,"unknown":0,"total":6},"items":[{"uid":"e310d6788a5bbab2","status":"failed","statusDetails":"Timed out after 500.031s.\nExpected\n    <bool>: false\nto be true","time":{"start":1693899867000,"stop":1693900389139,"duration":522139}},{"uid":"1f6d30ab5086eaff","status":"failed","statusDetails":"Timed out after 500.031s.\nExpected\n    <bool>: false\nto be true","time":{"start":1693828311000,"stop":1693828850406,"duration":539406}},{"uid":"e5bc5e2fe607b923","status":"failed","statusDetails":"Timed out after 500.014s.\nExpected\n    <bool>: false\nto be true","time":{"start":1690377721000,"stop":1690378221496,"duration":500496}},{"uid":"3298ab024528031c","status":"failed","statusDetails":"Timed out after 500.015s.\nExpected\n    <bool>: false\nto be true","time":{"start":1690371125000,"stop":1690371625278,"duration":500278}},{"uid":"cdcc37c6ecf1f84f","status":"failed","statusDetails":"Timed out after 500.017s.\nExpected\n    <bool>: false\nto be true","time":{"start":1689758771000,"stop":1689759271018,"duration":500018}},{"uid":"4b58394970be3af6","status":"failed","statusDetails":"Timed out after 500.000s.\nExpected\n    <bool>: false\nto be true","time":{"start":1689755137000,"stop":1689755637001,"duration":500001}}]},"Controller Suite:Controller Suite#[It] Slice node affinity Tests SliceNodeAffinity tests - includes creation and deletion slice node affinity creation -failed case if wild card * for cluster is repeated":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":1,"unknown":0,"total":2},"items":[{"uid":"3ee56103c6e71db3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693899867000,"stop":1693899867000,"duration":0}},{"uid":"572164c87adf18b0","status":"passed","time":{"start":1693828311000,"stop":1693828319829,"duration":8829}}]},"Controller Suite:Controller Suite#[It] Slice resource quota Tests ResourceQuota creation tests Create ResourceQuota fail test-namespace.limit.mem negative":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":1,"unknown":0,"total":2},"items":[{"uid":"4c0ac4fc74a15191","status":"passed","time":{"start":1693899867000,"stop":1693899871643,"duration":4643}},{"uid":"89af0e9d9b49a9c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Namespace reconcile test scenarios: when Slice is created with onboarded application namespaces Should have application namespaces created & labeled":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"bebb8b5823b80b","status":"passed","time":{"start":1709719453000,"stop":1709719471156,"duration":18156}}]},"Controller Suite:Controller Suite#[It] Slice resource quota Tests ResourceQuota creation tests Create ResourceQuota fail test - slice.limit.podcount negative  ":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":1,"unknown":0,"total":2},"items":[{"uid":"c3f830b2b9176cf6","status":"passed","time":{"start":1693899867000,"stop":1693899875203,"duration":8203}},{"uid":"e1f3c338224c14d0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Intracluster Suite:Intracluster Suite#[It] Intracluster slice tests Iperf connectivity should onboard deployment in application Namespaces without slice annotations":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"f345adf86187f0f9","status":"passed","time":{"start":1688025155000,"stop":1688025185414,"duration":30414}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting iperf-client pod Check ping between iperf-server and iperf-client after iperf-client pod restart":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":0,"unknown":0,"total":1},"items":[{"uid":"6e9a7be4ecaca66","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1709719453000,"stop":1709719453000,"duration":0}}]},"Controller Suite:Controller Suite#[It] Slice resource quota Tests ResourceQuota creation tests Create ResourceQuota fail test - namespace.DefaultLimitPerContainer.EphemeralStorage > namespace.Request.EphemeralStorage":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":1,"unknown":0,"total":2},"items":[{"uid":"620d2b7fa323ab3e","status":"passed","time":{"start":1693899867000,"stop":1693899870005,"duration":3005}},{"uid":"720927e16fc90c0c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests slice with netpol Should install networkpolicies in all application namespaces":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"b088c5a6ee2cc85d","status":"passed","time":{"start":1709719453000,"stop":1709719453414,"duration":414}}]},"Controller Suite:Controller Suite#[It] Slice resource quota Tests ResourceQuota creation tests Create ResourceQuota fail test- Request.CPU > Limit.CPU at namespace level":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":1,"unknown":0,"total":2},"items":[{"uid":"be08915d5080173b","status":"passed","time":{"start":1693899867000,"stop":1693899878197,"duration":11197}},{"uid":"40cad49419fd216f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Controller Suite:Controller Suite#[It] Slice resource quota Tests ResourceQuota creation tests Create ResourceQuota fail test - slice.DefaultRequestPerContainer.cpu > slice.Request.cpu":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":1,"unknown":0,"total":2},"items":[{"uid":"7d8a8e9ef9b16407","status":"passed","time":{"start":1693899867000,"stop":1693899881546,"duration":14546}},{"uid":"7a7fe46f5f8d4cc5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Hub netpol scenarios: Slice Configs Netpol scenarios Should fail when deploying clusters in namespaceisolationprofile with * and a cluster name in both applicationNamespace and allowedNamespace":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"3eb4c1ee5c39bce","status":"passed","time":{"start":1709719453000,"stop":1709719453408,"duration":408}}]},"Controller Suite:Controller Suite#[It] E2E tests for license job > when license config is tampered with should not create new secret upon restart":{"statistic":{"failed":1,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":2},"items":[{"uid":"4d43b553ced0d86d","status":"failed","statusDetails":"Timed out after 204.759s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1693899867000,"stop":1693900151147,"duration":284147}},{"uid":"6c4c9318bfd16ecc","status":"passed","time":{"start":1693828311000,"stop":1693828462922,"duration":151922}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Update Should update successfully when Project applied with valid service account name in Write users":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"863e7fd9c4f182ef","status":"passed","time":{"start":1688022039000,"stop":1688022044245,"duration":5245}}]},"Controller Suite:Controller Suite#[It] Slice resource quota Tests ResourceQuota creation tests Create ResourceQuota fail test-namespace not part of application namespaces":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":1,"unknown":0,"total":2},"items":[{"uid":"b2d4ca00365ec087","status":"passed","time":{"start":1693899867000,"stop":1693899877615,"duration":10615}},{"uid":"ebbd60bb6639db0d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Controller Suite:Controller Suite#[It] Slice resource quota Tests ResourceQuota creation tests Create ResourceQuota fail test-cluster not part":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":1,"unknown":0,"total":2},"items":[{"uid":"62c5d148ac1f6504","status":"passed","time":{"start":1693899867000,"stop":1693899896390,"duration":29390}},{"uid":"2e86578764a9a78c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Update Should fail when Project is Applied with service account name as combination of special characters in Read users":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"fce4b1cc82ddb816","status":"passed","time":{"start":1688022039000,"stop":1688022043868,"duration":4868}}]},"Controller Suite:Controller Suite#[It] Slice resource quota Tests ResourceQuota creation tests Create ResourceQuota fail test- Request.EphemeralStorage > Limit.EphemeralStorage at slice level":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":1,"unknown":0,"total":2},"items":[{"uid":"65b0cccdc8f9c41f","status":"passed","time":{"start":1693899867000,"stop":1693899904193,"duration":37193}},{"uid":"7dbd5cfd3ba08e17","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Controller Suite:Controller Suite#[It] Slice RBAC Tests SliceRoleBinding Tests Create SliceRoleBinding successful test: Creating SliceRoleBinding slice-red with SliceRoleTemplate":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":0,"unknown":0,"total":2},"items":[{"uid":"4091bfe7347e15d6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693899867000,"stop":1693899867000,"duration":0}},{"uid":"7874216c5d81a162","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Controller Suite:Controller Suite#[It] Events Test Events get recorded for slice resource quota deletion Validating events for slice resource quota deletion":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":0,"unknown":0,"total":2},"items":[{"uid":"b1dfa88cd26b2b58","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693899867000,"stop":1693899867000,"duration":0}},{"uid":"fafd0c859c24975e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Controller Suite:Controller Suite#[It] Slice resource quota Tests ResourceQuota creation tests Create ResourceQuota fail test- Request.Mem > Limit.Mem at namespace level":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":1,"unknown":0,"total":2},"items":[{"uid":"1efc1b1e49cb6c03","status":"passed","time":{"start":1693899867000,"stop":1693899917366,"duration":50366}},{"uid":"7d5195d7306b1ce","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"VPNKeyRotation Suite:VPNKeyRotation Suite#[It] Verify vpn key rotation functionality: rotation Interval should be 30 by default":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":3,"unknown":0,"total":3},"items":[{"uid":"b01c9f6174e51432","status":"passed","time":{"start":1710224686000,"stop":1710224686128,"duration":128}},{"uid":"3c8fc417d282c5c9","status":"passed","time":{"start":1709883716000,"stop":1709883716164,"duration":164}},{"uid":"d9d1c436caf1a501","status":"passed","time":{"start":1709717686000,"stop":1709717686199,"duration":199}}]},"Worker Suite:Worker Suite#[It] Namespace reconcile test scenarios: when Slice is created with onboarded application namespaces Should have slices on workers":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"5086eddbc57920da","status":"passed","time":{"start":1709719453000,"stop":1709719456014,"duration":3014}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests Iperf App connectivity should have vl3 router running":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"fd395ff812c4e7a0","status":"passed","time":{"start":1709719453000,"stop":1709719455069,"duration":2069}}]},"Controller Suite:Controller Suite#[It] Events Test Events get recorded for slice role binding creation and deletion Events get recorded for slice role binding deletion Validating events for slice role binding deletion":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":0,"unknown":0,"total":2},"items":[{"uid":"e11c8df742d787fc","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693899867000,"stop":1693899867000,"duration":0}},{"uid":"9123d4f4c65cc420","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice Config Negative Scenario tests Slice Config Negative Scenario Update Should fail while updating sliceconfig with bandwithguaranteedkbps more than bandwidthceilingkbps in qosprofiledetailsnvalid ":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"cfce53fc099ef753","status":"passed","time":{"start":1709719453000,"stop":1709719453543,"duration":543}}]},"Controller Suite:Controller Suite#[It] Slice node affinity Tests SliceNodeAffinity tests - includes creation and deletion slice node affinity creation success test":{"statistic":{"failed":1,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":2},"items":[{"uid":"72050e38879c10a3","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000017aa0>: {\n        Underlying: <*exec.ExitError | 0xc00072f240>{\n            ProcessState: {\n                pid: 6878,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 353409},\n                    Stime: {Sec: 0, Usec: 263177},\n                    Maxrss: 102340,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 13236,\n                    Majflt: 16,\n                    Nswap: 0,\n                    Inblock: 13016,\n                    Oublock: 20624,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 2442,\n                    Nivcsw: 3064,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Slice%20node%20affinity%20Tests%20SliceNodeAffinity%20tests%20-%20includes%20creation%20and%20deletion%20slice%20node%20affinity%20creation%20success%20test962941551\\\": Internal error occurred: failed calling webhook \\\"msliceconfig.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-sliceconfig?timeout=10s\\\": dial tcp 10.96.230.189:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Slice%20node%20affinity%20Tests%20SliceNodeAffinity%20tests%20-%20includes%20creation%20and%20deletion%20slice%20node%20affinity%20creation%20success%20test962941551\\\": Internal error occurred: failed calling webhook \\\"msliceconfig.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-sliceconfig?timeout=10s\\\": dial tcp 10.96.230.189:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Slice%20node%20affinity%20Tests%20SliceNodeAffinity%20tests%20-%20includes%20creation%20and%20deletion%20slice%20node%20affinity%20creation%20success%20test962941551\\\": Internal error occurred: failed calling webhook \\\"msliceconfig.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-sliceconfig?timeout=10s\\\": dial tcp 10.96.230.189:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Slice%20node%20affinity%20Tests%20SliceNodeAffinity%20tests%20-%20includes%20creation%20and%20deletion%20slice%20node%20affinity%20creation%20success%20test962941551\\\": Internal error occurred: failed calling webhook \\\"msliceconfig.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-sliceconfig?timeout=10s\\\": dial tcp 10.96.230.189:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Slice%20node%20affinity%20Tests%20SliceNodeAffinity%20tests%20-%20includes%20creation%20and%20deletion%20slice%20node%20affinity%20creation%20success%20test962941551\": Internal error occurred: failed calling webhook \"msliceconfig.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-sliceconfig?timeout=10s\": dial tcp 10.96.230.189:443: connect: connection refused\noccurred","time":{"start":1693899867000,"stop":1693899885859,"duration":18859}},{"uid":"198ca866a5f2163a","status":"passed","time":{"start":1693828311000,"stop":1693828333629,"duration":22629}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Update Should update successfully when Project Applied while removing a service account name in Write users":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"498044a4e2996b8a","status":"passed","time":{"start":1688022039000,"stop":1688022044696,"duration":5696}}]},"Controller Suite:Controller Suite#[It] Slice RBAC Tests RBAC Common Tests SliceRoleBinding status test: SliceRoleBinding should have error status for invalid namespace for custom roles":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":0,"unknown":0,"total":2},"items":[{"uid":"ffb69cf405135cce","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693899867000,"stop":1693899867000,"duration":0}},{"uid":"2093522b37843479","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Hub Suite:Hub Suite#[It] Cluster health check tests Cluster health check - without istio Updates health status when dns is not working":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":0,"unknown":0,"total":1},"items":[{"uid":"3b10059df4284c38","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1688022039000,"stop":1688022039000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice Config Events Test Events get recorded for slice config creation and deletion Events get recorded for slice config creation Validating events for slice gateway job creation":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"6031ea478b29ec78","status":"passed","time":{"start":1709719453000,"stop":1709719456907,"duration":3907}}]},"Worker Suite:Worker Suite#[It] Slice Config Negative Scenario tests Slice Config Negative Scenario Creation Should fail while creating sliceconfig with bandwithguaranteedkbps more than bandwidthceilingkbps in qosprofiledetailsnvalid ":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"b4de81ce628a82ac","status":"passed","time":{"start":1709719453000,"stop":1709719453000,"duration":0}}]},"Controller Suite:Controller Suite#[It] Slice RBAC Tests RBAC Common Tests WorkerSliceRoleBinding removal test: WorkerSliceRoleBinding should not be generated if SliceRoleTempate does not exit.":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":0,"unknown":0,"total":2},"items":[{"uid":"bf9ebacdd9102b15","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693899867000,"stop":1693899867000,"duration":0}},{"uid":"2fa78dbf6f7dc8ae","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1693828311000,"stop":1693828311000,"duration":0}}]},"Hub Suite:Hub Suite#[It] Cluster CR tests Cluster CR validation Register worker cluster":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"428bdf1bca524381","status":"passed","time":{"start":1688022039000,"stop":1688022686534,"duration":647534}}]}}