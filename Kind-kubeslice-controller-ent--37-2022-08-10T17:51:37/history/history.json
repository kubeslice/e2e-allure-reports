{"Worker Suite:Worker Suite#[It] Slice Config Negative Scenario tests Slice Config Negative Scenario Update Should fail while updating sliceconfig with invalid clusters":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":35,"unknown":0,"total":35},"items":[{"uid":"91f8c07fc6364c3f","status":"passed","time":{"start":1660113628000,"stop":1660113628215,"duration":215}},{"uid":"5db4ff86031bf856","status":"passed","time":{"start":1660104898000,"stop":1660104898207,"duration":207}},{"uid":"89d6070f92bcb80d","status":"passed","time":{"start":1660067259000,"stop":1660067259386,"duration":386}},{"uid":"cd0bbf108d6b54a2","status":"passed","time":{"start":1660047551000,"stop":1660047551502,"duration":502}},{"uid":"86e577fd9d856ab3","status":"passed","time":{"start":1659982549000,"stop":1659982549722,"duration":722}},{"uid":"84eea051792f19bc","status":"passed","time":{"start":1659970182000,"stop":1659970182344,"duration":344}},{"uid":"c99f4a151bc5c1a9","status":"passed","time":{"start":1659944495000,"stop":1659944495576,"duration":576}},{"uid":"f91e421b7b66d091","status":"passed","time":{"start":1659874977000,"stop":1659874977414,"duration":414}},{"uid":"cb821361306247ab","status":"passed","time":{"start":1659496050000,"stop":1659496050360,"duration":360}},{"uid":"b929d7f5ac5f1dba","status":"passed","time":{"start":1659496083000,"stop":1659496083592,"duration":592}},{"uid":"9a59f2a3aea9a0e0","status":"passed","time":{"start":1659451501000,"stop":1659451502130,"duration":1130}},{"uid":"c3c00816c21f2da7","status":"passed","time":{"start":1659450916000,"stop":1659450916470,"duration":470}},{"uid":"ad6dc42bb92e8932","status":"passed","time":{"start":1659451524000,"stop":1659451524460,"duration":460}},{"uid":"d474ce3b8d7c59da","status":"passed","time":{"start":1659447298000,"stop":1659447298367,"duration":367}},{"uid":"d4ab08c777ea2c11","status":"passed","time":{"start":1659447116000,"stop":1659447116375,"duration":375}},{"uid":"b72b5053e1b375ac","status":"passed","time":{"start":1659448205000,"stop":1659448205458,"duration":458}},{"uid":"d4eb41b961edbb15","status":"passed","time":{"start":1659448097000,"stop":1659448097477,"duration":477}},{"uid":"580abafe372f09f6","status":"passed","time":{"start":1659409279000,"stop":1659409279504,"duration":504}},{"uid":"99e253e3efd59b13","status":"passed","time":{"start":1659402555000,"stop":1659402555555,"duration":555}},{"uid":"9b69f961acc23033","status":"passed","time":{"start":1659291287000,"stop":1659291287323,"duration":323}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting worker-operator pod Check ping between iperf-server and iperf-client after worker-operator pod restart":{"statistic":{"failed":0,"broken":0,"skipped":35,"passed":0,"unknown":0,"total":35},"items":[{"uid":"287b484dbf2fa5ea","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"9c287adab578aaf5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"f2226ffe7a4c9f89","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"26f9a0d42688abd0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}},{"uid":"411b12010d759cff","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659982549000,"stop":1659982549000,"duration":0}},{"uid":"aab84b2848251745","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659970182000,"stop":1659970182000,"duration":0}},{"uid":"65ced3bb74049344","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659944495000,"stop":1659944495000,"duration":0}},{"uid":"be9f361998284ef2","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659874977000,"stop":1659874977000,"duration":0}},{"uid":"392cb59d501efc9a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496050000,"stop":1659496050000,"duration":0}},{"uid":"7e864c504cd22411","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496083000,"stop":1659496083000,"duration":0}},{"uid":"603953b95f02e5c7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"f547876d408079e4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659450916000,"stop":1659450916000,"duration":0}},{"uid":"15820a2aa9e5b224","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"c2e485325f290d3b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447298000,"stop":1659447298000,"duration":0}},{"uid":"e2b4431c4a93196e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447116000,"stop":1659447116000,"duration":0}},{"uid":"28c84f68459f7dc9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448205000,"stop":1659448205000,"duration":0}},{"uid":"44ef8425894736af","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448097000,"stop":1659448097000,"duration":0}},{"uid":"faa47d173b3eab2b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659409279000,"stop":1659409279000,"duration":0}},{"uid":"d052a721c1a3878e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659402555000,"stop":1659402555000,"duration":0}},{"uid":"d4a84e47b769ed00","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659291287000,"stop":1659291287000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice validation Slice with LOW QOS should reconcile low qos slice":{"statistic":{"failed":2,"broken":0,"skipped":3,"passed":4,"unknown":0,"total":9},"items":[{"uid":"163950f3c0e6d910","status":"passed","time":{"start":1659168739000,"stop":1659168739395,"duration":395}},{"uid":"de409261aa8cd036","status":"passed","time":{"start":1659164084000,"stop":1659164084315,"duration":315}},{"uid":"e4c1a18074ae344e","status":"passed","time":{"start":1659160188000,"stop":1659160188292,"duration":292}},{"uid":"34602a2dfa4121a2","status":"passed","time":{"start":1659119724000,"stop":1659119724310,"duration":310}},{"uid":"1e46ed45be6838ba","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000130ac8>: {\n        Underlying: <*exec.ExitError | 0xc00079e000>{\n            ProcessState: {\n                pid: 7527,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 50928},\n                    Stime: {Sec: 0, Usec: 18188},\n                    Maxrss: 45792,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2561,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 184,\n                    Nivcsw: 163,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicelowqos\\\" not found\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicelowqos\\\" not found\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicelowqos\\\" not found\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicelowqos\\\" not found\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (NotFound): slice.networking.kubeslice.io \"slicelowqos\" not found\noccurred","time":{"start":1659116511000,"stop":1659116511891,"duration":891}},{"uid":"4d3617c2bce05dfb","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00088d680>: {\n        Underlying: <*exec.ExitError | 0xc0007e2400>{\n            ProcessState: {\n                pid: 7608,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 61643},\n                    Stime: {Sec: 0, Usec: 14225},\n                    Maxrss: 41880,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2500,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 187,\n                    Nivcsw: 280,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicelowqos\\\" not found\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicelowqos\\\" not found\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicelowqos\\\" not found\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicelowqos\\\" not found\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (NotFound): slice.networking.kubeslice.io \"slicelowqos\" not found\noccurred","time":{"start":1659109470000,"stop":1659109470829,"duration":829}},{"uid":"70e2f9f641c12986","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659106836000,"stop":1659106836000,"duration":0}},{"uid":"5d48e97edd82140c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659101522000,"stop":1659101522000,"duration":0}},{"uid":"758a38f347a57677","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659082816000,"stop":1659082816000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Intracluster slice tests Slice with netpol should reconcile netpol in app NS":{"statistic":{"failed":0,"broken":0,"skipped":20,"passed":15,"unknown":0,"total":35},"items":[{"uid":"625cff30c35f4ecf","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"516b5a95f9a8362b","status":"passed","time":{"start":1660104898000,"stop":1660104898190,"duration":190}},{"uid":"9519a31c10379b4e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"d318c1fe9592a06d","status":"passed","time":{"start":1660047551000,"stop":1660047551147,"duration":147}},{"uid":"a57a9aa7fa31ce9e","status":"passed","time":{"start":1659982549000,"stop":1659982549292,"duration":292}},{"uid":"671680ea2c147f4e","status":"passed","time":{"start":1659970182000,"stop":1659970182191,"duration":191}},{"uid":"7a401cf17587a736","status":"passed","time":{"start":1659944495000,"stop":1659944495339,"duration":339}},{"uid":"738cff4f7117659f","status":"passed","time":{"start":1659874977000,"stop":1659874977135,"duration":135}},{"uid":"d3da3d440157aec9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496050000,"stop":1659496050000,"duration":0}},{"uid":"3281d07c26395e16","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496083000,"stop":1659496083000,"duration":0}},{"uid":"425caf17792147f7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"8da1ab0e5413633e","status":"passed","time":{"start":1659450916000,"stop":1659450916166,"duration":166}},{"uid":"fe41b30072a9c422","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"62cfb6ca6a3ef0c","status":"passed","time":{"start":1659447298000,"stop":1659447298134,"duration":134}},{"uid":"58ab373f90d48379","status":"passed","time":{"start":1659447116000,"stop":1659447116229,"duration":229}},{"uid":"ea20309d7318db8","status":"passed","time":{"start":1659448205000,"stop":1659448205139,"duration":139}},{"uid":"1c5034d7550243b1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448097000,"stop":1659448097000,"duration":0}},{"uid":"12e4dfecd3b6217f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659409279000,"stop":1659409279000,"duration":0}},{"uid":"81c8e1bf450e78c5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659402555000,"stop":1659402555000,"duration":0}},{"uid":"7d75f2ae0ed6f989","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659291287000,"stop":1659291287000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting vl3 pod Check ping between iperf-server and iperf-client after vl3 pod restart":{"statistic":{"failed":0,"broken":0,"skipped":35,"passed":0,"unknown":0,"total":35},"items":[{"uid":"2c49fbb5f5ddb3c0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"e3852ee003573a86","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"b11735feb6c8e0fe","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"9c06e997af9ed3e4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}},{"uid":"945d7d094f0b3280","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659982549000,"stop":1659982549000,"duration":0}},{"uid":"238bbe129130275a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659970182000,"stop":1659970182000,"duration":0}},{"uid":"329a149cd0bfaed6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659944495000,"stop":1659944495000,"duration":0}},{"uid":"5cc26af20c3799fe","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659874977000,"stop":1659874977000,"duration":0}},{"uid":"bb3370e2bb312359","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496050000,"stop":1659496050000,"duration":0}},{"uid":"12ed68d51b213ed9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496083000,"stop":1659496083000,"duration":0}},{"uid":"90c7add9006dbf55","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"ab0d0c15e1d7c3bd","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659450916000,"stop":1659450916000,"duration":0}},{"uid":"8dbe4ba521abb97b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"f095cf381e3cea45","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447298000,"stop":1659447298000,"duration":0}},{"uid":"7ddfb23b194fbb7d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447116000,"stop":1659447116000,"duration":0}},{"uid":"60203d46de37efce","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448205000,"stop":1659448205000,"duration":0}},{"uid":"72a914fc8dda4609","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448097000,"stop":1659448097000,"duration":0}},{"uid":"40eff90ee7c7b619","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659409279000,"stop":1659409279000,"duration":0}},{"uid":"dec88519dd864fe2","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659402555000,"stop":1659402555000,"duration":0}},{"uid":"1607efd90261337","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659291287000,"stop":1659291287000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Hub netpol scenarios: Slice Configs Netpol scenarios Should create slice for valid namespace and valid clusters in applicationNamespaces of sliceconfigs manifest":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":35,"unknown":0,"total":35},"items":[{"uid":"b0cd5625a8c65951","status":"passed","time":{"start":1660113628000,"stop":1660113638508,"duration":10508}},{"uid":"6fb85775730300cf","status":"passed","time":{"start":1660104898000,"stop":1660104898482,"duration":482}},{"uid":"347b84b841b98233","status":"passed","time":{"start":1660067259000,"stop":1660067269513,"duration":10513}},{"uid":"c7a92ac4d03e8b95","status":"passed","time":{"start":1660047551000,"stop":1660047561644,"duration":10644}},{"uid":"609f221d2e51426d","status":"passed","time":{"start":1659982549000,"stop":1659982559875,"duration":10875}},{"uid":"82b8a889eb3761af","status":"passed","time":{"start":1659970182000,"stop":1659970192699,"duration":10699}},{"uid":"3cc2871a535536a3","status":"passed","time":{"start":1659944495000,"stop":1659944505752,"duration":10752}},{"uid":"86dd66947bf128d9","status":"passed","time":{"start":1659874977000,"stop":1659874987965,"duration":10965}},{"uid":"74ee52337a2d01b2","status":"passed","time":{"start":1659496050000,"stop":1659496060417,"duration":10417}},{"uid":"89ca4a2be446ce6d","status":"passed","time":{"start":1659496083000,"stop":1659496093897,"duration":10897}},{"uid":"5a88319a2c6ccf0d","status":"passed","time":{"start":1659451501000,"stop":1659451511840,"duration":10840}},{"uid":"192a2406bf19822","status":"passed","time":{"start":1659450916000,"stop":1659450926936,"duration":10936}},{"uid":"42dd74920cdaa73c","status":"passed","time":{"start":1659451524000,"stop":1659451535359,"duration":11359}},{"uid":"bbf2bcb388703ac9","status":"passed","time":{"start":1659447298000,"stop":1659447308968,"duration":10968}},{"uid":"734c570b11324124","status":"passed","time":{"start":1659447116000,"stop":1659447126412,"duration":10412}},{"uid":"1e9190532732496f","status":"passed","time":{"start":1659448205000,"stop":1659448215725,"duration":10725}},{"uid":"b83d3f4461216f11","status":"passed","time":{"start":1659448097000,"stop":1659448108002,"duration":11002}},{"uid":"dfc2ad8bfc79ad5e","status":"passed","time":{"start":1659409279000,"stop":1659409289722,"duration":10722}},{"uid":"17ff328768a186e7","status":"passed","time":{"start":1659402555000,"stop":1659402565505,"duration":10505}},{"uid":"3205618434050ba6","status":"passed","time":{"start":1659291287000,"stop":1659291297514,"duration":10514}}]},"Worker Suite:Worker Suite#[It] Slice Config Negative Scenario tests Slice Config Negative Scenario Creation Should fail while creating sliceconfig with invalid clusters":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":35,"unknown":0,"total":35},"items":[{"uid":"da5cb7d723fd8f4f","status":"passed","time":{"start":1660113628000,"stop":1660113628291,"duration":291}},{"uid":"48badc49b55e6bcd","status":"passed","time":{"start":1660104898000,"stop":1660104898409,"duration":409}},{"uid":"b96df0ef959fd578","status":"passed","time":{"start":1660067259000,"stop":1660067259174,"duration":174}},{"uid":"b5cac23adb5289fe","status":"passed","time":{"start":1660047551000,"stop":1660047551264,"duration":264}},{"uid":"78899f06b4b0e667","status":"passed","time":{"start":1659982549000,"stop":1659982549395,"duration":395}},{"uid":"ddfde65d3047cf6b","status":"passed","time":{"start":1659970182000,"stop":1659970182210,"duration":210}},{"uid":"2e6bc7e5b307543f","status":"passed","time":{"start":1659944495000,"stop":1659944495309,"duration":309}},{"uid":"b4d68a4a3f0c2680","status":"passed","time":{"start":1659874977000,"stop":1659874977259,"duration":259}},{"uid":"3e74f451885c0c08","status":"passed","time":{"start":1659496050000,"stop":1659496050238,"duration":238}},{"uid":"646d016f32e1da33","status":"passed","time":{"start":1659496083000,"stop":1659496083377,"duration":377}},{"uid":"4e6c3b50899e6449","status":"passed","time":{"start":1659451501000,"stop":1659451501507,"duration":507}},{"uid":"6fbe6290cff2c8d1","status":"passed","time":{"start":1659450916000,"stop":1659450916301,"duration":301}},{"uid":"8e87201815ae6d1b","status":"passed","time":{"start":1659451524000,"stop":1659451524237,"duration":237}},{"uid":"672854f9e9ea8c6f","status":"passed","time":{"start":1659447298000,"stop":1659447298449,"duration":449}},{"uid":"36e161dd2f69c2e","status":"passed","time":{"start":1659447116000,"stop":1659447116212,"duration":212}},{"uid":"ce1acedaf786d4fb","status":"passed","time":{"start":1659448205000,"stop":1659448205246,"duration":246}},{"uid":"7685a55393cb83b2","status":"passed","time":{"start":1659448097000,"stop":1659448097447,"duration":447}},{"uid":"53008dc2e39284c5","status":"passed","time":{"start":1659409279000,"stop":1659409279276,"duration":276}},{"uid":"1c19022c9cbf89","status":"passed","time":{"start":1659402555000,"stop":1659402555264,"duration":264}},{"uid":"bf721c34ee7e08f6","status":"passed","time":{"start":1659291287000,"stop":1659291287355,"duration":355}}]},"Worker Suite:Worker Suite#[It] Intracluster slice tests Slice with netpol should remove network policy resource from application namespace once the isolationEnabled is set to false":{"statistic":{"failed":1,"broken":0,"skipped":20,"passed":14,"unknown":0,"total":35},"items":[{"uid":"e5efa63f8f60e74","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"8feb3246cec61f5a","status":"passed","time":{"start":1660104898000,"stop":1660104905082,"duration":7082}},{"uid":"10295f1f4963b982","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"b43b7d108b078b7d","status":"passed","time":{"start":1660047551000,"stop":1660047557836,"duration":6836}},{"uid":"4c71a33499abdf6c","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000a44660>: {\n        Underlying: <*exec.ExitError | 0xc00014ed80>{\n            ProcessState: {\n                pid: 7392,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 83422},\n                    Stime: {Sec: 0, Usec: 31779},\n                    Maxrss: 42544,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2522,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 148,\n                    Nivcsw: 163,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: ServiceExportConfig: Forbidden: The SliceConfig can only be deleted after all the service export configs are deleted for the slice.\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: ServiceExportConfig: Forbidden: The SliceConfig can only be deleted after all the service export configs are deleted for the slice.\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: ServiceExportConfig: Forbidden: The SliceConfig can only be deleted after all the service export configs are deleted for the slice.\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: ServiceExportConfig: Forbidden: The SliceConfig can only be deleted after all the service export configs are deleted for the slice.\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: ServiceExportConfig: Forbidden: The SliceConfig can only be deleted after all the service export configs are deleted for the slice.\noccurred","time":{"start":1659982549000,"stop":1659982558710,"duration":9710}},{"uid":"8c55b6e8a27891a2","status":"passed","time":{"start":1659970182000,"stop":1659970188871,"duration":6871}},{"uid":"a8f89acf60f00a65","status":"passed","time":{"start":1659944495000,"stop":1659944504977,"duration":9977}},{"uid":"720d85b8f5c728fc","status":"passed","time":{"start":1659874977000,"stop":1659874983803,"duration":6803}},{"uid":"a8f494a04a1a6a0b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496050000,"stop":1659496050000,"duration":0}},{"uid":"6b2e149b3f483171","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496083000,"stop":1659496083000,"duration":0}},{"uid":"15527b71eda095ed","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"b59f25d1d2bcd5ca","status":"passed","time":{"start":1659450916000,"stop":1659450925129,"duration":9129}},{"uid":"30e8aa9f71c4fc5d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"83efdde840eee843","status":"passed","time":{"start":1659447298000,"stop":1659447304843,"duration":6843}},{"uid":"630cd32864a9d27","status":"passed","time":{"start":1659447116000,"stop":1659447123133,"duration":7133}},{"uid":"e1371e4e21df0476","status":"passed","time":{"start":1659448205000,"stop":1659448211878,"duration":6878}},{"uid":"5e37c4d55d1a092","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448097000,"stop":1659448097000,"duration":0}},{"uid":"2bf2b56291f4e7ca","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659409279000,"stop":1659409279000,"duration":0}},{"uid":"b9b317f22c6dd89a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659402555000,"stop":1659402555000,"duration":0}},{"uid":"e10804b69208fb6f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659291287000,"stop":1659291287000,"duration":0}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests Iperf App connectivity Verify Connectivity between iperf-server and iperf-client":{"statistic":{"failed":0,"broken":0,"skipped":28,"passed":7,"unknown":0,"total":35},"items":[{"uid":"c9e1f1608059fd00","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"cb80154ecd076dda","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"a7837f3cbb91d7ad","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"a7aac38e3105a7ba","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}},{"uid":"5eba677be11c7ac3","status":"passed","time":{"start":1659982549000,"stop":1659982556001,"duration":7001}},{"uid":"35fca064f4533a39","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659970182000,"stop":1659970182000,"duration":0}},{"uid":"82e1ff76cbf2f999","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659944495000,"stop":1659944495000,"duration":0}},{"uid":"23bb2287ab847206","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659874977000,"stop":1659874977000,"duration":0}},{"uid":"83ca3f1756ba941a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496050000,"stop":1659496050000,"duration":0}},{"uid":"e75e36307e73f88f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496083000,"stop":1659496083000,"duration":0}},{"uid":"36d72517ffd74f70","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"da6ba38caba6ccdc","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659450916000,"stop":1659450916000,"duration":0}},{"uid":"993f846fefafacd7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"a97fbaa4b5478b05","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447298000,"stop":1659447298000,"duration":0}},{"uid":"87ffd5b6dfddd1c5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447116000,"stop":1659447116000,"duration":0}},{"uid":"8858a4548cf0356f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448205000,"stop":1659448205000,"duration":0}},{"uid":"f944fbab77483f53","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448097000,"stop":1659448097000,"duration":0}},{"uid":"ce1f7f9ccaa20724","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659409279000,"stop":1659409279000,"duration":0}},{"uid":"180ef03d986b81c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659402555000,"stop":1659402555000,"duration":0}},{"uid":"e2043b4d5253cbe8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659291287000,"stop":1659291287000,"duration":0}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests slice with netpol Should contain application namespaces":{"statistic":{"failed":0,"broken":0,"skipped":25,"passed":10,"unknown":0,"total":35},"items":[{"uid":"df21d7568dd7599d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"c247d6ba54f52876","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"1d6194afce7554b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"1235243d85ca5cea","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}},{"uid":"535159053f975706","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659982549000,"stop":1659982549000,"duration":0}},{"uid":"30acfb68f2137adb","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659970182000,"stop":1659970182000,"duration":0}},{"uid":"1a13aeaeaec63028","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659944495000,"stop":1659944495000,"duration":0}},{"uid":"d93213caebf850b3","status":"passed","time":{"start":1659874977000,"stop":1659874977151,"duration":151}},{"uid":"c5ba31a95e455ab8","status":"passed","time":{"start":1659496050000,"stop":1659496050128,"duration":128}},{"uid":"c95259b90823a79d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496083000,"stop":1659496083000,"duration":0}},{"uid":"d254c7cf2ed1d560","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"c7f96b2b233913d4","status":"passed","time":{"start":1659450916000,"stop":1659450916359,"duration":359}},{"uid":"64596d60711b3954","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"23b5cfc05a3b6727","status":"passed","time":{"start":1659447298000,"stop":1659447298235,"duration":235}},{"uid":"1655cb16e97ce896","status":"passed","time":{"start":1659447116000,"stop":1659447116126,"duration":126}},{"uid":"99f576424aca310c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448205000,"stop":1659448205000,"duration":0}},{"uid":"76142860e739be55","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448097000,"stop":1659448097000,"duration":0}},{"uid":"93c83c091a897022","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659409279000,"stop":1659409279000,"duration":0}},{"uid":"b3f559262abfae6e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659402555000,"stop":1659402555000,"duration":0}},{"uid":"d2d9a25133ee96a4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659291287000,"stop":1659291287000,"duration":0}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests Iperf App connectivity should have gateway pod running":{"statistic":{"failed":1,"broken":0,"skipped":20,"passed":14,"unknown":0,"total":35},"items":[{"uid":"2861f06811efcc4a","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1660113628000,"stop":1660113810641,"duration":182641}},{"uid":"2435eab733dae1c3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"1255de5c12de14b1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"fd7704c54474e0f0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}},{"uid":"249074e117f93794","status":"passed","time":{"start":1659982549000,"stop":1659982585327,"duration":36327}},{"uid":"d169c26865f6bbb7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659970182000,"stop":1659970182000,"duration":0}},{"uid":"37e3cd1e6b2a7f58","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659944495000,"stop":1659944495000,"duration":0}},{"uid":"699a0a58f888de65","status":"passed","time":{"start":1659874977000,"stop":1659874977043,"duration":43}},{"uid":"1177505f1df6a7ba","status":"passed","time":{"start":1659496050000,"stop":1659496050048,"duration":48}},{"uid":"568dfa885ec9ac9a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496083000,"stop":1659496083000,"duration":0}},{"uid":"8f24e815e07f8581","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"8c3a94f498c5caf7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659450916000,"stop":1659450916000,"duration":0}},{"uid":"9fd2b3e0720b13ef","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"970f18afbe85c363","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447298000,"stop":1659447298000,"duration":0}},{"uid":"94e2ebd5289e3996","status":"passed","time":{"start":1659447116000,"stop":1659447116032,"duration":32}},{"uid":"c4e39d0392284a94","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448205000,"stop":1659448205000,"duration":0}},{"uid":"27333139f4f62c32","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448097000,"stop":1659448097000,"duration":0}},{"uid":"b270268fc6d7ceda","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659409279000,"stop":1659409279000,"duration":0}},{"uid":"74500268415ddb68","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659402555000,"stop":1659402555000,"duration":0}},{"uid":"cf5d97d3f2f3632d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659291287000,"stop":1659291287000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice Config Negative Scenario tests Slice Config Negative Scenario Update Should fail while updating sliceconfig with invalid values on slice subnet":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":35,"unknown":0,"total":35},"items":[{"uid":"a7c3da3595e9f755","status":"passed","time":{"start":1660113628000,"stop":1660113628214,"duration":214}},{"uid":"96793728a6e27a30","status":"passed","time":{"start":1660104898000,"stop":1660104898211,"duration":211}},{"uid":"dc3e813d284a352a","status":"passed","time":{"start":1660067259000,"stop":1660067259395,"duration":395}},{"uid":"356d03c3c897fa1b","status":"passed","time":{"start":1660047551000,"stop":1660047551432,"duration":432}},{"uid":"ce9987f5ff87d7ba","status":"passed","time":{"start":1659982549000,"stop":1659982549740,"duration":740}},{"uid":"5d724ccc782abd42","status":"passed","time":{"start":1659970182000,"stop":1659970182471,"duration":471}},{"uid":"3eb784e1301fbf5e","status":"passed","time":{"start":1659944495000,"stop":1659944495528,"duration":528}},{"uid":"f2eee1de6a193726","status":"passed","time":{"start":1659874977000,"stop":1659874977462,"duration":462}},{"uid":"44bd298d16b818e6","status":"passed","time":{"start":1659496050000,"stop":1659496050360,"duration":360}},{"uid":"35fcf1431f5ec517","status":"passed","time":{"start":1659496083000,"stop":1659496083474,"duration":474}},{"uid":"7160f6bf40aa2cf6","status":"passed","time":{"start":1659451501000,"stop":1659451501406,"duration":406}},{"uid":"63178a893a1ea710","status":"passed","time":{"start":1659450916000,"stop":1659450916570,"duration":570}},{"uid":"689edf0a40b9c2ea","status":"passed","time":{"start":1659451524000,"stop":1659451524371,"duration":371}},{"uid":"db7b915489b89837","status":"passed","time":{"start":1659447298000,"stop":1659447298428,"duration":428}},{"uid":"ae751069e4a72188","status":"passed","time":{"start":1659447116000,"stop":1659447116421,"duration":421}},{"uid":"15dacae28907463f","status":"passed","time":{"start":1659448205000,"stop":1659448205694,"duration":694}},{"uid":"938d38e274de233f","status":"passed","time":{"start":1659448097000,"stop":1659448097619,"duration":619}},{"uid":"c4d36b85acbd2416","status":"passed","time":{"start":1659409279000,"stop":1659409279802,"duration":802}},{"uid":"9d5dd9f0abeb618d","status":"passed","time":{"start":1659402555000,"stop":1659402555515,"duration":515}},{"uid":"3537bf54dc057c26","status":"passed","time":{"start":1659291287000,"stop":1659291287245,"duration":245}}]},"Worker Suite:Worker Suite#[It] Slice Config Negative Scenario tests Slice Config Negative Scenario Creation Should fail while creating sliceconfig with with invalid DSCP class in qosprofiledetails":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":35,"unknown":0,"total":35},"items":[{"uid":"32ed2535f1490246","status":"passed","time":{"start":1660113628000,"stop":1660113628295,"duration":295}},{"uid":"13e3eb7ce308bc4c","status":"passed","time":{"start":1660104898000,"stop":1660104898243,"duration":243}},{"uid":"e49f09d80140a40b","status":"passed","time":{"start":1660067259000,"stop":1660067259219,"duration":219}},{"uid":"49d64be485e17926","status":"passed","time":{"start":1660047551000,"stop":1660047551349,"duration":349}},{"uid":"f8edf56e8d5df854","status":"passed","time":{"start":1659982549000,"stop":1659982549526,"duration":526}},{"uid":"4ff3fc24efcc9987","status":"passed","time":{"start":1659970182000,"stop":1659970182257,"duration":257}},{"uid":"3ca2bfe14018086","status":"passed","time":{"start":1659944495000,"stop":1659944495370,"duration":370}},{"uid":"17758bfb37752799","status":"passed","time":{"start":1659874977000,"stop":1659874977403,"duration":403}},{"uid":"39b71752fa07c658","status":"passed","time":{"start":1659496050000,"stop":1659496050287,"duration":287}},{"uid":"ef4c607459eba344","status":"passed","time":{"start":1659496083000,"stop":1659496083452,"duration":452}},{"uid":"fb73f3c33a6c676e","status":"passed","time":{"start":1659451501000,"stop":1659451501621,"duration":621}},{"uid":"94bf21b209bcf354","status":"passed","time":{"start":1659450916000,"stop":1659450916315,"duration":315}},{"uid":"99877107e20ce419","status":"passed","time":{"start":1659451524000,"stop":1659451524415,"duration":415}},{"uid":"4a7756fc3f17c390","status":"passed","time":{"start":1659447298000,"stop":1659447298546,"duration":546}},{"uid":"9070b03cabf6da8c","status":"passed","time":{"start":1659447116000,"stop":1659447116285,"duration":285}},{"uid":"fb766d2595a33a64","status":"passed","time":{"start":1659448205000,"stop":1659448205328,"duration":328}},{"uid":"e382d6617de333df","status":"passed","time":{"start":1659448097000,"stop":1659448097581,"duration":581}},{"uid":"2a8ef07541df39b3","status":"passed","time":{"start":1659409279000,"stop":1659409279324,"duration":324}},{"uid":"3d6830a73c2eb0df","status":"passed","time":{"start":1659402555000,"stop":1659402555315,"duration":315}},{"uid":"e07522fd13a4dde7","status":"passed","time":{"start":1659291287000,"stop":1659291287366,"duration":366}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests Iperf App connectivity Should install iperf-sleep on client cluster":{"statistic":{"failed":7,"broken":0,"skipped":21,"passed":7,"unknown":0,"total":35},"items":[{"uid":"b29aed39f4f9ab9a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"ace7e813770a747f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"952e2e1421f79e7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"ae18ebba5777f587","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}},{"uid":"510284485a2fff87","status":"passed","time":{"start":1659982549000,"stop":1659982550090,"duration":1090}},{"uid":"466344827719a82f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659970182000,"stop":1659970182000,"duration":0}},{"uid":"154cb936a3753e5d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659944495000,"stop":1659944495000,"duration":0}},{"uid":"e7495404ab2acfcf","status":"failed","statusDetails":"Timed out after 120.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1659874977000,"stop":1659875099985,"duration":122985}},{"uid":"a61df080062ca9a6","status":"failed","statusDetails":"Timed out after 120.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1659496050000,"stop":1659496173270,"duration":123270}},{"uid":"52194caef4d8c838","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496083000,"stop":1659496083000,"duration":0}},{"uid":"7a0765ea8cd914ac","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"25e9166eff99b05a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659450916000,"stop":1659450916000,"duration":0}},{"uid":"b402cc5cc3b41952","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"1735c40150944cf","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447298000,"stop":1659447298000,"duration":0}},{"uid":"bf8f923cbbe8dbad","status":"failed","statusDetails":"Timed out after 120.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1659447116000,"stop":1659447239709,"duration":123709}},{"uid":"8961f60c5a71307","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448205000,"stop":1659448205000,"duration":0}},{"uid":"89611098e6ff30cd","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448097000,"stop":1659448097000,"duration":0}},{"uid":"a8b24eebc964f4db","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659409279000,"stop":1659409279000,"duration":0}},{"uid":"7dc16885be8d75ad","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659402555000,"stop":1659402555000,"duration":0}},{"uid":"b389c53e07b99844","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659291287000,"stop":1659291287000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice creation scenarios: when slice red is installed install slice blue workers should have vl3 routers from both slices":{"statistic":{"failed":1,"broken":0,"skipped":23,"passed":11,"unknown":0,"total":35},"items":[{"uid":"a18f7f1ce2d18999","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"93bff7dfa90cf20d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"4ff872de7cc98ab1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"f5ca1143b95130ca","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}},{"uid":"d3127f715cb5b818","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659982549000,"stop":1659982549000,"duration":0}},{"uid":"9ec6abe3ace13022","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659970182000,"stop":1659970182000,"duration":0}},{"uid":"213c0844d879d369","status":"passed","time":{"start":1659944495000,"stop":1659944495053,"duration":53}},{"uid":"2ff9a8b29adfd93","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659874977000,"stop":1659874977000,"duration":0}},{"uid":"69c7820d0e162af5","status":"passed","time":{"start":1659496050000,"stop":1659496053077,"duration":3077}},{"uid":"e24f3b1085693d7f","status":"passed","time":{"start":1659496083000,"stop":1659496085052,"duration":2052}},{"uid":"20344023803e3742","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"922f978314c2dab9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659450916000,"stop":1659450916000,"duration":0}},{"uid":"f6013e34bd66aa3b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"a68e4ee9c47d1386","status":"passed","time":{"start":1659447298000,"stop":1659447300041,"duration":2041}},{"uid":"198d61d2b91c0f26","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447116000,"stop":1659447116000,"duration":0}},{"uid":"aa22f1587132edc3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448205000,"stop":1659448205000,"duration":0}},{"uid":"3033ce570488d126","status":"passed","time":{"start":1659448097000,"stop":1659448100138,"duration":3138}},{"uid":"b0aa9eaa2d2a1cac","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659409279000,"stop":1659409279000,"duration":0}},{"uid":"6cf6b6d123f6704d","status":"passed","time":{"start":1659402555000,"stop":1659402555033,"duration":33}},{"uid":"9b1a985d4b74436a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659291287000,"stop":1659291287000,"duration":0}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Update Should fail when Project is applied with service account name as blank in Write users":{"statistic":{"failed":7,"broken":0,"skipped":0,"passed":79,"unknown":0,"total":86},"items":[{"uid":"5b55d3f1351e02cc","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000532360>: {\n        Underlying: <*exec.ExitError | 0xc0007d90c0>{\n            ProcessState: {\n                pid: 6262,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 157784},\n                    Stime: {Sec: 0, Usec: 38380},\n                    Maxrss: 81996,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4083,\n                    Majflt: 2,\n                    Nswap: 0,\n                    Inblock: 384,\n                    Oublock: 13376,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 354,\n                    Nivcsw: 351,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The Project \"projectupdatetest\" is invalid: project namespace: Invalid value: \"kubeslice-projectupdatetest\": already exists\noccurred","time":{"start":1660145110000,"stop":1660145170250,"duration":60250}},{"uid":"b27b6aa75b80d0f2","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0001e8030>: {\n        Underlying: <*exec.ExitError | 0xc0005d4040>{\n            ProcessState: {\n                pid: 6363,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 150638},\n                    Stime: {Sec: 0, Usec: 67390},\n                    Maxrss: 79392,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 13225,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13392,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 372,\n                    Nivcsw: 269,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The Project \"projectupdatetest\" is invalid: project namespace: Invalid value: \"kubeslice-projectupdatetest\": already exists\noccurred","time":{"start":1660142645000,"stop":1660142705271,"duration":60271}},{"uid":"c7a4e9ad779f12e5","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000351998>: {\n        Underlying: <*exec.ExitError | 0xc000799980>{\n            ProcessState: {\n                pid: 6185,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 133374},\n                    Stime: {Sec: 0, Usec: 43023},\n                    Maxrss: 76804,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 10351,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13392,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 315,\n                    Nivcsw: 275,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The Project \"projectupdatetest\" is invalid: project namespace: Invalid value: \"kubeslice-projectupdatetest\": already exists\noccurred","time":{"start":1660141834000,"stop":1660141894247,"duration":60247}},{"uid":"d4b3ab160ec3680e","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0007401c8>: {\n        Underlying: <*exec.ExitError | 0xc000883f40>{\n            ProcessState: {\n                pid: 6222,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 142304},\n                    Stime: {Sec: 0, Usec: 44724},\n                    Maxrss: 78268,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 15397,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13376,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 453,\n                    Nivcsw: 315,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The Project \"projectupdatetest\" is invalid: project namespace: Invalid value: \"kubeslice-projectupdatetest\": already exists\noccurred","time":{"start":1660138561000,"stop":1660138621239,"duration":60239}},{"uid":"480aa4b210c303c8","status":"passed","time":{"start":1660116744000,"stop":1660116748858,"duration":4858}},{"uid":"70e367eda9cf801e","status":"passed","time":{"start":1660116144000,"stop":1660116148650,"duration":4650}},{"uid":"9bf940b12c505b39","status":"passed","time":{"start":1660112860000,"stop":1660112864711,"duration":4711}},{"uid":"d82a8faa3fcd011d","status":"passed","time":{"start":1660108970000,"stop":1660108974744,"duration":4744}},{"uid":"22d9c699d7ce86df","status":"passed","time":{"start":1660104124000,"stop":1660104128873,"duration":4873}},{"uid":"eca4402b3c4545e3","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0007d5ba8>: {\n        Underlying: <*exec.ExitError | 0xc0007a05e0>{\n            ProcessState: {\n                pid: 6336,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 168829},\n                    Stime: {Sec: 0, Usec: 40197},\n                    Maxrss: 79732,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 14102,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13392,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 444,\n                    Nivcsw: 295,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The Project \"projectupdatetest\" is invalid: project namespace: Invalid value: \"kubeslice-projectupdatetest\": already exists\noccurred","time":{"start":1660104760000,"stop":1660104820282,"duration":60282}},{"uid":"e9c81ad7e523b9c8","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004f4060>: {\n        Underlying: <*exec.ExitError | 0xc00047c040>{\n            ProcessState: {\n                pid: 6171,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 124393},\n                    Stime: {Sec: 0, Usec: 20732},\n                    Maxrss: 81136,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3413,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13376,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 415,\n                    Nivcsw: 233,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The Project \"projectupdatetest\" is invalid: project namespace: Invalid value: \"kubeslice-projectupdatetest\": already exists\noccurred","time":{"start":1660101116000,"stop":1660101176217,"duration":60217}},{"uid":"900acba08129081c","status":"passed","time":{"start":1660100168000,"stop":1660100172914,"duration":4914}},{"uid":"a4ecaa30b05b11d7","status":"passed","time":{"start":1660066476000,"stop":1660066480690,"duration":4690}},{"uid":"4244b338115a49f","status":"passed","time":{"start":1660064121000,"stop":1660064125767,"duration":4767}},{"uid":"52be067d7ecb51c2","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0005f9b90>: {\n        Underlying: <*exec.ExitError | 0xc000620f40>{\n            ProcessState: {\n                pid: 6244,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 135504},\n                    Stime: {Sec: 0, Usec: 58073},\n                    Maxrss: 73148,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 12986,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13392,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 393,\n                    Nivcsw: 214,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The Project \"projectupdatetest\" is invalid: project namespace: Invalid value: \"kubeslice-projectupdatetest\": already exists\noccurred","time":{"start":1660060680000,"stop":1660060740250,"duration":60250}},{"uid":"3b2b547f4674804a","status":"passed","time":{"start":1660052169000,"stop":1660052173923,"duration":4923}},{"uid":"70da38982e249378","status":"passed","time":{"start":1660050481000,"stop":1660050485780,"duration":4780}},{"uid":"3375a81b3791135d","status":"passed","time":{"start":1660046776000,"stop":1660046780631,"duration":4631}},{"uid":"d3967a7ab182c62f","status":"passed","time":{"start":1660046048000,"stop":1660046052687,"duration":4687}},{"uid":"f3a6b4ce7ba4b38d","status":"passed","time":{"start":1660044532000,"stop":1660044537761,"duration":5761}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests slice with netpol should contain the allowed namespaces":{"statistic":{"failed":0,"broken":0,"skipped":25,"passed":10,"unknown":0,"total":35},"items":[{"uid":"1ec5cbb3af9551d4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"a8cac207d479ac21","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"2189a2fd8853074e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"f1df71237ae45d4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}},{"uid":"1f38300a9af4105e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659982549000,"stop":1659982549000,"duration":0}},{"uid":"4a3b39b6bf2fa331","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659970182000,"stop":1659970182000,"duration":0}},{"uid":"84fd0e33325d5198","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659944495000,"stop":1659944495000,"duration":0}},{"uid":"9f50d9da2d0bb7b8","status":"passed","time":{"start":1659874977000,"stop":1659874977167,"duration":167}},{"uid":"9b8147a59780fcb5","status":"passed","time":{"start":1659496050000,"stop":1659496050154,"duration":154}},{"uid":"ed7a8fe0e7f80fdc","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496083000,"stop":1659496083000,"duration":0}},{"uid":"106604f9c1af435","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"5e259450ea477653","status":"passed","time":{"start":1659450916000,"stop":1659450916259,"duration":259}},{"uid":"cd102e93ce051232","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"a403c2ddc6316ef2","status":"passed","time":{"start":1659447298000,"stop":1659447298198,"duration":198}},{"uid":"92f651ad40aa21fa","status":"passed","time":{"start":1659447116000,"stop":1659447116125,"duration":125}},{"uid":"a1463043b5599998","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448205000,"stop":1659448205000,"duration":0}},{"uid":"212a85d35f238e90","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448097000,"stop":1659448097000,"duration":0}},{"uid":"cf7c141d7b78bc21","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659409279000,"stop":1659409279000,"duration":0}},{"uid":"147ab25d48c5a5d5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659402555000,"stop":1659402555000,"duration":0}},{"uid":"e48cae27c91f1d1f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659291287000,"stop":1659291287000,"duration":0}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Deletion Should fail when deleting a project that does not exist":{"statistic":{"failed":5,"broken":0,"skipped":0,"passed":81,"unknown":0,"total":86},"items":[{"uid":"38d61a4c058ec797","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected an error to have occurred.  Got:\n    <nil>: nil","time":{"start":1660145110000,"stop":1660145171662,"duration":61662}},{"uid":"684a526c9b38c0e3","status":"passed","time":{"start":1660142645000,"stop":1660142653576,"duration":8576}},{"uid":"e91ab6153ee52f62","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected an error to have occurred.  Got:\n    <nil>: nil","time":{"start":1660141834000,"stop":1660141895592,"duration":61592}},{"uid":"237efaea7021a1fc","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected an error to have occurred.  Got:\n    <nil>: nil","time":{"start":1660138561000,"stop":1660138622639,"duration":61639}},{"uid":"2ca2d27cc6080938","status":"passed","time":{"start":1660116744000,"stop":1660116751791,"duration":7791}},{"uid":"3f92595f7fdbbf47","status":"passed","time":{"start":1660116144000,"stop":1660116152229,"duration":8229}},{"uid":"f8847cde9f2103dd","status":"passed","time":{"start":1660112860000,"stop":1660112867693,"duration":7693}},{"uid":"930ed499a3224176","status":"passed","time":{"start":1660108970000,"stop":1660108978720,"duration":8720}},{"uid":"9324a5f80076fab1","status":"passed","time":{"start":1660104124000,"stop":1660104132586,"duration":8586}},{"uid":"6d020fa9a4403eb","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected an error to have occurred.  Got:\n    <nil>: nil","time":{"start":1660104760000,"stop":1660104821672,"duration":61672}},{"uid":"7b27d5bc664c3aa1","status":"passed","time":{"start":1660101116000,"stop":1660101124824,"duration":8824}},{"uid":"e3bf745922cf7839","status":"passed","time":{"start":1660100168000,"stop":1660100176939,"duration":8939}},{"uid":"5d839d694b307a87","status":"passed","time":{"start":1660066476000,"stop":1660066484273,"duration":8273}},{"uid":"6b517e9b075f1a08","status":"passed","time":{"start":1660064121000,"stop":1660064129334,"duration":8334}},{"uid":"cd8efa348f4bffb9","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected an error to have occurred.  Got:\n    <nil>: nil","time":{"start":1660060680000,"stop":1660060741664,"duration":61664}},{"uid":"f1060575b6eb16d2","status":"passed","time":{"start":1660052169000,"stop":1660052177855,"duration":8855}},{"uid":"2816ab38715b6517","status":"passed","time":{"start":1660050481000,"stop":1660050488736,"duration":7736}},{"uid":"8c9e68521cef6130","status":"passed","time":{"start":1660046776000,"stop":1660046783584,"duration":7584}},{"uid":"af6ca80b149fb088","status":"passed","time":{"start":1660046048000,"stop":1660046055567,"duration":7567}},{"uid":"715750412e0a1075","status":"passed","time":{"start":1660044532000,"stop":1660044539624,"duration":7624}}]},"Worker Suite:Worker Suite#[It] Hub netpol scenarios: Slice Configs Netpol scenarios Should successfully deploy slice for valid namespace and valid clusters in allowedNamespaces of sliceconfigs manifest":{"statistic":{"failed":9,"broken":0,"skipped":0,"passed":26,"unknown":0,"total":35},"items":[{"uid":"531f2cd9e5077f58","status":"failed","statusDetails":"Expected\n    <[]string | len:0, cap:0>: nil\nto equal\n    <[]string | len:1, cap:1>: [\"kube-system\"]","time":{"start":1660113628000,"stop":1660113628535,"duration":535}},{"uid":"56aa6694efc8c246","status":"failed","statusDetails":"Expected\n    <[]string | len:0, cap:0>: nil\nto equal\n    <[]string | len:1, cap:1>: [\"kube-system\"]","time":{"start":1660104898000,"stop":1660104898627,"duration":627}},{"uid":"727e6e902469b4f9","status":"passed","time":{"start":1660067259000,"stop":1660067259539,"duration":539}},{"uid":"3ea312df00826f38","status":"passed","time":{"start":1660047551000,"stop":1660047551447,"duration":447}},{"uid":"7d6ec9711cff326e","status":"passed","time":{"start":1659982549000,"stop":1659982550136,"duration":1136}},{"uid":"51ee76f4032baa9a","status":"passed","time":{"start":1659970182000,"stop":1659970182548,"duration":548}},{"uid":"a195397df16ca08c","status":"passed","time":{"start":1659944495000,"stop":1659944495605,"duration":605}},{"uid":"e4c11bc42fce7c05","status":"passed","time":{"start":1659874977000,"stop":1659874977537,"duration":537}},{"uid":"b86acdaec62bc845","status":"passed","time":{"start":1659496050000,"stop":1659496050326,"duration":326}},{"uid":"e43f950617c7e444","status":"passed","time":{"start":1659496083000,"stop":1659496083591,"duration":591}},{"uid":"5e32d67a88629605","status":"failed","statusDetails":"Expected\n    <[]string | len:0, cap:0>: nil\nto equal\n    <[]string | len:1, cap:1>: [\"kube-system\"]","time":{"start":1659451501000,"stop":1659451501608,"duration":608}},{"uid":"f4a219e9153ab9ef","status":"passed","time":{"start":1659450916000,"stop":1659450916423,"duration":423}},{"uid":"b1cc0a57c1d0f7b0","status":"passed","time":{"start":1659451524000,"stop":1659451524680,"duration":680}},{"uid":"812ea94f932d5965","status":"passed","time":{"start":1659447298000,"stop":1659447298361,"duration":361}},{"uid":"42e3d3722f30ba19","status":"passed","time":{"start":1659447116000,"stop":1659447116389,"duration":389}},{"uid":"222cea2b21cac4b7","status":"passed","time":{"start":1659448205000,"stop":1659448205568,"duration":568}},{"uid":"441262ec514d728b","status":"passed","time":{"start":1659448097000,"stop":1659448097657,"duration":657}},{"uid":"d71daaa5e0651d9","status":"failed","statusDetails":"Expected\n    <[]string | len:0, cap:0>: nil\nto equal\n    <[]string | len:1, cap:1>: [\"kube-system\"]","time":{"start":1659409279000,"stop":1659409279512,"duration":512}},{"uid":"33796d99e5d0f414","status":"passed","time":{"start":1659402555000,"stop":1659402555571,"duration":571}},{"uid":"a6809d073248768f","status":"failed","statusDetails":"Expected\n    <[]string | len:0, cap:0>: nil\nto equal\n    <[]string | len:1, cap:1>: [\"kube-system\"]","time":{"start":1659291287000,"stop":1659291287519,"duration":519}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Creation Should create Multiple Projects in controller using valid manifest":{"statistic":{"failed":5,"broken":0,"skipped":0,"passed":84,"unknown":0,"total":89},"items":[{"uid":"ad3811598e59d36f","status":"passed","time":{"start":1660145110000,"stop":1660145117635,"duration":7635}},{"uid":"8282bb5a95d92dee","status":"passed","time":{"start":1660142645000,"stop":1660142652644,"duration":7644}},{"uid":"b0818303972bbfaf","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected an error to have occurred.  Got:\n    <nil>: nil","time":{"start":1660141834000,"stop":1660141955627,"duration":121627}},{"uid":"84fa00f504583871","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected an error to have occurred.  Got:\n    <nil>: nil","time":{"start":1660138561000,"stop":1660138682572,"duration":121572}},{"uid":"d471444e1cc671bc","status":"passed","time":{"start":1660116744000,"stop":1660116751745,"duration":7745}},{"uid":"50ee5f84f6d8f252","status":"passed","time":{"start":1660116144000,"stop":1660116151649,"duration":7649}},{"uid":"ca8255e5d0c5abbe","status":"passed","time":{"start":1660112860000,"stop":1660112867639,"duration":7639}},{"uid":"fde3f08ced4e77d3","status":"passed","time":{"start":1660108970000,"stop":1660108977661,"duration":7661}},{"uid":"b657a7d24c179150","status":"passed","time":{"start":1660104124000,"stop":1660104131738,"duration":7738}},{"uid":"5c45e4acb58c60e7","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected an error to have occurred.  Got:\n    <nil>: nil","time":{"start":1660104760000,"stop":1660104881591,"duration":121591}},{"uid":"3ac6cdbca55df7e4","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected an error to have occurred.  Got:\n    <nil>: nil","time":{"start":1660101116000,"stop":1660101237881,"duration":121881}},{"uid":"2a1627093111e3fc","status":"passed","time":{"start":1660100168000,"stop":1660100175705,"duration":7705}},{"uid":"865766cf0dcd5580","status":"passed","time":{"start":1660066476000,"stop":1660066483572,"duration":7572}},{"uid":"621f14a05b5f3ed2","status":"passed","time":{"start":1660064121000,"stop":1660064128625,"duration":7625}},{"uid":"bedd78fdd1861db2","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected an error to have occurred.  Got:\n    <nil>: nil","time":{"start":1660060680000,"stop":1660060801582,"duration":121582}},{"uid":"a4b50e580b257d3f","status":"passed","time":{"start":1660052169000,"stop":1660052176762,"duration":7762}},{"uid":"f0862ec4ea89aac2","status":"passed","time":{"start":1660050481000,"stop":1660050488727,"duration":7727}},{"uid":"9bd10255f997c6fe","status":"passed","time":{"start":1660046776000,"stop":1660046783602,"duration":7602}},{"uid":"64cd55f8d055bbc0","status":"passed","time":{"start":1660046048000,"stop":1660046055585,"duration":7585}},{"uid":"7a9845d53dbf2fe2","status":"passed","time":{"start":1660044532000,"stop":1660044539659,"duration":7659}}]},"Worker Suite:Worker Suite#[It] Slice Config Negative Scenario tests Slice Config Negative Scenario Creation Should fail while creating sliceconfig with invalid values on slice subnet":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":35,"unknown":0,"total":35},"items":[{"uid":"c046516ad4b3191a","status":"passed","time":{"start":1660113628000,"stop":1660113628403,"duration":403}},{"uid":"2a6b0223922ffa5d","status":"passed","time":{"start":1660104898000,"stop":1660104898388,"duration":388}},{"uid":"420942bc34bbbd50","status":"passed","time":{"start":1660067259000,"stop":1660067259222,"duration":222}},{"uid":"23627fff15ec10a0","status":"passed","time":{"start":1660047551000,"stop":1660047551314,"duration":314}},{"uid":"e1d0d8fc7c77748e","status":"passed","time":{"start":1659982549000,"stop":1659982549498,"duration":498}},{"uid":"160506ba5dcbd77a","status":"passed","time":{"start":1659970182000,"stop":1659970182200,"duration":200}},{"uid":"17904ef9501a8845","status":"passed","time":{"start":1659944495000,"stop":1659944495310,"duration":310}},{"uid":"6835329ee47e1603","status":"passed","time":{"start":1659874977000,"stop":1659874977254,"duration":254}},{"uid":"61788abb29ec968e","status":"passed","time":{"start":1659496050000,"stop":1659496050250,"duration":250}},{"uid":"b442481458c9b723","status":"passed","time":{"start":1659496083000,"stop":1659496083279,"duration":279}},{"uid":"36c3b66dc708c916","status":"passed","time":{"start":1659451501000,"stop":1659451501498,"duration":498}},{"uid":"b59d16f356eeaa6f","status":"passed","time":{"start":1659450916000,"stop":1659450916392,"duration":392}},{"uid":"d03b96d20b511421","status":"passed","time":{"start":1659451524000,"stop":1659451524236,"duration":236}},{"uid":"e3c78fa2357ad6d2","status":"passed","time":{"start":1659447298000,"stop":1659447298525,"duration":525}},{"uid":"86cae6f53c76bf88","status":"passed","time":{"start":1659447116000,"stop":1659447116246,"duration":246}},{"uid":"a92276c8fbe148b4","status":"passed","time":{"start":1659448205000,"stop":1659448205288,"duration":288}},{"uid":"a45572e6113fd0e9","status":"passed","time":{"start":1659448097000,"stop":1659448097462,"duration":462}},{"uid":"519e61c5ecb247ea","status":"passed","time":{"start":1659409279000,"stop":1659409279217,"duration":217}},{"uid":"fcb0188198abd360","status":"passed","time":{"start":1659402555000,"stop":1659402555301,"duration":301}},{"uid":"466a58b94d7b5612","status":"passed","time":{"start":1659291287000,"stop":1659291287533,"duration":533}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Update Should fail when Project is Applied with service account name as combination of special characters in Write users":{"statistic":{"failed":7,"broken":0,"skipped":0,"passed":79,"unknown":0,"total":86},"items":[{"uid":"84e64a49fdc0bd2e","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000811920>: {\n        Underlying: <*exec.ExitError | 0xc0005352c0>{\n            ProcessState: {\n                pid: 6252,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 135526},\n                    Stime: {Sec: 0, Usec: 49282},\n                    Maxrss: 75420,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3896,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13376,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 322,\n                    Nivcsw: 199,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The Project \"projectupdatetest\" is invalid: project namespace: Invalid value: \"kubeslice-projectupdatetest\": already exists\noccurred","time":{"start":1660145110000,"stop":1660145170240,"duration":60240}},{"uid":"c98d3d8013bb8815","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00066b578>: {\n        Underlying: <*exec.ExitError | 0xc0005892c0>{\n            ProcessState: {\n                pid: 6354,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 139900},\n                    Stime: {Sec: 0, Usec: 59957},\n                    Maxrss: 77424,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 10595,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13392,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 430,\n                    Nivcsw: 398,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The Project \"projectupdatetest\" is invalid: project namespace: Invalid value: \"kubeslice-projectupdatetest\": already exists\noccurred","time":{"start":1660142645000,"stop":1660142705254,"duration":60254}},{"uid":"f1858e30208727f3","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004befd8>: {\n        Underlying: <*exec.ExitError | 0xc00066a4e0>{\n            ProcessState: {\n                pid: 6176,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 140571},\n                    Stime: {Sec: 0, Usec: 46857},\n                    Maxrss: 73304,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 9816,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13392,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 276,\n                    Nivcsw: 248,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The Project \"projectupdatetest\" is invalid: project namespace: Invalid value: \"kubeslice-projectupdatetest\": already exists\noccurred","time":{"start":1660141834000,"stop":1660141894231,"duration":60231}},{"uid":"77175311e1f78a79","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000740810>: {\n        Underlying: <*exec.ExitError | 0xc000605860>{\n            ProcessState: {\n                pid: 6213,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 124256},\n                    Stime: {Sec: 0, Usec: 44377},\n                    Maxrss: 76504,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 14290,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13376,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 331,\n                    Nivcsw: 223,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The Project \"projectupdatetest\" is invalid: project namespace: Invalid value: \"kubeslice-projectupdatetest\": already exists\noccurred","time":{"start":1660138561000,"stop":1660138621228,"duration":60228}},{"uid":"b2117e062906673a","status":"passed","time":{"start":1660116744000,"stop":1660116749872,"duration":5872}},{"uid":"7dce8ca57c6f697d","status":"passed","time":{"start":1660116144000,"stop":1660116149663,"duration":5663}},{"uid":"3bd182145db0b9cd","status":"passed","time":{"start":1660112860000,"stop":1660112865663,"duration":5663}},{"uid":"2279a47635beef6c","status":"passed","time":{"start":1660108970000,"stop":1660108975737,"duration":5737}},{"uid":"a5502ed6582b0943","status":"passed","time":{"start":1660104124000,"stop":1660104128834,"duration":4834}},{"uid":"3a8f74dd82546b16","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000130c00>: {\n        Underlying: <*exec.ExitError | 0xc0006c0420>{\n            ProcessState: {\n                pid: 6326,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 181820},\n                    Stime: {Sec: 0, Usec: 43478},\n                    Maxrss: 74008,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 13565,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13392,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 526,\n                    Nivcsw: 355,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The Project \"projectupdatetest\" is invalid: project namespace: Invalid value: \"kubeslice-projectupdatetest\": already exists\noccurred","time":{"start":1660104760000,"stop":1660104820321,"duration":60321}},{"uid":"de662d49e6d64d9c","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00060fc38>: {\n        Underlying: <*exec.ExitError | 0xc000524c00>{\n            ProcessState: {\n                pid: 6161,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 122521},\n                    Stime: {Sec: 0, Usec: 18017},\n                    Maxrss: 81556,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 5058,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13376,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 408,\n                    Nivcsw: 160,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The Project \"projectupdatetest\" is invalid: project namespace: Invalid value: \"kubeslice-projectupdatetest\": already exists\noccurred","time":{"start":1660101116000,"stop":1660101176217,"duration":60217}},{"uid":"71b8bbb31acb74ec","status":"passed","time":{"start":1660100168000,"stop":1660100173798,"duration":5798}},{"uid":"5c586b0d783e76ae","status":"passed","time":{"start":1660066476000,"stop":1660066481651,"duration":5651}},{"uid":"50a0402c67dd4534","status":"passed","time":{"start":1660064121000,"stop":1660064126777,"duration":5777}},{"uid":"aed5fa0a2ff89887","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0005f9128>: {\n        Underlying: <*exec.ExitError | 0xc000499aa0>{\n            ProcessState: {\n                pid: 6234,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 141945},\n                    Stime: {Sec: 0, Usec: 66797},\n                    Maxrss: 78596,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 14907,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13392,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 339,\n                    Nivcsw: 256,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The Project \"projectupdatetest\" is invalid: project namespace: Invalid value: \"kubeslice-projectupdatetest\": already exists\noccurred","time":{"start":1660060680000,"stop":1660060740269,"duration":60269}},{"uid":"51e1b146e0ae4ace","status":"passed","time":{"start":1660052169000,"stop":1660052173850,"duration":4850}},{"uid":"2c9c7194ea7d8c7a","status":"passed","time":{"start":1660050481000,"stop":1660050485754,"duration":4754}},{"uid":"a6a89b6f8952e6f7","status":"passed","time":{"start":1660046776000,"stop":1660046781668,"duration":5668}},{"uid":"3150471999b9f0ee","status":"passed","time":{"start":1660046048000,"stop":1660046053737,"duration":5737}},{"uid":"4a7c652c0f8136a5","status":"passed","time":{"start":1660044532000,"stop":1660044536732,"duration":4732}}]},"Worker Suite:Worker Suite#[It] Slice validation Slice with MEDIUM QOS should have gateway pod running":{"statistic":{"failed":1,"broken":0,"skipped":3,"passed":5,"unknown":0,"total":9},"items":[{"uid":"b3b6e4a8fa0c91c1","status":"passed","time":{"start":1659168739000,"stop":1659168739027,"duration":27}},{"uid":"8230e5aa7484ed6b","status":"passed","time":{"start":1659164084000,"stop":1659164084038,"duration":38}},{"uid":"a6c5b4296cbc546a","status":"passed","time":{"start":1659160188000,"stop":1659160188098,"duration":98}},{"uid":"b68db7ec5e46e791","status":"passed","time":{"start":1659119724000,"stop":1659119724043,"duration":43}},{"uid":"c82a0259af00b654","status":"passed","time":{"start":1659116511000,"stop":1659116511028,"duration":28}},{"uid":"a7cc472df3f4a042","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659109470000,"stop":1659109470000,"duration":0}},{"uid":"1b5e1219b3c47024","status":"failed","statusDetails":"Timed out after 180.004s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1659106836000,"stop":1659107016302,"duration":180302}},{"uid":"da28f429be91085e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659101522000,"stop":1659101522000,"duration":0}},{"uid":"5fb9844f703d68ba","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659082816000,"stop":1659082816000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Attach/Deattach Scenario Attach cluster from slice  when Deattach cluster from slice  should have vl3 router running on attached cluster":{"statistic":{"failed":1,"broken":0,"skipped":22,"passed":3,"unknown":0,"total":26},"items":[{"uid":"3289fe47f5013d48","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"735a5e166091b1a4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"d0f24a524844458e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"694071bc5a3889df","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}},{"uid":"3654934211e0aee7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659982549000,"stop":1659982549000,"duration":0}},{"uid":"3c530b5d5c7d9c27","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659970182000,"stop":1659970182000,"duration":0}},{"uid":"5e7377a4071080bb","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659944495000,"stop":1659944495000,"duration":0}},{"uid":"32a6a4f83c6183ff","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659874977000,"stop":1659874977000,"duration":0}},{"uid":"c39f03e99cd43672","status":"passed","time":{"start":1659496050000,"stop":1659496050004,"duration":4}},{"uid":"c2c3fe78f84cd62d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496083000,"stop":1659496083000,"duration":0}},{"uid":"17262dac39e31aaa","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"e6940ede380b704","status":"passed","time":{"start":1659450916000,"stop":1659450916010,"duration":10}},{"uid":"6af39d089f161a6c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"51a330b9d3865d0b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447298000,"stop":1659447298000,"duration":0}},{"uid":"76a70ede7b0c3c08","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447116000,"stop":1659447116000,"duration":0}},{"uid":"8ad83e4468e229ce","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448205000,"stop":1659448205000,"duration":0}},{"uid":"82fca52b22481fe4","status":"passed","time":{"start":1659448097000,"stop":1659448097006,"duration":6}},{"uid":"60e9b2473e35b6f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659409279000,"stop":1659409279000,"duration":0}},{"uid":"99b186d6f01aeee2","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659402555000,"stop":1659402555000,"duration":0}},{"uid":"5139f1291d5a02a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659291287000,"stop":1659291287000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting nsm-kernel-forwarder pod Iperf cleanup":{"statistic":{"failed":0,"broken":0,"skipped":35,"passed":0,"unknown":0,"total":35},"items":[{"uid":"f44600f22877f25","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"52f42cba52cf9c10","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"c69073c6064b5f56","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"5c14f502ae6b7d0f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}},{"uid":"76aba77214a9e69f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659982549000,"stop":1659982549000,"duration":0}},{"uid":"8dc20c5bb402487f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659970182000,"stop":1659970182000,"duration":0}},{"uid":"3019686f91deff55","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659944495000,"stop":1659944495000,"duration":0}},{"uid":"57a51c0c230f0d42","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659874977000,"stop":1659874977000,"duration":0}},{"uid":"56635be81460e006","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496050000,"stop":1659496050000,"duration":0}},{"uid":"ea9546db01da9460","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496083000,"stop":1659496083000,"duration":0}},{"uid":"a1dabe74d7f35002","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"da517c8d81bc9190","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659450916000,"stop":1659450916000,"duration":0}},{"uid":"88ea2970231c1c92","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"33f4a46a9a804c52","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447298000,"stop":1659447298000,"duration":0}},{"uid":"5a92b248513dce25","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447116000,"stop":1659447116000,"duration":0}},{"uid":"e73501cc652415d9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448205000,"stop":1659448205000,"duration":0}},{"uid":"cab1e59f900d2e6a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448097000,"stop":1659448097000,"duration":0}},{"uid":"f021b263f7f6616","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659409279000,"stop":1659409279000,"duration":0}},{"uid":"c0b4c0a5d783e7f4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659402555000,"stop":1659402555000,"duration":0}},{"uid":"e103e06b3573f460","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659291287000,"stop":1659291287000,"duration":0}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Deletion Should Delete an existing project successfully":{"statistic":{"failed":5,"broken":0,"skipped":0,"passed":81,"unknown":0,"total":86},"items":[{"uid":"a299f1aaabeaba43","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0003a1500>: {\n        Underlying: <*exec.ExitError | 0xc0006c5e60>{\n            ProcessState: {\n                pid: 6343,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 153963},\n                    Stime: {Sec: 0, Usec: 20258},\n                    Maxrss: 79204,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4732,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13392,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 355,\n                    Nivcsw: 266,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectdeletetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectdeletetest\\\": already exists\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The Project \\\"projectdeletetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectdeletetest\\\": already exists\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectdeletetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectdeletetest\\\": already exists\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The Project \\\"projectdeletetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectdeletetest\\\": already exists\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The Project \"projectdeletetest\" is invalid: project namespace: Invalid value: \"kubeslice-projectdeletetest\": already exists\noccurred","time":{"start":1660145110000,"stop":1660145170232,"duration":60232}},{"uid":"f96828ebb8337011","status":"passed","time":{"start":1660142645000,"stop":1660142649644,"duration":4644}},{"uid":"92baf3a3c36dca41","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0003501f8>: {\n        Underlying: <*exec.ExitError | 0xc0007982a0>{\n            ProcessState: {\n                pid: 6043,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 143521},\n                    Stime: {Sec: 0, Usec: 31893},\n                    Maxrss: 77588,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4840,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13376,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 297,\n                    Nivcsw: 369,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectdeletetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectdeletetest\\\": already exists\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The Project \\\"projectdeletetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectdeletetest\\\": already exists\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectdeletetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectdeletetest\\\": already exists\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The Project \\\"projectdeletetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectdeletetest\\\": already exists\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The Project \"projectdeletetest\" is invalid: project namespace: Invalid value: \"kubeslice-projectdeletetest\": already exists\noccurred","time":{"start":1660141834000,"stop":1660141894220,"duration":60220}},{"uid":"bbdc98003265002c","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000fc68>: {\n        Underlying: <*exec.ExitError | 0xc0006f68c0>{\n            ProcessState: {\n                pid: 6285,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 122839},\n                    Stime: {Sec: 0, Usec: 40946},\n                    Maxrss: 81816,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 8674,\n                    Majflt: 1,\n                    Nswap: 0,\n                    Inblock: 144,\n                    Oublock: 13392,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 341,\n                    Nivcsw: 340,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectdeletetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectdeletetest\\\": already exists\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The Project \\\"projectdeletetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectdeletetest\\\": already exists\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectdeletetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectdeletetest\\\": already exists\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The Project \\\"projectdeletetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectdeletetest\\\": already exists\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The Project \"projectdeletetest\" is invalid: project namespace: Invalid value: \"kubeslice-projectdeletetest\": already exists\noccurred","time":{"start":1660138561000,"stop":1660138621215,"duration":60215}},{"uid":"ff57a6060ebacfb9","status":"passed","time":{"start":1660116744000,"stop":1660116748723,"duration":4723}},{"uid":"c94497c93b713bf7","status":"passed","time":{"start":1660116144000,"stop":1660116148589,"duration":4589}},{"uid":"6a1ec7cd2ba4b995","status":"passed","time":{"start":1660112860000,"stop":1660112864584,"duration":4584}},{"uid":"f60ebc063361d148","status":"passed","time":{"start":1660108970000,"stop":1660108974680,"duration":4680}},{"uid":"c9df39ec000f0fcc","status":"passed","time":{"start":1660104124000,"stop":1660104128690,"duration":4690}},{"uid":"5cd4f8c21ba54212","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004be180>: {\n        Underlying: <*exec.ExitError | 0xc0006c4040>{\n            ProcessState: {\n                pid: 6077,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 118194},\n                    Stime: {Sec: 0, Usec: 54876},\n                    Maxrss: 80540,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4097,\n                    Majflt: 1,\n                    Nswap: 0,\n                    Inblock: 256,\n                    Oublock: 13376,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 352,\n                    Nivcsw: 160,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectdeletetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectdeletetest\\\": already exists\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The Project \\\"projectdeletetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectdeletetest\\\": already exists\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectdeletetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectdeletetest\\\": already exists\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The Project \\\"projectdeletetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectdeletetest\\\": already exists\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The Project \"projectdeletetest\" is invalid: project namespace: Invalid value: \"kubeslice-projectdeletetest\": already exists\noccurred","time":{"start":1660104760000,"stop":1660104820252,"duration":60252}},{"uid":"7347edc5836e8cca","status":"passed","time":{"start":1660101116000,"stop":1660101120547,"duration":4547}},{"uid":"b7c8088afbe6176","status":"passed","time":{"start":1660100168000,"stop":1660100172840,"duration":4840}},{"uid":"e8b93ad37a1b8a22","status":"passed","time":{"start":1660066476000,"stop":1660066480582,"duration":4582}},{"uid":"cb7f4a1eb3750fce","status":"passed","time":{"start":1660064121000,"stop":1660064125586,"duration":4586}},{"uid":"e43ccc430109346b","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0005f9608>: {\n        Underlying: <*exec.ExitError | 0xc00083e3a0>{\n            ProcessState: {\n                pid: 6127,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 139948},\n                    Stime: {Sec: 0, Usec: 50890},\n                    Maxrss: 73884,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4488,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13392,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 331,\n                    Nivcsw: 250,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectdeletetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectdeletetest\\\": already exists\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The Project \\\"projectdeletetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectdeletetest\\\": already exists\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectdeletetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectdeletetest\\\": already exists\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The Project \\\"projectdeletetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectdeletetest\\\": already exists\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The Project \"projectdeletetest\" is invalid: project namespace: Invalid value: \"kubeslice-projectdeletetest\": already exists\noccurred","time":{"start":1660060680000,"stop":1660060740271,"duration":60271}},{"uid":"57424dadf11e7ed8","status":"passed","time":{"start":1660052169000,"stop":1660052173769,"duration":4769}},{"uid":"fa97e7416945ea18","status":"passed","time":{"start":1660050481000,"stop":1660050485695,"duration":4695}},{"uid":"e44a558108d7f759","status":"passed","time":{"start":1660046776000,"stop":1660046780599,"duration":4599}},{"uid":"b86baef80e411587","status":"passed","time":{"start":1660046048000,"stop":1660046052571,"duration":4571}},{"uid":"72d26ad872ebf9c2","status":"passed","time":{"start":1660044532000,"stop":1660044536623,"duration":4623}}]},"Worker Suite:Worker Suite#[It] Slice Config Negative Scenario tests Slice Config Negative Scenario Update Should update while deploying sliceconfig with existing slice name":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":35,"unknown":0,"total":35},"items":[{"uid":"e1088d93cfb7d6c6","status":"passed","time":{"start":1660113628000,"stop":1660113628487,"duration":487}},{"uid":"3bf78a3f3ee4de48","status":"passed","time":{"start":1660104898000,"stop":1660104898510,"duration":510}},{"uid":"27407775e2e36805","status":"passed","time":{"start":1660067259000,"stop":1660067259578,"duration":578}},{"uid":"7d2b2902410b0503","status":"passed","time":{"start":1660047551000,"stop":1660047551724,"duration":724}},{"uid":"a36e599b98daf010","status":"passed","time":{"start":1659982549000,"stop":1659982550101,"duration":1101}},{"uid":"26f6948d71d7d1b3","status":"passed","time":{"start":1659970182000,"stop":1659970182592,"duration":592}},{"uid":"3d161a5475ac8a39","status":"passed","time":{"start":1659944495000,"stop":1659944495643,"duration":643}},{"uid":"b8e93b0b10a14592","status":"passed","time":{"start":1659874977000,"stop":1659874977814,"duration":814}},{"uid":"1d5c91043c275f53","status":"passed","time":{"start":1659496050000,"stop":1659496050508,"duration":508}},{"uid":"8f99755c905c768c","status":"passed","time":{"start":1659496083000,"stop":1659496083782,"duration":782}},{"uid":"35e510bdb15c348e","status":"passed","time":{"start":1659451501000,"stop":1659451502030,"duration":1030}},{"uid":"7226c2869e8379d4","status":"passed","time":{"start":1659450916000,"stop":1659450916879,"duration":879}},{"uid":"93b0635f72459da8","status":"passed","time":{"start":1659451524000,"stop":1659451524635,"duration":635}},{"uid":"d508846ad4b42898","status":"passed","time":{"start":1659447298000,"stop":1659447298586,"duration":586}},{"uid":"f8254281bb3f0d06","status":"passed","time":{"start":1659447116000,"stop":1659447116857,"duration":857}},{"uid":"a88714c4351883c0","status":"passed","time":{"start":1659448205000,"stop":1659448205771,"duration":771}},{"uid":"65dd7ff68001ea46","status":"passed","time":{"start":1659448097000,"stop":1659448097959,"duration":959}},{"uid":"40953c7a9a9642b5","status":"passed","time":{"start":1659409279000,"stop":1659409280083,"duration":1083}},{"uid":"64ab959e8ac6cb4c","status":"passed","time":{"start":1659402555000,"stop":1659402555807,"duration":807}},{"uid":"6641a73fe74cec11","status":"passed","time":{"start":1659291287000,"stop":1659291287800,"duration":800}}]},"Worker Suite:Worker Suite#[AfterSuite]":{"statistic":{"failed":7,"broken":12,"skipped":0,"passed":81,"unknown":0,"total":100},"items":[{"uid":"4e2b6ef0fc87ef91","status":"passed","time":{"start":1660153688000,"stop":1660153690261,"duration":2261}},{"uid":"b02f660b3f9f94b2","status":"passed","time":{"start":1660152164000,"stop":1660152166292,"duration":2292}},{"uid":"13a8957077bfe3d3","status":"failed","statusDetails":"Timed out after 210.007s.\nExpected an error to have occurred.  Got:\n    <nil>: nil","time":{"start":1660146582000,"stop":1660146792476,"duration":210476}},{"uid":"609b84cc9f9d129a","status":"failed","statusDetails":"Timed out after 210.009s.\nExpected an error to have occurred.  Got:\n    <nil>: nil","time":{"start":1660143888000,"stop":1660144098407,"duration":210407}},{"uid":"bb57f4c200210e82","status":"failed","statusDetails":"Timed out after 210.009s.\nExpected an error to have occurred.  Got:\n    <nil>: nil","time":{"start":1660143654000,"stop":1660143864360,"duration":210360}},{"uid":"e04308c4ddda2b80","status":"failed","statusDetails":"Timed out after 210.004s.\nExpected an error to have occurred.  Got:\n    <nil>: nil","time":{"start":1660140253000,"stop":1660140463399,"duration":210399}},{"uid":"ebed09ab06c7c745","status":"passed","time":{"start":1660137949000,"stop":1660137951156,"duration":2156}},{"uid":"a99a23fcb787a1c4","status":"passed","time":{"start":1660129303000,"stop":1660129305860,"duration":2860}},{"uid":"bfaf4038032539e6","status":"passed","time":{"start":1660128018000,"stop":1660128020252,"duration":2252}},{"uid":"2fce3bfaa4326d25","status":"passed","time":{"start":1660127085000,"stop":1660127088446,"duration":3446}},{"uid":"c06477159c5a5ea","status":"passed","time":{"start":1660117538000,"stop":1660117562788,"duration":24788}},{"uid":"169ab4f187c334dd","status":"passed","time":{"start":1660116889000,"stop":1660116912654,"duration":23654}},{"uid":"104c0fbd1e8852d8","status":"passed","time":{"start":1660113628000,"stop":1660113985511,"duration":357511}},{"uid":"983627137d59b1b2","status":"passed","time":{"start":1660114668000,"stop":1660114668376,"duration":376}},{"uid":"7b43e2c24272868f","status":"passed","time":{"start":1660109721000,"stop":1660109745299,"duration":24299}},{"uid":"a7a0488afe5d22ea","status":"broken","statusDetails":"interrupted","time":{"start":1660104898000,"stop":1660106079229,"duration":1181229}},{"uid":"f5c3dcab970438e3","status":"failed","statusDetails":"Timed out after 210.007s.\nExpected an error to have occurred.  Got:\n    <nil>: nil","time":{"start":1660106395000,"stop":1660106605441,"duration":210441}},{"uid":"2c993129effaebf7","status":"failed","statusDetails":"Timed out after 210.006s.\nExpected an error to have occurred.  Got:\n    <nil>: nil","time":{"start":1660102709000,"stop":1660102919381,"duration":210381}},{"uid":"61174749307f5de5","status":"passed","time":{"start":1660100787000,"stop":1660100811575,"duration":24575}},{"uid":"ae468cdb1eaed0a1","status":"passed","time":{"start":1660067259000,"stop":1660067615825,"duration":356825}}]},"Worker Suite:Worker Suite#[It] Attach/Deattach Scenario Attach cluster from slice  when Deattach cluster from slice  should have gateway pod deleted from deattached cluster":{"statistic":{"failed":0,"broken":0,"skipped":23,"passed":3,"unknown":0,"total":26},"items":[{"uid":"a07b40b4b758dab8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"5f73c8f35795ada","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"6d34a78e20f38e3d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"7803f71bb08dc67b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}},{"uid":"1a11755b98b63b69","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659982549000,"stop":1659982549000,"duration":0}},{"uid":"77e97ca1b13682c0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659970182000,"stop":1659970182000,"duration":0}},{"uid":"6e175a3dd7eed76c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659944495000,"stop":1659944495000,"duration":0}},{"uid":"5a8cf36d17005672","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659874977000,"stop":1659874977000,"duration":0}},{"uid":"c1b6b67e5de2a6b6","status":"passed","time":{"start":1659496050000,"stop":1659496050003,"duration":3}},{"uid":"395cd9faac31fe60","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496083000,"stop":1659496083000,"duration":0}},{"uid":"30cae155000ba6cf","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"84c70712657d4677","status":"passed","time":{"start":1659450916000,"stop":1659450916006,"duration":6}},{"uid":"6155591c65f5f356","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"b5ae6ec168fe4afa","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447298000,"stop":1659447298000,"duration":0}},{"uid":"b8b38178490dbc91","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447116000,"stop":1659447116000,"duration":0}},{"uid":"f0dbe3c680dd1432","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448205000,"stop":1659448205000,"duration":0}},{"uid":"bee2a09809f44ac3","status":"passed","time":{"start":1659448097000,"stop":1659448097005,"duration":5}},{"uid":"83c6fc5793653de0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659409279000,"stop":1659409279000,"duration":0}},{"uid":"7bb73961685a8929","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659402555000,"stop":1659402555000,"duration":0}},{"uid":"36dafe605ddf7ffe","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659291287000,"stop":1659291287000,"duration":0}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests Iperf App connectivity Should install iperf-server on server cluster":{"statistic":{"failed":0,"broken":0,"skipped":28,"passed":7,"unknown":0,"total":35},"items":[{"uid":"50234fff618b0fdd","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"c8b848eb04b8c53f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"168abb95e985e3b5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"bb9ebf09c77d6533","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}},{"uid":"1d8a3c288b98caa4","status":"passed","time":{"start":1659982549000,"stop":1659982580633,"duration":31633}},{"uid":"3597f86b72eec6a4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659970182000,"stop":1659970182000,"duration":0}},{"uid":"461f0e7c750d5488","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659944495000,"stop":1659944495000,"duration":0}},{"uid":"ccce999e5ec2b275","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659874977000,"stop":1659874977000,"duration":0}},{"uid":"27b355b863ba245e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496050000,"stop":1659496050000,"duration":0}},{"uid":"3c70b4028996091c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496083000,"stop":1659496083000,"duration":0}},{"uid":"490905978d350b02","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"3ca7b409931c8f32","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659450916000,"stop":1659450916000,"duration":0}},{"uid":"7029556ee23fa00b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"ac9bca7f9f51e13b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447298000,"stop":1659447298000,"duration":0}},{"uid":"a6314b9c001dd6c9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447116000,"stop":1659447116000,"duration":0}},{"uid":"d6365664de1a2f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448205000,"stop":1659448205000,"duration":0}},{"uid":"11aace1951b727a0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448097000,"stop":1659448097000,"duration":0}},{"uid":"54de1ac12a4ad713","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659409279000,"stop":1659409279000,"duration":0}},{"uid":"53ea139c6d7062dc","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659402555000,"stop":1659402555000,"duration":0}},{"uid":"8d659da0ac95e766","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659291287000,"stop":1659291287000,"duration":0}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests slice with netpol should remove network policy resource from application namespace once the isolationEnabled is set to false":{"statistic":{"failed":0,"broken":3,"skipped":25,"passed":7,"unknown":0,"total":35},"items":[{"uid":"c4ecdae0e74c8965","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"d666d99d343a5072","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"25f9d43772a122a1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"1d614e2ef6c93c5e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}},{"uid":"d372fb91f1a15f2b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659982549000,"stop":1659982549000,"duration":0}},{"uid":"149eef822efba33d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659970182000,"stop":1659970182000,"duration":0}},{"uid":"e20f4ec7e66dc3fb","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659944495000,"stop":1659944495000,"duration":0}},{"uid":"3e725336bbb60660","status":"broken","statusDetails":"runtime error: invalid memory address or nil pointer dereference","time":{"start":1659874977000,"stop":1659874996220,"duration":19220}},{"uid":"9ad3cbb10a056f9c","status":"passed","time":{"start":1659496050000,"stop":1659496065221,"duration":15221}},{"uid":"289f60f3cd624e96","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496083000,"stop":1659496083000,"duration":0}},{"uid":"2c1136a6022a8261","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"378ff2223d9af915","status":"passed","time":{"start":1659450916000,"stop":1659450932280,"duration":16280}},{"uid":"d7a0fdcc7dc09098","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"6d7f184fa5e0fe54","status":"broken","statusDetails":"runtime error: invalid memory address or nil pointer dereference","time":{"start":1659447298000,"stop":1659447328053,"duration":30053}},{"uid":"f2f8be5e7860eeec","status":"passed","time":{"start":1659447116000,"stop":1659447129404,"duration":13404}},{"uid":"aa3d7207a2c6634c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448205000,"stop":1659448205000,"duration":0}},{"uid":"b7444af250157db2","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448097000,"stop":1659448097000,"duration":0}},{"uid":"f4b792ddfa040b83","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659409279000,"stop":1659409279000,"duration":0}},{"uid":"1100307b3831faaa","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659402555000,"stop":1659402555000,"duration":0}},{"uid":"51a9d9130d4b372d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659291287000,"stop":1659291287000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice creation scenarios: when slice red is installed workers should get attached to slice":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":35,"unknown":0,"total":35},"items":[{"uid":"b78277e7475b8a33","status":"passed","time":{"start":1660113628000,"stop":1660113628340,"duration":340}},{"uid":"1da6b1671cd9086d","status":"passed","time":{"start":1660104898000,"stop":1660104898386,"duration":386}},{"uid":"49a8fbddbe632ff4","status":"passed","time":{"start":1660067259000,"stop":1660067259534,"duration":534}},{"uid":"5d15b2fb7d385713","status":"passed","time":{"start":1660047551000,"stop":1660047551685,"duration":685}},{"uid":"7b33ca5f146d6115","status":"passed","time":{"start":1659982549000,"stop":1659982552234,"duration":3234}},{"uid":"dda26ef6bd87456d","status":"passed","time":{"start":1659970182000,"stop":1659970184807,"duration":2807}},{"uid":"a72143e7dca53e27","status":"passed","time":{"start":1659944495000,"stop":1659944497946,"duration":2946}},{"uid":"5c45c80189be2453","status":"passed","time":{"start":1659874977000,"stop":1659874980196,"duration":3196}},{"uid":"98e8766b87f1f9ac","status":"passed","time":{"start":1659496050000,"stop":1659496053016,"duration":3016}},{"uid":"9529399196f130eb","status":"passed","time":{"start":1659496083000,"stop":1659496086630,"duration":3630}},{"uid":"1742215a1316d1ce","status":"passed","time":{"start":1659451501000,"stop":1659451503470,"duration":2470}},{"uid":"e9c29467b8fe7a53","status":"passed","time":{"start":1659450916000,"stop":1659450919065,"duration":3065}},{"uid":"7f14fcefaa265e0b","status":"passed","time":{"start":1659451524000,"stop":1659451527004,"duration":3004}},{"uid":"4b46bb22383a44f1","status":"passed","time":{"start":1659447298000,"stop":1659447301835,"duration":3835}},{"uid":"eb29a8b2afe14bec","status":"passed","time":{"start":1659447116000,"stop":1659447118699,"duration":2699}},{"uid":"dbeb581be7925275","status":"passed","time":{"start":1659448205000,"stop":1659448205705,"duration":705}},{"uid":"83a0626e9bb21e06","status":"passed","time":{"start":1659448097000,"stop":1659448099897,"duration":2897}},{"uid":"4ca60c617e9f3922","status":"passed","time":{"start":1659409279000,"stop":1659409279712,"duration":712}},{"uid":"47d54fb3ab63acbc","status":"passed","time":{"start":1659402555000,"stop":1659402557904,"duration":2904}},{"uid":"237a375debf0f1a4","status":"passed","time":{"start":1659291287000,"stop":1659291289750,"duration":2750}}]},"Worker Suite:Worker Suite#[It] Slice validation Slice with LOW QOS should have gateway pod running":{"statistic":{"failed":1,"broken":0,"skipped":2,"passed":6,"unknown":0,"total":9},"items":[{"uid":"37109e6a1fcf8e95","status":"passed","time":{"start":1659168739000,"stop":1659168769082,"duration":30082}},{"uid":"174f63e4d95b0be8","status":"passed","time":{"start":1659164084000,"stop":1659164109099,"duration":25099}},{"uid":"a62f8935b4aa5bb8","status":"passed","time":{"start":1659160188000,"stop":1659160208045,"duration":20045}},{"uid":"b0ef192b3b3a9205","status":"passed","time":{"start":1659119724000,"stop":1659119749068,"duration":25068}},{"uid":"8e90a32bd8a8113b","status":"passed","time":{"start":1659116511000,"stop":1659116511012,"duration":12}},{"uid":"f41770fc4484150","status":"passed","time":{"start":1659109470000,"stop":1659109470028,"duration":28}},{"uid":"8ad77caf63959cf7","status":"failed","statusDetails":"Timed out after 180.004s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1659106836000,"stop":1659107016285,"duration":180285}},{"uid":"17f1dcc7e00985cc","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659101522000,"stop":1659101522000,"duration":0}},{"uid":"6423839b465e949a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659082816000,"stop":1659082816000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Namespace reconcile test scenarios: Offboard slice get detached from app ns in cluster objects":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":35,"unknown":0,"total":35},"items":[{"uid":"24e36ef2c7ba214e","status":"passed","time":{"start":1660113628000,"stop":1660113628144,"duration":144}},{"uid":"9d12c51634e50f5e","status":"passed","time":{"start":1660104898000,"stop":1660104898170,"duration":170}},{"uid":"928412d23ea7283e","status":"passed","time":{"start":1660067259000,"stop":1660067259292,"duration":292}},{"uid":"698a909a502fe02b","status":"passed","time":{"start":1660047551000,"stop":1660047551231,"duration":231}},{"uid":"6a1097204066860e","status":"passed","time":{"start":1659982549000,"stop":1659982549526,"duration":526}},{"uid":"d926ea3cc7538073","status":"passed","time":{"start":1659970182000,"stop":1659970182175,"duration":175}},{"uid":"323860827362ece7","status":"passed","time":{"start":1659944495000,"stop":1659944495326,"duration":326}},{"uid":"3ee2323f677f1a91","status":"passed","time":{"start":1659874977000,"stop":1659874977320,"duration":320}},{"uid":"1e3120e38a62ebb5","status":"passed","time":{"start":1659496050000,"stop":1659496050135,"duration":135}},{"uid":"c30cbf485972bdc1","status":"passed","time":{"start":1659496083000,"stop":1659496083179,"duration":179}},{"uid":"7ea06c6e7d2a0c5f","status":"passed","time":{"start":1659451501000,"stop":1659451501161,"duration":161}},{"uid":"c7a56f4a04b914e4","status":"passed","time":{"start":1659450916000,"stop":1659450916385,"duration":385}},{"uid":"e79ef4f4c6e49eef","status":"passed","time":{"start":1659451524000,"stop":1659451524436,"duration":436}},{"uid":"a5830c870df8e938","status":"passed","time":{"start":1659447298000,"stop":1659447298147,"duration":147}},{"uid":"c682ef4b1f1027b9","status":"passed","time":{"start":1659447116000,"stop":1659447116222,"duration":222}},{"uid":"4066b6cfa4c392df","status":"passed","time":{"start":1659448205000,"stop":1659448205148,"duration":148}},{"uid":"7439d65b3b9b5e90","status":"passed","time":{"start":1659448097000,"stop":1659448097348,"duration":348}},{"uid":"c7c7b10094bc8ef8","status":"passed","time":{"start":1659409279000,"stop":1659409279276,"duration":276}},{"uid":"ea3a960b873d0173","status":"passed","time":{"start":1659402555000,"stop":1659402555188,"duration":188}},{"uid":"657d1672fb3f633a","status":"passed","time":{"start":1659291287000,"stop":1659291287165,"duration":165}}]},"Hub Suite:Hub Suite#[It] Cluster negative tests Worker Cluster Registration with Wrong endpoint":{"statistic":{"failed":17,"broken":0,"skipped":0,"passed":72,"unknown":0,"total":89},"items":[{"uid":"9be3fa1bdf0c1131","status":"failed","statusDetails":"Timed out after 120.000s.\nExpected an error to have occurred.  Got:\n    <nil>: nil","time":{"start":1660145110000,"stop":1660145569198,"duration":459198}},{"uid":"23cf9d88dc9349e0","status":"failed","statusDetails":"Timed out after 120.001s.\nExpected an error to have occurred.  Got:\n    <nil>: nil","time":{"start":1660142645000,"stop":1660143106804,"duration":461804}},{"uid":"68f09d7ef9940abe","status":"failed","statusDetails":"Timed out after 120.000s.\nExpected an error to have occurred.  Got:\n    <nil>: nil","time":{"start":1660141834000,"stop":1660142292973,"duration":458973}},{"uid":"7855071a04737240","status":"failed","statusDetails":"Timed out after 120.001s.\nExpected an error to have occurred.  Got:\n    <nil>: nil","time":{"start":1660138561000,"stop":1660139021359,"duration":460359}},{"uid":"97d969455478ab97","status":"passed","time":{"start":1660116744000,"stop":1660116879944,"duration":135944}},{"uid":"ed87bcc032c00146","status":"passed","time":{"start":1660116144000,"stop":1660116284198,"duration":140198}},{"uid":"6766c7e47881c64c","status":"passed","time":{"start":1660112860000,"stop":1660113001064,"duration":141064}},{"uid":"9299638a2c7ed9f3","status":"passed","time":{"start":1660108970000,"stop":1660109104869,"duration":134869}},{"uid":"715ef3cfb2a5d836","status":"passed","time":{"start":1660104124000,"stop":1660104258749,"duration":134749}},{"uid":"98cef60366e7606b","status":"failed","statusDetails":"Timed out after 120.001s.\nExpected an error to have occurred.  Got:\n    <nil>: nil","time":{"start":1660104760000,"stop":1660105098487,"duration":338487}},{"uid":"1ef94faca9d639b8","status":"failed","statusDetails":"Timed out after 120.001s.\nExpected an error to have occurred.  Got:\n    <nil>: nil","time":{"start":1660101116000,"stop":1660101578090,"duration":462090}},{"uid":"acf4b5be743a39f2","status":"passed","time":{"start":1660100168000,"stop":1660100304261,"duration":136261}},{"uid":"167f385983ccb43e","status":"passed","time":{"start":1660066476000,"stop":1660066610428,"duration":134428}},{"uid":"8c60489f23cb06ec","status":"passed","time":{"start":1660064121000,"stop":1660064262611,"duration":141611}},{"uid":"84b095835a3b8f2e","status":"failed","statusDetails":"Timed out after 120.001s.\nExpected an error to have occurred.  Got:\n    <nil>: nil","time":{"start":1660060680000,"stop":1660061018876,"duration":338876}},{"uid":"47908ae1bd73fa8c","status":"passed","time":{"start":1660052169000,"stop":1660052305650,"duration":136650}},{"uid":"2ef4a65f60f0ce8d","status":"failed","statusDetails":"\n\tError Trace:\tnamespace.go:70\n\t            \t\t\t\tt_cluster_negative_test.go:88\n\t            \t\t\t\tsuite.go:596\n\t            \t\t\t\tasm_amd64.s:1581\n\tError:      \tReceived unexpected error:\n\t            \tnamespaces \"kubeslice-system\" not found\n\tTest:       \tCluster negative tests Worker Cluster Registration with Wrong endpoint\n","time":{"start":1660050481000,"stop":1660050492956,"duration":11956}},{"uid":"b4647dff1bac686a","status":"passed","time":{"start":1660046776000,"stop":1660046915298,"duration":139298}},{"uid":"a272b3171b817b88","status":"failed","statusDetails":"\n\tError Trace:\tnamespace.go:70\n\t            \t\t\t\tt_cluster_negative_test.go:88\n\t            \t\t\t\tsuite.go:596\n\t            \t\t\t\tasm_amd64.s:1581\n\tError:      \tReceived unexpected error:\n\t            \tnamespaces \"kubeslice-system\" not found\n\tTest:       \tCluster negative tests Worker Cluster Registration with Wrong endpoint\n","time":{"start":1660046048000,"stop":1660046060928,"duration":12928}},{"uid":"2e7cc4339712dc14","status":"failed","statusDetails":"\n\tError Trace:\tnamespace.go:70\n\t            \t\t\t\tt_cluster_negative_test.go:88\n\t            \t\t\t\tsuite.go:596\n\t            \t\t\t\tasm_amd64.s:1581\n\tError:      \tReceived unexpected error:\n\t            \tnamespaces \"kubeslice-system\" not found\n\tTest:       \tCluster negative tests Worker Cluster Registration with Wrong endpoint\n","time":{"start":1660044532000,"stop":1660044544971,"duration":12971}}]},"Worker Suite:Worker Suite#[It] Slice Creation scenarios Slice with LOW QOS should reconcile low qos slice":{"statistic":{"failed":0,"broken":0,"skipped":8,"passed":18,"unknown":0,"total":26},"items":[{"uid":"c024270115cc5970","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"cda97b10df661bed","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"3bdde8547ea9cff","status":"passed","time":{"start":1660067259000,"stop":1660067259352,"duration":352}},{"uid":"3ee95c4806144a62","status":"passed","time":{"start":1660047551000,"stop":1660047551514,"duration":514}},{"uid":"bd86fd7e1923fbd5","status":"passed","time":{"start":1659982549000,"stop":1659982549724,"duration":724}},{"uid":"185b4e098271da1a","status":"passed","time":{"start":1659970182000,"stop":1659970182334,"duration":334}},{"uid":"934e980d89e28e49","status":"passed","time":{"start":1659944495000,"stop":1659944495501,"duration":501}},{"uid":"4d6a1e6c84f1b2bb","status":"passed","time":{"start":1659874977000,"stop":1659874977320,"duration":320}},{"uid":"dfddbc956563f8af","status":"passed","time":{"start":1659496050000,"stop":1659496050313,"duration":313}},{"uid":"2fb0eff080ceec37","status":"passed","time":{"start":1659496083000,"stop":1659496083358,"duration":358}},{"uid":"18ee77134f9d6e2d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"d44dcd6a5b884b4b","status":"passed","time":{"start":1659450916000,"stop":1659450916449,"duration":449}},{"uid":"f3291dfb8626c61b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"1106a8282b6e9d54","status":"passed","time":{"start":1659447298000,"stop":1659447298374,"duration":374}},{"uid":"11724a2eb5ec452","status":"passed","time":{"start":1659447116000,"stop":1659447116638,"duration":638}},{"uid":"d4b1319ab6182251","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448205000,"stop":1659448205000,"duration":0}},{"uid":"ac515d3566209716","status":"passed","time":{"start":1659448097000,"stop":1659448097438,"duration":438}},{"uid":"ba619990a7102cca","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659409279000,"stop":1659409279000,"duration":0}},{"uid":"616422c218937bf8","status":"passed","time":{"start":1659402555000,"stop":1659402555427,"duration":427}},{"uid":"eda9cf7086e29506","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659291287000,"stop":1659291287000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Hub netpol scenarios: Slice Configs Netpol scenarios Should successfully deploy for valid namespace creating clusters with * in both applicationNamespace and allowedNamespace":{"statistic":{"failed":0,"broken":0,"skipped":9,"passed":26,"unknown":0,"total":35},"items":[{"uid":"106a7e6a0eedaa5e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"8968ac8a980ba084","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"cde58556223c78a8","status":"passed","time":{"start":1660067259000,"stop":1660067259696,"duration":696}},{"uid":"1817a460312e12e5","status":"passed","time":{"start":1660047551000,"stop":1660047551474,"duration":474}},{"uid":"1acd960545e5fc6","status":"passed","time":{"start":1659982549000,"stop":1659982550116,"duration":1116}},{"uid":"d613384647c52e98","status":"passed","time":{"start":1659970182000,"stop":1659970182403,"duration":403}},{"uid":"2f1f10872d9174c5","status":"passed","time":{"start":1659944495000,"stop":1659944495795,"duration":795}},{"uid":"6886dcbc27580230","status":"passed","time":{"start":1659874977000,"stop":1659874977469,"duration":469}},{"uid":"5ecb611a60e9b3cd","status":"passed","time":{"start":1659496050000,"stop":1659496050375,"duration":375}},{"uid":"476a63ecbd11329b","status":"passed","time":{"start":1659496083000,"stop":1659496083517,"duration":517}},{"uid":"2a122e065760bbe0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"cbe3ccdf5d91f0b7","status":"passed","time":{"start":1659450916000,"stop":1659450916474,"duration":474}},{"uid":"89df494b1e4f0cd9","status":"passed","time":{"start":1659451524000,"stop":1659451524775,"duration":775}},{"uid":"6586a1b645b9aefc","status":"passed","time":{"start":1659447298000,"stop":1659447298377,"duration":377}},{"uid":"e5974338295322e1","status":"passed","time":{"start":1659447116000,"stop":1659447116451,"duration":451}},{"uid":"db368f376661734","status":"passed","time":{"start":1659448205000,"stop":1659448205531,"duration":531}},{"uid":"5833b08d72353123","status":"passed","time":{"start":1659448097000,"stop":1659448097613,"duration":613}},{"uid":"442a88ff8b8a1c31","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659409279000,"stop":1659409279000,"duration":0}},{"uid":"7dd2bab527290e0a","status":"passed","time":{"start":1659402555000,"stop":1659402555634,"duration":634}},{"uid":"7d0c4cc444f9ae0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659291287000,"stop":1659291287000,"duration":0}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Creation Should update Project while applying valid manifest with existing Project name":{"statistic":{"failed":5,"broken":0,"skipped":0,"passed":84,"unknown":0,"total":89},"items":[{"uid":"a869507350f409c6","status":"passed","time":{"start":1660145110000,"stop":1660145114563,"duration":4563}},{"uid":"1fb246f276b1eb51","status":"passed","time":{"start":1660142645000,"stop":1660142649567,"duration":4567}},{"uid":"6ccbd5f9cdfd89c6","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000655578>: {\n        Underlying: <*exec.ExitError | 0xc000515300>{\n            ProcessState: {\n                pid: 6221,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 114978},\n                    Stime: {Sec: 0, Usec: 53383},\n                    Maxrss: 77872,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 9747,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13392,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 341,\n                    Nivcsw: 295,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projecttest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projecttest\\\": already exists\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The Project \\\"projecttest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projecttest\\\": already exists\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projecttest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projecttest\\\": already exists\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The Project \\\"projecttest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projecttest\\\": already exists\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The Project \"projecttest\" is invalid: project namespace: Invalid value: \"kubeslice-projecttest\": already exists\noccurred","time":{"start":1660141834000,"stop":1660141894211,"duration":60211}},{"uid":"66b2b06f65494820","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000350318>: {\n        Underlying: <*exec.ExitError | 0xc00059a3e0>{\n            ProcessState: {\n                pid: 6042,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 148815},\n                    Stime: {Sec: 0, Usec: 27413},\n                    Maxrss: 78180,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 7722,\n                    Majflt: 1,\n                    Nswap: 0,\n                    Inblock: 256,\n                    Oublock: 13376,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 411,\n                    Nivcsw: 242,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projecttest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projecttest\\\": already exists\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The Project \\\"projecttest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projecttest\\\": already exists\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projecttest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projecttest\\\": already exists\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The Project \\\"projecttest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projecttest\\\": already exists\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The Project \"projecttest\" is invalid: project namespace: Invalid value: \"kubeslice-projecttest\": already exists\noccurred","time":{"start":1660138561000,"stop":1660138621227,"duration":60227}},{"uid":"249a4e3537ef619e","status":"passed","time":{"start":1660116744000,"stop":1660116748644,"duration":4644}},{"uid":"63caeddc1acbae7d","status":"passed","time":{"start":1660116144000,"stop":1660116148509,"duration":4509}},{"uid":"8002ad3217139c8c","status":"passed","time":{"start":1660112860000,"stop":1660112864559,"duration":4559}},{"uid":"499834f6e18118ad","status":"passed","time":{"start":1660108970000,"stop":1660108974561,"duration":4561}},{"uid":"2f201fce2987891f","status":"passed","time":{"start":1660104124000,"stop":1660104128594,"duration":4594}},{"uid":"1924dcad5906e2fb","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000353a58>: {\n        Underlying: <*exec.ExitError | 0xc0004233a0>{\n            ProcessState: {\n                pid: 6117,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 116564},\n                    Stime: {Sec: 0, Usec: 58282},\n                    Maxrss: 73060,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3586,\n                    Majflt: 1,\n                    Nswap: 0,\n                    Inblock: 128,\n                    Oublock: 13376,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 502,\n                    Nivcsw: 340,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projecttest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projecttest\\\": already exists\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The Project \\\"projecttest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projecttest\\\": already exists\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projecttest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projecttest\\\": already exists\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The Project \\\"projecttest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projecttest\\\": already exists\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The Project \"projecttest\" is invalid: project namespace: Invalid value: \"kubeslice-projecttest\": already exists\noccurred","time":{"start":1660104760000,"stop":1660104820255,"duration":60255}},{"uid":"a9386dc2dc6b1521","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00060fb30>: {\n        Underlying: <*exec.ExitError | 0xc00061a140>{\n            ProcessState: {\n                pid: 6245,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 131451},\n                    Stime: {Sec: 0, Usec: 27196},\n                    Maxrss: 66220,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3941,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13392,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 371,\n                    Nivcsw: 324,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projecttest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projecttest\\\": already exists\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The Project \\\"projecttest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projecttest\\\": already exists\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projecttest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projecttest\\\": already exists\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The Project \\\"projecttest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projecttest\\\": already exists\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The Project \"projecttest\" is invalid: project namespace: Invalid value: \"kubeslice-projecttest\": already exists\noccurred","time":{"start":1660101116000,"stop":1660101176242,"duration":60242}},{"uid":"38e8f0beb79d9a69","status":"passed","time":{"start":1660100168000,"stop":1660100172721,"duration":4721}},{"uid":"12245324deda3d49","status":"passed","time":{"start":1660066476000,"stop":1660066480489,"duration":4489}},{"uid":"2ea00993ff6f7745","status":"passed","time":{"start":1660064121000,"stop":1660064125540,"duration":4540}},{"uid":"e036f1fae258ef43","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0005f8018>: {\n        Underlying: <*exec.ExitError | 0xc000424040>{\n            ProcessState: {\n                pid: 6001,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 143873},\n                    Stime: {Sec: 0, Usec: 49327},\n                    Maxrss: 75196,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4591,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13376,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 330,\n                    Nivcsw: 387,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projecttest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projecttest\\\": already exists\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The Project \\\"projecttest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projecttest\\\": already exists\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projecttest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projecttest\\\": already exists\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The Project \\\"projecttest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projecttest\\\": already exists\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The Project \"projecttest\" is invalid: project namespace: Invalid value: \"kubeslice-projecttest\": already exists\noccurred","time":{"start":1660060680000,"stop":1660060740267,"duration":60267}},{"uid":"351e903e49ef4949","status":"passed","time":{"start":1660052169000,"stop":1660052173637,"duration":4637}},{"uid":"50d536e479dafee2","status":"passed","time":{"start":1660050481000,"stop":1660050485643,"duration":4643}},{"uid":"a1b6a025de3eb2c6","status":"passed","time":{"start":1660046776000,"stop":1660046780521,"duration":4521}},{"uid":"8d3dabd85d7f6328","status":"passed","time":{"start":1660046048000,"stop":1660046052486,"duration":4486}},{"uid":"9f3a23a66e6aa9a7","status":"passed","time":{"start":1660044532000,"stop":1660044536550,"duration":4550}}]},"Worker Suite:Worker Suite#[BeforeSuite]":{"statistic":{"failed":65,"broken":0,"skipped":0,"passed":35,"unknown":0,"total":100},"items":[{"uid":"55dee8d15666eaf","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000350030>: {\n        Underlying: <*exec.ExitError | 0xc0004d6000>{\n            ProcessState: {\n                pid: 6030,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 1, Usec: 918339},\n                    Stime: {Sec: 0, Usec: 265382},\n                    Maxrss: 80252,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 5278,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12856,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 22164,\n                    Nivcsw: 6010,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-0.4.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 39 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 39 resources with timeout of 2m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-alertmanager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-alertmanager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-0.4.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 39 resource(s)\n    wait.go:48: [debug] beginning wait for 39 resources with timeout of 2m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-alertmanager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-alertmanager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-controller\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-alertmanager\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-manager-metrics-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-prometheus-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-prometheus\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-alertmanager\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-kube-state-metrics\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-controller-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-prometheus\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-kube-state-metrics\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-editor-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-prometheus\" ClusterRole\n    client.go:310: [debug] Starting delete for \"sliceconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"sliceqosconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"sliceresourcequotaconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workerserviceimports.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workersliceconfigs.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workerslicegateways.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workersliceresourcequotas.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"clusters.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"projects.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"serviceexportconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-prometheus-server-conf\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-alertmanager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-serving-cert\" Certificate\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-selfsigned-issuer\" Issuer\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-validating-webhook-configuration\" ValidatingWebhookConfiguration\n    uninstall.go:144: [debug] purge requested for kubeslice-controller\n    Error: INSTALLATION FAILED: release kubeslice-controller failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-controller failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1660153688000,"stop":1660153891563,"duration":203563}},{"uid":"6b039605b6bc1494","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0001ca048>: {\n        Underlying: <*exec.ExitError | 0xc00049c000>{\n            ProcessState: {\n                pid: 6032,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 2, Usec: 162850},\n                    Stime: {Sec: 0, Usec: 258873},\n                    Maxrss: 80864,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 5066,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12856,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 24151,\n                    Nivcsw: 6226,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-0.4.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 39 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 39 resources with timeout of 2m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-alertmanager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-0.4.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 39 resource(s)\n    wait.go:48: [debug] beginning wait for 39 resources with timeout of 2m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-alertmanager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-controller\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-manager-metrics-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-alertmanager\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-prometheus-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-prometheus\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-alertmanager\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-kube-state-metrics\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-controller-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-prometheus\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-kube-state-metrics\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-editor-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-prometheus\" ClusterRole\n    client.go:310: [debug] Starting delete for \"sliceconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workersliceconfigs.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"sliceqosconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workerslicegateways.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workersliceresourcequotas.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"sliceresourcequotaconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"clusters.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"projects.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"serviceexportconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workerserviceimports.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-prometheus-server-conf\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-alertmanager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-serving-cert\" Certificate\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-selfsigned-issuer\" Issuer\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-validating-webhook-configuration\" ValidatingWebhookConfiguration\n    uninstall.go:144: [debug] purge requested for kubeslice-controller\n    Error: INSTALLATION FAILED: release kubeslice-controller failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-controller failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1660152164000,"stop":1660152367495,"duration":203495}},{"uid":"da6db45bf4335fdf","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004d9038>: {\n        Underlying: <*exec.ExitError | 0xc0007733e0>{\n            ProcessState: {\n                pid: 6412,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 1, Usec: 177155},\n                    Stime: {Sec: 0, Usec: 164801},\n                    Maxrss: 87336,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 12536,\n                    Majflt: 1,\n                    Nswap: 0,\n                    Inblock: 256,\n                    Oublock: 15224,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 2827,\n                    Nivcsw: 2043,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.4.1.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD serviceexports.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD serviceimports.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD slice.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD slicegateways.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD sliceresourcequotas.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"Error: INSTALLATION FAILED: create: failed to create: secrets \\\"sh.helm.release.v1.kubeslice-worker.v1\\\" is forbidden: unable to create new content in namespace kubeslice-system because it is being terminated\",\n                        \"helm.go:84: [debug] secrets \\\"sh.helm.release.v1.kubeslice-worker.v1\\\" is forbidden: unable to create new content in namespace kubeslice-system because it is being terminated\",\n                        \"create: failed to create\",\n                        \"helm.sh/helm/v3/pkg/storage/driver.(*Secrets).Create\",\n                        \"\\thelm.sh/helm/v3/pkg/storage/driver/secrets.go:164\",\n                        \"helm.sh/helm/v3/pkg/storage.(*Storage).Create\",\n                        \"\\thelm.sh/helm/v3/pkg/storage/storage.go:69\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:340\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:264\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:125\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n         ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.4.1.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD serviceexports.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD serviceimports.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD slice.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD slicegateways.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD sliceresourcequotas.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    Error: INSTALLATION FAILED: create: failed to create: secrets \"sh.helm.release.v1.kubeslice-worker.v1\" is forbidden: unable to create new content in namespace kubeslice-system because it is being terminated\n    helm.go:84: [debug] secrets \"sh.helm.release.v1.kubeslice-worker.v1\" is forbidden: unable to create new content in namespace kubeslice-system because it is being terminated\n    create: failed to create\n    helm.sh/helm/v3/pkg/storage/driver.(*Secrets).Create\n    \thelm.sh/helm/v3/pkg/storage/driver/secrets.go:164\n    helm.sh/helm/v3/pkg/storage.(*Storage).Create\n    \thelm.sh/helm/v3/pkg/storage/storage.go:69\n    helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\n    \thelm.sh/helm/v3/pkg/action/install.go:340\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:264\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:125\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1660146582000,"stop":1660146698382,"duration":116382}},{"uid":"41a6bf182bc11da3","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000463c80>: {\n        Underlying: <*exec.ExitError | 0xc0004071e0>{\n            ProcessState: {\n                pid: 6436,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 1, Usec: 506936},\n                    Stime: {Sec: 0, Usec: 138102},\n                    Maxrss: 83772,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 13100,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 15224,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 3234,\n                    Nivcsw: 2147,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.4.1.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD serviceexports.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD serviceimports.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD slice.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD slicegateways.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD sliceresourcequotas.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"Error: INSTALLATION FAILED: create: failed to create: secrets \\\"sh.helm.release.v1.kubeslice-worker.v1\\\" is forbidden: unable to create new content in namespace kubeslice-system because it is being terminated\",\n                        \"helm.go:84: [debug] secrets \\\"sh.helm.release.v1.kubeslice-worker.v1\\\" is forbidden: unable to create new content in namespace kubeslice-system because it is being terminated\",\n                        \"create: failed to create\",\n                        \"helm.sh/helm/v3/pkg/storage/driver.(*Secrets).Create\",\n                        \"\\thelm.sh/helm/v3/pkg/storage/driver/secrets.go:164\",\n                        \"helm.sh/helm/v3/pkg/storage.(*Storage).Create\",\n                        \"\\thelm.sh/helm/v3/pkg/storage/storage.go:69\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:340\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:264\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:125\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\"...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.4.1.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD serviceexports.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD serviceimports.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD slice.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD slicegateways.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD sliceresourcequotas.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    Error: INSTALLATION FAILED: create: failed to create: secrets \"sh.helm.release.v1.kubeslice-worker.v1\" is forbidden: unable to create new content in namespace kubeslice-system because it is being terminated\n    helm.go:84: [debug] secrets \"sh.helm.release.v1.kubeslice-worker.v1\" is forbidden: unable to create new content in namespace kubeslice-system because it is being terminated\n    create: failed to create\n    helm.sh/helm/v3/pkg/storage/driver.(*Secrets).Create\n    \thelm.sh/helm/v3/pkg/storage/driver/secrets.go:164\n    helm.sh/helm/v3/pkg/storage.(*Storage).Create\n    \thelm.sh/helm/v3/pkg/storage/storage.go:69\n    helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\n    \thelm.sh/helm/v3/pkg/action/install.go:340\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:264\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:125\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1660143888000,"stop":1660144007332,"duration":119332}},{"uid":"2db48b69fb336e6b","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0003f80f0>: {\n        Underlying: <*exec.ExitError | 0xc00089a040>{\n            ProcessState: {\n                pid: 6391,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 1, Usec: 635454},\n                    Stime: {Sec: 0, Usec: 167126},\n                    Maxrss: 85396,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 11751,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 15224,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 3103,\n                    Nivcsw: 1841,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.4.1.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD serviceexports.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD serviceimports.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD slice.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD slicegateways.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD sliceresourcequotas.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"Error: INSTALLATION FAILED: create: failed to create: secrets \\\"sh.helm.release.v1.kubeslice-worker.v1\\\" is forbidden: unable to create new content in namespace kubeslice-system because it is being terminated\",\n                        \"helm.go:84: [debug] secrets \\\"sh.helm.release.v1.kubeslice-worker.v1\\\" is forbidden: unable to create new content in namespace kubeslice-system because it is being terminated\",\n                        \"create: failed to create\",\n                        \"helm.sh/helm/v3/pkg/storage/driver.(*Secrets).Create\",\n                        \"\\thelm.sh/helm/v3/pkg/storage/driver/secrets.go:164\",\n                        \"helm.sh/helm/v3/pkg/storage.(*Storage).Create\",\n                        \"\\thelm.sh/helm/v3/pkg/storage/storage.go:69\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:340\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:264\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:125\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\"...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.4.1.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD serviceexports.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD serviceimports.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD slice.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD slicegateways.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD sliceresourcequotas.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    Error: INSTALLATION FAILED: create: failed to create: secrets \"sh.helm.release.v1.kubeslice-worker.v1\" is forbidden: unable to create new content in namespace kubeslice-system because it is being terminated\n    helm.go:84: [debug] secrets \"sh.helm.release.v1.kubeslice-worker.v1\" is forbidden: unable to create new content in namespace kubeslice-system because it is being terminated\n    create: failed to create\n    helm.sh/helm/v3/pkg/storage/driver.(*Secrets).Create\n    \thelm.sh/helm/v3/pkg/storage/driver/secrets.go:164\n    helm.sh/helm/v3/pkg/storage.(*Storage).Create\n    \thelm.sh/helm/v3/pkg/storage/storage.go:69\n    helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\n    \thelm.sh/helm/v3/pkg/action/install.go:340\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:264\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:125\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1660143654000,"stop":1660143767971,"duration":113971}},{"uid":"46c8e42a43003597","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0005160a8>: {\n        Underlying: <*exec.ExitError | 0xc00088a060>{\n            ProcessState: {\n                pid: 6387,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 1, Usec: 334564},\n                    Stime: {Sec: 0, Usec: 144612},\n                    Maxrss: 87036,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 15334,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 24,\n                    Oublock: 15224,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 2832,\n                    Nivcsw: 1603,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.4.1.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD serviceexports.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD serviceimports.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD slice.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD slicegateways.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD sliceresourcequotas.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"Error: INSTALLATION FAILED: create: failed to create: secrets \\\"sh.helm.release.v1.kubeslice-worker.v1\\\" is forbidden: unable to create new content in namespace kubeslice-system because it is being terminated\",\n                        \"helm.go:84: [debug] secrets \\\"sh.helm.release.v1.kubeslice-worker.v1\\\" is forbidden: unable to create new content in namespace kubeslice-system because it is being terminated\",\n                        \"create: failed to create\",\n                        \"helm.sh/helm/v3/pkg/storage/driver.(*Secrets).Create\",\n                        \"\\thelm.sh/helm/v3/pkg/storage/driver/secrets.go:164\",\n                        \"helm.sh/helm/v3/pkg/storage.(*Storage).Create\",\n                        \"\\thelm.sh/helm/v3/pkg/storage/storage.go:69\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:340\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:264\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:125\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.4.1.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD serviceexports.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD serviceimports.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD slice.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD slicegateways.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD sliceresourcequotas.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    Error: INSTALLATION FAILED: create: failed to create: secrets \"sh.helm.release.v1.kubeslice-worker.v1\" is forbidden: unable to create new content in namespace kubeslice-system because it is being terminated\n    helm.go:84: [debug] secrets \"sh.helm.release.v1.kubeslice-worker.v1\" is forbidden: unable to create new content in namespace kubeslice-system because it is being terminated\n    create: failed to create\n    helm.sh/helm/v3/pkg/storage/driver.(*Secrets).Create\n    \thelm.sh/helm/v3/pkg/storage/driver/secrets.go:164\n    helm.sh/helm/v3/pkg/storage.(*Storage).Create\n    \thelm.sh/helm/v3/pkg/storage/storage.go:69\n    helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\n    \thelm.sh/helm/v3/pkg/action/install.go:340\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:264\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:125\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1660140253000,"stop":1660140382402,"duration":129402}},{"uid":"3d04c6087a274939","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0003400a8>: {\n        Underlying: <*exec.ExitError | 0xc000388000>{\n            ProcessState: {\n                pid: 6018,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 1, Usec: 860638},\n                    Stime: {Sec: 0, Usec: 257367},\n                    Maxrss: 83500,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 6095,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12856,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 22312,\n                    Nivcsw: 5872,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-0.4.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 39 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 39 resources with timeout of 2m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-alertmanager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                   ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-0.4.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 39 resource(s)\n    wait.go:48: [debug] beginning wait for 39 resources with timeout of 2m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-alertmanager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-controller\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-alertmanager\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-manager-metrics-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-prometheus-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-prometheus\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-alertmanager\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-kube-state-metrics\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-controller-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-prometheus\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-kube-state-metrics\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-editor-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-prometheus\" ClusterRole\n    client.go:310: [debug] Starting delete for \"sliceconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"sliceqosconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"sliceresourcequotaconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workerserviceimports.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workersliceconfigs.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workerslicegateways.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workersliceresourcequotas.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"clusters.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"projects.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"serviceexportconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-prometheus-server-conf\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-alertmanager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-serving-cert\" Certificate\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-selfsigned-issuer\" Issuer\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-validating-webhook-configuration\" ValidatingWebhookConfiguration\n    uninstall.go:144: [debug] purge requested for kubeslice-controller\n    Error: INSTALLATION FAILED: release kubeslice-controller failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-controller failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1660137949000,"stop":1660138156620,"duration":207620}},{"uid":"e0799aeba6cb762b","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000352030>: {\n        Underlying: <*exec.ExitError | 0xc000424000>{\n            ProcessState: {\n                pid: 6004,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 2, Usec: 409846},\n                    Stime: {Sec: 0, Usec: 362101},\n                    Maxrss: 80800,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 8279,\n                    Majflt: 2,\n                    Nswap: 0,\n                    Inblock: 432,\n                    Oublock: 12848,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 25701,\n                    Nivcsw: 6297,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-0.4.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 39 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 39 resources with timeout of 2m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-alertmanager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                 ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-0.4.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 39 resource(s)\n    wait.go:48: [debug] beginning wait for 39 resources with timeout of 2m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-alertmanager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-controller\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-alertmanager\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-manager-metrics-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-prometheus-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-prometheus\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-alertmanager\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-kube-state-metrics\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-controller-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-prometheus\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-kube-state-metrics\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-editor-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-prometheus\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"sliceconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workersliceconfigs.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"sliceqosconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"sliceresourcequotaconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workerserviceimports.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"clusters.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workerslicegateways.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workersliceresourcequotas.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"projects.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"serviceexportconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-prometheus-server-conf\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-alertmanager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-serving-cert\" Certificate\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-selfsigned-issuer\" Issuer\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-validating-webhook-configuration\" ValidatingWebhookConfiguration\n    uninstall.go:144: [debug] purge requested for kubeslice-controller\n    Error: INSTALLATION FAILED: release kubeslice-controller failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-controller failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1660129303000,"stop":1660129507314,"duration":204314}},{"uid":"d17cc7599a3c0004","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0005b2018>: {\n        Underlying: <*exec.ExitError | 0xc000464000>{\n            ProcessState: {\n                pid: 6029,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 2, Usec: 125924},\n                    Stime: {Sec: 0, Usec: 232457},\n                    Maxrss: 80476,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 6419,\n                    Majflt: 1,\n                    Nswap: 0,\n                    Inblock: 128,\n                    Oublock: 12848,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 23949,\n                    Nivcsw: 6007,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-0.4.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 39 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 39 resources with timeout of 2m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-alertmanager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                 ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-0.4.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 39 resource(s)\n    wait.go:48: [debug] beginning wait for 39 resources with timeout of 2m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-alertmanager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-controller\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-manager-metrics-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-alertmanager\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-prometheus-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-prometheus\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-alertmanager\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-kube-state-metrics\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-controller-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-prometheus\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-kube-state-metrics\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-editor-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-prometheus\" ClusterRole\n    client.go:310: [debug] Starting delete for \"sliceconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"sliceqosconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"sliceresourcequotaconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workerserviceimports.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workersliceconfigs.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workerslicegateways.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workersliceresourcequotas.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"clusters.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"projects.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"serviceexportconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-prometheus-server-conf\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-alertmanager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-serving-cert\" Certificate\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-selfsigned-issuer\" Issuer\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-validating-webhook-configuration\" ValidatingWebhookConfiguration\n    uninstall.go:144: [debug] purge requested for kubeslice-controller\n    Error: INSTALLATION FAILED: release kubeslice-controller failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-controller failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1660128018000,"stop":1660128217316,"duration":199316}},{"uid":"b8ca655982521e15","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0000cc060>: {\n        Underlying: <*exec.ExitError | 0xc0001c8000>{\n            ProcessState: {\n                pid: 6002,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 2, Usec: 534883},\n                    Stime: {Sec: 0, Usec: 407092},\n                    Maxrss: 85048,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 6352,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12848,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 27811,\n                    Nivcsw: 7363,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-0.4.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 39 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 39 resources with timeout of 2m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-alertmanager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-alertmanager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n              ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-0.4.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 39 resource(s)\n    wait.go:48: [debug] beginning wait for 39 resources with timeout of 2m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-alertmanager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-alertmanager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-controller\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-alertmanager\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-manager-metrics-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-prometheus-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-prometheus\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-alertmanager\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-kube-state-metrics\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-controller-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-prometheus\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-kube-state-metrics\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-editor-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-prometheus\" ClusterRole\n    client.go:310: [debug] Starting delete for \"sliceconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"sliceqosconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"sliceresourcequotaconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workerserviceimports.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workersliceconfigs.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workerslicegateways.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workersliceresourcequotas.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"clusters.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"projects.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"serviceexportconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-prometheus-server-conf\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-alertmanager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-serving-cert\" Certificate\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-selfsigned-issuer\" Issuer\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-validating-webhook-configuration\" ValidatingWebhookConfiguration\n    uninstall.go:144: [debug] purge requested for kubeslice-controller\n    Error: INSTALLATION FAILED: release kubeslice-controller failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-controller failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1660127085000,"stop":1660127295407,"duration":210407}},{"uid":"a66d6c6a48995f17","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0003a2030>: {\n        Underlying: <*exec.ExitError | 0xc00065e000>{\n            ProcessState: {\n                pid: 6738,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 2, Usec: 609576},\n                    Stime: {Sec: 0, Usec: 363034},\n                    Maxrss: 85120,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 6779,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14168,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 23250,\n                    Nivcsw: 5897,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.2.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD serviceexports.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD serviceimports.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD slice.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD slicegateways.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 44 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 44 resources with timeout of 2m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.2.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD serviceexports.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD serviceimports.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD slice.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD slicegateways.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 44 resource(s)\n    wait.go:48: [debug] beginning wait for 44 resources with timeout of 2m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:310: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"jaeger\" Service\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-svc\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:310: [debug] Starting delete for \"jaeger\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook\" Deployment\n    client.go:310: [debug] Starting delete for \"prefix-service\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsm-kernel-forwarder\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:310: [debug] Starting delete for \"networkservices.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkserviceendpoints.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservicemanagers.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:310: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-cfg\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    uninstall.go:144: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1660117538000,"stop":1660117787060,"duration":249060}},{"uid":"872d77eaab752a95","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0006b6018>: {\n        Underlying: <*exec.ExitError | 0xc000494000>{\n            ProcessState: {\n                pid: 6722,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 2, Usec: 455063},\n                    Stime: {Sec: 0, Usec: 244149},\n                    Maxrss: 82520,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 7915,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14168,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 21022,\n                    Nivcsw: 4370,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.2.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD serviceexports.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD serviceimports.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD slice.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD slicegateways.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 44 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 44 resources with timeout of 2m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.2.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD serviceexports.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD serviceimports.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD slice.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD slicegateways.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 44 resource(s)\n    wait.go:48: [debug] beginning wait for 44 resources with timeout of 2m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:310: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-svc\" Service\n    client.go:310: [debug] Starting delete for \"jaeger\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:310: [debug] Starting delete for \"prefix-service\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook\" Deployment\n    client.go:310: [debug] Starting delete for \"jaeger\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsm-kernel-forwarder\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"networkservices.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkserviceendpoints.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservicemanagers.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-cfg\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    uninstall.go:144: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1660116889000,"stop":1660117120313,"duration":231313}},{"uid":"21f8ecfa649438aa","status":"passed","time":{"start":1660113628000,"stop":1660113778778,"duration":150778}},{"uid":"8c48a0727f6c0c5a","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0002140c0>: {\n        Underlying: <*exec.ExitError | 0xc00020e020>{\n            ProcessState: {\n                pid: 8919,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 140382},\n                    Stime: {Sec: 0, Usec: 19336},\n                    Maxrss: 42408,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4995,\n                    Majflt: 1,\n                    Nswap: 0,\n                    Inblock: 72,\n                    Oublock: 352,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 731,\n                    Nivcsw: 17,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"v1.8.0\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\",\n                        \"\",\n                        \"Error: INSTALLATION FAILED: Kubernetes cluster unreachable: Get \\\"https://172.18.0.2:6443/version\\\": x509: certificate is valid for 10.96.0.1, 172.18.0.3, 127.0.0.1, not 172.18.0.2\",\n                        \"helm.go:84: [debug] Get \\\"https://172.18.0.2:6443/version\\\": x509: certificate is valid for 10.96.0.1, 172.18.0.3, 127.0.0.1, not 172.18.0.2\",\n                        \"Kubernetes cluster unreachable\",\n                        \"helm.sh/helm/v3/pkg/kube.(*Client).IsReachable\",\n                        \"\\thelm.sh/helm/v3/pkg/kube/client.go:121\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:192\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:264\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:125\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                        \"INSTALLATION FAILED\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:127\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n          ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"v1.8.0\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\n    \n    Error: INSTALLATION FAILED: Kubernetes cluster unreachable: Get \"https://172.18.0.2:6443/version\": x509: certificate is valid for 10.96.0.1, 172.18.0.3, 127.0.0.1, not 172.18.0.2\n    helm.go:84: [debug] Get \"https://172.18.0.2:6443/version\": x509: certificate is valid for 10.96.0.1, 172.18.0.3, 127.0.0.1, not 172.18.0.2\n    Kubernetes cluster unreachable\n    helm.sh/helm/v3/pkg/kube.(*Client).IsReachable\n    \thelm.sh/helm/v3/pkg/kube/client.go:121\n    helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\n    \thelm.sh/helm/v3/pkg/action/install.go:192\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:264\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:125\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1660114668000,"stop":1660114668274,"duration":274}},{"uid":"ea69b456203614c9","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1660109721000,"stop":1660109936666,"duration":215666}},{"uid":"d6867cf74e00f864","status":"passed","time":{"start":1660104898000,"stop":1660105077844,"duration":179844}},{"uid":"c2b16fc92d65d0ec","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00052e018>: {\n        Underlying: <*exec.ExitError | 0xc000492000>{\n            ProcessState: {\n                pid: 6408,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 1, Usec: 774969},\n                    Stime: {Sec: 0, Usec: 213803},\n                    Maxrss: 88340,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 14407,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 15216,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 2777,\n                    Nivcsw: 2207,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.4.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD serviceexports.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD serviceimports.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD slice.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD slicegateways.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD sliceresourcequotas.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"Error: INSTALLATION FAILED: create: failed to create: secrets \\\"sh.helm.release.v1.kubeslice-worker.v1\\\" is forbidden: unable to create new content in namespace kubeslice-system because it is being terminated\",\n                        \"helm.go:84: [debug] secrets \\\"sh.helm.release.v1.kubeslice-worker.v1\\\" is forbidden: unable to create new content in namespace kubeslice-system because it is being terminated\",\n                        \"create: failed to create\",\n                        \"helm.sh/helm/v3/pkg/storage/driver.(*Secrets).Create\",\n                        \"\\thelm.sh/helm/v3/pkg/storage/driver/secrets.go:164\",\n                        \"helm.sh/helm/v3/pkg/storage.(*Storage).Create\",\n                        \"\\thelm.sh/helm/v3/pkg/storage/storage.go:69\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:340\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:264\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:125\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\"...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.4.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD serviceexports.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD serviceimports.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD slice.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD slicegateways.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD sliceresourcequotas.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    Error: INSTALLATION FAILED: create: failed to create: secrets \"sh.helm.release.v1.kubeslice-worker.v1\" is forbidden: unable to create new content in namespace kubeslice-system because it is being terminated\n    helm.go:84: [debug] secrets \"sh.helm.release.v1.kubeslice-worker.v1\" is forbidden: unable to create new content in namespace kubeslice-system because it is being terminated\n    create: failed to create\n    helm.sh/helm/v3/pkg/storage/driver.(*Secrets).Create\n    \thelm.sh/helm/v3/pkg/storage/driver/secrets.go:164\n    helm.sh/helm/v3/pkg/storage.(*Storage).Create\n    \thelm.sh/helm/v3/pkg/storage/storage.go:69\n    helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\n    \thelm.sh/helm/v3/pkg/action/install.go:340\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:264\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:125\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1660106395000,"stop":1660106505307,"duration":110307}},{"uid":"3a8207bc1d7d7975","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000724c90>: {\n        Underlying: <*exec.ExitError | 0xc000555320>{\n            ProcessState: {\n                pid: 6404,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 1, Usec: 197578},\n                    Stime: {Sec: 0, Usec: 99109},\n                    Maxrss: 85476,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 10335,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 15216,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 5224,\n                    Nivcsw: 1793,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.4.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD serviceexports.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD serviceimports.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD slice.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD slicegateways.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD sliceresourcequotas.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"Error: INSTALLATION FAILED: create: failed to create: secrets \\\"sh.helm.release.v1.kubeslice-worker.v1\\\" is forbidden: unable to create new content in namespace kubeslice-system because it is being terminated\",\n                        \"helm.go:84: [debug] secrets \\\"sh.helm.release.v1.kubeslice-worker.v1\\\" is forbidden: unable to create new content in namespace kubeslice-system because it is being terminated\",\n                        \"create: failed to create\",\n                        \"helm.sh/helm/v3/pkg/storage/driver.(*Secrets).Create\",\n                        \"\\thelm.sh/helm/v3/pkg/storage/driver/secrets.go:164\",\n                        \"helm.sh/helm/v3/pkg/storage.(*Storage).Create\",\n                        \"\\thelm.sh/helm/v3/pkg/storage/storage.go:69\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:340\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:264\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:125\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.4.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD serviceexports.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD serviceimports.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD slice.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD slicegateways.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD sliceresourcequotas.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    Error: INSTALLATION FAILED: create: failed to create: secrets \"sh.helm.release.v1.kubeslice-worker.v1\" is forbidden: unable to create new content in namespace kubeslice-system because it is being terminated\n    helm.go:84: [debug] secrets \"sh.helm.release.v1.kubeslice-worker.v1\" is forbidden: unable to create new content in namespace kubeslice-system because it is being terminated\n    create: failed to create\n    helm.sh/helm/v3/pkg/storage/driver.(*Secrets).Create\n    \thelm.sh/helm/v3/pkg/storage/driver/secrets.go:164\n    helm.sh/helm/v3/pkg/storage.(*Storage).Create\n    \thelm.sh/helm/v3/pkg/storage/storage.go:69\n    helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\n    \thelm.sh/helm/v3/pkg/action/install.go:340\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:264\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:125\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1660102709000,"stop":1660102815135,"duration":106135}},{"uid":"d5a8b4dab4bcc16d","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000490150>: {\n        Underlying: <*exec.ExitError | 0xc000494000>{\n            ProcessState: {\n                pid: 6753,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 3, Usec: 87021},\n                    Stime: {Sec: 0, Usec: 441576},\n                    Maxrss: 84976,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 5274,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14168,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 22538,\n                    Nivcsw: 5616,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.2.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD serviceexports.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD serviceimports.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD slice.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD slicegateways.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 44 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 44 resources with timeout of 2m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug]...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.2.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD serviceexports.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD serviceimports.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD slice.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD slicegateways.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 44 resource(s)\n    wait.go:48: [debug] beginning wait for 44 resources with timeout of 2m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:310: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-svc\" Service\n    client.go:310: [debug] Starting delete for \"jaeger\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:310: [debug] Starting delete for \"jaeger\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook\" Deployment\n    client.go:310: [debug] Starting delete for \"prefix-service\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsm-kernel-forwarder\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"networkservices.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkserviceendpoints.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservicemanagers.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-cfg\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    uninstall.go:144: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1660100787000,"stop":1660101022532,"duration":235532}},{"uid":"c8b17c1f9d23d406","status":"passed","time":{"start":1660067259000,"stop":1660067444707,"duration":185707}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting iperf-server pod Check ping between iperf-server and iperf-client after iperf-server pod restart":{"statistic":{"failed":0,"broken":0,"skipped":35,"passed":0,"unknown":0,"total":35},"items":[{"uid":"73958c213a534876","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"269d2763f0b7651a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"8e3191ea549b4b71","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"93d3c2492084715e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}},{"uid":"29077ae9cb07db6a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659982549000,"stop":1659982549000,"duration":0}},{"uid":"1c72f0991f86e2c6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659970182000,"stop":1659970182000,"duration":0}},{"uid":"4c036a8e62cd942a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659944495000,"stop":1659944495000,"duration":0}},{"uid":"724038d5d23dcff9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659874977000,"stop":1659874977000,"duration":0}},{"uid":"2db80c5c3f6afdea","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496050000,"stop":1659496050000,"duration":0}},{"uid":"ad8b2f4fc2b3c448","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496083000,"stop":1659496083000,"duration":0}},{"uid":"a49ddc57c488716","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"c36dfcc3b13bdcf5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659450916000,"stop":1659450916000,"duration":0}},{"uid":"14c7f9dcf55459a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"6afa278afecbd7b6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447298000,"stop":1659447298000,"duration":0}},{"uid":"970e5a37e53c1627","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447116000,"stop":1659447116000,"duration":0}},{"uid":"c668d1307ce5f731","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448205000,"stop":1659448205000,"duration":0}},{"uid":"e45f45b065fd49c0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448097000,"stop":1659448097000,"duration":0}},{"uid":"5ba97a6f624866e6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659409279000,"stop":1659409279000,"duration":0}},{"uid":"8b4cb727c81fc735","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659402555000,"stop":1659402555000,"duration":0}},{"uid":"54f067c3d8fd7010","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659291287000,"stop":1659291287000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice creation scenarios: when slice red is installed workers should have gateway pod running":{"statistic":{"failed":0,"broken":0,"skipped":23,"passed":12,"unknown":0,"total":35},"items":[{"uid":"ce4a08a1314c8cc5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"e04f21649a41ac9b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"49fd7587906d710a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"a867e929ed7b9bcf","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}},{"uid":"c68e7a949a0f8139","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659982549000,"stop":1659982549000,"duration":0}},{"uid":"2ecd29f09aa98c5f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659970182000,"stop":1659970182000,"duration":0}},{"uid":"fb6832e1e6e29a12","status":"passed","time":{"start":1659944495000,"stop":1659944495030,"duration":30}},{"uid":"151367ac16e08ad6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659874977000,"stop":1659874977000,"duration":0}},{"uid":"ad42895f0f513d94","status":"passed","time":{"start":1659496050000,"stop":1659496080133,"duration":30133}},{"uid":"dc15a74958854782","status":"passed","time":{"start":1659496083000,"stop":1659496103112,"duration":20112}},{"uid":"20632b5e0ae6afae","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"cf1d01eb267cd56","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659450916000,"stop":1659450916000,"duration":0}},{"uid":"ef12351cf493beb4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"e4eec20157befe4d","status":"passed","time":{"start":1659447298000,"stop":1659447366237,"duration":68237}},{"uid":"99724be27d73d2c9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447116000,"stop":1659447116000,"duration":0}},{"uid":"55814213c1d3f90f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448205000,"stop":1659448205000,"duration":0}},{"uid":"537574a258530655","status":"passed","time":{"start":1659448097000,"stop":1659448119144,"duration":22144}},{"uid":"407f4bc967554b50","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659409279000,"stop":1659409279000,"duration":0}},{"uid":"e658daee5b973ad7","status":"passed","time":{"start":1659402555000,"stop":1659402619309,"duration":64309}},{"uid":"7224f10287cfb41c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659291287000,"stop":1659291287000,"duration":0}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Update Should update successfully when Project Applied while removing a service account name in Read users":{"statistic":{"failed":7,"broken":0,"skipped":0,"passed":79,"unknown":0,"total":86},"items":[{"uid":"100fe7088c53f7b6","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000e258>: {\n        Underlying: <*exec.ExitError | 0xc00062bc40>{\n            ProcessState: {\n                pid: 6212,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 148586},\n                    Stime: {Sec: 0, Usec: 33019},\n                    Maxrss: 78848,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4060,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13376,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 428,\n                    Nivcsw: 299,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The Project \"projectupdatetest\" is invalid: project namespace: Invalid value: \"kubeslice-projectupdatetest\": already exists\noccurred","time":{"start":1660145110000,"stop":1660145170230,"duration":60230}},{"uid":"8e9c87b9967caa39","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0001e8180>: {\n        Underlying: <*exec.ExitError | 0xc000627240>{\n            ProcessState: {\n                pid: 6317,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 143117},\n                    Stime: {Sec: 0, Usec: 60259},\n                    Maxrss: 67092,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 8865,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13392,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 386,\n                    Nivcsw: 289,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The Project \"projectupdatetest\" is invalid: project namespace: Invalid value: \"kubeslice-projectupdatetest\": already exists\noccurred","time":{"start":1660142645000,"stop":1660142705274,"duration":60274}},{"uid":"6f6bb6de63888f9b","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000130870>: {\n        Underlying: <*exec.ExitError | 0xc000651c80>{\n            ProcessState: {\n                pid: 6138,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 112900},\n                    Stime: {Sec: 0, Usec: 67740},\n                    Maxrss: 77084,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 10270,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13392,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 280,\n                    Nivcsw: 305,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The Project \"projectupdatetest\" is invalid: project namespace: Invalid value: \"kubeslice-projectupdatetest\": already exists\noccurred","time":{"start":1660141834000,"stop":1660141894262,"duration":60262}},{"uid":"bbd5f5afa1ac313d","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000741698>: {\n        Underlying: <*exec.ExitError | 0xc0005708c0>{\n            ProcessState: {\n                pid: 6173,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 129582},\n                    Stime: {Sec: 0, Usec: 62939},\n                    Maxrss: 75236,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 8798,\n                    Majflt: 1,\n                    Nswap: 0,\n                    Inblock: 128,\n                    Oublock: 13376,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 409,\n                    Nivcsw: 200,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The Project \"projectupdatetest\" is invalid: project namespace: Invalid value: \"kubeslice-projectupdatetest\": already exists\noccurred","time":{"start":1660138561000,"stop":1660138621240,"duration":60240}},{"uid":"dcd7d7c94458dbc5","status":"passed","time":{"start":1660116744000,"stop":1660116748764,"duration":4764}},{"uid":"f8492d9f89442a76","status":"passed","time":{"start":1660116144000,"stop":1660116149401,"duration":5401}},{"uid":"f8eef12ce38fbb84","status":"passed","time":{"start":1660112860000,"stop":1660112865391,"duration":5391}},{"uid":"676733410662a278","status":"passed","time":{"start":1660108970000,"stop":1660108975489,"duration":5489}},{"uid":"36ddec4952811f41","status":"passed","time":{"start":1660104124000,"stop":1660104128740,"duration":4740}},{"uid":"4d87063f475f4627","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0007d4f18>: {\n        Underlying: <*exec.ExitError | 0xc000512760>{\n            ProcessState: {\n                pid: 6290,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 145295},\n                    Stime: {Sec: 0, Usec: 43195},\n                    Maxrss: 75844,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 7399,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13392,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 292,\n                    Nivcsw: 484,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The Project \"projectupdatetest\" is invalid: project namespace: Invalid value: \"kubeslice-projectupdatetest\": already exists\noccurred","time":{"start":1660104760000,"stop":1660104820270,"duration":60270}},{"uid":"2912f9b5f3897725","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00060f638>: {\n        Underlying: <*exec.ExitError | 0xc00070c340>{\n            ProcessState: {\n                pid: 6121,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 148592},\n                    Stime: {Sec: 0, Usec: 7248},\n                    Maxrss: 77604,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 6225,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13376,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 408,\n                    Nivcsw: 331,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The Project \"projectupdatetest\" is invalid: project namespace: Invalid value: \"kubeslice-projectupdatetest\": already exists\noccurred","time":{"start":1660101116000,"stop":1660101176229,"duration":60229}},{"uid":"1a468ce4d819720a","status":"passed","time":{"start":1660100168000,"stop":1660100172746,"duration":4746}},{"uid":"946c582eb61ea54c","status":"passed","time":{"start":1660066476000,"stop":1660066481373,"duration":5373}},{"uid":"ca0637a4eaac1dce","status":"passed","time":{"start":1660064121000,"stop":1660064126469,"duration":5469}},{"uid":"e98d1bd13a3a07c6","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00060cdb0>: {\n        Underlying: <*exec.ExitError | 0xc0004252e0>{\n            ProcessState: {\n                pid: 6197,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 149465},\n                    Stime: {Sec: 0, Usec: 35399},\n                    Maxrss: 78268,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 8179,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13392,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 378,\n                    Nivcsw: 213,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The Project \"projectupdatetest\" is invalid: project namespace: Invalid value: \"kubeslice-projectupdatetest\": already exists\noccurred","time":{"start":1660060680000,"stop":1660060740234,"duration":60234}},{"uid":"8534321406521820","status":"passed","time":{"start":1660052169000,"stop":1660052173910,"duration":4910}},{"uid":"812ba189b0bef4db","status":"passed","time":{"start":1660050481000,"stop":1660050485640,"duration":4640}},{"uid":"9adf9dfd0bd0ca13","status":"passed","time":{"start":1660046776000,"stop":1660046780487,"duration":4487}},{"uid":"25994320c012772","status":"passed","time":{"start":1660046048000,"stop":1660046052428,"duration":4428}},{"uid":"2df91964e16e1344","status":"passed","time":{"start":1660044532000,"stop":1660044536526,"duration":4526}}]},"Worker Suite:Worker Suite#[It] NodeRestartSlice Testing node restart should re-establish connection on node restart":{"statistic":{"failed":0,"broken":0,"skipped":26,"passed":0,"unknown":0,"total":26},"items":[{"uid":"5c48cb060fe5b172","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"a0e7ad08cc242e71","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"e788046be4165370","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"db48046b039b4aa7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}},{"uid":"6fcbc28b4eaed6e9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659982549000,"stop":1659982549000,"duration":0}},{"uid":"5f2162242f2e886d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659970182000,"stop":1659970182000,"duration":0}},{"uid":"56c9f62414fd4a9d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659944495000,"stop":1659944495000,"duration":0}},{"uid":"201556a2f06ca154","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659874977000,"stop":1659874977000,"duration":0}},{"uid":"c83e4eb5ec7a436e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496050000,"stop":1659496050000,"duration":0}},{"uid":"e0f43405c6aee648","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496083000,"stop":1659496083000,"duration":0}},{"uid":"f6cc0dc15447168","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"206af700116ea313","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659450916000,"stop":1659450916000,"duration":0}},{"uid":"54cf17c19cd1289c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"d742ebbf5a414e6b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447298000,"stop":1659447298000,"duration":0}},{"uid":"81572141d096d69b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447116000,"stop":1659447116000,"duration":0}},{"uid":"4a893ba3b4a339bd","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448205000,"stop":1659448205000,"duration":0}},{"uid":"b668eda383258090","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448097000,"stop":1659448097000,"duration":0}},{"uid":"cfc9fd1217191b04","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659409279000,"stop":1659409279000,"duration":0}},{"uid":"f79aaea744fdbacb","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659402555000,"stop":1659402555000,"duration":0}},{"uid":"b43dd88877709725","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659291287000,"stop":1659291287000,"duration":0}}]},"Hub Suite:Hub Suite#[It]  QosProfile tests QosProfile delete validation Delete qosProfile positive scenario":{"statistic":{"failed":0,"broken":0,"skipped":81,"passed":0,"unknown":0,"total":81},"items":[{"uid":"ffa7ed4a78d4072c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660145110000,"stop":1660145110000,"duration":0}},{"uid":"3c747d463f12f749","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660142645000,"stop":1660142645000,"duration":0}},{"uid":"c96d7351c476fd47","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660141834000,"stop":1660141834000,"duration":0}},{"uid":"29b44f45710f0039","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660138561000,"stop":1660138561000,"duration":0}},{"uid":"e2ca600041fc2baa","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660116744000,"stop":1660116744000,"duration":0}},{"uid":"7ff8f7b44cd9c6f5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660116144000,"stop":1660116144000,"duration":0}},{"uid":"4fc271e75aee6b9e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660112860000,"stop":1660112860000,"duration":0}},{"uid":"7b1dcf7fb1c8163a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660108970000,"stop":1660108970000,"duration":0}},{"uid":"1013980f8764e3e3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104124000,"stop":1660104124000,"duration":0}},{"uid":"4bfdbe5ae8240aa0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104760000,"stop":1660104760000,"duration":0}},{"uid":"4f9b828422bd018d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660101116000,"stop":1660101116000,"duration":0}},{"uid":"73f3cef3036d12b9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660100168000,"stop":1660100168000,"duration":0}},{"uid":"481b29fccabf2b11","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660066476000,"stop":1660066476000,"duration":0}},{"uid":"91fca651749c16f2","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660064121000,"stop":1660064121000,"duration":0}},{"uid":"80fcef50d301135f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660060680000,"stop":1660060680000,"duration":0}},{"uid":"6ebd32af51271575","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660052169000,"stop":1660052169000,"duration":0}},{"uid":"5d299a3865cf25a7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660050481000,"stop":1660050481000,"duration":0}},{"uid":"9111cb05f4f70b9d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660046776000,"stop":1660046776000,"duration":0}},{"uid":"132996b221c9b884","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660046048000,"stop":1660046048000,"duration":0}},{"uid":"eb4979d6529a9949","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660044532000,"stop":1660044532000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Hub netpol scenarios: Slice Configs Netpol scenarios Should fail while Deleting Slice without removing the namespaceIsolationProfile part in slice configs":{"statistic":{"failed":0,"broken":0,"skipped":9,"passed":26,"unknown":0,"total":35},"items":[{"uid":"afe7880f99255487","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"349aee741123efd3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"bda3aabdfcaab334","status":"passed","time":{"start":1660067259000,"stop":1660067259112,"duration":112}},{"uid":"883524a3e0e84518","status":"passed","time":{"start":1660047551000,"stop":1660047551102,"duration":102}},{"uid":"91cbc227a85fc1ca","status":"passed","time":{"start":1659982549000,"stop":1659982549278,"duration":278}},{"uid":"a099f350ef435649","status":"passed","time":{"start":1659970182000,"stop":1659970182092,"duration":92}},{"uid":"d12d26cac3536fb7","status":"passed","time":{"start":1659944495000,"stop":1659944495135,"duration":135}},{"uid":"829e0817fa336760","status":"passed","time":{"start":1659874977000,"stop":1659874977096,"duration":96}},{"uid":"9318b5335d66291e","status":"passed","time":{"start":1659496050000,"stop":1659496050071,"duration":71}},{"uid":"49f06b443176dd2f","status":"passed","time":{"start":1659496083000,"stop":1659496083151,"duration":151}},{"uid":"3c969bc0619c24e7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"50cbbcaec9ec25a0","status":"passed","time":{"start":1659450916000,"stop":1659450916095,"duration":95}},{"uid":"853847a4e0aabcdf","status":"passed","time":{"start":1659451524000,"stop":1659451524099,"duration":99}},{"uid":"226e0b16cc6f34fc","status":"passed","time":{"start":1659447298000,"stop":1659447298085,"duration":85}},{"uid":"2dd95b143d31686","status":"passed","time":{"start":1659447116000,"stop":1659447116103,"duration":103}},{"uid":"85e8aabfa263ec84","status":"passed","time":{"start":1659448205000,"stop":1659448205137,"duration":137}},{"uid":"accd3799b390cc10","status":"passed","time":{"start":1659448097000,"stop":1659448097125,"duration":125}},{"uid":"69f16b5724e18d91","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659409279000,"stop":1659409279000,"duration":0}},{"uid":"7fa1f6098072f2ad","status":"passed","time":{"start":1659402555000,"stop":1659402555116,"duration":116}},{"uid":"76e3f3f9098d57ed","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659291287000,"stop":1659291287000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice deletion tests: Slice is installed & validated Slice object is deleted from hub should remove vl3 router from spoke":{"statistic":{"failed":27,"broken":0,"skipped":0,"passed":8,"unknown":0,"total":35},"items":[{"uid":"f8a6b9a3bda62f3","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 3\nto equal\n    <int>: 1","time":{"start":1660113628000,"stop":1660113688522,"duration":60522}},{"uid":"41c28d0a63c60f32","status":"failed","statusDetails":"Timed out after 120.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1660104898000,"stop":1660105018717,"duration":120717}},{"uid":"faa2b3f81db94314","status":"failed","statusDetails":"Timed out after 120.001s.\nExpected\n    <int>: 1\nto equal\n    <int>: 0","time":{"start":1660067259000,"stop":1660067394447,"duration":135447}},{"uid":"4b6f6352fa75b6fe","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 3\nto equal\n    <int>: 1","time":{"start":1660047551000,"stop":1660047612720,"duration":61720}},{"uid":"363d4a36387567e0","status":"failed","statusDetails":"Timed out after 120.000s.\nExpected\n    <int>: 1\nto equal\n    <int>: 0","time":{"start":1659982549000,"stop":1659982672024,"duration":123024}},{"uid":"df8bf4f9d2daafcb","status":"passed","time":{"start":1659970182000,"stop":1659970200079,"duration":18079}},{"uid":"222f5671504f5d04","status":"failed","statusDetails":"Timed out after 120.000s.\nExpected\n    <int>: 1\nto equal\n    <int>: 0","time":{"start":1659944495000,"stop":1659944649698,"duration":154698}},{"uid":"8839ecdb832b8a01","status":"passed","time":{"start":1659874977000,"stop":1659875010032,"duration":33032}},{"uid":"70b52fade9421f17","status":"failed","statusDetails":"Timed out after 120.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1659496050000,"stop":1659496172442,"duration":122442}},{"uid":"87086431c21db65c","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 3\nto equal\n    <int>: 1","time":{"start":1659496083000,"stop":1659496144892,"duration":61892}},{"uid":"c2fd07ad7e2241b6","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1659451501000,"stop":1659451562996,"duration":61996}},{"uid":"96e73c69b1850357","status":"passed","time":{"start":1659450916000,"stop":1659450994022,"duration":78022}},{"uid":"cfa1c868a9413b1b","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1659451524000,"stop":1659451586160,"duration":62160}},{"uid":"5c0a78000ba61832","status":"passed","time":{"start":1659447298000,"stop":1659447321143,"duration":23143}},{"uid":"cbea9a0d981ad7a3","status":"passed","time":{"start":1659447116000,"stop":1659447162061,"duration":46061}},{"uid":"ba41f21e460a46b2","status":"failed","statusDetails":"Timed out after 120.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1659448205000,"stop":1659448349898,"duration":144898}},{"uid":"9d5764b6e97fa845","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <int>: 2\nto equal\n    <int>: 1","time":{"start":1659448097000,"stop":1659448160069,"duration":63069}},{"uid":"43177be5f1ba41ad","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1659409279000,"stop":1659409341272,"duration":62272}},{"uid":"ef3ff7f25725e0d9","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <int>: 2\nto equal\n    <int>: 1","time":{"start":1659402555000,"stop":1659402619248,"duration":64248}},{"uid":"14fd26b56515b3ea","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 4\nto equal\n    <int>: 1","time":{"start":1659291287000,"stop":1659291348713,"duration":61713}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests slice with netpol should have gateway pod running":{"statistic":{"failed":1,"broken":0,"skipped":23,"passed":11,"unknown":0,"total":35},"items":[{"uid":"d75e2c660ebd79d8","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1660113628000,"stop":1660113822741,"duration":194741}},{"uid":"f9f595d398047778","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"a72d51888be345ba","status":"passed","time":{"start":1660067259000,"stop":1660067259010,"duration":10}},{"uid":"539dd318245b41e1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}},{"uid":"e5a328f0c074de7d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659982549000,"stop":1659982549000,"duration":0}},{"uid":"703a25fd5b578a18","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659970182000,"stop":1659970182000,"duration":0}},{"uid":"e02e754f6da995eb","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659944495000,"stop":1659944495000,"duration":0}},{"uid":"cbff7e3721535a0e","status":"passed","time":{"start":1659874977000,"stop":1659875007124,"duration":30124}},{"uid":"14b3b0e88672c386","status":"passed","time":{"start":1659496050000,"stop":1659496082127,"duration":32127}},{"uid":"5a2dfb615bf7eabe","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496083000,"stop":1659496083000,"duration":0}},{"uid":"8ac39ad2745b4bf9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"58e0d3b34d9cf21e","status":"passed","time":{"start":1659450916000,"stop":1659450972343,"duration":56343}},{"uid":"c069523638914937","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"fa0378ee58601d81","status":"passed","time":{"start":1659447298000,"stop":1659447298014,"duration":14}},{"uid":"e13085f8833772bf","status":"passed","time":{"start":1659447116000,"stop":1659447146148,"duration":30148}},{"uid":"279c9740b6bbce96","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448205000,"stop":1659448205000,"duration":0}},{"uid":"ed56978c37d0026c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448097000,"stop":1659448097000,"duration":0}},{"uid":"42e99ff2c229f028","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659409279000,"stop":1659409279000,"duration":0}},{"uid":"d6d7c5757a76610b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659402555000,"stop":1659402555000,"duration":0}},{"uid":"c9399fe83c358b16","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659291287000,"stop":1659291287000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice Config Negative Scenario tests Slice Config Negative Scenario Update Should fail while updating sliceconfig with with invalid DSCP class in qosprofiledetails":{"statistic":{"failed":1,"broken":0,"skipped":0,"passed":34,"unknown":0,"total":35},"items":[{"uid":"1123c64f65947f2f","status":"passed","time":{"start":1660113628000,"stop":1660113628359,"duration":359}},{"uid":"c2bbf1c2465db96e","status":"passed","time":{"start":1660104898000,"stop":1660104898511,"duration":511}},{"uid":"1817a5b1dca82056","status":"passed","time":{"start":1660067259000,"stop":1660067259309,"duration":309}},{"uid":"23037119b86b1c6f","status":"passed","time":{"start":1660047551000,"stop":1660047552326,"duration":1326}},{"uid":"a3bbdca48e2de4fa","status":"passed","time":{"start":1659982549000,"stop":1659982550447,"duration":1447}},{"uid":"5dae4f57370144ec","status":"passed","time":{"start":1659970182000,"stop":1659970182578,"duration":578}},{"uid":"f31d5bbd01606ed","status":"passed","time":{"start":1659944495000,"stop":1659944495984,"duration":984}},{"uid":"95f26a6ec4c4026b","status":"passed","time":{"start":1659874977000,"stop":1659874977811,"duration":811}},{"uid":"853561a32391d1b3","status":"passed","time":{"start":1659496050000,"stop":1659496050537,"duration":537}},{"uid":"1ed821a7e4febfe3","status":"passed","time":{"start":1659496083000,"stop":1659496083825,"duration":825}},{"uid":"189b26cad1f5c6b0","status":"passed","time":{"start":1659451501000,"stop":1659451501933,"duration":933}},{"uid":"293e85ea61fa2596","status":"passed","time":{"start":1659450916000,"stop":1659450916622,"duration":622}},{"uid":"e067886a38c8e886","status":"passed","time":{"start":1659451524000,"stop":1659451524736,"duration":736}},{"uid":"384d004e1737474","status":"passed","time":{"start":1659447298000,"stop":1659447298799,"duration":799}},{"uid":"d9c49a12fddd1caa","status":"passed","time":{"start":1659447116000,"stop":1659447117009,"duration":1009}},{"uid":"f17d19fff35677af","status":"passed","time":{"start":1659448205000,"stop":1659448206226,"duration":1226}},{"uid":"4d18ad87e896b0b8","status":"passed","time":{"start":1659448097000,"stop":1659448098022,"duration":1022}},{"uid":"182a49027cea5b1a","status":"passed","time":{"start":1659409279000,"stop":1659409279899,"duration":899}},{"uid":"369eb777192113d0","status":"passed","time":{"start":1659402555000,"stop":1659402555914,"duration":914}},{"uid":"bdebec30f0e4f8b7","status":"passed","time":{"start":1659291287000,"stop":1659291287415,"duration":415}}]},"Worker Suite:Worker Suite#[It] Intracluster slice tests Slice with netpol should have vl3 router running":{"statistic":{"failed":14,"broken":0,"skipped":0,"passed":21,"unknown":0,"total":35},"items":[{"uid":"8695f0e807b3dc0a","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 3\nto equal\n    <int>: 1","time":{"start":1660113628000,"stop":1660113697033,"duration":69033}},{"uid":"2a6c5000e4ecf209","status":"passed","time":{"start":1660104898000,"stop":1660104911531,"duration":13531}},{"uid":"134545b34a325aa4","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 2\nto equal\n    <int>: 1","time":{"start":1660067259000,"stop":1660067327837,"duration":68837}},{"uid":"1c906c1908f5806c","status":"passed","time":{"start":1660047551000,"stop":1660047564042,"duration":13042}},{"uid":"530d28c3da1c8fbd","status":"passed","time":{"start":1659982549000,"stop":1659982554283,"duration":5283}},{"uid":"cecedf8545ea7d97","status":"passed","time":{"start":1659970182000,"stop":1659970182776,"duration":776}},{"uid":"e133129792e6b7f1","status":"passed","time":{"start":1659944495000,"stop":1659944495675,"duration":675}},{"uid":"7b1d5934f03e2ce3","status":"passed","time":{"start":1659874977000,"stop":1659874979193,"duration":2193}},{"uid":"a734d4aaae4912e","status":"passed","time":{"start":1659496050000,"stop":1659496050196,"duration":196}},{"uid":"7ece1f268fe05f22","status":"passed","time":{"start":1659496083000,"stop":1659496083332,"duration":332}},{"uid":"ca4b9edaa4586262","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1659451501000,"stop":1659451568038,"duration":67038}},{"uid":"7f71746b6da6edfa","status":"passed","time":{"start":1659450916000,"stop":1659450916544,"duration":544}},{"uid":"5ea01e835e95bdcb","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1659451524000,"stop":1659451592812,"duration":68812}},{"uid":"d9dbf78d8f0d8e07","status":"passed","time":{"start":1659447298000,"stop":1659447299324,"duration":1324}},{"uid":"34204c28b21bd3b9","status":"passed","time":{"start":1659447116000,"stop":1659447116666,"duration":666}},{"uid":"9df4236d25e4e996","status":"passed","time":{"start":1659448205000,"stop":1659448205238,"duration":238}},{"uid":"dd2c7149baf05e7a","status":"passed","time":{"start":1659448097000,"stop":1659448100847,"duration":3847}},{"uid":"2733fb3ee87fa950","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1659409279000,"stop":1659409348157,"duration":69157}},{"uid":"71277d74a440dad1","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 3\nto equal\n    <int>: 1","time":{"start":1659402555000,"stop":1659402615890,"duration":60890}},{"uid":"881449b1a7b98ed8","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 3\nto equal\n    <int>: 1","time":{"start":1659291287000,"stop":1659291347730,"duration":60730}}]},"Worker Suite:Worker Suite#[It] Hub netpol scenarios: Slice Configs Netpol scenarios Should fail when deploying with invalid clusters in both applicationNamespace and allowedNamespace":{"statistic":{"failed":0,"broken":0,"skipped":9,"passed":26,"unknown":0,"total":35},"items":[{"uid":"97a9135b25dc1db1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"cc77ad471859959d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"4e512b6ecc5ca831","status":"passed","time":{"start":1660067259000,"stop":1660067259951,"duration":951}},{"uid":"d04536ee29a113d2","status":"passed","time":{"start":1660047551000,"stop":1660047551847,"duration":847}},{"uid":"65c6d5adb8d816e1","status":"passed","time":{"start":1659982549000,"stop":1659982551208,"duration":2208}},{"uid":"16bca63d16632b71","status":"passed","time":{"start":1659970182000,"stop":1659970182845,"duration":845}},{"uid":"eb238d493077b714","status":"passed","time":{"start":1659944495000,"stop":1659944496129,"duration":1129}},{"uid":"53e9bdbd7b973b2e","status":"passed","time":{"start":1659874977000,"stop":1659874977733,"duration":733}},{"uid":"a8f2ff274bfcb57e","status":"passed","time":{"start":1659496050000,"stop":1659496050633,"duration":633}},{"uid":"ea5305b8bfabad47","status":"passed","time":{"start":1659496083000,"stop":1659496084020,"duration":1020}},{"uid":"dfc37b36f0d3643f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"82e47dbf4d2f55c0","status":"passed","time":{"start":1659450916000,"stop":1659450916740,"duration":740}},{"uid":"1c857c61369cb7be","status":"passed","time":{"start":1659451524000,"stop":1659451525012,"duration":1012}},{"uid":"e1eb2f64ae44238d","status":"passed","time":{"start":1659447298000,"stop":1659447298695,"duration":695}},{"uid":"f23fe3e1bcd60a67","status":"passed","time":{"start":1659447116000,"stop":1659447116831,"duration":831}},{"uid":"cd78b242a915b6cb","status":"passed","time":{"start":1659448205000,"stop":1659448206157,"duration":1157}},{"uid":"49b9ce587bacec0e","status":"passed","time":{"start":1659448097000,"stop":1659448098040,"duration":1040}},{"uid":"5230c21ccde1b162","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659409279000,"stop":1659409279000,"duration":0}},{"uid":"e0bdc9c3a29e92d2","status":"passed","time":{"start":1659402555000,"stop":1659402555980,"duration":980}},{"uid":"1a1b5a55cf28e8e3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659291287000,"stop":1659291287000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Intracluster slice tests Iperf connectivity Verify Connectivity between iperf-server and iperf-client":{"statistic":{"failed":1,"broken":0,"skipped":29,"passed":5,"unknown":0,"total":35},"items":[{"uid":"77dbe44598075993","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"6d4e04110e2a88ec","status":"passed","time":{"start":1660104898000,"stop":1660104904754,"duration":6754}},{"uid":"c8a1aaa7af80547a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"6722ccd10c859182","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}},{"uid":"a8c49b95007f5f95","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659982549000,"stop":1659982549000,"duration":0}},{"uid":"d665e2ecc605a8d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659970182000,"stop":1659970182000,"duration":0}},{"uid":"84274267ff16eb3c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659944495000,"stop":1659944495000,"duration":0}},{"uid":"d0f1d009a5c9426f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659874977000,"stop":1659874977000,"duration":0}},{"uid":"1d9b542e08465503","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496050000,"stop":1659496050000,"duration":0}},{"uid":"185199e10f0eab04","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496083000,"stop":1659496083000,"duration":0}},{"uid":"f6f24f1fa300dcbe","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"9562505a882387b4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659450916000,"stop":1659450916000,"duration":0}},{"uid":"453e5f5e2531620c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"e6b65184d9cebec5","status":"passed","time":{"start":1659447298000,"stop":1659447312923,"duration":14923}},{"uid":"2fb2eb4ba1e9faf9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447116000,"stop":1659447116000,"duration":0}},{"uid":"fe569fd1070cd3d3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448205000,"stop":1659448205000,"duration":0}},{"uid":"116afe32b8961b73","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448097000,"stop":1659448097000,"duration":0}},{"uid":"17b38ac86e3b4baf","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659409279000,"stop":1659409279000,"duration":0}},{"uid":"d77db3cd9d09feb5","status":"passed","time":{"start":1659402555000,"stop":1659402561977,"duration":6977}},{"uid":"9c87443596994a2b","status":"passed","time":{"start":1659291287000,"stop":1659291302095,"duration":15095}}]},"Worker Suite:Worker Suite#[It] Hub netpol scenarios: Slice Configs Netpol scenarios Should fail when deploying with invalid namespace in both applicationNamespace and allowedNamespace":{"statistic":{"failed":0,"broken":0,"skipped":9,"passed":26,"unknown":0,"total":35},"items":[{"uid":"eb939a8da12694af","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"9c399eb1f6d9191c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"cd09dd02ce7594c8","status":"passed","time":{"start":1660067259000,"stop":1660067259765,"duration":765}},{"uid":"27c52a30ecc3a82d","status":"passed","time":{"start":1660047551000,"stop":1660047551497,"duration":497}},{"uid":"a1e489095f262be9","status":"passed","time":{"start":1659982549000,"stop":1659982550401,"duration":1401}},{"uid":"6628aa031d37d368","status":"passed","time":{"start":1659970182000,"stop":1659970182667,"duration":667}},{"uid":"a00e32029eb941a2","status":"passed","time":{"start":1659944495000,"stop":1659944495760,"duration":760}},{"uid":"d29704c48c6d8ccf","status":"passed","time":{"start":1659874977000,"stop":1659874977598,"duration":598}},{"uid":"503c485190b7244b","status":"passed","time":{"start":1659496050000,"stop":1659496050441,"duration":441}},{"uid":"d2a83b1f6930c1c","status":"passed","time":{"start":1659496083000,"stop":1659496083693,"duration":693}},{"uid":"431605936bbed0dc","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"f978a69218ba4613","status":"passed","time":{"start":1659450916000,"stop":1659450916752,"duration":752}},{"uid":"4b44f6ce4e273a5e","status":"passed","time":{"start":1659451524000,"stop":1659451524879,"duration":879}},{"uid":"92014a6ebeabb14f","status":"passed","time":{"start":1659447298000,"stop":1659447298440,"duration":440}},{"uid":"a3f635cffa641b1d","status":"passed","time":{"start":1659447116000,"stop":1659447116577,"duration":577}},{"uid":"15591f7fba9052e","status":"passed","time":{"start":1659448205000,"stop":1659448205725,"duration":725}},{"uid":"d2bc4962972ad20c","status":"passed","time":{"start":1659448097000,"stop":1659448097798,"duration":798}},{"uid":"a03dfc90fac851ab","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659409279000,"stop":1659409279000,"duration":0}},{"uid":"77f1e7a3f2ecae2","status":"passed","time":{"start":1659402555000,"stop":1659402555863,"duration":863}},{"uid":"a97cfc00c5227f66","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659291287000,"stop":1659291287000,"duration":0}}]},"Hub Suite:Hub Suite#[AfterSuite]":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":100,"unknown":0,"total":100},"items":[{"uid":"a9d7bb926afc3117","status":"passed","time":{"start":1660153542000,"stop":1660153543897,"duration":1897}},{"uid":"b3f17d000e7a452e","status":"passed","time":{"start":1660152016000,"stop":1660152017873,"duration":1873}},{"uid":"3f520e3c94395d37","status":"passed","time":{"start":1660145110000,"stop":1660145112566,"duration":2566}},{"uid":"86417726f9b0bc8f","status":"passed","time":{"start":1660142645000,"stop":1660142647605,"duration":2605}},{"uid":"3152fc9786a29f5a","status":"passed","time":{"start":1660141834000,"stop":1660141836473,"duration":2473}},{"uid":"c605287b47dcad78","status":"passed","time":{"start":1660138561000,"stop":1660138563335,"duration":2335}},{"uid":"f36b2eb87a3d61f2","status":"passed","time":{"start":1660137797000,"stop":1660137798540,"duration":1540}},{"uid":"3b4935ec3f1da26f","status":"passed","time":{"start":1660129158000,"stop":1660129160082,"duration":2082}},{"uid":"eda98485e5a58b04","status":"passed","time":{"start":1660127869000,"stop":1660127870909,"duration":1909}},{"uid":"b54db2f9ef746fd2","status":"passed","time":{"start":1660126937000,"stop":1660126939518,"duration":2518}},{"uid":"60ff7c7890df1c35","status":"passed","time":{"start":1660116744000,"stop":1660116746993,"duration":2993}},{"uid":"3b84650ff4b439f5","status":"passed","time":{"start":1660116144000,"stop":1660116146230,"duration":2230}},{"uid":"6b9c0e1bab395f0f","status":"passed","time":{"start":1660112860000,"stop":1660112862425,"duration":2425}},{"uid":"a40c3318e5b611c8","status":"passed","time":{"start":1660114666000,"stop":1660114666082,"duration":82}},{"uid":"8b2748b3964047f1","status":"passed","time":{"start":1660108970000,"stop":1660108971711,"duration":1711}},{"uid":"af593c87f3344c19","status":"passed","time":{"start":1660104124000,"stop":1660104127605,"duration":3605}},{"uid":"a1d1cec45411681d","status":"passed","time":{"start":1660104760000,"stop":1660104762779,"duration":2779}},{"uid":"2d192e541b229448","status":"passed","time":{"start":1660101116000,"stop":1660101120319,"duration":4319}},{"uid":"ad148d017006d3bc","status":"passed","time":{"start":1660100168000,"stop":1660100170556,"duration":2556}},{"uid":"8082776a52629f81","status":"passed","time":{"start":1660066476000,"stop":1660066478058,"duration":2058}}]},"Hub Suite:Hub Suite#[It] Cluster CR tests Cluster CR validation Creates cluster secrets":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":89,"unknown":0,"total":89},"items":[{"uid":"59970895b224448b","status":"passed","time":{"start":1660145110000,"stop":1660145113443,"duration":3443}},{"uid":"4b68bf6f4bdb4e8","status":"passed","time":{"start":1660142645000,"stop":1660142648472,"duration":3472}},{"uid":"257a6d3011004e15","status":"passed","time":{"start":1660141834000,"stop":1660141838377,"duration":4377}},{"uid":"2eedca379f6e70d","status":"passed","time":{"start":1660138561000,"stop":1660138564474,"duration":3474}},{"uid":"7eaa0ca08c02d3ac","status":"passed","time":{"start":1660116744000,"stop":1660116747615,"duration":3615}},{"uid":"8564037432b40b11","status":"passed","time":{"start":1660116144000,"stop":1660116147427,"duration":3427}},{"uid":"c2d88234b13319f3","status":"passed","time":{"start":1660112860000,"stop":1660112863521,"duration":3521}},{"uid":"79fdbfa77a10c62c","status":"passed","time":{"start":1660108970000,"stop":1660108973519,"duration":3519}},{"uid":"f7629e25c3f495a3","status":"passed","time":{"start":1660104124000,"stop":1660104128647,"duration":4647}},{"uid":"9779c810f35b96fb","status":"passed","time":{"start":1660104760000,"stop":1660104763540,"duration":3540}},{"uid":"2ce93b414cad557e","status":"passed","time":{"start":1660101116000,"stop":1660101119416,"duration":3416}},{"uid":"2c8116e00f7fc520","status":"passed","time":{"start":1660100168000,"stop":1660100171513,"duration":3513}},{"uid":"5bb0c2ccd4bf46a3","status":"passed","time":{"start":1660066476000,"stop":1660066479399,"duration":3399}},{"uid":"a81dd1f759fc685f","status":"passed","time":{"start":1660064121000,"stop":1660064124470,"duration":3470}},{"uid":"5309cd3da77df22a","status":"passed","time":{"start":1660060680000,"stop":1660060683467,"duration":3467}},{"uid":"9e3f2453b71273c4","status":"passed","time":{"start":1660052169000,"stop":1660052172616,"duration":3616}},{"uid":"35c2ac6f6c4431b3","status":"passed","time":{"start":1660050481000,"stop":1660050484542,"duration":3542}},{"uid":"6a8efd41142620b9","status":"passed","time":{"start":1660046776000,"stop":1660046780301,"duration":4301}},{"uid":"a537e8984a1851b7","status":"passed","time":{"start":1660046048000,"stop":1660046051504,"duration":3504}},{"uid":"86814041e12c31fa","status":"passed","time":{"start":1660044532000,"stop":1660044535583,"duration":3583}}]},"Worker Suite:Worker Suite#[It] Hub netpol scenarios: Slice Configs Netpol scenarios Should remove successfully while Deleting Slice after removing the applicationNamespace in namespaceIsolationProfile part in slice configs":{"statistic":{"failed":0,"broken":0,"skipped":9,"passed":26,"unknown":0,"total":35},"items":[{"uid":"8dede603bdf60bf","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"ee48266bbafa92a5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"91c3685558aaa853","status":"passed","time":{"start":1660067259000,"stop":1660067269623,"duration":10623}},{"uid":"c034f5d350da7db5","status":"passed","time":{"start":1660047551000,"stop":1660047561652,"duration":10652}},{"uid":"29575edc2e725031","status":"passed","time":{"start":1659982549000,"stop":1659982560771,"duration":11771}},{"uid":"3d4038dd3c38ec45","status":"passed","time":{"start":1659970182000,"stop":1659970192804,"duration":10804}},{"uid":"1dfa514fd214225c","status":"passed","time":{"start":1659944495000,"stop":1659944505902,"duration":10902}},{"uid":"a0e2ff4f6735c742","status":"passed","time":{"start":1659874977000,"stop":1659874987725,"duration":10725}},{"uid":"24f937bf3fbdaa29","status":"passed","time":{"start":1659496050000,"stop":1659496060723,"duration":10723}},{"uid":"823c84e41cd22093","status":"passed","time":{"start":1659496083000,"stop":1659496103819,"duration":20819}},{"uid":"e0d4aae4a83a55ca","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"57ad33fb0eaa5f9","status":"passed","time":{"start":1659450916000,"stop":1659450926683,"duration":10683}},{"uid":"8829c7c1edd56e97","status":"passed","time":{"start":1659451524000,"stop":1659451534686,"duration":10686}},{"uid":"5acd1e6f4b0617c3","status":"passed","time":{"start":1659447298000,"stop":1659447308701,"duration":10701}},{"uid":"bc96cf1e27ca1d80","status":"passed","time":{"start":1659447116000,"stop":1659447126608,"duration":10608}},{"uid":"d2c1891e6a4347cd","status":"passed","time":{"start":1659448205000,"stop":1659448215781,"duration":10781}},{"uid":"497d9fef4760c3d0","status":"passed","time":{"start":1659448097000,"stop":1659448107847,"duration":10847}},{"uid":"39499d872d2441b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659409279000,"stop":1659409279000,"duration":0}},{"uid":"bd77381f32c2d46","status":"passed","time":{"start":1659402555000,"stop":1659402565981,"duration":10981}},{"uid":"1ef007a0e5658694","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659291287000,"stop":1659291287000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Intracluster slice tests Slice with netpol should label app ns with kubeslice label":{"statistic":{"failed":0,"broken":0,"skipped":20,"passed":15,"unknown":0,"total":35},"items":[{"uid":"bfe5d6d1deb00c32","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"5edf25e216623b2f","status":"passed","time":{"start":1660104898000,"stop":1660104898090,"duration":90}},{"uid":"e3057d1f988f5870","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"46644bab25a7659b","status":"passed","time":{"start":1660047551000,"stop":1660047551064,"duration":64}},{"uid":"e3c5a43c4e16d554","status":"passed","time":{"start":1659982549000,"stop":1659982549127,"duration":127}},{"uid":"ee37d770ef63c77c","status":"passed","time":{"start":1659970182000,"stop":1659970182070,"duration":70}},{"uid":"d2faa314c1286ede","status":"passed","time":{"start":1659944495000,"stop":1659944495190,"duration":190}},{"uid":"dbf96b4b039ee320","status":"passed","time":{"start":1659874977000,"stop":1659874977060,"duration":60}},{"uid":"6115a4e440e42165","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496050000,"stop":1659496050000,"duration":0}},{"uid":"b7cc655d84198ebb","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496083000,"stop":1659496083000,"duration":0}},{"uid":"2c3c798b96546973","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"63314094c9aeece","status":"passed","time":{"start":1659450916000,"stop":1659450916105,"duration":105}},{"uid":"b8904633f7bdac32","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"4bbf3c589546d9c8","status":"passed","time":{"start":1659447298000,"stop":1659447298064,"duration":64}},{"uid":"1029d59624cde5de","status":"passed","time":{"start":1659447116000,"stop":1659447116124,"duration":124}},{"uid":"c5991ed4d7bb5695","status":"passed","time":{"start":1659448205000,"stop":1659448205064,"duration":64}},{"uid":"9fc15f595bac2b7d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448097000,"stop":1659448097000,"duration":0}},{"uid":"3bbeefcfd5e4b5a1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659409279000,"stop":1659409279000,"duration":0}},{"uid":"714e3c97738f233","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659402555000,"stop":1659402555000,"duration":0}},{"uid":"76912fab7cd30c9a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659291287000,"stop":1659291287000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Iperf connectivity should onboard deployment in application Namespaces without slice annotations":{"statistic":{"failed":18,"broken":0,"skipped":16,"passed":1,"unknown":0,"total":35},"items":[{"uid":"2fd7096c25e04333","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"1209d2fb5dd692a2","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"d6667b5009dc2e81","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 1\nto equal\n    <int>: 2","time":{"start":1660067259000,"stop":1660067440053,"duration":181053}},{"uid":"3e82acd47805ef73","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 1\nto equal\n    <int>: 2","time":{"start":1660047551000,"stop":1660047732291,"duration":181291}},{"uid":"ecbe12fffc6f2fc2","status":"failed","statusDetails":"Timed out after 180.113s.\nExpected\n    <int>: 1\nto equal\n    <int>: 2","time":{"start":1659982549000,"stop":1659982730823,"duration":181823}},{"uid":"515b232973533e68","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 2","time":{"start":1659970182000,"stop":1659970363279,"duration":181279}},{"uid":"231bebb33f179f73","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659944495000,"stop":1659944495000,"duration":0}},{"uid":"a655fa59ce18c093","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659874977000,"stop":1659874977000,"duration":0}},{"uid":"212ff630a85f8f4c","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 2","time":{"start":1659496050000,"stop":1659496230994,"duration":180994}},{"uid":"4f99a9b9d93fc74a","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 1\nto equal\n    <int>: 2","time":{"start":1659496083000,"stop":1659496265526,"duration":182526}},{"uid":"2600f8f8a8665b91","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 2","time":{"start":1659451501000,"stop":1659451681685,"duration":180685}},{"uid":"8a69ccbccdfa5ba0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659450916000,"stop":1659450916000,"duration":0}},{"uid":"3c712671ed8f4c23","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 2","time":{"start":1659451524000,"stop":1659451704899,"duration":180899}},{"uid":"22380fbc43156edb","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447298000,"stop":1659447298000,"duration":0}},{"uid":"65c258ad2f05e8aa","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447116000,"stop":1659447116000,"duration":0}},{"uid":"8d836a9cc9b558e5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448205000,"stop":1659448205000,"duration":0}},{"uid":"e2cfbe40004d5b23","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 1\nto equal\n    <int>: 2","time":{"start":1659448097000,"stop":1659448278562,"duration":181562}},{"uid":"69fe05c4cb6af834","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 2","time":{"start":1659409279000,"stop":1659409460315,"duration":181315}},{"uid":"60a5a40e12d2c1f6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659402555000,"stop":1659402555000,"duration":0}},{"uid":"97a74d5cc00d121b","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 1\nto equal\n    <int>: 2","time":{"start":1659291287000,"stop":1659291468393,"duration":181393}}]},"Worker Suite:Worker Suite#[It] Namespace reconcile test scenarios: Offboard remove label from app ns":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":35,"unknown":0,"total":35},"items":[{"uid":"c592ef78edda75fd","status":"passed","time":{"start":1660113628000,"stop":1660113628146,"duration":146}},{"uid":"8d7e85c04a75cc84","status":"passed","time":{"start":1660104898000,"stop":1660104898168,"duration":168}},{"uid":"5a057c865089ab3f","status":"passed","time":{"start":1660067259000,"stop":1660067259293,"duration":293}},{"uid":"f84cf2131e852cf7","status":"passed","time":{"start":1660047551000,"stop":1660047551172,"duration":172}},{"uid":"475e46aeb5950aa4","status":"passed","time":{"start":1659982549000,"stop":1659982549580,"duration":580}},{"uid":"c96ba7f9487f77ad","status":"passed","time":{"start":1659970182000,"stop":1659970182169,"duration":169}},{"uid":"c4bcbe9b30eb7fe6","status":"passed","time":{"start":1659944495000,"stop":1659944495330,"duration":330}},{"uid":"7cdb927b81367b62","status":"passed","time":{"start":1659874977000,"stop":1659874977293,"duration":293}},{"uid":"bad66ac4d56419f","status":"passed","time":{"start":1659496050000,"stop":1659496050169,"duration":169}},{"uid":"58723cc69358a373","status":"passed","time":{"start":1659496083000,"stop":1659496083175,"duration":175}},{"uid":"8e848df75fd7ccde","status":"passed","time":{"start":1659451501000,"stop":1659451501164,"duration":164}},{"uid":"a65990e896883e65","status":"passed","time":{"start":1659450916000,"stop":1659450916326,"duration":326}},{"uid":"82cfc5628770e70c","status":"passed","time":{"start":1659451524000,"stop":1659451524311,"duration":311}},{"uid":"1618f9311d8562ba","status":"passed","time":{"start":1659447298000,"stop":1659447298176,"duration":176}},{"uid":"4494683dc7d456ab","status":"passed","time":{"start":1659447116000,"stop":1659447116334,"duration":334}},{"uid":"6b6d098c40028976","status":"passed","time":{"start":1659448205000,"stop":1659448205151,"duration":151}},{"uid":"47015e90812544e9","status":"passed","time":{"start":1659448097000,"stop":1659448097263,"duration":263}},{"uid":"ed1fef1b700657e","status":"passed","time":{"start":1659409279000,"stop":1659409279335,"duration":335}},{"uid":"245434035871ff2f","status":"passed","time":{"start":1659402555000,"stop":1659402555207,"duration":207}},{"uid":"fb0af20dae48de72","status":"passed","time":{"start":1659291287000,"stop":1659291287185,"duration":185}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting nsm-kernel-forwarder pod Check ping between iperf-server and iperf-client after nsm-kernel-forwarder pod restart":{"statistic":{"failed":0,"broken":0,"skipped":35,"passed":0,"unknown":0,"total":35},"items":[{"uid":"cb93d54740688c87","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"1f9053edbbdc4cec","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"d5adb1757d825c89","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"ff2adf10077a1759","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}},{"uid":"49b5c0effe699e0c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659982549000,"stop":1659982549000,"duration":0}},{"uid":"896402dc7b639bd2","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659970182000,"stop":1659970182000,"duration":0}},{"uid":"ebe8b3e4d7a5106a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659944495000,"stop":1659944495000,"duration":0}},{"uid":"7a8c6b97a9e7a693","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659874977000,"stop":1659874977000,"duration":0}},{"uid":"481e2245ac2762d8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496050000,"stop":1659496050000,"duration":0}},{"uid":"4ba1b9a425ffe5a3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496083000,"stop":1659496083000,"duration":0}},{"uid":"6da10e1560b2d62a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"ac448248b82bac62","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659450916000,"stop":1659450916000,"duration":0}},{"uid":"20a18f47047560b9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"5bcc292d25fb135e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447298000,"stop":1659447298000,"duration":0}},{"uid":"e302a7de03d1c598","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447116000,"stop":1659447116000,"duration":0}},{"uid":"255c055f8fe216cc","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448205000,"stop":1659448205000,"duration":0}},{"uid":"81459562b2f78976","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448097000,"stop":1659448097000,"duration":0}},{"uid":"b2aaab545e2a0a2","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659409279000,"stop":1659409279000,"duration":0}},{"uid":"cde8d842627daaca","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659402555000,"stop":1659402555000,"duration":0}},{"uid":"a9ce54a41ff04c97","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659291287000,"stop":1659291287000,"duration":0}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Creation Should fail while creating Project with  project name as blank":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":89,"unknown":0,"total":89},"items":[{"uid":"ae3daa890bb07816","status":"passed","time":{"start":1660145110000,"stop":1660145110242,"duration":242}},{"uid":"d875915b5838823f","status":"passed","time":{"start":1660142645000,"stop":1660142645202,"duration":202}},{"uid":"fed6d5d03c6baacb","status":"passed","time":{"start":1660141834000,"stop":1660141834257,"duration":257}},{"uid":"8368ccd2f872902d","status":"passed","time":{"start":1660138561000,"stop":1660138561194,"duration":194}},{"uid":"57a4c47167f6b98c","status":"passed","time":{"start":1660116744000,"stop":1660116744255,"duration":255}},{"uid":"a6f7fae389b3b1f8","status":"passed","time":{"start":1660116144000,"stop":1660116144216,"duration":216}},{"uid":"5b8f00d3c96d403","status":"passed","time":{"start":1660112860000,"stop":1660112860239,"duration":239}},{"uid":"ad21f274480ce510","status":"passed","time":{"start":1660108970000,"stop":1660108970236,"duration":236}},{"uid":"58f8b20a7df497b4","status":"passed","time":{"start":1660104124000,"stop":1660104124265,"duration":265}},{"uid":"bcc2c2f5549553d0","status":"passed","time":{"start":1660104760000,"stop":1660104760236,"duration":236}},{"uid":"f971c233873a2886","status":"passed","time":{"start":1660101116000,"stop":1660101116287,"duration":287}},{"uid":"4ea9d59cc9e16d6b","status":"passed","time":{"start":1660100168000,"stop":1660100168245,"duration":245}},{"uid":"63dcefcbf4ff113b","status":"passed","time":{"start":1660066476000,"stop":1660066476204,"duration":204}},{"uid":"9430cb9207bf6ecb","status":"passed","time":{"start":1660064121000,"stop":1660064121227,"duration":227}},{"uid":"e77e790ce2b60b40","status":"passed","time":{"start":1660060680000,"stop":1660060680254,"duration":254}},{"uid":"d6b965989da1a214","status":"passed","time":{"start":1660052169000,"stop":1660052169256,"duration":256}},{"uid":"557a1b041dc0d9d0","status":"passed","time":{"start":1660050481000,"stop":1660050481237,"duration":237}},{"uid":"236d50decf8b264","status":"passed","time":{"start":1660046776000,"stop":1660046776204,"duration":204}},{"uid":"6be785b6315a171d","status":"passed","time":{"start":1660046048000,"stop":1660046048188,"duration":188}},{"uid":"8b7c468ebd8da9b3","status":"passed","time":{"start":1660044532000,"stop":1660044532249,"duration":249}}]},"Worker Suite:Worker Suite#[It] Slice Creation scenarios Slice with MEDIUM QOS should have gateway pod running":{"statistic":{"failed":4,"broken":0,"skipped":4,"passed":18,"unknown":0,"total":26},"items":[{"uid":"bd69a0674582e107","status":"failed","statusDetails":"Timed out after 180.004s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1660113628000,"stop":1660113808378,"duration":180378}},{"uid":"741098da46901686","status":"failed","statusDetails":"Timed out after 180.004s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1660104898000,"stop":1660105078192,"duration":180192}},{"uid":"b9eec9322c615330","status":"passed","time":{"start":1660067259000,"stop":1660067314130,"duration":55130}},{"uid":"af70513e880982f5","status":"passed","time":{"start":1660047551000,"stop":1660047571086,"duration":20086}},{"uid":"314decebdc74be0","status":"passed","time":{"start":1659982549000,"stop":1659982564116,"duration":15116}},{"uid":"8db7ec436bace433","status":"passed","time":{"start":1659970182000,"stop":1659970267207,"duration":85207}},{"uid":"f0aa9620e97bdf2a","status":"passed","time":{"start":1659944495000,"stop":1659944565226,"duration":70226}},{"uid":"7472208dc64691e7","status":"passed","time":{"start":1659874977000,"stop":1659875042154,"duration":65154}},{"uid":"39c2fec0ae0a8a95","status":"passed","time":{"start":1659496050000,"stop":1659496095111,"duration":45111}},{"uid":"74022a8e4572a784","status":"passed","time":{"start":1659496083000,"stop":1659496103054,"duration":20054}},{"uid":"aca437232317c056","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"a2eaaf0de29fac99","status":"passed","time":{"start":1659450916000,"stop":1659450941084,"duration":25084}},{"uid":"b92ab2ba7e2fe05","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"603367f12194637","status":"passed","time":{"start":1659447298000,"stop":1659447333064,"duration":35064}},{"uid":"b42fc16ddac01857","status":"passed","time":{"start":1659447116000,"stop":1659447151117,"duration":35117}},{"uid":"c571dfc4b376bac","status":"failed","statusDetails":"Timed out after 180.004s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1659448205000,"stop":1659448385271,"duration":180271}},{"uid":"51e514fb36bcaf00","status":"passed","time":{"start":1659448097000,"stop":1659448152155,"duration":55155}},{"uid":"afb6473115e68b94","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659409279000,"stop":1659409279000,"duration":0}},{"uid":"a23bec597572b603","status":"passed","time":{"start":1659402555000,"stop":1659402575080,"duration":20080}},{"uid":"ebda0168c7d76947","status":"failed","statusDetails":"Timed out after 180.004s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1659291287000,"stop":1659291467317,"duration":180317}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests Iperf App connectivity Verify Iperf traffic":{"statistic":{"failed":0,"broken":0,"skipped":28,"passed":7,"unknown":0,"total":35},"items":[{"uid":"641fe44b3bdaeb15","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"d19d5ebb840dfb4f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"90100b52a9767d0b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"f5feb2f611247554","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}},{"uid":"6c0fe63647f97c8a","status":"passed","time":{"start":1659982549000,"stop":1659982610912,"duration":61912}},{"uid":"b9710d8c2178aa33","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659970182000,"stop":1659970182000,"duration":0}},{"uid":"a5051b74f896dbb0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659944495000,"stop":1659944495000,"duration":0}},{"uid":"10a127afc20b747c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659874977000,"stop":1659874977000,"duration":0}},{"uid":"11a8b2cd8b141cc4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496050000,"stop":1659496050000,"duration":0}},{"uid":"98bc7359231964ba","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496083000,"stop":1659496083000,"duration":0}},{"uid":"97362200db55efc5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"9fabdb0144309f20","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659450916000,"stop":1659450916000,"duration":0}},{"uid":"eda5aba09a87b585","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"422d95be88aef462","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447298000,"stop":1659447298000,"duration":0}},{"uid":"5dcb0a2807e8881a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447116000,"stop":1659447116000,"duration":0}},{"uid":"6e07759cf7c004c1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448205000,"stop":1659448205000,"duration":0}},{"uid":"6204c4c43d6eb4f8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448097000,"stop":1659448097000,"duration":0}},{"uid":"30d88a61dba47be1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659409279000,"stop":1659409279000,"duration":0}},{"uid":"6b564739d50fb8cb","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659402555000,"stop":1659402555000,"duration":0}},{"uid":"49c81da4a82463b4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659291287000,"stop":1659291287000,"duration":0}}]},"Worker Suite:Worker Suite#[It] NodeRestartSlice Testing node restart should have vl3 router running":{"statistic":{"failed":18,"broken":0,"skipped":0,"passed":8,"unknown":0,"total":26},"items":[{"uid":"6c1e22e56abddc59","status":"passed","time":{"start":1660113628000,"stop":1660113629169,"duration":1169}},{"uid":"b640df5e971a0db2","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 2\nto equal\n    <int>: 1","time":{"start":1660104898000,"stop":1660104960822,"duration":62822}},{"uid":"d00e68b993ab4788","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1660067259000,"stop":1660067321581,"duration":62581}},{"uid":"c33b3f5965b650ec","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1660047551000,"stop":1660047611568,"duration":60568}},{"uid":"b1e5ef14f3683a9d","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1659982549000,"stop":1659982612188,"duration":63188}},{"uid":"16ea56aba9a47d07","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1659970182000,"stop":1659970244642,"duration":62642}},{"uid":"6e2a0f54d6b2e2e2","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1659944495000,"stop":1659944557958,"duration":62958}},{"uid":"addc8e564feecc57","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <int>: 2\nto equal\n    <int>: 1","time":{"start":1659874977000,"stop":1659875039856,"duration":62856}},{"uid":"96407de582dc8a1d","status":"passed","time":{"start":1659496050000,"stop":1659496053130,"duration":3130}},{"uid":"2bd6e89e54b21bab","status":"passed","time":{"start":1659496083000,"stop":1659496085231,"duration":2231}},{"uid":"83ff197bc09c9c5a","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1659451501000,"stop":1659451563892,"duration":62892}},{"uid":"9ac703d735ddfdb9","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1659450916000,"stop":1659450976645,"duration":60645}},{"uid":"e0c541f67e7d9756","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1659451524000,"stop":1659451586765,"duration":62765}},{"uid":"da43ed19f9613155","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1659447298000,"stop":1659447360620,"duration":62620}},{"uid":"5cc13e3aac5f58f1","status":"passed","time":{"start":1659447116000,"stop":1659447118224,"duration":2224}},{"uid":"2a8612777758ed03","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 3\nto equal\n    <int>: 1","time":{"start":1659448205000,"stop":1659448267976,"duration":62976}},{"uid":"2319831257df5d52","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1659448097000,"stop":1659448157717,"duration":60717}},{"uid":"149b9e5af8b531f6","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1659409279000,"stop":1659409341775,"duration":62775}},{"uid":"80d679d20265d3d0","status":"passed","time":{"start":1659402555000,"stop":1659402555218,"duration":218}},{"uid":"ba4efd49687cb118","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 2\nto equal\n    <int>: 1","time":{"start":1659291287000,"stop":1659291349776,"duration":62776}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests slice with netpol Should label all application namespaces with kubeslice namespace":{"statistic":{"failed":0,"broken":0,"skipped":25,"passed":10,"unknown":0,"total":35},"items":[{"uid":"569ccbb531b9abc3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"22e718ca030d26df","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"3857180907e76007","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"67ee87bb3198af92","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}},{"uid":"403d70710b91dc8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659982549000,"stop":1659982549000,"duration":0}},{"uid":"dec92fde9f73d64f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659970182000,"stop":1659970182000,"duration":0}},{"uid":"ed85f7f5eb8f6bc7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659944495000,"stop":1659944495000,"duration":0}},{"uid":"54a86c6bfc9b6d72","status":"passed","time":{"start":1659874977000,"stop":1659874977148,"duration":148}},{"uid":"30e9a9d997ef4688","status":"passed","time":{"start":1659496050000,"stop":1659496050139,"duration":139}},{"uid":"f81f67cea2e0304b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496083000,"stop":1659496083000,"duration":0}},{"uid":"4b1fea54f6f64bb9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"e49b868bfb6a2953","status":"passed","time":{"start":1659450916000,"stop":1659450916260,"duration":260}},{"uid":"48c72a15bbff68f5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"26847aaab7706cd7","status":"passed","time":{"start":1659447298000,"stop":1659447298219,"duration":219}},{"uid":"eae0b162075194cf","status":"passed","time":{"start":1659447116000,"stop":1659447116137,"duration":137}},{"uid":"a3056aab6bc1cb13","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448205000,"stop":1659448205000,"duration":0}},{"uid":"da0b03778c1f5ea1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448097000,"stop":1659448097000,"duration":0}},{"uid":"8217f103c3154c40","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659409279000,"stop":1659409279000,"duration":0}},{"uid":"ed25d3e6796b62f5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659402555000,"stop":1659402555000,"duration":0}},{"uid":"a2816763abd40d81","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659291287000,"stop":1659291287000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice Creation scenarios Slice with MEDIUM QOS should reconcile medium qos slice":{"statistic":{"failed":0,"broken":0,"skipped":8,"passed":18,"unknown":0,"total":26},"items":[{"uid":"5ba4f84babc0480a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"9ef9632d99cd5c01","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"e716b45d51b81686","status":"passed","time":{"start":1660067259000,"stop":1660067259509,"duration":509}},{"uid":"150e6c7673077fbd","status":"passed","time":{"start":1660047551000,"stop":1660047551461,"duration":461}},{"uid":"c51130d08eef9594","status":"passed","time":{"start":1659982549000,"stop":1659982550042,"duration":1042}},{"uid":"3cb95c89d23e1acb","status":"passed","time":{"start":1659970182000,"stop":1659970182324,"duration":324}},{"uid":"246b3d843e07aba4","status":"passed","time":{"start":1659944495000,"stop":1659944495499,"duration":499}},{"uid":"60c55cb8baf95399","status":"passed","time":{"start":1659874977000,"stop":1659874977400,"duration":400}},{"uid":"557294ffe7d5b72b","status":"passed","time":{"start":1659496050000,"stop":1659496050330,"duration":330}},{"uid":"d32002dc559f0163","status":"passed","time":{"start":1659496083000,"stop":1659496083505,"duration":505}},{"uid":"ece47bd48dea4cae","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"57f071e5e850192","status":"passed","time":{"start":1659450916000,"stop":1659450916408,"duration":408}},{"uid":"490f343779140d7f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"76d4318ce77a623f","status":"passed","time":{"start":1659447298000,"stop":1659447298301,"duration":301}},{"uid":"d60e2ea00e0c1712","status":"passed","time":{"start":1659447116000,"stop":1659447116484,"duration":484}},{"uid":"165fdb58f13b18aa","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448205000,"stop":1659448205000,"duration":0}},{"uid":"a97947a211dbab48","status":"passed","time":{"start":1659448097000,"stop":1659448097463,"duration":463}},{"uid":"68acce76a232ab71","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659409279000,"stop":1659409279000,"duration":0}},{"uid":"a09fc7cee9237e78","status":"passed","time":{"start":1659402555000,"stop":1659402555595,"duration":595}},{"uid":"fcb7645ccf570d19","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659291287000,"stop":1659291287000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Iperf connectivity Verify Connectivity between iperf-server and iperf-client":{"statistic":{"failed":1,"broken":0,"skipped":34,"passed":0,"unknown":0,"total":35},"items":[{"uid":"b04bcacbaa34becd","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"1fbdf937900a2a80","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"63f31b30f54bd7e4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"79a6b75a1e593857","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}},{"uid":"9f940cdab7e2e09","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659982549000,"stop":1659982549000,"duration":0}},{"uid":"9eacca028fa18ed2","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659970182000,"stop":1659970182000,"duration":0}},{"uid":"489ed7a1245efa1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659944495000,"stop":1659944495000,"duration":0}},{"uid":"40b80cc2fb0c403","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659874977000,"stop":1659874977000,"duration":0}},{"uid":"50271fbbf5702175","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496050000,"stop":1659496050000,"duration":0}},{"uid":"c1e0c5d8e8cb29c8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496083000,"stop":1659496083000,"duration":0}},{"uid":"aed1670f527a1e73","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"a84445354affae75","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659450916000,"stop":1659450916000,"duration":0}},{"uid":"dbe62a3cedbfa3e6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"727f043ed905c18c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447298000,"stop":1659447298000,"duration":0}},{"uid":"d1dfbcdc6a688d99","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447116000,"stop":1659447116000,"duration":0}},{"uid":"30b44fe10693cf2e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448205000,"stop":1659448205000,"duration":0}},{"uid":"a6de3879a4050b9f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448097000,"stop":1659448097000,"duration":0}},{"uid":"91ce4da9a653978b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659409279000,"stop":1659409279000,"duration":0}},{"uid":"70b9ad837f1f4106","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659402555000,"stop":1659402555000,"duration":0}},{"uid":"4dad55d90afb41be","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659291287000,"stop":1659291287000,"duration":0}}]},"Worker Suite:Worker Suite#[It] NodeRestartSlice Testing node restart iperf connectivity across multi cluster":{"statistic":{"failed":8,"broken":0,"skipped":18,"passed":0,"unknown":0,"total":26},"items":[{"uid":"c40ac25977c86333","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <bool>: false\nto be true","time":{"start":1660113628000,"stop":1660113688454,"duration":60454}},{"uid":"cc87a733b2c3b626","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"8b10849df7b7784c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"92a4cbe8f2ecc3e3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}},{"uid":"ab20c80024237ae4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659982549000,"stop":1659982549000,"duration":0}},{"uid":"58a6c30bd1570603","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659970182000,"stop":1659970182000,"duration":0}},{"uid":"96f1b4a11a1b3aaa","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659944495000,"stop":1659944495000,"duration":0}},{"uid":"4c1b284d48408fbd","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659874977000,"stop":1659874977000,"duration":0}},{"uid":"b05022508c6db13d","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <bool>: false\nto be true","time":{"start":1659496050000,"stop":1659496112426,"duration":62426}},{"uid":"8ce9054290a1b90e","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <bool>: false\nto be true","time":{"start":1659496083000,"stop":1659496145658,"duration":62658}},{"uid":"bd36be5e7f75aafb","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"b093d0e7d364fdcc","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659450916000,"stop":1659450916000,"duration":0}},{"uid":"7ad701133e3dab39","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"dd4284be57569697","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447298000,"stop":1659447298000,"duration":0}},{"uid":"e9b3bef974962fb6","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <bool>: false\nto be true","time":{"start":1659447116000,"stop":1659447176433,"duration":60433}},{"uid":"4c1ee93a3bb85218","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448205000,"stop":1659448205000,"duration":0}},{"uid":"3cc31ce4558c953b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448097000,"stop":1659448097000,"duration":0}},{"uid":"19a5edd97ded5434","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659409279000,"stop":1659409279000,"duration":0}},{"uid":"eb89609335a56b93","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <bool>: false\nto be true","time":{"start":1659402555000,"stop":1659402617629,"duration":62629}},{"uid":"a066f87b46bede02","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659291287000,"stop":1659291287000,"duration":0}}]},"Hub Suite:Hub Suite#[It] Cluster negative tests Worker Cluster Registration with Wrong token":{"statistic":{"failed":0,"broken":0,"skipped":17,"passed":72,"unknown":0,"total":89},"items":[{"uid":"9a56d5db04b878d9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660145110000,"stop":1660145110000,"duration":0}},{"uid":"8b639839926254b7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660142645000,"stop":1660142645000,"duration":0}},{"uid":"e670b504124d90c4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660141834000,"stop":1660141834000,"duration":0}},{"uid":"b662573582a2889f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660138561000,"stop":1660138561000,"duration":0}},{"uid":"c4afdfe51f7d6d8f","status":"passed","time":{"start":1660116744000,"stop":1660116879852,"duration":135852}},{"uid":"3eb9e157e001e74e","status":"passed","time":{"start":1660116144000,"stop":1660116273753,"duration":129753}},{"uid":"d7f62327a4fdd067","status":"passed","time":{"start":1660112860000,"stop":1660112994287,"duration":134287}},{"uid":"588aed1e955b32f1","status":"passed","time":{"start":1660108970000,"stop":1660109099705,"duration":129705}},{"uid":"a736ca9544c3dff1","status":"passed","time":{"start":1660104124000,"stop":1660104254105,"duration":130105}},{"uid":"d211c624c300c045","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104760000,"stop":1660104760000,"duration":0}},{"uid":"b9e293724e6a8d33","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660101116000,"stop":1660101116000,"duration":0}},{"uid":"dfe18be361c7565c","status":"passed","time":{"start":1660100168000,"stop":1660100298070,"duration":130070}},{"uid":"f8206d5333f0d1ba","status":"passed","time":{"start":1660066476000,"stop":1660066610003,"duration":134003}},{"uid":"1ccbcb4eef4fe914","status":"passed","time":{"start":1660064121000,"stop":1660064255244,"duration":134244}},{"uid":"eeefa65256f4e51a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660060680000,"stop":1660060680000,"duration":0}},{"uid":"bf33f3c460bd21c1","status":"passed","time":{"start":1660052169000,"stop":1660052304617,"duration":135617}},{"uid":"340effc80dc39398","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660050481000,"stop":1660050481000,"duration":0}},{"uid":"26aa857073464f8","status":"passed","time":{"start":1660046776000,"stop":1660046910321,"duration":134321}},{"uid":"8496d5c6353c429f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660046048000,"stop":1660046048000,"duration":0}},{"uid":"cd6b1d10776abf91","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660044532000,"stop":1660044532000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice Creation scenarios Slice with MEDIUM QOS should have vl3 router running":{"statistic":{"failed":4,"broken":0,"skipped":0,"passed":22,"unknown":0,"total":26},"items":[{"uid":"e38253dca24054bf","status":"passed","time":{"start":1660113628000,"stop":1660113632531,"duration":4531}},{"uid":"343fb057e308248e","status":"passed","time":{"start":1660104898000,"stop":1660104901714,"duration":3714}},{"uid":"7c86720bcb17df48","status":"passed","time":{"start":1660067259000,"stop":1660067263669,"duration":4669}},{"uid":"8d6b9f872be941b2","status":"passed","time":{"start":1660047551000,"stop":1660047556593,"duration":5593}},{"uid":"4ce983ee76a077d1","status":"passed","time":{"start":1659982549000,"stop":1659982556616,"duration":7616}},{"uid":"880190ff7de57490","status":"passed","time":{"start":1659970182000,"stop":1659970187260,"duration":5260}},{"uid":"d6789f77bf7c3e98","status":"passed","time":{"start":1659944495000,"stop":1659944501407,"duration":6407}},{"uid":"28dd605da7d0715","status":"passed","time":{"start":1659874977000,"stop":1659874980749,"duration":3749}},{"uid":"124ec6feabc6bea5","status":"passed","time":{"start":1659496050000,"stop":1659496053591,"duration":3591}},{"uid":"245770b0b64e24b6","status":"passed","time":{"start":1659496083000,"stop":1659496087546,"duration":4546}},{"uid":"ac6a05b8a9137709","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1659451501000,"stop":1659451561819,"duration":60819}},{"uid":"18a6f537a2dee605","status":"passed","time":{"start":1659450916000,"stop":1659450921872,"duration":5872}},{"uid":"c04a8f6ad9f09ae9","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1659451524000,"stop":1659451584543,"duration":60543}},{"uid":"712bdba7ba597a31","status":"passed","time":{"start":1659447298000,"stop":1659447302601,"duration":4601}},{"uid":"64b73e70e4709cee","status":"passed","time":{"start":1659447116000,"stop":1659447120992,"duration":4992}},{"uid":"f5179fa4c3bdc493","status":"passed","time":{"start":1659448205000,"stop":1659448209618,"duration":4618}},{"uid":"c7b2a1d570a202ed","status":"passed","time":{"start":1659448097000,"stop":1659448102880,"duration":5880}},{"uid":"c941551cc033135a","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1659409279000,"stop":1659409339960,"duration":60960}},{"uid":"c9f3a1b789f76a41","status":"passed","time":{"start":1659402555000,"stop":1659402560699,"duration":5699}},{"uid":"fedb91286437fb6c","status":"passed","time":{"start":1659291287000,"stop":1659291291843,"duration":4843}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Iperf connectivity Verify Iperf traffic":{"statistic":{"failed":0,"broken":0,"skipped":35,"passed":0,"unknown":0,"total":35},"items":[{"uid":"74b0c4a68123bb79","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"4bd7a17867fd253d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"df9833c409829e1a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"3c3b6674a8ab1e1a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}},{"uid":"aa80ac5a4fb50875","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659982549000,"stop":1659982549000,"duration":0}},{"uid":"90a38c60b8396fa3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659970182000,"stop":1659970182000,"duration":0}},{"uid":"32fafc6d1906cf8d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659944495000,"stop":1659944495000,"duration":0}},{"uid":"47994d772fcba1c0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659874977000,"stop":1659874977000,"duration":0}},{"uid":"9b1b1e1655e0c561","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496050000,"stop":1659496050000,"duration":0}},{"uid":"adfed59c32fe81d3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496083000,"stop":1659496083000,"duration":0}},{"uid":"d6d67eb717698198","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"7f4a98e403823f55","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659450916000,"stop":1659450916000,"duration":0}},{"uid":"9fbd45a658c67d16","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"1bba72c4190a9aa5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447298000,"stop":1659447298000,"duration":0}},{"uid":"d0300741100a5695","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447116000,"stop":1659447116000,"duration":0}},{"uid":"a15cde49066aac2d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448205000,"stop":1659448205000,"duration":0}},{"uid":"9791c3477b7a35e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448097000,"stop":1659448097000,"duration":0}},{"uid":"91de731aa05ae097","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659409279000,"stop":1659409279000,"duration":0}},{"uid":"f01e9affc1020b55","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659402555000,"stop":1659402555000,"duration":0}},{"uid":"4e78671c0a546a1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659291287000,"stop":1659291287000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice validation Slice with LOW QOS should have vl3 router running":{"statistic":{"failed":2,"broken":0,"skipped":0,"passed":7,"unknown":0,"total":9},"items":[{"uid":"46aca11723b30ab4","status":"passed","time":{"start":1659168739000,"stop":1659168753412,"duration":14412}},{"uid":"23c755beac37ab66","status":"passed","time":{"start":1659164084000,"stop":1659164087272,"duration":3272}},{"uid":"88103c88c0b75f1c","status":"passed","time":{"start":1659160188000,"stop":1659160191197,"duration":3197}},{"uid":"3d2e40ff3ef5d0e5","status":"passed","time":{"start":1659119724000,"stop":1659119725567,"duration":1567}},{"uid":"502e7078fbd0aa84","status":"passed","time":{"start":1659116511000,"stop":1659116511230,"duration":230}},{"uid":"b2daf468d54fc42b","status":"passed","time":{"start":1659109470000,"stop":1659109470577,"duration":577}},{"uid":"e5cba288b1a3ee92","status":"passed","time":{"start":1659106836000,"stop":1659106864747,"duration":28747}},{"uid":"9676d1b60703e180","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1659101522000,"stop":1659101582708,"duration":60708}},{"uid":"3834bb8efced8f8","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 4\nto equal\n    <int>: 1","time":{"start":1659082816000,"stop":1659082876703,"duration":60703}}]},"Hub Suite:Hub Suite#[It]  QosProfile tests QosProfile Create validation Create qos profile- validation of BandwidthCeilingKbps > BandwidthGuaranteedKbps":{"statistic":{"failed":0,"broken":0,"skipped":81,"passed":0,"unknown":0,"total":81},"items":[{"uid":"4bdc1298b191dc07","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660145110000,"stop":1660145110000,"duration":0}},{"uid":"2472d2187fff28a3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660142645000,"stop":1660142645000,"duration":0}},{"uid":"c6f76398fbbe8be0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660141834000,"stop":1660141834000,"duration":0}},{"uid":"910b544e3d73cbcf","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660138561000,"stop":1660138561000,"duration":0}},{"uid":"7f7679891467b174","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660116744000,"stop":1660116744000,"duration":0}},{"uid":"8af58bf38624556","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660116144000,"stop":1660116144000,"duration":0}},{"uid":"430b3f39e33fbdd7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660112860000,"stop":1660112860000,"duration":0}},{"uid":"e6e7c68adf9a533","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660108970000,"stop":1660108970000,"duration":0}},{"uid":"3b158577d11419d6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104124000,"stop":1660104124000,"duration":0}},{"uid":"cc69ff1d9934f9d3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104760000,"stop":1660104760000,"duration":0}},{"uid":"4dd9c418e397110d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660101116000,"stop":1660101116000,"duration":0}},{"uid":"42f930b8beb48d8b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660100168000,"stop":1660100168000,"duration":0}},{"uid":"4318fc86ca3807eb","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660066476000,"stop":1660066476000,"duration":0}},{"uid":"6be1b88207a23524","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660064121000,"stop":1660064121000,"duration":0}},{"uid":"c17b34fb66ccd9ec","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660060680000,"stop":1660060680000,"duration":0}},{"uid":"70387f32355dacab","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660052169000,"stop":1660052169000,"duration":0}},{"uid":"890003f6c421a451","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660050481000,"stop":1660050481000,"duration":0}},{"uid":"375e9e72eb6eace4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660046776000,"stop":1660046776000,"duration":0}},{"uid":"1b959ee1a36f0b2b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660046048000,"stop":1660046048000,"duration":0}},{"uid":"d40b2564baae95e5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660044532000,"stop":1660044532000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Attach/Deattach Scenario Attach cluster from slice  when Deattach cluster from slice  should have vl3 router deleted from deattach cluster":{"statistic":{"failed":21,"broken":1,"skipped":0,"passed":4,"unknown":0,"total":26},"items":[{"uid":"75aedaf35c3f7f72","status":"failed","statusDetails":"Timed out after 180.004s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1660113628000,"stop":1660113808241,"duration":180241}},{"uid":"b9f986577f69cc6c","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1660104898000,"stop":1660105081644,"duration":183644}},{"uid":"f9394230d84aa75f","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 3\nto equal\n    <int>: 1","time":{"start":1660067259000,"stop":1660067319210,"duration":60210}},{"uid":"b5795c2e99782bba","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <int>: 4\nto equal\n    <int>: 1","time":{"start":1660047551000,"stop":1660047611447,"duration":60447}},{"uid":"5dfea7eeb7b7746","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 2\nto equal\n    <int>: 1","time":{"start":1659982549000,"stop":1659982646211,"duration":97211}},{"uid":"f7403465a20a1ee9","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 3\nto equal\n    <int>: 1","time":{"start":1659970182000,"stop":1659970242235,"duration":60235}},{"uid":"a47b4af4ddb3d5ff","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 3\nto equal\n    <int>: 1","time":{"start":1659944495000,"stop":1659944555311,"duration":60311}},{"uid":"feaf5ec21a88c59e","status":"broken","statusDetails":"runtime error: invalid memory address or nil pointer dereference","time":{"start":1659874977000,"stop":1659874977334,"duration":334}},{"uid":"6f5ceffbf9614acc","status":"passed","time":{"start":1659496050000,"stop":1659496071077,"duration":21077}},{"uid":"10f58fe5b924960e","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0008c0018>: {\n        Underlying: <*exec.ExitError | 0xc000880000>{\n            ProcessState: {\n                pid: 7197,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 54634},\n                    Stime: {Sec: 0, Usec: 22764},\n                    Maxrss: 43780,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3367,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 221,\n                    Nivcsw: 142,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicetwocluster\\\" not found\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicetwocluster\\\" not found\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicetwocluster\\\" not found\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicetwocluster\\\" not found\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (NotFound): slice.networking.kubeslice.io \"slicetwocluster\" not found\noccurred","time":{"start":1659496083000,"stop":1659496083788,"duration":788}},{"uid":"6a8bb385dc65c2f4","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1659451501000,"stop":1659451561389,"duration":60389}},{"uid":"4bf6c4ba19bba614","status":"passed","time":{"start":1659450916000,"stop":1659450996988,"duration":80988}},{"uid":"1df4795bd2dec759","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1659451524000,"stop":1659451584157,"duration":60157}},{"uid":"62edf51a2bf1168","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <int>: 1\nto equal\n    <int>: 0","time":{"start":1659447298000,"stop":1659447361412,"duration":63412}},{"uid":"dd9b9b783b5a4ba6","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0006c36f8>: {\n        Underlying: <*exec.ExitError | 0xc0009bcb40>{\n            ProcessState: {\n                pid: 8065,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 66541},\n                    Stime: {Sec: 0, Usec: 11090},\n                    Maxrss: 44300,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2467,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 260,\n                    Nivcsw: 237,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicetwocluster\\\" not found\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicetwocluster\\\" not found\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicetwocluster\\\" not found\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicetwocluster\\\" not found\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (NotFound): slice.networking.kubeslice.io \"slicetwocluster\" not found\noccurred","time":{"start":1659447116000,"stop":1659447116652,"duration":652}},{"uid":"c94791318239a8df","status":"failed","statusDetails":"Timed out after 180.004s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1659448205000,"stop":1659448385427,"duration":180427}},{"uid":"11a20436799fe84d","status":"passed","time":{"start":1659448097000,"stop":1659448156934,"duration":59934}},{"uid":"c1cf79f5fb4f58d7","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1659409279000,"stop":1659409339369,"duration":60369}},{"uid":"503a3b66516eb09c","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <int>: 3\nto equal\n    <int>: 1","time":{"start":1659402555000,"stop":1659402615296,"duration":60296}},{"uid":"6884b44296d93a21","status":"failed","statusDetails":"Timed out after 180.003s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1659291287000,"stop":1659291493478,"duration":206478}}]},"Hub Suite:Hub Suite#[BeforeSuite]":{"statistic":{"failed":11,"broken":0,"skipped":0,"passed":89,"unknown":0,"total":100},"items":[{"uid":"f0e93417cac453e2","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004c41b0>: {\n        Underlying: <*exec.ExitError | 0xc000074000>{\n            ProcessState: {\n                pid: 5985,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 1, Usec: 968976},\n                    Stime: {Sec: 0, Usec: 233391},\n                    Maxrss: 82324,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 9488,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12856,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 22311,\n                    Nivcsw: 5665,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-0.4.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 39 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 39 resources with timeout of 2m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-alertmanager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-alertmanager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-alertmanager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-alertmanager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-alertmanager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-alertmanager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-0.4.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 39 resource(s)\n    wait.go:48: [debug] beginning wait for 39 resources with timeout of 2m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-alertmanager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-alertmanager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-alertmanager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-alertmanager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-alertmanager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-alertmanager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-controller\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-manager-metrics-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-prometheus-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-alertmanager\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-prometheus\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-alertmanager\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-kube-state-metrics\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-controller-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-prometheus\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-kube-state-metrics\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-editor-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-prometheus\" ClusterRole\n    client.go:310: [debug] Starting delete for \"sliceconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"sliceqosconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"sliceresourcequotaconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workerserviceimports.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workersliceconfigs.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workerslicegateways.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workersliceresourcequotas.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"clusters.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"projects.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"serviceexportconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-prometheus-server-conf\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-alertmanager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-serving-cert\" Certificate\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-selfsigned-issuer\" Issuer\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-validating-webhook-configuration\" ValidatingWebhookConfiguration\n    uninstall.go:144: [debug] purge requested for kubeslice-controller\n    Error: INSTALLATION FAILED: release kubeslice-controller failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-controller failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1660153542000,"stop":1660153686086,"duration":144086}},{"uid":"ebb0d77e07912bca","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004fc018>: {\n        Underlying: <*exec.ExitError | 0xc0003ec020>{\n            ProcessState: {\n                pid: 5986,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 2, Usec: 143649},\n                    Stime: {Sec: 0, Usec: 347067},\n                    Maxrss: 83256,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 7224,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12856,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 25847,\n                    Nivcsw: 6532,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-0.4.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 39 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 39 resources with timeout of 2m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-alertmanager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-alertmanager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-0.4.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 39 resource(s)\n    wait.go:48: [debug] beginning wait for 39 resources with timeout of 2m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-alertmanager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-alertmanager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-controller\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-alertmanager\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-manager-metrics-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-prometheus-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-prometheus\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-alertmanager\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-kube-state-metrics\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-controller-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-prometheus\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-kube-state-metrics\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-editor-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-prometheus\" ClusterRole\n    client.go:310: [debug] Starting delete for \"sliceconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"sliceqosconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"sliceresourcequotaconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workersliceresourcequotas.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"clusters.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"projects.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"serviceexportconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workersliceconfigs.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workerserviceimports.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workerslicegateways.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-prometheus-server-conf\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-alertmanager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-serving-cert\" Certificate\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-selfsigned-issuer\" Issuer\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-validating-webhook-configuration\" ValidatingWebhookConfiguration\n    uninstall.go:144: [debug] purge requested for kubeslice-controller\n    Error: INSTALLATION FAILED: release kubeslice-controller failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-controller failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1660152016000,"stop":1660152162049,"duration":146049}},{"uid":"9bf98484801f66d8","status":"passed","time":{"start":1660145110000,"stop":1660145139203,"duration":29203}},{"uid":"bc5f0f34f7ee81b2","status":"passed","time":{"start":1660142645000,"stop":1660142675980,"duration":30980}},{"uid":"3049b2b63691212c","status":"passed","time":{"start":1660141834000,"stop":1660141881137,"duration":47137}},{"uid":"23189d6a23535088","status":"passed","time":{"start":1660138561000,"stop":1660138601025,"duration":40025}},{"uid":"ba4907c38bd7d8fe","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000132480>: {\n        Underlying: <*exec.ExitError | 0xc0003e2200>{\n            ProcessState: {\n                pid: 5973,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 1, Usec: 911912},\n                    Stime: {Sec: 0, Usec: 252355},\n                    Maxrss: 78480,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 9454,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12856,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 22438,\n                    Nivcsw: 5899,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-0.4.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 39 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 39 resources with timeout of 2m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-alertmanager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-alertmanager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n              ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-0.4.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 39 resource(s)\n    wait.go:48: [debug] beginning wait for 39 resources with timeout of 2m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-alertmanager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-alertmanager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-controller\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-alertmanager\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-manager-metrics-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-prometheus-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-prometheus\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-alertmanager\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-kube-state-metrics\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-controller-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-prometheus\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-kube-state-metrics\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-editor-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-prometheus\" ClusterRole\n    client.go:310: [debug] Starting delete for \"sliceconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"sliceqosconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workersliceresourcequotas.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"sliceresourcequotaconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workerserviceimports.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workersliceconfigs.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workerslicegateways.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"projects.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"clusters.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"serviceexportconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-prometheus-server-conf\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-alertmanager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-serving-cert\" Certificate\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-selfsigned-issuer\" Issuer\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-validating-webhook-configuration\" ValidatingWebhookConfiguration\n    uninstall.go:144: [debug] purge requested for kubeslice-controller\n    Error: INSTALLATION FAILED: release kubeslice-controller failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-controller failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1660137797000,"stop":1660137946997,"duration":149997}},{"uid":"263facb3d080e532","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004b8198>: {\n        Underlying: <*exec.ExitError | 0xc00014a020>{\n            ProcessState: {\n                pid: 5958,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 2, Usec: 330388},\n                    Stime: {Sec: 0, Usec: 399623},\n                    Maxrss: 80720,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3502,\n                    Majflt: 3,\n                    Nswap: 0,\n                    Inblock: 632,\n                    Oublock: 12848,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 25920,\n                    Nivcsw: 6713,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-0.4.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 39 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 39 resources with timeout of 2m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-alertmanager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-alertmanager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n            ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-0.4.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 39 resource(s)\n    wait.go:48: [debug] beginning wait for 39 resources with timeout of 2m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-alertmanager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-alertmanager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-controller\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-manager-metrics-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-alertmanager\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-prometheus-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-prometheus\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-alertmanager\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-kube-state-metrics\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-controller-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-prometheus\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-kube-state-metrics\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-editor-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-prometheus\" ClusterRole\n    client.go:310: [debug] Starting delete for \"sliceconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"sliceqosconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"sliceresourcequotaconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workerserviceimports.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workersliceconfigs.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workerslicegateways.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workersliceresourcequotas.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"clusters.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"projects.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"serviceexportconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-prometheus-server-conf\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-alertmanager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-serving-cert\" Certificate\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-selfsigned-issuer\" Issuer\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-validating-webhook-configuration\" ValidatingWebhookConfiguration\n    uninstall.go:144: [debug] purge requested for kubeslice-controller\n    Error: INSTALLATION FAILED: release kubeslice-controller failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-controller failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1660129158000,"stop":1660129300624,"duration":142624}},{"uid":"15efb78c9bb7bbfc","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000e060>: {\n        Underlying: <*exec.ExitError | 0xc00048c020>{\n            ProcessState: {\n                pid: 5979,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 2, Usec: 23475},\n                    Stime: {Sec: 0, Usec: 363593},\n                    Maxrss: 80968,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 7917,\n                    Majflt: 2,\n                    Nswap: 0,\n                    Inblock: 280,\n                    Oublock: 12848,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 23896,\n                    Nivcsw: 5538,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-0.4.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 39 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 39 resources with timeout of 2m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-alertmanager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-alertmanager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-alertmanager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n        ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-0.4.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 39 resource(s)\n    wait.go:48: [debug] beginning wait for 39 resources with timeout of 2m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-alertmanager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-alertmanager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-alertmanager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-controller\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-alertmanager\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-manager-metrics-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-prometheus-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-prometheus\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-alertmanager\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-kube-state-metrics\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-controller-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-prometheus\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-kube-state-metrics\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-editor-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-prometheus\" ClusterRole\n    client.go:310: [debug] Starting delete for \"sliceconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"sliceqosconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"sliceresourcequotaconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workerserviceimports.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workersliceconfigs.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workerslicegateways.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workersliceresourcequotas.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"clusters.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"projects.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"serviceexportconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-prometheus-server-conf\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-alertmanager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-serving-cert\" Certificate\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-selfsigned-issuer\" Issuer\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-validating-webhook-configuration\" ValidatingWebhookConfiguration\n    uninstall.go:144: [debug] purge requested for kubeslice-controller\n    Error: INSTALLATION FAILED: release kubeslice-controller failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-controller failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1660127869000,"stop":1660128016051,"duration":147051}},{"uid":"1b97f7f4f04634e9","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0001cc030>: {\n        Underlying: <*exec.ExitError | 0xc000498000>{\n            ProcessState: {\n                pid: 5958,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 2, Usec: 497539},\n                    Stime: {Sec: 0, Usec: 496172},\n                    Maxrss: 82016,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3903,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12848,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 27435,\n                    Nivcsw: 7702,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-0.4.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 39 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 39 resources with timeout of 2m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-alertmanager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-alertmanager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n              ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-0.4.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 39 resource(s)\n    wait.go:48: [debug] beginning wait for 39 resources with timeout of 2m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-alertmanager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-alertmanager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-controller\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-alertmanager\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-manager-metrics-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-prometheus-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-prometheus\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-alertmanager\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-kube-state-metrics\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-controller-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-prometheus\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-kube-state-metrics\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-editor-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-prometheus\" ClusterRole\n    client.go:310: [debug] Starting delete for \"sliceconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workersliceconfigs.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"sliceqosconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"sliceresourcequotaconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workerserviceimports.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"clusters.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workerslicegateways.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workersliceresourcequotas.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"projects.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"serviceexportconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-prometheus-server-conf\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-alertmanager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-serving-cert\" Certificate\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-selfsigned-issuer\" Issuer\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-validating-webhook-configuration\" ValidatingWebhookConfiguration\n    uninstall.go:144: [debug] purge requested for kubeslice-controller\n    Error: INSTALLATION FAILED: release kubeslice-controller failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-controller failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1660126937000,"stop":1660127082250,"duration":145250}},{"uid":"bd4949594af3578a","status":"passed","time":{"start":1660116744000,"stop":1660116786458,"duration":42458}},{"uid":"b45e6c99ecf5c12e","status":"passed","time":{"start":1660116144000,"stop":1660116172992,"duration":28992}},{"uid":"1a02676c238de8ef","status":"passed","time":{"start":1660112860000,"stop":1660112902247,"duration":42247}},{"uid":"9291671c1ca203f2","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0005823a8>: {\n        Underlying: <*exec.ExitError | 0xc000288220>{\n            ProcessState: {\n                pid: 8857,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 126876},\n                    Stime: {Sec: 0, Usec: 118633},\n                    Maxrss: 45076,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4882,\n                    Majflt: 421,\n                    Nswap: 0,\n                    Inblock: 75936,\n                    Oublock: 352,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 1754,\n                    Nivcsw: 28,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"v1.8.0\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\",\n                        \"\",\n                        \"Error: INSTALLATION FAILED: Kubernetes cluster unreachable: Get \\\"https://172.18.0.2:6443/version\\\": x509: certificate is valid for 10.96.0.1, 172.18.0.3, 127.0.0.1, not 172.18.0.2\",\n                        \"helm.go:84: [debug] Get \\\"https://172.18.0.2:6443/version\\\": x509: certificate is valid for 10.96.0.1, 172.18.0.3, 127.0.0.1, not 172.18.0.2\",\n                        \"Kubernetes cluster unreachable\",\n                        \"helm.sh/helm/v3/pkg/kube.(*Client).IsReachable\",\n                        \"\\thelm.sh/helm/v3/pkg/kube/client.go:121\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:192\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:264\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:125\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                        \"INSTALLATION FAILED\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:127\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n   ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"v1.8.0\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\n    \n    Error: INSTALLATION FAILED: Kubernetes cluster unreachable: Get \"https://172.18.0.2:6443/version\": x509: certificate is valid for 10.96.0.1, 172.18.0.3, 127.0.0.1, not 172.18.0.2\n    helm.go:84: [debug] Get \"https://172.18.0.2:6443/version\": x509: certificate is valid for 10.96.0.1, 172.18.0.3, 127.0.0.1, not 172.18.0.2\n    Kubernetes cluster unreachable\n    helm.sh/helm/v3/pkg/kube.(*Client).IsReachable\n    \thelm.sh/helm/v3/pkg/kube/client.go:121\n    helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\n    \thelm.sh/helm/v3/pkg/action/install.go:192\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:264\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:125\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1660114666000,"stop":1660114667171,"duration":1171}},{"uid":"ee301c5eb1cad6c8","status":"passed","time":{"start":1660108970000,"stop":1660109050128,"duration":80128}},{"uid":"3283decb5a433118","status":"passed","time":{"start":1660104124000,"stop":1660104165857,"duration":41857}},{"uid":"42197d6356ec7bb6","status":"passed","time":{"start":1660104760000,"stop":1660104789879,"duration":29879}},{"uid":"7c5eb220a7fec995","status":"passed","time":{"start":1660101116000,"stop":1660101159300,"duration":43300}},{"uid":"3d08432468418c51","status":"passed","time":{"start":1660100168000,"stop":1660100205053,"duration":37053}},{"uid":"5f8db170c94824bc","status":"passed","time":{"start":1660066476000,"stop":1660066517071,"duration":41071}}]},"Worker Suite:Worker Suite#[It] Slice creation scenarios: when slice red is installed workers should have vl3 router pods running":{"statistic":{"failed":23,"broken":0,"skipped":0,"passed":12,"unknown":0,"total":35},"items":[{"uid":"a00e71172c2c972f","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 3\nto equal\n    <int>: 1","time":{"start":1660113628000,"stop":1660113688206,"duration":60206}},{"uid":"b00389e53c48f1bc","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 4\nto equal\n    <int>: 1","time":{"start":1660104898000,"stop":1660104958219,"duration":60219}},{"uid":"f86ec96bb86b5ca1","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 4\nto equal\n    <int>: 1","time":{"start":1660067259000,"stop":1660067319170,"duration":60170}},{"uid":"f42a9323b9da382f","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 4\nto equal\n    <int>: 1","time":{"start":1660047551000,"stop":1660047611173,"duration":60173}},{"uid":"96f4cdf3d853cbf4","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 3\nto equal\n    <int>: 1","time":{"start":1659982549000,"stop":1659982609489,"duration":60489}},{"uid":"a1b9429af1fefd86","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 2\nto equal\n    <int>: 1","time":{"start":1659970182000,"stop":1659970242243,"duration":60243}},{"uid":"d38fc465464d3b03","status":"passed","time":{"start":1659944495000,"stop":1659944495037,"duration":37}},{"uid":"fb0adbe226e7eebc","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 4\nto equal\n    <int>: 1","time":{"start":1659874977000,"stop":1659875037171,"duration":60171}},{"uid":"161d6ba71ae99227","status":"passed","time":{"start":1659496050000,"stop":1659496052034,"duration":2034}},{"uid":"709610fc56f0eaf9","status":"passed","time":{"start":1659496083000,"stop":1659496101155,"duration":18155}},{"uid":"3561c55d9522702c","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1659451501000,"stop":1659451561303,"duration":60303}},{"uid":"20632dd0710f130d","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <int>: 3\nto equal\n    <int>: 1","time":{"start":1659450916000,"stop":1659450976330,"duration":60330}},{"uid":"7f55e6b2fdce0ad2","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1659451524000,"stop":1659451584630,"duration":60630}},{"uid":"868cb5385e377f11","status":"passed","time":{"start":1659447298000,"stop":1659447316620,"duration":18620}},{"uid":"bcc04a83a2a9ca14","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 2\nto equal\n    <int>: 1","time":{"start":1659447116000,"stop":1659447176209,"duration":60209}},{"uid":"d77b66ff284f9d70","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <int>: 4\nto equal\n    <int>: 1","time":{"start":1659448205000,"stop":1659448265264,"duration":60264}},{"uid":"f6a6e755ec2170c6","status":"passed","time":{"start":1659448097000,"stop":1659448097069,"duration":69}},{"uid":"4b71a7d8ab2c7ba0","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1659409279000,"stop":1659409339359,"duration":60359}},{"uid":"a1d7d895de40885f","status":"passed","time":{"start":1659402555000,"stop":1659402555039,"duration":39}},{"uid":"c0a1cb5c15c9296b","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <int>: 5\nto equal\n    <int>: 1","time":{"start":1659291287000,"stop":1659291347187,"duration":60187}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests Iperf App connectivity Application namespaces should be isolated":{"statistic":{"failed":7,"broken":0,"skipped":28,"passed":0,"unknown":0,"total":35},"items":[{"uid":"4fc9286750e399a5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"6d3f1759fe352220","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"2aa3f287d14b3323","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"e500e1e7697014b0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}},{"uid":"6dafe66778e09cf6","status":"failed","statusDetails":"Expected\n    <bool>: true\nto equal\n    <bool>: false","time":{"start":1659982549000,"stop":1659982607206,"duration":58206}},{"uid":"8ad464614fa56159","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659970182000,"stop":1659970182000,"duration":0}},{"uid":"957c5eb6879ca2dc","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659944495000,"stop":1659944495000,"duration":0}},{"uid":"8d77c83742c71467","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659874977000,"stop":1659874977000,"duration":0}},{"uid":"b947f1c580de7e5f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496050000,"stop":1659496050000,"duration":0}},{"uid":"3c2dbc7ec3cb362b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496083000,"stop":1659496083000,"duration":0}},{"uid":"222a8d9257f10a02","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"ca88c92d44c3d43a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659450916000,"stop":1659450916000,"duration":0}},{"uid":"ca25fbf18bd36441","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"868353a5dd249005","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447298000,"stop":1659447298000,"duration":0}},{"uid":"1efc0bb70e3ee8e4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447116000,"stop":1659447116000,"duration":0}},{"uid":"9ca811ff865e1c89","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448205000,"stop":1659448205000,"duration":0}},{"uid":"6e2da14540f7386","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448097000,"stop":1659448097000,"duration":0}},{"uid":"a6990091026f33ef","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659409279000,"stop":1659409279000,"duration":0}},{"uid":"977ed55ff69ad899","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659402555000,"stop":1659402555000,"duration":0}},{"uid":"387d01a7b4b78fa1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659291287000,"stop":1659291287000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice deletion tests: Slice is installed & validated Slice object is deleted from hub should remove gateway pod from spoke":{"statistic":{"failed":0,"broken":0,"skipped":27,"passed":8,"unknown":0,"total":35},"items":[{"uid":"5327462f8ff4e436","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"7d7598ffdfd14785","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"e41c10057cdaa0b4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"ea78d1c0af1b8979","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}},{"uid":"cd61a4c5d6e3c81c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659982549000,"stop":1659982549000,"duration":0}},{"uid":"2ca53197a2dfc03d","status":"passed","time":{"start":1659970182000,"stop":1659970182011,"duration":11}},{"uid":"f38f3492bc4b244d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659944495000,"stop":1659944495000,"duration":0}},{"uid":"eb28c6c3b5ac162c","status":"passed","time":{"start":1659874977000,"stop":1659874979024,"duration":2024}},{"uid":"a8441c605f162d21","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496050000,"stop":1659496050000,"duration":0}},{"uid":"c5be3bdf46b67c32","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496083000,"stop":1659496083000,"duration":0}},{"uid":"5def494e5dfa3f01","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"d8617e93d29c6af","status":"passed","time":{"start":1659450916000,"stop":1659450916009,"duration":9}},{"uid":"a01c7449c4bab27d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"3a60b3b6d50e4293","status":"passed","time":{"start":1659447298000,"stop":1659447298018,"duration":18}},{"uid":"c0b127fd5935a97","status":"passed","time":{"start":1659447116000,"stop":1659447118029,"duration":2029}},{"uid":"aef61c0b1d6f4ea6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448205000,"stop":1659448205000,"duration":0}},{"uid":"603d6ec9a8050852","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448097000,"stop":1659448097000,"duration":0}},{"uid":"ec88f1f9157c3b45","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659409279000,"stop":1659409279000,"duration":0}},{"uid":"3dc1c5c3a59aed36","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659402555000,"stop":1659402555000,"duration":0}},{"uid":"b8f6f23b04bbee32","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659291287000,"stop":1659291287000,"duration":0}}]},"Hub Suite:Hub Suite#[It] Cluster negative tests Worker Cluster Registration with Wrong ca.cert":{"statistic":{"failed":0,"broken":0,"skipped":17,"passed":72,"unknown":0,"total":89},"items":[{"uid":"426c8bba6a68d638","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660145110000,"stop":1660145110000,"duration":0}},{"uid":"9202afb9677bf4d4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660142645000,"stop":1660142645000,"duration":0}},{"uid":"3a90e4a89860836e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660141834000,"stop":1660141834000,"duration":0}},{"uid":"cf0cbd70c2b7857e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660138561000,"stop":1660138561000,"duration":0}},{"uid":"a7df184ad572cd1c","status":"passed","time":{"start":1660116744000,"stop":1660116880399,"duration":136399}},{"uid":"9d416f6de65b2b11","status":"passed","time":{"start":1660116144000,"stop":1660116273355,"duration":129355}},{"uid":"dd865fbf480bada0","status":"passed","time":{"start":1660112860000,"stop":1660112995502,"duration":135502}},{"uid":"abb2915ca10be2e2","status":"passed","time":{"start":1660108970000,"stop":1660109105844,"duration":135844}},{"uid":"1f2d890dcd776248","status":"passed","time":{"start":1660104124000,"stop":1660104254553,"duration":130553}},{"uid":"88b80ddc98c7d482","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104760000,"stop":1660104760000,"duration":0}},{"uid":"786a3b3c5ad10b91","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660101116000,"stop":1660101116000,"duration":0}},{"uid":"dea23ff10980829a","status":"passed","time":{"start":1660100168000,"stop":1660100304924,"duration":136924}},{"uid":"3d6648328c5daec1","status":"passed","time":{"start":1660066476000,"stop":1660066605848,"duration":129848}},{"uid":"dd24409f6bcffa8e","status":"passed","time":{"start":1660064121000,"stop":1660064256570,"duration":135570}},{"uid":"fa3ea09f4f9d497b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660060680000,"stop":1660060680000,"duration":0}},{"uid":"7e44aae7b6541afe","status":"passed","time":{"start":1660052169000,"stop":1660052299558,"duration":130558}},{"uid":"5c03021b544d30ff","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660050481000,"stop":1660050481000,"duration":0}},{"uid":"b1c8b17e4d55124","status":"passed","time":{"start":1660046776000,"stop":1660046911375,"duration":135375}},{"uid":"c9aeb583f877d152","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660046048000,"stop":1660046048000,"duration":0}},{"uid":"50f817740c6b2b92","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660044532000,"stop":1660044532000,"duration":0}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Creation Should fail while creating service accounts as combination of special characters":{"statistic":{"failed":5,"broken":0,"skipped":0,"passed":84,"unknown":0,"total":89},"items":[{"uid":"368e38dbcb08fc57","status":"passed","time":{"start":1660145110000,"stop":1660145110257,"duration":257}},{"uid":"a983e33fb23e622","status":"passed","time":{"start":1660142645000,"stop":1660142645290,"duration":290}},{"uid":"302e5f3587b3e76d","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected an error to have occurred.  Got:\n    <nil>: nil","time":{"start":1660141834000,"stop":1660141894241,"duration":60241}},{"uid":"ab95fec6ec0ed1c1","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected an error to have occurred.  Got:\n    <nil>: nil","time":{"start":1660138561000,"stop":1660138621258,"duration":60258}},{"uid":"7fa189555bfb7e93","status":"passed","time":{"start":1660116744000,"stop":1660116744307,"duration":307}},{"uid":"fbfa14d8e399f566","status":"passed","time":{"start":1660116144000,"stop":1660116144224,"duration":224}},{"uid":"c34fd64945faea65","status":"passed","time":{"start":1660112860000,"stop":1660112860253,"duration":253}},{"uid":"1aeeeb696312a97e","status":"passed","time":{"start":1660108970000,"stop":1660108970257,"duration":257}},{"uid":"b367f41ab79c263f","status":"passed","time":{"start":1660104124000,"stop":1660104124272,"duration":272}},{"uid":"a9bdaf8c6de7576d","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected an error to have occurred.  Got:\n    <nil>: nil","time":{"start":1660104760000,"stop":1660104820241,"duration":60241}},{"uid":"f4d9c412011fec1f","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected an error to have occurred.  Got:\n    <nil>: nil","time":{"start":1660101116000,"stop":1660101176248,"duration":60248}},{"uid":"779cf52e551b2068","status":"passed","time":{"start":1660100168000,"stop":1660100168270,"duration":270}},{"uid":"665ee2cbfd7c33b3","status":"passed","time":{"start":1660066476000,"stop":1660066476248,"duration":248}},{"uid":"4ca24ae73e4a3bdf","status":"passed","time":{"start":1660064121000,"stop":1660064121230,"duration":230}},{"uid":"7ef5d2d43892af9e","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected an error to have occurred.  Got:\n    <nil>: nil","time":{"start":1660060680000,"stop":1660060740246,"duration":60246}},{"uid":"e4ee1fd9712ee721","status":"passed","time":{"start":1660052169000,"stop":1660052169314,"duration":314}},{"uid":"5155d74bccf93b0f","status":"passed","time":{"start":1660050481000,"stop":1660050481263,"duration":263}},{"uid":"cd64239af4216cba","status":"passed","time":{"start":1660046776000,"stop":1660046776226,"duration":226}},{"uid":"c0d3b135eb0830e4","status":"passed","time":{"start":1660046048000,"stop":1660046048198,"duration":198}},{"uid":"c394cc8ec1a5c585","status":"passed","time":{"start":1660044532000,"stop":1660044532248,"duration":248}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting nsm-kernel-forwarder pod Should restart nsm-kernel-forwarder pod":{"statistic":{"failed":0,"broken":0,"skipped":35,"passed":0,"unknown":0,"total":35},"items":[{"uid":"88de0417b8c590c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"8c4d338133296a87","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"1a9fba1a18cfb37b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"be3b4ae9d6ba3d61","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}},{"uid":"f3e3a50375e11f8e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659982549000,"stop":1659982549000,"duration":0}},{"uid":"e422917c6e20a0ce","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659970182000,"stop":1659970182000,"duration":0}},{"uid":"78d530d9c466458a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659944495000,"stop":1659944495000,"duration":0}},{"uid":"13d446a1ff1670a7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659874977000,"stop":1659874977000,"duration":0}},{"uid":"4dabd04863b110eb","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496050000,"stop":1659496050000,"duration":0}},{"uid":"e6563d7c1c9d5780","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496083000,"stop":1659496083000,"duration":0}},{"uid":"b348a625d25e2449","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"78d7949da523b81d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659450916000,"stop":1659450916000,"duration":0}},{"uid":"4151a1cde5221116","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"b853f6d33ed920de","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447298000,"stop":1659447298000,"duration":0}},{"uid":"4342fa2c914fd39c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447116000,"stop":1659447116000,"duration":0}},{"uid":"bd5f3cea55997272","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448205000,"stop":1659448205000,"duration":0}},{"uid":"5134688ca8eb68ab","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448097000,"stop":1659448097000,"duration":0}},{"uid":"8bf1910f50dc295c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659409279000,"stop":1659409279000,"duration":0}},{"uid":"ee9e5bcaccd0cbd8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659402555000,"stop":1659402555000,"duration":0}},{"uid":"ebf28694fd2c7fd3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659291287000,"stop":1659291287000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting nsm-manager pod Check ping between iperf-server and iperf-client after nsm-manager pod restart":{"statistic":{"failed":0,"broken":0,"skipped":35,"passed":0,"unknown":0,"total":35},"items":[{"uid":"5c30549487d0ea09","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"6ea7cf1fd8397dbe","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"9c041fb0e17fd02c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"78cd510da75148c6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}},{"uid":"6641048f7e846c73","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659982549000,"stop":1659982549000,"duration":0}},{"uid":"900b3a47b1a7b84c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659970182000,"stop":1659970182000,"duration":0}},{"uid":"b049cdf76a474a04","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659944495000,"stop":1659944495000,"duration":0}},{"uid":"2aac0f693e869d88","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659874977000,"stop":1659874977000,"duration":0}},{"uid":"4079753729c9fab9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496050000,"stop":1659496050000,"duration":0}},{"uid":"f610e4a78c4cc661","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496083000,"stop":1659496083000,"duration":0}},{"uid":"7cad46dc35e05df7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"5cf858e00fa369e2","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659450916000,"stop":1659450916000,"duration":0}},{"uid":"5a7f153872854ded","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"f3406e9d98c05d50","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447298000,"stop":1659447298000,"duration":0}},{"uid":"5a4315b44449ff05","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447116000,"stop":1659447116000,"duration":0}},{"uid":"ef28204785b4f6b5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448205000,"stop":1659448205000,"duration":0}},{"uid":"2922b3847c12a86d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448097000,"stop":1659448097000,"duration":0}},{"uid":"6bc9552c2627dec1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659409279000,"stop":1659409279000,"duration":0}},{"uid":"98689f929f5ab22e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659402555000,"stop":1659402555000,"duration":0}},{"uid":"d77bbeb1798cb486","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659291287000,"stop":1659291287000,"duration":0}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Creation Should fail while creating Project with project name as combination of special characters":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":89,"unknown":0,"total":89},"items":[{"uid":"1ec38f4a20082111","status":"passed","time":{"start":1660145110000,"stop":1660145110218,"duration":218}},{"uid":"81fdee8a758307ca","status":"passed","time":{"start":1660142645000,"stop":1660142645219,"duration":219}},{"uid":"68fc1aecbeaf6d0c","status":"passed","time":{"start":1660141834000,"stop":1660141834227,"duration":227}},{"uid":"4f7cae27693c9f89","status":"passed","time":{"start":1660138561000,"stop":1660138561205,"duration":205}},{"uid":"48c9832398ce78ae","status":"passed","time":{"start":1660116744000,"stop":1660116744270,"duration":270}},{"uid":"a0c1ae15329b78fd","status":"passed","time":{"start":1660116144000,"stop":1660116144196,"duration":196}},{"uid":"61156738b8d7b4b2","status":"passed","time":{"start":1660112860000,"stop":1660112860236,"duration":236}},{"uid":"ebc40ab18404fde4","status":"passed","time":{"start":1660108970000,"stop":1660108970231,"duration":231}},{"uid":"c624a1434b6ea7d9","status":"passed","time":{"start":1660104124000,"stop":1660104124232,"duration":232}},{"uid":"fac65fb0b835ef34","status":"passed","time":{"start":1660104760000,"stop":1660104760236,"duration":236}},{"uid":"a1550ebe9826a43a","status":"passed","time":{"start":1660101116000,"stop":1660101116433,"duration":433}},{"uid":"3a76f7f8926c0630","status":"passed","time":{"start":1660100168000,"stop":1660100168280,"duration":280}},{"uid":"8ec1dad711c17944","status":"passed","time":{"start":1660066476000,"stop":1660066476199,"duration":199}},{"uid":"dc8f7f0b25911839","status":"passed","time":{"start":1660064121000,"stop":1660064121217,"duration":217}},{"uid":"2fd2c43567919879","status":"passed","time":{"start":1660060680000,"stop":1660060680242,"duration":242}},{"uid":"f82e1ca76528b4a5","status":"passed","time":{"start":1660052169000,"stop":1660052169277,"duration":277}},{"uid":"223329ddeab4ada","status":"passed","time":{"start":1660050481000,"stop":1660050481251,"duration":251}},{"uid":"820cd2eab03e917a","status":"passed","time":{"start":1660046776000,"stop":1660046776209,"duration":209}},{"uid":"b8c05269b4ae5d85","status":"passed","time":{"start":1660046048000,"stop":1660046048207,"duration":207}},{"uid":"1f74033c816e6805","status":"passed","time":{"start":1660044532000,"stop":1660044532246,"duration":246}}]},"Worker Suite:Worker Suite#[It] Hub netpol scenarios: Slice Configs Netpol scenarios Should fail while creating namespaces with same name in both allowedNamespace and applicationNamespace":{"statistic":{"failed":0,"broken":0,"skipped":9,"passed":26,"unknown":0,"total":35},"items":[{"uid":"b295c3bc52481d14","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"cacb1bf0c65dc3b6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"b18e8be3034e07a0","status":"passed","time":{"start":1660067259000,"stop":1660067259327,"duration":327}},{"uid":"a599479aa6ba95f3","status":"passed","time":{"start":1660047551000,"stop":1660047551345,"duration":345}},{"uid":"9bc6bca39f0a7d59","status":"passed","time":{"start":1659982549000,"stop":1659982549628,"duration":628}},{"uid":"2498530d4846db75","status":"passed","time":{"start":1659970182000,"stop":1659970182322,"duration":322}},{"uid":"832517f5de86c819","status":"passed","time":{"start":1659944495000,"stop":1659944495386,"duration":386}},{"uid":"2ff4039d50974e18","status":"passed","time":{"start":1659874977000,"stop":1659874977283,"duration":283}},{"uid":"a029f32e5c86f789","status":"passed","time":{"start":1659496050000,"stop":1659496050168,"duration":168}},{"uid":"41d5bc15d27e6a38","status":"passed","time":{"start":1659496083000,"stop":1659496083365,"duration":365}},{"uid":"c1b0ca2f6a405bf2","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"f1743a0ff792d686","status":"passed","time":{"start":1659450916000,"stop":1659450916297,"duration":297}},{"uid":"469ecc33613c0103","status":"passed","time":{"start":1659451524000,"stop":1659451524404,"duration":404}},{"uid":"9425d7edb3cc2f5","status":"passed","time":{"start":1659447298000,"stop":1659447298229,"duration":229}},{"uid":"7ca373fc25b70669","status":"passed","time":{"start":1659447116000,"stop":1659447116350,"duration":350}},{"uid":"b98e46c025868716","status":"passed","time":{"start":1659448205000,"stop":1659448205280,"duration":280}},{"uid":"71c912d4b7b68f4d","status":"passed","time":{"start":1659448097000,"stop":1659448097377,"duration":377}},{"uid":"2fc98a5460a153fa","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659409279000,"stop":1659409279000,"duration":0}},{"uid":"c36bf0ba0c37f919","status":"passed","time":{"start":1659402555000,"stop":1659402555372,"duration":372}},{"uid":"373762e6f0604a8d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659291287000,"stop":1659291287000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting iperf-server pod Should restart iperf-server pod":{"statistic":{"failed":0,"broken":0,"skipped":35,"passed":0,"unknown":0,"total":35},"items":[{"uid":"34ba068544ff09c7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"475d48b671990726","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"fd93fcd5dcf92398","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"e81f0f8c906863c6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}},{"uid":"74ad7195449d0090","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659982549000,"stop":1659982549000,"duration":0}},{"uid":"16aba6005575c616","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659970182000,"stop":1659970182000,"duration":0}},{"uid":"770b3e272b6fbe5c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659944495000,"stop":1659944495000,"duration":0}},{"uid":"af4ecae8c2e4f678","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659874977000,"stop":1659874977000,"duration":0}},{"uid":"c5400e164577490c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496050000,"stop":1659496050000,"duration":0}},{"uid":"c1002e7e0f88e284","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496083000,"stop":1659496083000,"duration":0}},{"uid":"7520b702322fd05c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"ca57d9641bf170b2","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659450916000,"stop":1659450916000,"duration":0}},{"uid":"eea836a2fc70917e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"5331423252706652","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447298000,"stop":1659447298000,"duration":0}},{"uid":"9ff8601400b33284","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447116000,"stop":1659447116000,"duration":0}},{"uid":"9c87ed8fb261e00c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448205000,"stop":1659448205000,"duration":0}},{"uid":"297907b08c07d7a0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448097000,"stop":1659448097000,"duration":0}},{"uid":"c356597690d03ce0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659409279000,"stop":1659409279000,"duration":0}},{"uid":"dbe79d294f4b538b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659402555000,"stop":1659402555000,"duration":0}},{"uid":"6ce4b9aee78d3b4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659291287000,"stop":1659291287000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting worker-operator pod Should restart worker-operator pod":{"statistic":{"failed":0,"broken":0,"skipped":35,"passed":0,"unknown":0,"total":35},"items":[{"uid":"bbd2eacb60b58a3f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"bf242a58aac28486","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"cb543711d9edb312","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"be4c88760885ea1b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}},{"uid":"109b4eef62c498bd","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659982549000,"stop":1659982549000,"duration":0}},{"uid":"7f40a8b1881c8c47","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659970182000,"stop":1659970182000,"duration":0}},{"uid":"1daac68858ec0398","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659944495000,"stop":1659944495000,"duration":0}},{"uid":"d611d4d272ab3305","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659874977000,"stop":1659874977000,"duration":0}},{"uid":"66b7895ed7b4b752","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496050000,"stop":1659496050000,"duration":0}},{"uid":"43fd7a55012e8361","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496083000,"stop":1659496083000,"duration":0}},{"uid":"9f09c751a09c57f0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"8f31bdf379b8c4b9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659450916000,"stop":1659450916000,"duration":0}},{"uid":"16c5f501f46907d4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"fe2217ea1fbe8e76","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447298000,"stop":1659447298000,"duration":0}},{"uid":"2a149e7f9fbfd44a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447116000,"stop":1659447116000,"duration":0}},{"uid":"5c625a61c957d866","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448205000,"stop":1659448205000,"duration":0}},{"uid":"5e8ba4535b8f4d95","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448097000,"stop":1659448097000,"duration":0}},{"uid":"1fa969a6d0567ce8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659409279000,"stop":1659409279000,"duration":0}},{"uid":"f495fb1063e9e6c8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659402555000,"stop":1659402555000,"duration":0}},{"uid":"4c75ca0a395a3ec0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659291287000,"stop":1659291287000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice Creation scenarios Slice with LOW QOS should have vl3 router running":{"statistic":{"failed":4,"broken":0,"skipped":0,"passed":22,"unknown":0,"total":26},"items":[{"uid":"94c4e83ac5902d5","status":"passed","time":{"start":1660113628000,"stop":1660113631562,"duration":3562}},{"uid":"2f74166b31149e41","status":"passed","time":{"start":1660104898000,"stop":1660104902701,"duration":4701}},{"uid":"90ab2f86812d46bd","status":"passed","time":{"start":1660067259000,"stop":1660067262335,"duration":3335}},{"uid":"83ca3d46dbaac9da","status":"passed","time":{"start":1660047551000,"stop":1660047554342,"duration":3342}},{"uid":"6d80b4ff5f31083a","status":"passed","time":{"start":1659982549000,"stop":1659982553592,"duration":4592}},{"uid":"86d596614a5d7b54","status":"passed","time":{"start":1659970182000,"stop":1659970186679,"duration":4679}},{"uid":"fba8cf35a94aaafd","status":"passed","time":{"start":1659944495000,"stop":1659944524059,"duration":29059}},{"uid":"38a742fa72b5bcfb","status":"passed","time":{"start":1659874977000,"stop":1659874980291,"duration":3291}},{"uid":"2b04f2855ad7c278","status":"passed","time":{"start":1659496050000,"stop":1659496075448,"duration":25448}},{"uid":"82b45c9001fb3a6a","status":"passed","time":{"start":1659496083000,"stop":1659496087249,"duration":4249}},{"uid":"33b73272ae51e309","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1659451501000,"stop":1659451561498,"duration":60498}},{"uid":"525f0d7132df5920","status":"passed","time":{"start":1659450916000,"stop":1659450952639,"duration":36639}},{"uid":"c331e1b7d91ad959","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1659451524000,"stop":1659451584358,"duration":60358}},{"uid":"edc7dd8185a598bb","status":"passed","time":{"start":1659447298000,"stop":1659447301305,"duration":3305}},{"uid":"45c09e1f261b960","status":"passed","time":{"start":1659447116000,"stop":1659447138934,"duration":22934}},{"uid":"44c564d73bc0f695","status":"passed","time":{"start":1659448205000,"stop":1659448209454,"duration":4454}},{"uid":"36c907ffa7d37e0c","status":"passed","time":{"start":1659448097000,"stop":1659448101473,"duration":4473}},{"uid":"c4c172f50291357","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1659409279000,"stop":1659409339947,"duration":60947}},{"uid":"98a727d564a41f4c","status":"passed","time":{"start":1659402555000,"stop":1659402585583,"duration":30583}},{"uid":"d2acfc38f25b69ff","status":"passed","time":{"start":1659291287000,"stop":1659291291512,"duration":4512}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests slice with netpol should install slice on each worker cluster with correct namespaceisolationprofile":{"statistic":{"failed":0,"broken":1,"skipped":24,"passed":10,"unknown":0,"total":35},"items":[{"uid":"ec51288878fc5a11","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"76ba8125e909631a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"90307464dcdff2ea","status":"broken","statusDetails":"runtime error: invalid memory address or nil pointer dereference","time":{"start":1660067259000,"stop":1660067272200,"duration":13200}},{"uid":"6cbfa9832ad737b5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}},{"uid":"403ffcb6c198bd03","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659982549000,"stop":1659982549000,"duration":0}},{"uid":"fa9b719fe109b074","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659970182000,"stop":1659970182000,"duration":0}},{"uid":"d5aae204fb47f7fc","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659944495000,"stop":1659944495000,"duration":0}},{"uid":"59c02aef97f9e552","status":"passed","time":{"start":1659874977000,"stop":1659874977132,"duration":132}},{"uid":"9987cde71c4b5d60","status":"passed","time":{"start":1659496050000,"stop":1659496050134,"duration":134}},{"uid":"5b454783fba4bc6c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496083000,"stop":1659496083000,"duration":0}},{"uid":"4971df4f3f5485c9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"e8f20504e5a02d03","status":"passed","time":{"start":1659450916000,"stop":1659450916255,"duration":255}},{"uid":"afd3f0f77a6bb672","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"cf49f17dfff515d4","status":"passed","time":{"start":1659447298000,"stop":1659447300329,"duration":2329}},{"uid":"22b7bf83539454f","status":"passed","time":{"start":1659447116000,"stop":1659447116154,"duration":154}},{"uid":"54522e06a9c70b1f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448205000,"stop":1659448205000,"duration":0}},{"uid":"bb177bfa1a7cf266","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448097000,"stop":1659448097000,"duration":0}},{"uid":"53d016de1d398ed7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659409279000,"stop":1659409279000,"duration":0}},{"uid":"66a4f1fa57b050fd","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659402555000,"stop":1659402555000,"duration":0}},{"uid":"322be089f56c2811","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659291287000,"stop":1659291287000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Namespace reconcile test scenarios: Delete deboarded app ns gets deleted":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":35,"unknown":0,"total":35},"items":[{"uid":"2831a67a404a7357","status":"passed","time":{"start":1660113628000,"stop":1660113638487,"duration":10487}},{"uid":"8f676df91669f63","status":"passed","time":{"start":1660104898000,"stop":1660104908574,"duration":10574}},{"uid":"cd93f51a1ab83ad3","status":"passed","time":{"start":1660067259000,"stop":1660067269534,"duration":10534}},{"uid":"13fcd3859cc66725","status":"passed","time":{"start":1660047551000,"stop":1660047562318,"duration":11318}},{"uid":"b9ac4c54f7882360","status":"passed","time":{"start":1659982549000,"stop":1659982560511,"duration":11511}},{"uid":"7078296c0949d1b2","status":"passed","time":{"start":1659970182000,"stop":1659970193563,"duration":11563}},{"uid":"82b61b916305481b","status":"passed","time":{"start":1659944495000,"stop":1659944505858,"duration":10858}},{"uid":"e3e4c9cca4c428fa","status":"passed","time":{"start":1659874977000,"stop":1659874987822,"duration":10822}},{"uid":"302f5fc2e67c7319","status":"passed","time":{"start":1659496050000,"stop":1659496060456,"duration":10456}},{"uid":"b49a5854e44e3829","status":"passed","time":{"start":1659496083000,"stop":1659496093545,"duration":10545}},{"uid":"cb718d0bfdd9f774","status":"passed","time":{"start":1659451501000,"stop":1659451511554,"duration":10554}},{"uid":"2e6fea764baab7e0","status":"passed","time":{"start":1659450916000,"stop":1659450927053,"duration":11053}},{"uid":"3df2d0f6ed5e9fd3","status":"passed","time":{"start":1659451524000,"stop":1659451535530,"duration":11530}},{"uid":"ced8dd2e69aa5a8","status":"passed","time":{"start":1659447298000,"stop":1659447308599,"duration":10599}},{"uid":"bd596f220a630598","status":"passed","time":{"start":1659447116000,"stop":1659447126500,"duration":10500}},{"uid":"c79eb12720ea847d","status":"passed","time":{"start":1659448205000,"stop":1659448215609,"duration":10609}},{"uid":"d40f1210e901db9","status":"passed","time":{"start":1659448097000,"stop":1659448107736,"duration":10736}},{"uid":"3f14d94699658626","status":"passed","time":{"start":1659409279000,"stop":1659409289945,"duration":10945}},{"uid":"82cf4a20e9b716f2","status":"passed","time":{"start":1659402555000,"stop":1659402565774,"duration":10774}},{"uid":"e9971212ef59f5e1","status":"passed","time":{"start":1659291287000,"stop":1659291297605,"duration":10605}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting mesh-dns pod Check ping between iperf-server and iperf-client after mesh-dns pod restart":{"statistic":{"failed":0,"broken":0,"skipped":35,"passed":0,"unknown":0,"total":35},"items":[{"uid":"9a90dd3bcc877f76","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"d67a0bffcb44f2a0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"5ebbcb010527076","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"2664ad03adc8ecca","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}},{"uid":"6a7c247e6bf8fd66","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659982549000,"stop":1659982549000,"duration":0}},{"uid":"37ac04dd656c246f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659970182000,"stop":1659970182000,"duration":0}},{"uid":"be287ec5f222a53b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659944495000,"stop":1659944495000,"duration":0}},{"uid":"2bf4cd3d8954f19d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659874977000,"stop":1659874977000,"duration":0}},{"uid":"8f4690fb2dbbbf06","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496050000,"stop":1659496050000,"duration":0}},{"uid":"1a7a52786a0a8e13","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496083000,"stop":1659496083000,"duration":0}},{"uid":"12dfb30642958397","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"ff5e7d4726cbb999","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659450916000,"stop":1659450916000,"duration":0}},{"uid":"890b78ee163eae11","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"c84b9a00f55c6c8d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447298000,"stop":1659447298000,"duration":0}},{"uid":"62beff7c1453800e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447116000,"stop":1659447116000,"duration":0}},{"uid":"7777d458be9e5e75","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448205000,"stop":1659448205000,"duration":0}},{"uid":"3749f76d89b3d67e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448097000,"stop":1659448097000,"duration":0}},{"uid":"3cd6e1cb2ba2f28a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659409279000,"stop":1659409279000,"duration":0}},{"uid":"5679f603a1a420c9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659402555000,"stop":1659402555000,"duration":0}},{"uid":"ce0df9f340ec0662","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659291287000,"stop":1659291287000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice creation scenarios: when slice red is installed install slice blue workers should get attached to slice blue":{"statistic":{"failed":0,"broken":0,"skipped":23,"passed":12,"unknown":0,"total":35},"items":[{"uid":"915e33568f1b60f0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"6318749ab84150f0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"20ab880307c7c7a8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"ca501e21f13e3dbf","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}},{"uid":"4660aacdce62685a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659982549000,"stop":1659982549000,"duration":0}},{"uid":"f3c33ef94876bd2b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659970182000,"stop":1659970182000,"duration":0}},{"uid":"ecf08bf8622b2ee","status":"passed","time":{"start":1659944495000,"stop":1659944497277,"duration":2277}},{"uid":"1db3c3d39b20d8bc","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659874977000,"stop":1659874977000,"duration":0}},{"uid":"49f77de0f662eb1b","status":"passed","time":{"start":1659496050000,"stop":1659496050459,"duration":459}},{"uid":"5f8b1d4a52c83682","status":"passed","time":{"start":1659496083000,"stop":1659496084771,"duration":1771}},{"uid":"1701bfa31be17b24","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"d081d016ea6b2cab","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659450916000,"stop":1659450916000,"duration":0}},{"uid":"8f4eaacdd311b8c4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"fabdac3c5410891e","status":"passed","time":{"start":1659447298000,"stop":1659447299728,"duration":1728}},{"uid":"827d13e1354407a4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447116000,"stop":1659447116000,"duration":0}},{"uid":"810f052b5bf059c8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448205000,"stop":1659448205000,"duration":0}},{"uid":"150fe0fec73aa79","status":"passed","time":{"start":1659448097000,"stop":1659448097593,"duration":593}},{"uid":"ef3cd3e274bd7373","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659409279000,"stop":1659409279000,"duration":0}},{"uid":"c5781d94232959f6","status":"passed","time":{"start":1659402555000,"stop":1659402557128,"duration":2128}},{"uid":"c99e417922da91f7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659291287000,"stop":1659291287000,"duration":0}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests slice with netpol should have vl3 router running":{"statistic":{"failed":23,"broken":0,"skipped":0,"passed":12,"unknown":0,"total":35},"items":[{"uid":"9006817b9c332608","status":"passed","time":{"start":1660113628000,"stop":1660113649243,"duration":21243}},{"uid":"e827510464e03727","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 4\nto equal\n    <int>: 1","time":{"start":1660104898000,"stop":1660105093414,"duration":195414}},{"uid":"d064732320ea6e1e","status":"passed","time":{"start":1660067259000,"stop":1660067259349,"duration":349}},{"uid":"7e2022ee47a9ac83","status":"failed","statusDetails":"Unexpected error:\n    <*errors.StatusError | 0xc00069a280>: {\n        ErrStatus: {\n            TypeMeta: {Kind: \"\", APIVersion: \"\"},\n            ListMeta: {\n                SelfLink: \"\",\n                ResourceVersion: \"\",\n                Continue: \"\",\n                RemainingItemCount: nil,\n            },\n            Status: \"Failure\",\n            Message: \"namespaces \\\"iperf\\\" already exists\",\n            Reason: \"AlreadyExists\",\n            Details: {Name: \"iperf\", Group: \"\", Kind: \"namespaces\", UID: \"\", Causes: nil, RetryAfterSeconds: 0},\n            Code: 409,\n        },\n    }\n    namespaces \"iperf\" already exists\noccurred","time":{"start":1660047551000,"stop":1660047559604,"duration":8604}},{"uid":"228eb1717e3625a5","status":"failed","statusDetails":"Unexpected error:\n    <*errors.StatusError | 0xc000394280>: {\n        ErrStatus: {\n            TypeMeta: {Kind: \"\", APIVersion: \"\"},\n            ListMeta: {\n                SelfLink: \"\",\n                ResourceVersion: \"\",\n                Continue: \"\",\n                RemainingItemCount: nil,\n            },\n            Status: \"Failure\",\n            Message: \"namespaces \\\"iperf\\\" already exists\",\n            Reason: \"AlreadyExists\",\n            Details: {Name: \"iperf\", Group: \"\", Kind: \"namespaces\", UID: \"\", Causes: nil, RetryAfterSeconds: 0},\n            Code: 409,\n        },\n    }\n    namespaces \"iperf\" already exists\noccurred","time":{"start":1659982549000,"stop":1659982559138,"duration":10138}},{"uid":"7a13ad7261e61eeb","status":"failed","statusDetails":"Unexpected error:\n    <*errors.StatusError | 0xc0006e0dc0>: {\n        ErrStatus: {\n            TypeMeta: {Kind: \"\", APIVersion: \"\"},\n            ListMeta: {\n                SelfLink: \"\",\n                ResourceVersion: \"\",\n                Continue: \"\",\n                RemainingItemCount: nil,\n            },\n            Status: \"Failure\",\n            Message: \"namespaces \\\"iperf\\\" already exists\",\n            Reason: \"AlreadyExists\",\n            Details: {Name: \"iperf\", Group: \"\", Kind: \"namespaces\", UID: \"\", Causes: nil, RetryAfterSeconds: 0},\n            Code: 409,\n        },\n    }\n    namespaces \"iperf\" already exists\noccurred","time":{"start":1659970182000,"stop":1659970190617,"duration":8617}},{"uid":"14d087b532cce9ff","status":"failed","statusDetails":"Unexpected error:\n    <*errors.StatusError | 0xc000623b80>: {\n        ErrStatus: {\n            TypeMeta: {Kind: \"\", APIVersion: \"\"},\n            ListMeta: {\n                SelfLink: \"\",\n                ResourceVersion: \"\",\n                Continue: \"\",\n                RemainingItemCount: nil,\n            },\n            Status: \"Failure\",\n            Message: \"namespaces \\\"iperf\\\" already exists\",\n            Reason: \"AlreadyExists\",\n            Details: {Name: \"iperf\", Group: \"\", Kind: \"namespaces\", UID: \"\", Causes: nil, RetryAfterSeconds: 0},\n            Code: 409,\n        },\n    }\n    namespaces \"iperf\" already exists\noccurred","time":{"start":1659944495000,"stop":1659944504718,"duration":9718}},{"uid":"b62d7ebb05e5a750","status":"passed","time":{"start":1659874977000,"stop":1659874981259,"duration":4259}},{"uid":"4a93dfd562fc842c","status":"passed","time":{"start":1659496050000,"stop":1659496052423,"duration":2423}},{"uid":"61fb6ffe68b4292d","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 4\nto equal\n    <int>: 1","time":{"start":1659496083000,"stop":1659496278462,"duration":195462}},{"uid":"1d9ce49b513f6cd2","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1659451501000,"stop":1659451697487,"duration":196487}},{"uid":"5665676d1af1e9e","status":"passed","time":{"start":1659450916000,"stop":1659450916458,"duration":458}},{"uid":"e3ec03f44d19d720","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1659451524000,"stop":1659451720050,"duration":196050}},{"uid":"2077c6886480d7c0","status":"passed","time":{"start":1659447298000,"stop":1659447298229,"duration":229}},{"uid":"4461fd0744d8df7","status":"passed","time":{"start":1659447116000,"stop":1659447120325,"duration":4325}},{"uid":"85380ea922ac0c39","status":"failed","statusDetails":"Unexpected error:\n    <*errors.StatusError | 0xc000a152c0>: {\n        ErrStatus: {\n            TypeMeta: {Kind: \"\", APIVersion: \"\"},\n            ListMeta: {\n                SelfLink: \"\",\n                ResourceVersion: \"\",\n                Continue: \"\",\n                RemainingItemCount: nil,\n            },\n            Status: \"Failure\",\n            Message: \"namespaces \\\"iperf\\\" already exists\",\n            Reason: \"AlreadyExists\",\n            Details: {Name: \"iperf\", Group: \"\", Kind: \"namespaces\", UID: \"\", Causes: nil, RetryAfterSeconds: 0},\n            Code: 409,\n        },\n    }\n    namespaces \"iperf\" already exists\noccurred","time":{"start":1659448205000,"stop":1659448208194,"duration":3194}},{"uid":"318172e2bbb19364","status":"failed","statusDetails":"Timed out after 180.002s.\nExpected\n    <int>: 3\nto equal\n    <int>: 1","time":{"start":1659448097000,"stop":1659448292524,"duration":195524}},{"uid":"ccbf91f96e5e4ad5","status":"failed","statusDetails":"Unexpected error:\n    <*errors.StatusError | 0xc0001b0820>: {\n        ErrStatus: {\n            TypeMeta: {Kind: \"\", APIVersion: \"\"},\n            ListMeta: {\n                SelfLink: \"\",\n                ResourceVersion: \"\",\n                Continue: \"\",\n                RemainingItemCount: nil,\n            },\n            Status: \"Failure\",\n            Message: \"namespaces \\\"iperf\\\" already exists\",\n            Reason: \"AlreadyExists\",\n            Details: {Name: \"iperf\", Group: \"\", Kind: \"namespaces\", UID: \"\", Causes: nil, RetryAfterSeconds: 0},\n            Code: 409,\n        },\n    }\n    namespaces \"iperf\" already exists\noccurred","time":{"start":1659409279000,"stop":1659409287997,"duration":8997}},{"uid":"2f886b25f537bdd","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 4\nto equal\n    <int>: 1","time":{"start":1659402555000,"stop":1659402750804,"duration":195804}},{"uid":"8459a6810e10a07d","status":"failed","statusDetails":"Unexpected error:\n    <*errors.StatusError | 0xc0003bf360>: {\n        ErrStatus: {\n            TypeMeta: {Kind: \"\", APIVersion: \"\"},\n            ListMeta: {\n                SelfLink: \"\",\n                ResourceVersion: \"\",\n                Continue: \"\",\n                RemainingItemCount: nil,\n            },\n            Status: \"Failure\",\n            Message: \"namespaces \\\"iperf\\\" already exists\",\n            Reason: \"AlreadyExists\",\n            Details: {Name: \"iperf\", Group: \"\", Kind: \"namespaces\", UID: \"\", Causes: nil, RetryAfterSeconds: 0},\n            Code: 409,\n        },\n    }\n    namespaces \"iperf\" already exists\noccurred","time":{"start":1659291287000,"stop":1659291302524,"duration":15524}}]},"Worker Suite:Worker Suite#[It] Slice validation Slice with MEDIUM QOS should have vl3 router running":{"statistic":{"failed":3,"broken":0,"skipped":0,"passed":6,"unknown":0,"total":9},"items":[{"uid":"24dbffb43308dcfe","status":"passed","time":{"start":1659168739000,"stop":1659168739689,"duration":689}},{"uid":"32d8b5c697e6c847","status":"passed","time":{"start":1659164084000,"stop":1659164084631,"duration":631}},{"uid":"49c7996737d1d261","status":"passed","time":{"start":1659160188000,"stop":1659160188690,"duration":690}},{"uid":"dad5cb3f70e541bb","status":"passed","time":{"start":1659119724000,"stop":1659119724538,"duration":538}},{"uid":"3696a0b392e3f2eb","status":"passed","time":{"start":1659116511000,"stop":1659116511419,"duration":419}},{"uid":"df9e0e068ede27d4","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 3\nto equal\n    <int>: 1","time":{"start":1659109470000,"stop":1659109530750,"duration":60750}},{"uid":"b166459c6ae18a80","status":"passed","time":{"start":1659106836000,"stop":1659106836774,"duration":774}},{"uid":"55bf1b7112ff65f6","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1659101522000,"stop":1659101582765,"duration":60765}},{"uid":"6b9bc19dfb598b06","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <int>: 4\nto equal\n    <int>: 1","time":{"start":1659082816000,"stop":1659082876914,"duration":60914}}]},"Worker Suite:Worker Suite#[It] Slice validation Slice with MEDIUM QOS should reconcile medium qos slice":{"statistic":{"failed":5,"broken":0,"skipped":4,"passed":0,"unknown":0,"total":9},"items":[{"uid":"531abd47defaec11","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004a9788>: {\n        Underlying: <*exec.ExitError | 0xc00040d860>{\n            ProcessState: {\n                pid: 7260,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 72127},\n                    Stime: {Sec: 0, Usec: 18031},\n                    Maxrss: 43584,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4297,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 230,\n                    Nivcsw: 359,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicemediumqos\\\" not found\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicemediumqos\\\" not found\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicemediumqos\\\" not found\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicemediumqos\\\" not found\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (NotFound): slice.networking.kubeslice.io \"slicemediumqos\" not found\noccurred","time":{"start":1659168739000,"stop":1659168740414,"duration":1414}},{"uid":"46130730b1f11a1a","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00087c120>: {\n        Underlying: <*exec.ExitError | 0xc00069c5e0>{\n            ProcessState: {\n                pid: 7406,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 74896},\n                    Stime: {Sec: 0, Usec: 12482},\n                    Maxrss: 43348,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3354,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 199,\n                    Nivcsw: 297,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicemediumqos\\\" not found\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicemediumqos\\\" not found\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicemediumqos\\\" not found\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicemediumqos\\\" not found\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (NotFound): slice.networking.kubeslice.io \"slicemediumqos\" not found\noccurred","time":{"start":1659164084000,"stop":1659164085098,"duration":1098}},{"uid":"cdf275d41a8481c7","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00049ad20>: {\n        Underlying: <*exec.ExitError | 0xc000843c20>{\n            ProcessState: {\n                pid: 7452,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 44988},\n                    Stime: {Sec: 0, Usec: 22494},\n                    Maxrss: 41588,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2497,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 210,\n                    Nivcsw: 113,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicemediumqos\\\" not found\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicemediumqos\\\" not found\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicemediumqos\\\" not found\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicemediumqos\\\" not found\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (NotFound): slice.networking.kubeslice.io \"slicemediumqos\" not found\noccurred","time":{"start":1659160188000,"stop":1659160188732,"duration":732}},{"uid":"7f1c6ad507808e9d","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000347c38>: {\n        Underlying: <*exec.ExitError | 0xc000778740>{\n            ProcessState: {\n                pid: 8030,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 59177},\n                    Stime: {Sec: 0, Usec: 12680},\n                    Maxrss: 42032,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2537,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 257,\n                    Nivcsw: 140,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicemediumqos\\\" not found\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicemediumqos\\\" not found\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicemediumqos\\\" not found\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicemediumqos\\\" not found\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (NotFound): slice.networking.kubeslice.io \"slicemediumqos\" not found\noccurred","time":{"start":1659119724000,"stop":1659119725126,"duration":1126}},{"uid":"88a740814282788f","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004e9560>: {\n        Underlying: <*exec.ExitError | 0xc0004510c0>{\n            ProcessState: {\n                pid: 7542,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 66414},\n                    Stime: {Sec: 0, Usec: 15626},\n                    Maxrss: 45792,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2977,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 225,\n                    Nivcsw: 318,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicemediumqos\\\" not found\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicemediumqos\\\" not found\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicemediumqos\\\" not found\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicemediumqos\\\" not found\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (NotFound): slice.networking.kubeslice.io \"slicemediumqos\" not found\noccurred","time":{"start":1659116511000,"stop":1659116511874,"duration":874}},{"uid":"f1ae2dd55b9e07f8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659109470000,"stop":1659109470000,"duration":0}},{"uid":"afe8c318d5877363","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659106836000,"stop":1659106836000,"duration":0}},{"uid":"cffa1fbdfce28b40","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659101522000,"stop":1659101522000,"duration":0}},{"uid":"ce0be5d753f333ab","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659082816000,"stop":1659082816000,"duration":0}}]},"Hub Suite:Hub Suite#[It] Cluster negative tests Worker Cluster Registration with Wrong clustername":{"statistic":{"failed":0,"broken":0,"skipped":17,"passed":72,"unknown":0,"total":89},"items":[{"uid":"27f87dbe1f7fb25d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660145110000,"stop":1660145110000,"duration":0}},{"uid":"fe98ebb54179e92b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660142645000,"stop":1660142645000,"duration":0}},{"uid":"c9f78f25f7d1eccc","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660141834000,"stop":1660141834000,"duration":0}},{"uid":"b36e37142ba16fbb","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660138561000,"stop":1660138561000,"duration":0}},{"uid":"41608ae8f4917cb5","status":"passed","time":{"start":1660116744000,"stop":1660116933853,"duration":189853}},{"uid":"862225b6a290f019","status":"passed","time":{"start":1660116144000,"stop":1660116331012,"duration":187012}},{"uid":"970b64e15b5fb3ca","status":"passed","time":{"start":1660112860000,"stop":1660113047288,"duration":187288}},{"uid":"1e293131b59847df","status":"passed","time":{"start":1660108970000,"stop":1660109109020,"duration":139020}},{"uid":"d6200db92ea8da02","status":"passed","time":{"start":1660104124000,"stop":1660104313006,"duration":189006}},{"uid":"eb4e0478a540f543","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104760000,"stop":1660104760000,"duration":0}},{"uid":"c401e09ce5a20d1e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660101116000,"stop":1660101116000,"duration":0}},{"uid":"39476d224ff9e509","status":"passed","time":{"start":1660100168000,"stop":1660100212696,"duration":44696}},{"uid":"f24f6dd8cf4faef9","status":"passed","time":{"start":1660066476000,"stop":1660066669333,"duration":193333}},{"uid":"8946d86a5fcff73a","status":"passed","time":{"start":1660064121000,"stop":1660064308796,"duration":187796}},{"uid":"5b85983cb2973b2d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660060680000,"stop":1660060680000,"duration":0}},{"uid":"5df9944e40738c65","status":"passed","time":{"start":1660052169000,"stop":1660052357956,"duration":188956}},{"uid":"13aa53b755cae249","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660050481000,"stop":1660050481000,"duration":0}},{"uid":"2a76f3c78e079756","status":"passed","time":{"start":1660046776000,"stop":1660046963385,"duration":187385}},{"uid":"f04e9b747b779c54","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660046048000,"stop":1660046048000,"duration":0}},{"uid":"38b2fef929ce14e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660044532000,"stop":1660044532000,"duration":0}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Update Should fail when Project is Applied with service account name as blank in Read users":{"statistic":{"failed":7,"broken":0,"skipped":0,"passed":79,"unknown":0,"total":86},"items":[{"uid":"b7d1ef421cb847ca","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0003a1ad0>: {\n        Underlying: <*exec.ExitError | 0xc00049cec0>{\n            ProcessState: {\n                pid: 6241,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 149900},\n                    Stime: {Sec: 0, Usec: 72809},\n                    Maxrss: 82512,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 6131,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13376,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 454,\n                    Nivcsw: 178,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The Project \"projectupdatetest\" is invalid: project namespace: Invalid value: \"kubeslice-projectupdatetest\": already exists\noccurred","time":{"start":1660145110000,"stop":1660145170319,"duration":60319}},{"uid":"2a139f2782d56d40","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0008daa38>: {\n        Underlying: <*exec.ExitError | 0xc000795ec0>{\n            ProcessState: {\n                pid: 6345,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 169006},\n                    Stime: {Sec: 0, Usec: 37099},\n                    Maxrss: 73244,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 10457,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13392,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 392,\n                    Nivcsw: 329,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The Project \"projectupdatetest\" is invalid: project namespace: Invalid value: \"kubeslice-projectupdatetest\": already exists\noccurred","time":{"start":1660142645000,"stop":1660142705253,"duration":60253}},{"uid":"6d200ad150d039a6","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004bf740>: {\n        Underlying: <*exec.ExitError | 0xc0005e62a0>{\n            ProcessState: {\n                pid: 6167,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 129860},\n                    Stime: {Sec: 0, Usec: 37102},\n                    Maxrss: 78348,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 11115,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13392,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 585,\n                    Nivcsw: 218,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The Project \"projectupdatetest\" is invalid: project namespace: Invalid value: \"kubeslice-projectupdatetest\": already exists\noccurred","time":{"start":1660141834000,"stop":1660141894217,"duration":60217}},{"uid":"e10473aa50dee902","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000eb10>: {\n        Underlying: <*exec.ExitError | 0xc00044a340>{\n            ProcessState: {\n                pid: 6203,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 123735},\n                    Stime: {Sec: 0, Usec: 55880},\n                    Maxrss: 80836,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 14145,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13376,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 355,\n                    Nivcsw: 335,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The Project \"projectupdatetest\" is invalid: project namespace: Invalid value: \"kubeslice-projectupdatetest\": already exists\noccurred","time":{"start":1660138561000,"stop":1660138621251,"duration":60251}},{"uid":"d3cbdfc53212702b","status":"passed","time":{"start":1660116744000,"stop":1660116748889,"duration":4889}},{"uid":"c525a0c0178d7761","status":"passed","time":{"start":1660116144000,"stop":1660116148689,"duration":4689}},{"uid":"d34a7dda223c5c6c","status":"passed","time":{"start":1660112860000,"stop":1660112864703,"duration":4703}},{"uid":"4fa290d1f1fe69b5","status":"passed","time":{"start":1660108970000,"stop":1660108974746,"duration":4746}},{"uid":"1a61ba88b76e7d38","status":"passed","time":{"start":1660104124000,"stop":1660104129832,"duration":5832}},{"uid":"7b4acf8dfe655038","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000864f78>: {\n        Underlying: <*exec.ExitError | 0xc0006485c0>{\n            ProcessState: {\n                pid: 6317,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 134367},\n                    Stime: {Sec: 0, Usec: 50387},\n                    Maxrss: 76040,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 9294,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13392,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 394,\n                    Nivcsw: 324,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The Project \"projectupdatetest\" is invalid: project namespace: Invalid value: \"kubeslice-projectupdatetest\": already exists\noccurred","time":{"start":1660104760000,"stop":1660104820280,"duration":60280}},{"uid":"9e4e7cc4e080c93","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00069c018>: {\n        Underlying: <*exec.ExitError | 0xc0007f8040>{\n            ProcessState: {\n                pid: 6151,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 113830},\n                    Stime: {Sec: 0, Usec: 27476},\n                    Maxrss: 78888,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3854,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13376,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 561,\n                    Nivcsw: 254,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The Project \"projectupdatetest\" is invalid: project namespace: Invalid value: \"kubeslice-projectupdatetest\": already exists\noccurred","time":{"start":1660101116000,"stop":1660101176222,"duration":60222}},{"uid":"b2c7adfe19922c3a","status":"passed","time":{"start":1660100168000,"stop":1660100172816,"duration":4816}},{"uid":"c62ef947c0de1022","status":"passed","time":{"start":1660066476000,"stop":1660066480670,"duration":4670}},{"uid":"409ad06a880e860e","status":"passed","time":{"start":1660064121000,"stop":1660064125704,"duration":4704}},{"uid":"3cf1120f7d333096","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00034ec18>: {\n        Underlying: <*exec.ExitError | 0xc00061cf80>{\n            ProcessState: {\n                pid: 6226,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 148110},\n                    Stime: {Sec: 0, Usec: 59244},\n                    Maxrss: 74228,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 12641,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13392,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 449,\n                    Nivcsw: 456,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The Project \"projectupdatetest\" is invalid: project namespace: Invalid value: \"kubeslice-projectupdatetest\": already exists\noccurred","time":{"start":1660060680000,"stop":1660060740265,"duration":60265}},{"uid":"f0da5356a76a6dae","status":"passed","time":{"start":1660052169000,"stop":1660052174888,"duration":5888}},{"uid":"4ca09f85c146a4d7","status":"passed","time":{"start":1660050481000,"stop":1660050486834,"duration":5834}},{"uid":"2452ac2ab139d1f3","status":"passed","time":{"start":1660046776000,"stop":1660046780660,"duration":4660}},{"uid":"6d68eaec30337363","status":"passed","time":{"start":1660046048000,"stop":1660046052649,"duration":4649}},{"uid":"2776ced793b66d94","status":"passed","time":{"start":1660044532000,"stop":1660044537696,"duration":5696}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting mesh-dns pod Should restart mesh-dns pod":{"statistic":{"failed":0,"broken":0,"skipped":35,"passed":0,"unknown":0,"total":35},"items":[{"uid":"be37a4982c5d2ea4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"22b3e07e8562cb","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"6087ee1466eafde6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"24b8df24665b65fc","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}},{"uid":"53c64af72ee9723d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659982549000,"stop":1659982549000,"duration":0}},{"uid":"9838ea87ba03d216","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659970182000,"stop":1659970182000,"duration":0}},{"uid":"df430113d776a1f3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659944495000,"stop":1659944495000,"duration":0}},{"uid":"7824264acb7db787","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659874977000,"stop":1659874977000,"duration":0}},{"uid":"f7f85e0279f839e0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496050000,"stop":1659496050000,"duration":0}},{"uid":"af58336646fdf682","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496083000,"stop":1659496083000,"duration":0}},{"uid":"f5f09aefb76b112d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"dc6080f579816089","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659450916000,"stop":1659450916000,"duration":0}},{"uid":"9b3658bbd778a2e6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"59fe015355d48e66","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447298000,"stop":1659447298000,"duration":0}},{"uid":"b866e7882a56e578","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447116000,"stop":1659447116000,"duration":0}},{"uid":"ac2e9a9a0c725964","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448205000,"stop":1659448205000,"duration":0}},{"uid":"f0de354044da0a8f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448097000,"stop":1659448097000,"duration":0}},{"uid":"e67fddc545935b47","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659409279000,"stop":1659409279000,"duration":0}},{"uid":"eebfb553b011f322","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659402555000,"stop":1659402555000,"duration":0}},{"uid":"51c9d3b769808fe2","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659291287000,"stop":1659291287000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Namespace reconcile test scenarios: Delete deleted app ns entry should get removed from cluster objs":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":35,"unknown":0,"total":35},"items":[{"uid":"5f0118be17c85c16","status":"passed","time":{"start":1660113628000,"stop":1660113638889,"duration":10889}},{"uid":"8df3fb4cbb9def6c","status":"passed","time":{"start":1660104898000,"stop":1660104909074,"duration":11074}},{"uid":"65a0fe8d141d0c32","status":"passed","time":{"start":1660067259000,"stop":1660067269991,"duration":10991}},{"uid":"42d748857c27a54b","status":"passed","time":{"start":1660047551000,"stop":1660047562383,"duration":11383}},{"uid":"b209b72dc0422951","status":"passed","time":{"start":1659982549000,"stop":1659982561359,"duration":12359}},{"uid":"53a9db4c1f7db2f3","status":"passed","time":{"start":1659970182000,"stop":1659970193965,"duration":11965}},{"uid":"66834cb18738a6f0","status":"passed","time":{"start":1659944495000,"stop":1659944506259,"duration":11259}},{"uid":"f12fe9161ce48b7d","status":"passed","time":{"start":1659874977000,"stop":1659874988105,"duration":11105}},{"uid":"c86fdcd27d17cab","status":"passed","time":{"start":1659496050000,"stop":1659496060888,"duration":10888}},{"uid":"4787146e1cbb6641","status":"passed","time":{"start":1659496083000,"stop":1659496094084,"duration":11084}},{"uid":"e60c7e6600ef522b","status":"passed","time":{"start":1659451501000,"stop":1659451511948,"duration":10948}},{"uid":"67d27253ffbd920e","status":"passed","time":{"start":1659450916000,"stop":1659450927991,"duration":11991}},{"uid":"ab220c411c1137d7","status":"passed","time":{"start":1659451524000,"stop":1659451536762,"duration":12762}},{"uid":"2444cc84e12d8565","status":"passed","time":{"start":1659447298000,"stop":1659447308923,"duration":10923}},{"uid":"4e7b8e9a180536a6","status":"passed","time":{"start":1659447116000,"stop":1659447126973,"duration":10973}},{"uid":"46a0d5a4cf2840a2","status":"passed","time":{"start":1659448205000,"stop":1659448216160,"duration":11160}},{"uid":"c2130a156c0d55f","status":"passed","time":{"start":1659448097000,"stop":1659448108345,"duration":11345}},{"uid":"af4fcb0a6171900d","status":"passed","time":{"start":1659409279000,"stop":1659409290962,"duration":11962}},{"uid":"37fac44050e6c084","status":"passed","time":{"start":1659402555000,"stop":1659402566396,"duration":11396}},{"uid":"9b1bf0abde58e134","status":"passed","time":{"start":1659291287000,"stop":1659291298071,"duration":11071}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting nsm-manager pod Should restart nsm-manager pod":{"statistic":{"failed":0,"broken":0,"skipped":35,"passed":0,"unknown":0,"total":35},"items":[{"uid":"db55039887229fe","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"1c60e4002c5b7046","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"5cce28e3a0c00b9f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"ea20699f2b725d82","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}},{"uid":"48578733b94b5969","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659982549000,"stop":1659982549000,"duration":0}},{"uid":"2b9a2c67aafc1981","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659970182000,"stop":1659970182000,"duration":0}},{"uid":"ef688033641e9cd3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659944495000,"stop":1659944495000,"duration":0}},{"uid":"d13f544a0c920b38","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659874977000,"stop":1659874977000,"duration":0}},{"uid":"3a74c94948d60ce4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496050000,"stop":1659496050000,"duration":0}},{"uid":"d2588fcbbf054d3a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496083000,"stop":1659496083000,"duration":0}},{"uid":"8d235d38f7079c70","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"69a5604d0e834f69","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659450916000,"stop":1659450916000,"duration":0}},{"uid":"51c3833b2de4f5fc","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"5618f416588183c3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447298000,"stop":1659447298000,"duration":0}},{"uid":"3d3d67ba339b1737","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447116000,"stop":1659447116000,"duration":0}},{"uid":"71dc6633c6def77b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448205000,"stop":1659448205000,"duration":0}},{"uid":"f9d50d60326b6522","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448097000,"stop":1659448097000,"duration":0}},{"uid":"16d65effa33ed6b4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659409279000,"stop":1659409279000,"duration":0}},{"uid":"cccbb11feeee323e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659402555000,"stop":1659402555000,"duration":0}},{"uid":"552ca9168e85303f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659291287000,"stop":1659291287000,"duration":0}}]},"Hub Suite:Hub Suite#[It]  QosProfile tests QosProfile Create validation Creates qosprofile":{"statistic":{"failed":81,"broken":0,"skipped":0,"passed":0,"unknown":0,"total":81},"items":[{"uid":"6613b1c42f5fa37b","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000532408>: {\n        Underlying: <*exec.ExitError | 0xc0004ee1a0>{\n            ProcessState: {\n                pid: 6092,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 150451},\n                    Stime: {Sec: 0, Usec: 59388},\n                    Maxrss: 78632,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 5057,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13616,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 1001,\n                    Nivcsw: 446,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile2940049709\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile2940049709\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile2940049709\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile2940049709\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile2940049709\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1660145110000,"stop":1660145117576,"duration":7576}},{"uid":"b73b9a0410850db8","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0005d73c8>: {\n        Underlying: <*exec.ExitError | 0xc000629080>{\n            ProcessState: {\n                pid: 6043,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 134135},\n                    Stime: {Sec: 0, Usec: 81294},\n                    Maxrss: 80212,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 11292,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13616,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 851,\n                    Nivcsw: 518,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile2832119038\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile2832119038\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile2832119038\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile2832119038\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile2832119038\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1660142645000,"stop":1660142652579,"duration":7579}},{"uid":"ebd20dbe700632f7","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0003510e0>: {\n        Underlying: <*exec.ExitError | 0xc00061a7e0>{\n            ProcessState: {\n                pid: 6057,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 134812},\n                    Stime: {Sec: 0, Usec: 50073},\n                    Maxrss: 80236,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3292,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14288,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 700,\n                    Nivcsw: 324,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile2241604685\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile2241604685\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile2241604685\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile2241604685\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile2241604685\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1660141834000,"stop":1660141895476,"duration":61476}},{"uid":"cb10c1293ec9584b","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000480ac8>: {\n        Underlying: <*exec.ExitError | 0xc00060e900>{\n            ProcessState: {\n                pid: 6236,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 159343},\n                    Stime: {Sec: 0, Usec: 62182},\n                    Maxrss: 72504,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 15031,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14288,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 1017,\n                    Nivcsw: 499,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile4265601235\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile4265601235\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile4265601235\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile4265601235\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile4265601235\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1660138561000,"stop":1660138622535,"duration":61535}},{"uid":"f10f685ef6f57def","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004c04f8>: {\n        Underlying: <*exec.ExitError | 0xc00085a060>{\n            ProcessState: {\n                pid: 6651,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 179448},\n                    Stime: {Sec: 0, Usec: 81567},\n                    Maxrss: 75880,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4179,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14104,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 857,\n                    Nivcsw: 425,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile514527415\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile514527415\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile514527415\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile514527415\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile514527415\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1660116744000,"stop":1660116751637,"duration":7637}},{"uid":"1160155c643d5d83","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0001313f8>: {\n        Underlying: <*exec.ExitError | 0xc00071d340>{\n            ProcessState: {\n                pid: 6593,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 139767},\n                    Stime: {Sec: 0, Usec: 49594},\n                    Maxrss: 75276,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 5447,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14104,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 620,\n                    Nivcsw: 332,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile3561285268\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile3561285268\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile3561285268\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile3561285268\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile3561285268\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1660116144000,"stop":1660116151516,"duration":7516}},{"uid":"2010c0727f119f9d","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000374090>: {\n        Underlying: <*exec.ExitError | 0xc000442160>{\n            ProcessState: {\n                pid: 6016,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 149421},\n                    Stime: {Sec: 0, Usec: 62548},\n                    Maxrss: 75992,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 8009,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13616,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 725,\n                    Nivcsw: 664,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1486138854\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1486138854\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1486138854\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1486138854\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1486138854\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1660112860000,"stop":1660112867440,"duration":7440}},{"uid":"9b8291e4203acabc","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000741cc8>: {\n        Underlying: <*exec.ExitError | 0xc000080320>{\n            ProcessState: {\n                pid: 9258,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 219238},\n                    Stime: {Sec: 0, Usec: 107655},\n                    Maxrss: 77064,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 12995,\n                    Majflt: 1,\n                    Nswap: 0,\n                    Inblock: 256,\n                    Oublock: 14104,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 1407,\n                    Nivcsw: 50,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile3011025426\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile3011025426\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile3011025426\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile3011025426\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile3011025426\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1660108970000,"stop":1660108977670,"duration":7670}},{"uid":"4c7a28f5ae2325a6","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000848318>: {\n        Underlying: <*exec.ExitError | 0xc0009c6520>{\n            ProcessState: {\n                pid: 6409,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 159515},\n                    Stime: {Sec: 0, Usec: 74193},\n                    Maxrss: 81432,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4330,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14104,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 950,\n                    Nivcsw: 399,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1663281511\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1663281511\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1663281511\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1663281511\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1663281511\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1660104124000,"stop":1660104131606,"duration":7606}},{"uid":"3eda2fcc984127a1","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004bfad0>: {\n        Underlying: <*exec.ExitError | 0xc000449fc0>{\n            ProcessState: {\n                pid: 5986,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 141499},\n                    Stime: {Sec: 0, Usec: 78610},\n                    Maxrss: 78548,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 10522,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13616,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 938,\n                    Nivcsw: 423,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1283148996\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1283148996\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1283148996\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1283148996\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1283148996\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1660104760000,"stop":1660104768439,"duration":8439}},{"uid":"3334de3d29ff01cc","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000889500>: {\n        Underlying: <*exec.ExitError | 0xc00067fac0>{\n            ProcessState: {\n                pid: 6324,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 139587},\n                    Stime: {Sec: 0, Usec: 43621},\n                    Maxrss: 83256,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 8209,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14304,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 1659,\n                    Nivcsw: 484,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile381256948\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile381256948\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile381256948\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile381256948\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile381256948\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1660101116000,"stop":1660101177979,"duration":61979}},{"uid":"a816478afe549845","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004e44f8>: {\n        Underlying: <*exec.ExitError | 0xc0008d05e0>{\n            ProcessState: {\n                pid: 6058,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 182616},\n                    Stime: {Sec: 0, Usec: 100004},\n                    Maxrss: 73640,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 9025,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13616,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 925,\n                    Nivcsw: 663,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile3960368217\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile3960368217\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile3960368217\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile3960368217\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile3960368217\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1660100168000,"stop":1660100175784,"duration":7784}},{"uid":"3d91b05f869130d6","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004c22a0>: {\n        Underlying: <*exec.ExitError | 0xc00044afc0>{\n            ProcessState: {\n                pid: 5989,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 145514},\n                    Stime: {Sec: 0, Usec: 61269},\n                    Maxrss: 69428,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 6401,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13616,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 948,\n                    Nivcsw: 467,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile2595255433\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile2595255433\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile2595255433\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile2595255433\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile2595255433\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1660066476000,"stop":1660066483394,"duration":7394}},{"uid":"ae2f6d8da0953ef7","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000788ba0>: {\n        Underlying: <*exec.ExitError | 0xc0004c8480>{\n            ProcessState: {\n                pid: 6654,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 169097},\n                    Stime: {Sec: 0, Usec: 40261},\n                    Maxrss: 75916,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 5097,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14104,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 719,\n                    Nivcsw: 468,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1618450347\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1618450347\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1618450347\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1618450347\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1618450347\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1660064121000,"stop":1660064129215,"duration":8215}},{"uid":"5c446640a8e55d0d","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000f950>: {\n        Underlying: <*exec.ExitError | 0xc000640300>{\n            ProcessState: {\n                pid: 6075,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 163693},\n                    Stime: {Sec: 0, Usec: 59888},\n                    Maxrss: 73128,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3526,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14288,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 743,\n                    Nivcsw: 533,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile2182344557\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile2182344557\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile2182344557\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile2182344557\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile2182344557\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1660060680000,"stop":1660060741499,"duration":61499}},{"uid":"35e056c29befb629","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000440ea0>: {\n        Underlying: <*exec.ExitError | 0xc0007e3f40>{\n            ProcessState: {\n                pid: 6504,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 207100},\n                    Stime: {Sec: 0, Usec: 64972},\n                    Maxrss: 77400,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 5147,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14272,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 901,\n                    Nivcsw: 539,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile2890320144\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile2890320144\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile2890320144\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile2890320144\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile2890320144\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1660052169000,"stop":1660052176709,"duration":7709}},{"uid":"f71211a4f96ff1f9","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000e900>: {\n        Underlying: <*exec.ExitError | 0xc00048c260>{\n            ProcessState: {\n                pid: 5955,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 178782},\n                    Stime: {Sec: 0, Usec: 66071},\n                    Maxrss: 76044,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 5639,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13616,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 819,\n                    Nivcsw: 451,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1171217446\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1171217446\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1171217446\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1171217446\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1171217446\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1660050481000,"stop":1660050489788,"duration":8788}},{"uid":"ee50543b3c667e49","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0007f30f8>: {\n        Underlying: <*exec.ExitError | 0xc0004f2820>{\n            ProcessState: {\n                pid: 6404,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 169489},\n                    Stime: {Sec: 0, Usec: 26365},\n                    Maxrss: 79024,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4667,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14104,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 871,\n                    Nivcsw: 401,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile3336909834\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile3336909834\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile3336909834\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile3336909834\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile3336909834\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1660046776000,"stop":1660046783514,"duration":7514}},{"uid":"690f475d585130a5","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000694930>: {\n        Underlying: <*exec.ExitError | 0xc0005ede00>{\n            ProcessState: {\n                pid: 6072,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 153268},\n                    Stime: {Sec: 0, Usec: 65138},\n                    Maxrss: 74772,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 13523,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13616,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 710,\n                    Nivcsw: 399,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1846151223\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1846151223\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1846151223\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1846151223\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1846151223\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1660046048000,"stop":1660046055522,"duration":7522}},{"uid":"7b044782467108de","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000f7a0>: {\n        Underlying: <*exec.ExitError | 0xc00014b260>{\n            ProcessState: {\n                pid: 6191,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 137062},\n                    Stime: {Sec: 0, Usec: 75163},\n                    Maxrss: 82920,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 6128,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13616,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 751,\n                    Nivcsw: 491,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile2761086443\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile2761086443\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile2761086443\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile2761086443\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile2761086443\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1660044532000,"stop":1660044539588,"duration":7588}}]},"Hub Suite:Hub Suite#[It]  QosProfile tests QosProfile delete validation Delete qos profile - negative scenario":{"statistic":{"failed":81,"broken":0,"skipped":0,"passed":0,"unknown":0,"total":81},"items":[{"uid":"5cf443230f7fd33b","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0008104c8>: {\n        Underlying: <*exec.ExitError | 0xc000840fa0>{\n            ProcessState: {\n                pid: 6107,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 156466},\n                    Stime: {Sec: 0, Usec: 56167},\n                    Maxrss: 78100,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3919,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13616,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 892,\n                    Nivcsw: 451,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario2687826274\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario2687826274\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario2687826274\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario2687826274\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario2687826274\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1660145110000,"stop":1660145114542,"duration":4542}},{"uid":"626676254810d72c","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0005d6360>: {\n        Underlying: <*exec.ExitError | 0xc00014ab20>{\n            ProcessState: {\n                pid: 6058,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 162965},\n                    Stime: {Sec: 0, Usec: 62679},\n                    Maxrss: 80064,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 8290,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13616,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 761,\n                    Nivcsw: 477,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario3392838470\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario3392838470\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario3392838470\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario3392838470\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario3392838470\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1660142645000,"stop":1660142649532,"duration":4532}},{"uid":"48f6f086c4490b13","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000350378>: {\n        Underlying: <*exec.ExitError | 0xc0005e7780>{\n            ProcessState: {\n                pid: 6067,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 123920},\n                    Stime: {Sec: 0, Usec: 47969},\n                    Maxrss: 78576,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3599,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13392,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 291,\n                    Nivcsw: 280,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"qostest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-qostest\\\": already exists\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The Project \\\"qostest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-qostest\\\": already exists\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"qostest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-qostest\\\": already exists\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The Project \\\"qostest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-qostest\\\": already exists\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The Project \"qostest\" is invalid: project namespace: Invalid value: \"kubeslice-qostest\": already exists\noccurred","time":{"start":1660141834000,"stop":1660141834217,"duration":217}},{"uid":"78b4d108e91f188a","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000350030>: {\n        Underlying: <*exec.ExitError | 0xc000882040>{\n            ProcessState: {\n                pid: 6245,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 140282},\n                    Stime: {Sec: 0, Usec: 31173},\n                    Maxrss: 75660,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 11930,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13392,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 308,\n                    Nivcsw: 306,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"qostest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-qostest\\\": already exists\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The Project \\\"qostest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-qostest\\\": already exists\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"qostest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-qostest\\\": already exists\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The Project \\\"qostest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-qostest\\\": already exists\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The Project \"qostest\" is invalid: project namespace: Invalid value: \"kubeslice-qostest\": already exists\noccurred","time":{"start":1660138561000,"stop":1660138561246,"duration":246}},{"uid":"c2734bfd191e392c","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004c1e78>: {\n        Underlying: <*exec.ExitError | 0xc0007e7dc0>{\n            ProcessState: {\n                pid: 6666,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 187066},\n                    Stime: {Sec: 0, Usec: 55722},\n                    Maxrss: 66676,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3590,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14104,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 912,\n                    Nivcsw: 648,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario3440597362\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario3440597362\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario3440597362\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario3440597362\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario3440597362\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1660116744000,"stop":1660116748624,"duration":4624}},{"uid":"2e1105a41f457fe1","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0006474e8>: {\n        Underlying: <*exec.ExitError | 0xc0006cfca0>{\n            ProcessState: {\n                pid: 6609,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 114393},\n                    Stime: {Sec: 0, Usec: 63552},\n                    Maxrss: 79996,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3379,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14104,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 731,\n                    Nivcsw: 372,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario2240283581\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario2240283581\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario2240283581\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario2240283581\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario2240283581\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1660116144000,"stop":1660116148458,"duration":4458}},{"uid":"5c3a9396442071c1","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00065e9d8>: {\n        Underlying: <*exec.ExitError | 0xc000751e00>{\n            ProcessState: {\n                pid: 6032,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 163430},\n                    Stime: {Sec: 0, Usec: 75089},\n                    Maxrss: 75756,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 9874,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13616,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 775,\n                    Nivcsw: 542,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario3675848940\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario3675848940\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario3675848940\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario3675848940\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario3675848940\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1660112860000,"stop":1660112864578,"duration":4578}},{"uid":"221cf2e0e0d07c2d","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000eb28>: {\n        Underlying: <*exec.ExitError | 0xc0001536c0>{\n            ProcessState: {\n                pid: 9288,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 254106},\n                    Stime: {Sec: 0, Usec: 107422},\n                    Maxrss: 78204,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 12471,\n                    Majflt: 2,\n                    Nswap: 0,\n                    Inblock: 136,\n                    Oublock: 14104,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 1567,\n                    Nivcsw: 94,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario1552089460\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario1552089460\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario1552089460\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario1552089460\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario1552089460\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1660108970000,"stop":1660108974670,"duration":4670}},{"uid":"2b112b3539584f56","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0006b8ae0>: {\n        Underlying: <*exec.ExitError | 0xc000721ee0>{\n            ProcessState: {\n                pid: 6423,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 178867},\n                    Stime: {Sec: 0, Usec: 63881},\n                    Maxrss: 81340,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3708,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14104,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 1263,\n                    Nivcsw: 457,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario3446959830\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario3446959830\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario3446959830\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario3446959830\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario3446959830\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1660104124000,"stop":1660104128686,"duration":4686}},{"uid":"50831e9a46c58133","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000442030>: {\n        Underlying: <*exec.ExitError | 0xc0007b60e0>{\n            ProcessState: {\n                pid: 6001,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 158078},\n                    Stime: {Sec: 0, Usec: 60799},\n                    Maxrss: 74468,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 10064,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13616,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 767,\n                    Nivcsw: 491,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario1985936476\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario1985936476\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario1985936476\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario1985936476\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario1985936476\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1660104760000,"stop":1660104764534,"duration":4534}},{"uid":"e51173d11b9485e","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000889428>: {\n        Underlying: <*exec.ExitError | 0xc0008a2f80>{\n            ProcessState: {\n                pid: 6335,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 137192},\n                    Stime: {Sec: 0, Usec: 32280},\n                    Maxrss: 75792,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 8129,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13392,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 419,\n                    Nivcsw: 279,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"qostest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-qostest\\\": already exists\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The Project \\\"qostest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-qostest\\\": already exists\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"qostest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-qostest\\\": already exists\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The Project \\\"qostest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-qostest\\\": already exists\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The Project \"qostest\" is invalid: project namespace: Invalid value: \"kubeslice-qostest\": already exists\noccurred","time":{"start":1660101116000,"stop":1660101116249,"duration":249}},{"uid":"5321668723517c4e","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00018d7b8>: {\n        Underlying: <*exec.ExitError | 0xc0006bab00>{\n            ProcessState: {\n                pid: 6074,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 170784},\n                    Stime: {Sec: 0, Usec: 80897},\n                    Maxrss: 79132,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 9589,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13616,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 995,\n                    Nivcsw: 466,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario3961551625\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario3961551625\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario3961551625\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario3961551625\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario3961551625\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1660100168000,"stop":1660100172639,"duration":4639}},{"uid":"81cbc903fa3a6d02","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000445248>: {\n        Underlying: <*exec.ExitError | 0xc000708b00>{\n            ProcessState: {\n                pid: 6003,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 103319},\n                    Stime: {Sec: 0, Usec: 103319},\n                    Maxrss: 86696,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 11418,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13616,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 657,\n                    Nivcsw: 421,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario2362546225\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario2362546225\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario2362546225\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario2362546225\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario2362546225\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1660066476000,"stop":1660066480471,"duration":4471}},{"uid":"bb0c8271c46a59d8","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000ea80>: {\n        Underlying: <*exec.ExitError | 0xc0009b1da0>{\n            ProcessState: {\n                pid: 6668,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 134347},\n                    Stime: {Sec: 0, Usec: 72931},\n                    Maxrss: 78852,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3446,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14104,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 989,\n                    Nivcsw: 411,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario2010303816\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario2010303816\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario2010303816\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario2010303816\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario2010303816\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1660064121000,"stop":1660064125492,"duration":4492}},{"uid":"5166ede12ab5468a","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00048d080>: {\n        Underlying: <*exec.ExitError | 0xc00039e0e0>{\n            ProcessState: {\n                pid: 6085,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 150563},\n                    Stime: {Sec: 0, Usec: 50187},\n                    Maxrss: 77652,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 7353,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13392,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 351,\n                    Nivcsw: 408,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"qostest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-qostest\\\": already exists\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The Project \\\"qostest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-qostest\\\": already exists\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"qostest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-qostest\\\": already exists\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The Project \\\"qostest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-qostest\\\": already exists\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The Project \"qostest\" is invalid: project namespace: Invalid value: \"kubeslice-qostest\": already exists\noccurred","time":{"start":1660060680000,"stop":1660060680267,"duration":267}},{"uid":"ec2da3bd70b1fdf8","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000413968>: {\n        Underlying: <*exec.ExitError | 0xc000704680>{\n            ProcessState: {\n                pid: 6520,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 189625},\n                    Stime: {Sec: 0, Usec: 64645},\n                    Maxrss: 82836,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 6611,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14272,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 922,\n                    Nivcsw: 424,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario912325257\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario912325257\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario912325257\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario912325257\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario912325257\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1660052169000,"stop":1660052173670,"duration":4670}},{"uid":"e7ee2141897d93a1","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000669188>: {\n        Underlying: <*exec.ExitError | 0xc000737300>{\n            ProcessState: {\n                pid: 5971,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 171808},\n                    Stime: {Sec: 0, Usec: 67924},\n                    Maxrss: 73204,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 6068,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13616,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 820,\n                    Nivcsw: 434,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario3487611922\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario3487611922\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario3487611922\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario3487611922\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario3487611922\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1660050481000,"stop":1660050485594,"duration":4594}},{"uid":"d0c6fcdc6f4d7d16","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000f488>: {\n        Underlying: <*exec.ExitError | 0xc0006a74a0>{\n            ProcessState: {\n                pid: 6419,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 144293},\n                    Stime: {Sec: 0, Usec: 37104},\n                    Maxrss: 79228,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4584,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14104,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 673,\n                    Nivcsw: 314,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario124085076\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario124085076\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario124085076\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario124085076\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario124085076\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1660046776000,"stop":1660046780496,"duration":4496}},{"uid":"a2ce3226a6eb3da1","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004b0fa8>: {\n        Underlying: <*exec.ExitError | 0xc000a03140>{\n            ProcessState: {\n                pid: 6086,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 151110},\n                    Stime: {Sec: 0, Usec: 71357},\n                    Maxrss: 67748,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 11242,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13616,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 780,\n                    Nivcsw: 594,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario621967535\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario621967535\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario621967535\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario621967535\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario621967535\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1660046048000,"stop":1660046052496,"duration":4496}},{"uid":"27b7a8769cd98fa3","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000623ed8>: {\n        Underlying: <*exec.ExitError | 0xc000757020>{\n            ProcessState: {\n                pid: 6206,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 159193},\n                    Stime: {Sec: 0, Usec: 58650},\n                    Maxrss: 75888,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4890,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13616,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 836,\n                    Nivcsw: 536,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario3904806138\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario3904806138\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario3904806138\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario3904806138\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario3904806138\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1660044532000,"stop":1660044536616,"duration":4616}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting iperf-client pod Check ping between iperf-server and iperf-client after iperf-client pod restart":{"statistic":{"failed":0,"broken":0,"skipped":35,"passed":0,"unknown":0,"total":35},"items":[{"uid":"13bccfdb1f55ccf","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"5ecdc7763ea8b5fb","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"1c4d34acdc179a9c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"e229c74d458f3317","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}},{"uid":"c73be2c4b27fd8f6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659982549000,"stop":1659982549000,"duration":0}},{"uid":"3a32e9e78162c55","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659970182000,"stop":1659970182000,"duration":0}},{"uid":"358dfcc0dc8b5dce","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659944495000,"stop":1659944495000,"duration":0}},{"uid":"b854455b8c235a4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659874977000,"stop":1659874977000,"duration":0}},{"uid":"e9a0fcdd64c9c343","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496050000,"stop":1659496050000,"duration":0}},{"uid":"278e0b5ae2f71f95","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496083000,"stop":1659496083000,"duration":0}},{"uid":"53c5df1e3f905c3e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"e956951cf4f06639","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659450916000,"stop":1659450916000,"duration":0}},{"uid":"5a3fd0bb4385e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"b1fac386eb77f16c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447298000,"stop":1659447298000,"duration":0}},{"uid":"3bf717aac14ac4d6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447116000,"stop":1659447116000,"duration":0}},{"uid":"147d7caaa1265c2e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448205000,"stop":1659448205000,"duration":0}},{"uid":"2b38831452857257","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448097000,"stop":1659448097000,"duration":0}},{"uid":"6de3c17391d6edaa","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659409279000,"stop":1659409279000,"duration":0}},{"uid":"62671c90226f298a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659402555000,"stop":1659402555000,"duration":0}},{"uid":"8747a7255855ad02","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659291287000,"stop":1659291287000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice Creation scenarios Slice with LOW QOS should have gateway pod running":{"statistic":{"failed":4,"broken":0,"skipped":4,"passed":18,"unknown":0,"total":26},"items":[{"uid":"6d97e2f3ddbf091a","status":"failed","statusDetails":"Timed out after 180.003s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1660113628000,"stop":1660113808230,"duration":180230}},{"uid":"db984b14975dc419","status":"failed","statusDetails":"Timed out after 180.003s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1660104898000,"stop":1660105078329,"duration":180329}},{"uid":"52d0b1af77db83e0","status":"passed","time":{"start":1660067259000,"stop":1660067304107,"duration":45107}},{"uid":"10963f3710a2b8f1","status":"passed","time":{"start":1660047551000,"stop":1660047576084,"duration":25084}},{"uid":"6a23f74930b7586c","status":"passed","time":{"start":1659982549000,"stop":1659982584171,"duration":35171}},{"uid":"4ef23a2bbe351250","status":"passed","time":{"start":1659970182000,"stop":1659970217082,"duration":35082}},{"uid":"d9c410853aab26ec","status":"passed","time":{"start":1659944495000,"stop":1659944550162,"duration":55162}},{"uid":"4e53c9a27403fe40","status":"passed","time":{"start":1659874977000,"stop":1659875032141,"duration":55141}},{"uid":"687960afb231088a","status":"passed","time":{"start":1659496050000,"stop":1659496120156,"duration":70156}},{"uid":"4869c986697247ea","status":"passed","time":{"start":1659496083000,"stop":1659496108078,"duration":25078}},{"uid":"cc936e6180a68c30","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"4f720b4f88133e92","status":"passed","time":{"start":1659450916000,"stop":1659450996230,"duration":80230}},{"uid":"2cf4f207593ec1a4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"ad8b9a5883d85e98","status":"passed","time":{"start":1659447298000,"stop":1659447343120,"duration":45120}},{"uid":"de0007b1bf0344c8","status":"passed","time":{"start":1659447116000,"stop":1659447156140,"duration":40140}},{"uid":"650dc9f682427718","status":"failed","statusDetails":"Timed out after 180.005s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1659448205000,"stop":1659448385251,"duration":180251}},{"uid":"97e3b3643670bc46","status":"passed","time":{"start":1659448097000,"stop":1659448152242,"duration":55242}},{"uid":"66724eea2625f9dd","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659409279000,"stop":1659409279000,"duration":0}},{"uid":"401a3e4568a86d85","status":"passed","time":{"start":1659402555000,"stop":1659402655263,"duration":100263}},{"uid":"127956c265586547","status":"failed","statusDetails":"Timed out after 180.004s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1659291287000,"stop":1659291467279,"duration":180279}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests slice with netpol Should install networkpolicies in all application namespaces":{"statistic":{"failed":0,"broken":0,"skipped":25,"passed":10,"unknown":0,"total":35},"items":[{"uid":"f9790e32a3152ebf","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"abe1a1dc4ce71d5a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"c829bdfb31dc1fa1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"d7a74d3deea8d4ca","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}},{"uid":"39d84f2a4b374136","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659982549000,"stop":1659982549000,"duration":0}},{"uid":"854d70b4e5ac7799","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659970182000,"stop":1659970182000,"duration":0}},{"uid":"5bd79e0e96d204f9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659944495000,"stop":1659944495000,"duration":0}},{"uid":"956e4120761878e0","status":"passed","time":{"start":1659874977000,"stop":1659874977242,"duration":242}},{"uid":"fb5679dba00f6ba2","status":"passed","time":{"start":1659496050000,"stop":1659496050250,"duration":250}},{"uid":"7b9c3d8cd7fbb14a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496083000,"stop":1659496083000,"duration":0}},{"uid":"8234d4f86fdad30e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"d048edf82722d872","status":"passed","time":{"start":1659450916000,"stop":1659450916527,"duration":527}},{"uid":"fba6609343a8d195","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"657393e3b8c9c4f1","status":"passed","time":{"start":1659447298000,"stop":1659447298378,"duration":378}},{"uid":"edde7c7a8b93c34c","status":"passed","time":{"start":1659447116000,"stop":1659447116267,"duration":267}},{"uid":"96bf8586a973df6d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448205000,"stop":1659448205000,"duration":0}},{"uid":"b8ca43a80dce827d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448097000,"stop":1659448097000,"duration":0}},{"uid":"8fe7b17829d1be","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659409279000,"stop":1659409279000,"duration":0}},{"uid":"9a4e167933330ba8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659402555000,"stop":1659402555000,"duration":0}},{"uid":"88887f1298b69b73","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659291287000,"stop":1659291287000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Hub netpol scenarios: Slice Configs Netpol scenarios Should fail when deploying clusters in namespaceisolationprofile with * and a cluster name in both applicationNamespace and allowedNamespace":{"statistic":{"failed":0,"broken":0,"skipped":9,"passed":26,"unknown":0,"total":35},"items":[{"uid":"b03d36df7f44a05d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"600d71a75a9351de","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"d07887859a71b553","status":"passed","time":{"start":1660067259000,"stop":1660067259358,"duration":358}},{"uid":"4ed7671a8dd4de62","status":"passed","time":{"start":1660047551000,"stop":1660047551304,"duration":304}},{"uid":"1cd73755f593d30e","status":"passed","time":{"start":1659982549000,"stop":1659982549960,"duration":960}},{"uid":"e7394558af825f00","status":"passed","time":{"start":1659970182000,"stop":1659970182361,"duration":361}},{"uid":"99b393b3444d56ea","status":"passed","time":{"start":1659944495000,"stop":1659944495418,"duration":418}},{"uid":"b31a378ea97b28e3","status":"passed","time":{"start":1659874977000,"stop":1659874977397,"duration":397}},{"uid":"dd6b4aac0dac3ed4","status":"passed","time":{"start":1659496050000,"stop":1659496050232,"duration":232}},{"uid":"b8014ebfb643c74b","status":"passed","time":{"start":1659496083000,"stop":1659496083329,"duration":329}},{"uid":"9c131efb0c4eea64","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"726bee7babff25ae","status":"passed","time":{"start":1659450916000,"stop":1659450916346,"duration":346}},{"uid":"149cdb14074a0f1d","status":"passed","time":{"start":1659451524000,"stop":1659451524442,"duration":442}},{"uid":"a75d7a0deec4b7f6","status":"passed","time":{"start":1659447298000,"stop":1659447298313,"duration":313}},{"uid":"f8b49d107112dd01","status":"passed","time":{"start":1659447116000,"stop":1659447116303,"duration":303}},{"uid":"1783e1607e2794d4","status":"passed","time":{"start":1659448205000,"stop":1659448205464,"duration":464}},{"uid":"289becf06bc72efa","status":"passed","time":{"start":1659448097000,"stop":1659448097398,"duration":398}},{"uid":"91fd603f4cb5839","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659409279000,"stop":1659409279000,"duration":0}},{"uid":"e5f9a0d91033ab7b","status":"passed","time":{"start":1659402555000,"stop":1659402555569,"duration":569}},{"uid":"58983cacc63d90d3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659291287000,"stop":1659291287000,"duration":0}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Update Should update successfully when Project applied with valid service account name in Write users":{"statistic":{"failed":7,"broken":0,"skipped":0,"passed":79,"unknown":0,"total":86},"items":[{"uid":"3fa20d483604cbce","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000532030>: {\n        Underlying: <*exec.ExitError | 0xc00014a040>{\n            ProcessState: {\n                pid: 6203,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 155782},\n                    Stime: {Sec: 0, Usec: 32796},\n                    Maxrss: 73384,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4618,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13376,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 428,\n                    Nivcsw: 249,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The Project \"projectupdatetest\" is invalid: project namespace: Invalid value: \"kubeslice-projectupdatetest\": already exists\noccurred","time":{"start":1660145110000,"stop":1660145170255,"duration":60255}},{"uid":"ef12a8b79b8527a8","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0001e8d08>: {\n        Underlying: <*exec.ExitError | 0xc000629660>{\n            ProcessState: {\n                pid: 6308,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 146677},\n                    Stime: {Sec: 0, Usec: 71356},\n                    Maxrss: 77360,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 12361,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13392,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 365,\n                    Nivcsw: 320,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The Project \"projectupdatetest\" is invalid: project namespace: Invalid value: \"kubeslice-projectupdatetest\": already exists\noccurred","time":{"start":1660142645000,"stop":1660142705291,"duration":60291}},{"uid":"baa8fb2e23aa00b7","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000130a68>: {\n        Underlying: <*exec.ExitError | 0xc00083c0a0>{\n            ProcessState: {\n                pid: 6129,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 129398},\n                    Stime: {Sec: 0, Usec: 48524},\n                    Maxrss: 75972,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 7218,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13392,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 344,\n                    Nivcsw: 372,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The Project \"projectupdatetest\" is invalid: project namespace: Invalid value: \"kubeslice-projectupdatetest\": already exists\noccurred","time":{"start":1660141834000,"stop":1660141894225,"duration":60225}},{"uid":"4b27d2266b825be1","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000e2d0>: {\n        Underlying: <*exec.ExitError | 0xc00070d9a0>{\n            ProcessState: {\n                pid: 6163,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 125726},\n                    Stime: {Sec: 0, Usec: 50290},\n                    Maxrss: 80680,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 11880,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13376,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 357,\n                    Nivcsw: 221,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The Project \"projectupdatetest\" is invalid: project namespace: Invalid value: \"kubeslice-projectupdatetest\": already exists\noccurred","time":{"start":1660138561000,"stop":1660138621220,"duration":60220}},{"uid":"35ef45d08771a581","status":"passed","time":{"start":1660116744000,"stop":1660116749244,"duration":5244}},{"uid":"751adc387a4f79ec","status":"passed","time":{"start":1660116144000,"stop":1660116148977,"duration":4977}},{"uid":"8ccbaf2a7231e926","status":"passed","time":{"start":1660112860000,"stop":1660112865036,"duration":5036}},{"uid":"166ff1d41f0a912f","status":"passed","time":{"start":1660108970000,"stop":1660108975040,"duration":5040}},{"uid":"99dc99d4be0b5190","status":"passed","time":{"start":1660104124000,"stop":1660104129209,"duration":5209}},{"uid":"7a2ca23b8f91ad21","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000864450>: {\n        Underlying: <*exec.ExitError | 0xc0006d0e80>{\n            ProcessState: {\n                pid: 6281,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 115535},\n                    Stime: {Sec: 0, Usec: 63743},\n                    Maxrss: 75512,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 7635,\n                    Majflt: 1,\n                    Nswap: 0,\n                    Inblock: 64,\n                    Oublock: 13392,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 408,\n                    Nivcsw: 330,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The Project \"projectupdatetest\" is invalid: project namespace: Invalid value: \"kubeslice-projectupdatetest\": already exists\noccurred","time":{"start":1660104760000,"stop":1660104820258,"duration":60258}},{"uid":"67c1d7d48286a50f","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004f5128>: {\n        Underlying: <*exec.ExitError | 0xc00045fee0>{\n            ProcessState: {\n                pid: 6111,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 129458},\n                    Stime: {Sec: 0, Usec: 26653},\n                    Maxrss: 76660,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3674,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13376,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 440,\n                    Nivcsw: 280,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The Project \"projectupdatetest\" is invalid: project namespace: Invalid value: \"kubeslice-projectupdatetest\": already exists\noccurred","time":{"start":1660101116000,"stop":1660101176214,"duration":60214}},{"uid":"864547b2efd4c7a8","status":"passed","time":{"start":1660100168000,"stop":1660100173227,"duration":5227}},{"uid":"36ae62c3aa96cce","status":"passed","time":{"start":1660066476000,"stop":1660066480952,"duration":4952}},{"uid":"9d43c4eded7d4f30","status":"passed","time":{"start":1660064121000,"stop":1660064126079,"duration":5079}},{"uid":"617f2816f7cb3ff6","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0005f8018>: {\n        Underlying: <*exec.ExitError | 0xc00014a040>{\n            ProcessState: {\n                pid: 6188,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 130925},\n                    Stime: {Sec: 0, Usec: 50059},\n                    Maxrss: 75668,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 6343,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13392,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 370,\n                    Nivcsw: 270,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The Project \"projectupdatetest\" is invalid: project namespace: Invalid value: \"kubeslice-projectupdatetest\": already exists\noccurred","time":{"start":1660060680000,"stop":1660060740241,"duration":60241}},{"uid":"fa1a78aed956929e","status":"passed","time":{"start":1660052169000,"stop":1660052174401,"duration":5401}},{"uid":"2f19c6df540c7695","status":"passed","time":{"start":1660050481000,"stop":1660050486122,"duration":5122}},{"uid":"c880db19ea3dc1a4","status":"passed","time":{"start":1660046776000,"stop":1660046781020,"duration":5020}},{"uid":"3715a5fd146cce50","status":"passed","time":{"start":1660046048000,"stop":1660046053162,"duration":5162}},{"uid":"dbbc072986fd0e44","status":"passed","time":{"start":1660044532000,"stop":1660044537094,"duration":5094}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Update Should fail when Project is Applied with service account name as combination of special characters in Read users":{"statistic":{"failed":7,"broken":0,"skipped":0,"passed":79,"unknown":0,"total":86},"items":[{"uid":"3d1be6be762ba98","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000810e10>: {\n        Underlying: <*exec.ExitError | 0xc000685fc0>{\n            ProcessState: {\n                pid: 6231,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 132954},\n                    Stime: {Sec: 0, Usec: 48750},\n                    Maxrss: 72660,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3359,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13376,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 471,\n                    Nivcsw: 301,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The Project \"projectupdatetest\" is invalid: project namespace: Invalid value: \"kubeslice-projectupdatetest\": already exists\noccurred","time":{"start":1660145110000,"stop":1660145170248,"duration":60248}},{"uid":"775b7459222985de","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00066bb90>: {\n        Underlying: <*exec.ExitError | 0xc000657300>{\n            ProcessState: {\n                pid: 6335,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 165566},\n                    Stime: {Sec: 0, Usec: 47927},\n                    Maxrss: 73616,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 9135,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13392,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 423,\n                    Nivcsw: 333,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The Project \"projectupdatetest\" is invalid: project namespace: Invalid value: \"kubeslice-projectupdatetest\": already exists\noccurred","time":{"start":1660142645000,"stop":1660142705268,"duration":60268}},{"uid":"9ca72dd1ff8c839b","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004be8e8>: {\n        Underlying: <*exec.ExitError | 0xc000667f60>{\n            ProcessState: {\n                pid: 6157,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 130521},\n                    Stime: {Sec: 0, Usec: 53024},\n                    Maxrss: 74136,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 9591,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13392,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 424,\n                    Nivcsw: 319,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The Project \"projectupdatetest\" is invalid: project namespace: Invalid value: \"kubeslice-projectupdatetest\": already exists\noccurred","time":{"start":1660141834000,"stop":1660141894229,"duration":60229}},{"uid":"181139210755628e","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0003519f8>: {\n        Underlying: <*exec.ExitError | 0xc0006b1c00>{\n            ProcessState: {\n                pid: 6193,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 137348},\n                    Stime: {Sec: 0, Usec: 43166},\n                    Maxrss: 68720,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 8756,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13376,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 286,\n                    Nivcsw: 251,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The Project \"projectupdatetest\" is invalid: project namespace: Invalid value: \"kubeslice-projectupdatetest\": already exists\noccurred","time":{"start":1660138561000,"stop":1660138621221,"duration":60221}},{"uid":"316f229514af46e9","status":"passed","time":{"start":1660116744000,"stop":1660116748861,"duration":4861}},{"uid":"a648dce338037bf2","status":"passed","time":{"start":1660116144000,"stop":1660116149725,"duration":5725}},{"uid":"ace5867ed936e636","status":"passed","time":{"start":1660112860000,"stop":1660112865735,"duration":5735}},{"uid":"2127bdddd5081dd5","status":"passed","time":{"start":1660108970000,"stop":1660108975749,"duration":5749}},{"uid":"8465b9d9e2902a00","status":"passed","time":{"start":1660104124000,"stop":1660104128811,"duration":4811}},{"uid":"657ccb66db2aeb72","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0007d4048>: {\n        Underlying: <*exec.ExitError | 0xc0007a0060>{\n            ProcessState: {\n                pid: 6308,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 127626},\n                    Stime: {Sec: 0, Usec: 63813},\n                    Maxrss: 68820,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 10681,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13392,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 416,\n                    Nivcsw: 341,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The Project \"projectupdatetest\" is invalid: project namespace: Invalid value: \"kubeslice-projectupdatetest\": already exists\noccurred","time":{"start":1660104760000,"stop":1660104820264,"duration":60264}},{"uid":"899f1b919a84e14a","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000889bd8>: {\n        Underlying: <*exec.ExitError | 0xc000645800>{\n            ProcessState: {\n                pid: 6141,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 110562},\n                    Stime: {Sec: 0, Usec: 34019},\n                    Maxrss: 80124,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 5094,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13376,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 360,\n                    Nivcsw: 270,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The Project \"projectupdatetest\" is invalid: project namespace: Invalid value: \"kubeslice-projectupdatetest\": already exists\noccurred","time":{"start":1660101116000,"stop":1660101176224,"duration":60224}},{"uid":"db6271e3897e4587","status":"passed","time":{"start":1660100168000,"stop":1660100172829,"duration":4829}},{"uid":"4b1814b34fc3b06e","status":"passed","time":{"start":1660066476000,"stop":1660066481648,"duration":5648}},{"uid":"533419f4d2ae823f","status":"passed","time":{"start":1660064121000,"stop":1660064126711,"duration":5711}},{"uid":"ab9c2128b56ca70d","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000131680>: {\n        Underlying: <*exec.ExitError | 0xc00039faa0>{\n            ProcessState: {\n                pid: 6216,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 153469},\n                    Stime: {Sec: 0, Usec: 58069},\n                    Maxrss: 73716,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 10019,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13392,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 450,\n                    Nivcsw: 332,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The Project \"projectupdatetest\" is invalid: project namespace: Invalid value: \"kubeslice-projectupdatetest\": already exists\noccurred","time":{"start":1660060680000,"stop":1660060740258,"duration":60258}},{"uid":"83bf2d7899933f61","status":"passed","time":{"start":1660052169000,"stop":1660052174082,"duration":5082}},{"uid":"3ff334d6c9db2781","status":"passed","time":{"start":1660050481000,"stop":1660050485793,"duration":4793}},{"uid":"fe3448553e25a591","status":"passed","time":{"start":1660046776000,"stop":1660046781734,"duration":5734}},{"uid":"94b9dde08083dcaf","status":"passed","time":{"start":1660046048000,"stop":1660046053651,"duration":5651}},{"uid":"1c7547a21af9d1a3","status":"passed","time":{"start":1660044532000,"stop":1660044536762,"duration":4762}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Creation Should create Project while using valid manifest":{"statistic":{"failed":5,"broken":0,"skipped":0,"passed":84,"unknown":0,"total":89},"items":[{"uid":"4ac9ae87286f4956","status":"passed","time":{"start":1660145110000,"stop":1660145118577,"duration":8577}},{"uid":"242c3d537adb3be5","status":"passed","time":{"start":1660142645000,"stop":1660142652569,"duration":7569}},{"uid":"a5d9149c8f9ffe36","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected an error to have occurred.  Got:\n    <nil>: nil","time":{"start":1660141834000,"stop":1660141895539,"duration":61539}},{"uid":"557742b7efd909c4","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected an error to have occurred.  Got:\n    <nil>: nil","time":{"start":1660138561000,"stop":1660138622496,"duration":61496}},{"uid":"7cff006aae36c152","status":"passed","time":{"start":1660116744000,"stop":1660116751691,"duration":7691}},{"uid":"8ce37603699a2b64","status":"passed","time":{"start":1660116144000,"stop":1660116151523,"duration":7523}},{"uid":"d0ee95ec76d6debf","status":"passed","time":{"start":1660112860000,"stop":1660112868252,"duration":8252}},{"uid":"e6372be499683f82","status":"passed","time":{"start":1660108970000,"stop":1660108977553,"duration":7553}},{"uid":"7c2ec8f69d8a6675","status":"passed","time":{"start":1660104124000,"stop":1660104131665,"duration":7665}},{"uid":"61a3e357ea2cca8c","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected an error to have occurred.  Got:\n    <nil>: nil","time":{"start":1660104760000,"stop":1660104821559,"duration":61559}},{"uid":"3e899087e8485be","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected an error to have occurred.  Got:\n    <nil>: nil","time":{"start":1660101116000,"stop":1660101177656,"duration":61656}},{"uid":"c834c75a35ce7198","status":"passed","time":{"start":1660100168000,"stop":1660100175669,"duration":7669}},{"uid":"e76abf6fe2a1564e","status":"passed","time":{"start":1660066476000,"stop":1660066483519,"duration":7519}},{"uid":"2a93e4fd7fef1564","status":"passed","time":{"start":1660064121000,"stop":1660064128525,"duration":7525}},{"uid":"aad422cbc9db9a0e","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected an error to have occurred.  Got:\n    <nil>: nil","time":{"start":1660060680000,"stop":1660060741576,"duration":61576}},{"uid":"c1a46db33fb16279","status":"passed","time":{"start":1660052169000,"stop":1660052176657,"duration":7657}},{"uid":"37395fbf6938e77","status":"passed","time":{"start":1660050481000,"stop":1660050488707,"duration":7707}},{"uid":"a5742ea87348e64c","status":"passed","time":{"start":1660046776000,"stop":1660046784267,"duration":8267}},{"uid":"88b7a04a97345666","status":"passed","time":{"start":1660046048000,"stop":1660046055529,"duration":7529}},{"uid":"cdd8ee15e693e273","status":"passed","time":{"start":1660044532000,"stop":1660044539618,"duration":7618}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting iperf-client pod Should restart iperf-client pod":{"statistic":{"failed":0,"broken":0,"skipped":35,"passed":0,"unknown":0,"total":35},"items":[{"uid":"3771a5d1186d5a76","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"9f15fa0a8bdf138c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"bd27a581f053c534","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"74c77ba825a795ea","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}},{"uid":"fa7e0fde0999af65","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659982549000,"stop":1659982549000,"duration":0}},{"uid":"a7eba0463f8331a5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659970182000,"stop":1659970182000,"duration":0}},{"uid":"15087d2c61f8519d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659944495000,"stop":1659944495000,"duration":0}},{"uid":"576ab208e6a9166b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659874977000,"stop":1659874977000,"duration":0}},{"uid":"ca72aaf31565f0fe","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496050000,"stop":1659496050000,"duration":0}},{"uid":"c431f7869332068d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496083000,"stop":1659496083000,"duration":0}},{"uid":"6bafe7cc23dff0d3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"6ea339cda5774440","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659450916000,"stop":1659450916000,"duration":0}},{"uid":"db5eed0a94715173","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"344c107adbb82c00","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447298000,"stop":1659447298000,"duration":0}},{"uid":"8a380485849242bc","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447116000,"stop":1659447116000,"duration":0}},{"uid":"5b9a95fa94e9f4f4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448205000,"stop":1659448205000,"duration":0}},{"uid":"36df249590372691","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448097000,"stop":1659448097000,"duration":0}},{"uid":"5cb198d86ff9ea3d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659409279000,"stop":1659409279000,"duration":0}},{"uid":"141be80313a502f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659402555000,"stop":1659402555000,"duration":0}},{"uid":"60954cfe2a8cd99a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659291287000,"stop":1659291287000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Namespace reconcile test scenarios: Onboard cluster objs should have app ns & attached slice entry":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":35,"unknown":0,"total":35},"items":[{"uid":"fd20553455f463a","status":"passed","time":{"start":1660113628000,"stop":1660113628155,"duration":155}},{"uid":"a122f0fcd157933f","status":"passed","time":{"start":1660104898000,"stop":1660104898158,"duration":158}},{"uid":"5bf00521790334c7","status":"passed","time":{"start":1660067259000,"stop":1660067259351,"duration":351}},{"uid":"ea7a254be89923dc","status":"passed","time":{"start":1660047551000,"stop":1660047551258,"duration":258}},{"uid":"32daea97c2051a5d","status":"passed","time":{"start":1659982549000,"stop":1659982549671,"duration":671}},{"uid":"c03e2dac62f95e1d","status":"passed","time":{"start":1659970182000,"stop":1659970182159,"duration":159}},{"uid":"abcd49dec62e3530","status":"passed","time":{"start":1659944495000,"stop":1659944495409,"duration":409}},{"uid":"b072b14bebdd0ef7","status":"passed","time":{"start":1659874977000,"stop":1659874977351,"duration":351}},{"uid":"b6e2ead0b840609e","status":"passed","time":{"start":1659496050000,"stop":1659496050133,"duration":133}},{"uid":"57f90a9dfaebed06","status":"passed","time":{"start":1659496083000,"stop":1659496083172,"duration":172}},{"uid":"d60e9b0959ec0831","status":"passed","time":{"start":1659451501000,"stop":1659451501136,"duration":136}},{"uid":"50b6c3d323723146","status":"passed","time":{"start":1659450916000,"stop":1659450916327,"duration":327}},{"uid":"2fab010ec05ab108","status":"passed","time":{"start":1659451524000,"stop":1659451524304,"duration":304}},{"uid":"898896a2a3aa0b1d","status":"passed","time":{"start":1659447298000,"stop":1659447298134,"duration":134}},{"uid":"eaa4fa13b2b85a62","status":"passed","time":{"start":1659447116000,"stop":1659447116241,"duration":241}},{"uid":"142cc5f6cbbc2326","status":"passed","time":{"start":1659448205000,"stop":1659448205141,"duration":141}},{"uid":"318f504c341a738d","status":"passed","time":{"start":1659448097000,"stop":1659448097306,"duration":306}},{"uid":"3dd47236beef9c27","status":"passed","time":{"start":1659409279000,"stop":1659409279268,"duration":268}},{"uid":"9763296267a226fa","status":"passed","time":{"start":1659402555000,"stop":1659402555215,"duration":215}},{"uid":"174b87459e9e235e","status":"passed","time":{"start":1659291287000,"stop":1659291287202,"duration":202}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Iperf connectivity Should install slice":{"statistic":{"failed":16,"broken":0,"skipped":0,"passed":19,"unknown":0,"total":35},"items":[{"uid":"82afcbf8277b6889","status":"failed","statusDetails":"Unexpected error:\n    <*errors.StatusError | 0xc0000f6a00>: {\n        ErrStatus: {\n            TypeMeta: {Kind: \"\", APIVersion: \"\"},\n            ListMeta: {\n                SelfLink: \"\",\n                ResourceVersion: \"\",\n                Continue: \"\",\n                RemainingItemCount: nil,\n            },\n            Status: \"Failure\",\n            Message: \"namespaces \\\"iperf\\\" already exists\",\n            Reason: \"AlreadyExists\",\n            Details: {Name: \"iperf\", Group: \"\", Kind: \"namespaces\", UID: \"\", Causes: nil, RetryAfterSeconds: 0},\n            Code: 409,\n        },\n    }\n    namespaces \"iperf\" already exists\noccurred","time":{"start":1660113628000,"stop":1660113628360,"duration":360}},{"uid":"8898fd59cdc037cd","status":"failed","statusDetails":"Unexpected error:\n    <*errors.StatusError | 0xc00098bf40>: {\n        ErrStatus: {\n            TypeMeta: {Kind: \"\", APIVersion: \"\"},\n            ListMeta: {\n                SelfLink: \"\",\n                ResourceVersion: \"\",\n                Continue: \"\",\n                RemainingItemCount: nil,\n            },\n            Status: \"Failure\",\n            Message: \"namespaces \\\"iperf\\\" already exists\",\n            Reason: \"AlreadyExists\",\n            Details: {Name: \"iperf\", Group: \"\", Kind: \"namespaces\", UID: \"\", Causes: nil, RetryAfterSeconds: 0},\n            Code: 409,\n        },\n    }\n    namespaces \"iperf\" already exists\noccurred","time":{"start":1660104898000,"stop":1660104898528,"duration":528}},{"uid":"f7f482a66199998b","status":"passed","time":{"start":1660067259000,"stop":1660067259240,"duration":240}},{"uid":"16cb0f9377c13402","status":"passed","time":{"start":1660047551000,"stop":1660047551904,"duration":904}},{"uid":"81397e1af8087c56","status":"passed","time":{"start":1659982549000,"stop":1659982550984,"duration":1984}},{"uid":"55f4eddab014d14","status":"passed","time":{"start":1659970182000,"stop":1659970182838,"duration":838}},{"uid":"1a9027a95c501b97","status":"failed","statusDetails":"Unexpected error:\n    <*errors.StatusError | 0xc0006235e0>: {\n        ErrStatus: {\n            TypeMeta: {Kind: \"\", APIVersion: \"\"},\n            ListMeta: {\n                SelfLink: \"\",\n                ResourceVersion: \"\",\n                Continue: \"\",\n                RemainingItemCount: nil,\n            },\n            Status: \"Failure\",\n            Message: \"object is being deleted: namespaces \\\"appns1\\\" already exists\",\n            Reason: \"AlreadyExists\",\n            Details: {Name: \"appns1\", Group: \"\", Kind: \"namespaces\", UID: \"\", Causes: nil, RetryAfterSeconds: 0},\n            Code: 409,\n        },\n    }\n    object is being deleted: namespaces \"appns1\" already exists\noccurred","time":{"start":1659944495000,"stop":1659944495678,"duration":678}},{"uid":"171ce26b91b9b9c2","status":"failed","statusDetails":"Unexpected error:\n    <*errors.StatusError | 0xc0006d1a40>: {\n        ErrStatus: {\n            TypeMeta: {Kind: \"\", APIVersion: \"\"},\n            ListMeta: {\n                SelfLink: \"\",\n                ResourceVersion: \"\",\n                Continue: \"\",\n                RemainingItemCount: nil,\n            },\n            Status: \"Failure\",\n            Message: \"namespaces \\\"iperf\\\" already exists\",\n            Reason: \"AlreadyExists\",\n            Details: {Name: \"iperf\", Group: \"\", Kind: \"namespaces\", UID: \"\", Causes: nil, RetryAfterSeconds: 0},\n            Code: 409,\n        },\n    }\n    namespaces \"iperf\" already exists\noccurred","time":{"start":1659874977000,"stop":1659874977443,"duration":443}},{"uid":"251467de527f8996","status":"passed","time":{"start":1659496050000,"stop":1659496050675,"duration":675}},{"uid":"69b97791bdc803f8","status":"passed","time":{"start":1659496083000,"stop":1659496083628,"duration":628}},{"uid":"4eac7f9f63d3d13d","status":"passed","time":{"start":1659451501000,"stop":1659451502083,"duration":1083}},{"uid":"9b029ad9105096bf","status":"failed","statusDetails":"Unexpected error:\n    <*errors.StatusError | 0xc0006ff220>: {\n        ErrStatus: {\n            TypeMeta: {Kind: \"\", APIVersion: \"\"},\n            ListMeta: {\n                SelfLink: \"\",\n                ResourceVersion: \"\",\n                Continue: \"\",\n                RemainingItemCount: nil,\n            },\n            Status: \"Failure\",\n            Message: \"namespaces \\\"iperf\\\" already exists\",\n            Reason: \"AlreadyExists\",\n            Details: {Name: \"iperf\", Group: \"\", Kind: \"namespaces\", UID: \"\", Causes: nil, RetryAfterSeconds: 0},\n            Code: 409,\n        },\n    }\n    namespaces \"iperf\" already exists\noccurred","time":{"start":1659450916000,"stop":1659450916321,"duration":321}},{"uid":"6772042a3518b2a","status":"passed","time":{"start":1659451524000,"stop":1659451524462,"duration":462}},{"uid":"6dce2773b75aa3cd","status":"failed","statusDetails":"Unexpected error:\n    <*errors.StatusError | 0xc000752000>: {\n        ErrStatus: {\n            TypeMeta: {Kind: \"\", APIVersion: \"\"},\n            ListMeta: {\n                SelfLink: \"\",\n                ResourceVersion: \"\",\n                Continue: \"\",\n                RemainingItemCount: nil,\n            },\n            Status: \"Failure\",\n            Message: \"namespaces \\\"iperf\\\" already exists\",\n            Reason: \"AlreadyExists\",\n            Details: {Name: \"iperf\", Group: \"\", Kind: \"namespaces\", UID: \"\", Causes: nil, RetryAfterSeconds: 0},\n            Code: 409,\n        },\n    }\n    namespaces \"iperf\" already exists\noccurred","time":{"start":1659447298000,"stop":1659447298248,"duration":248}},{"uid":"ef8909c34d0ed3cd","status":"failed","statusDetails":"Unexpected error:\n    <*errors.StatusError | 0xc00043ca00>: {\n        ErrStatus: {\n            TypeMeta: {Kind: \"\", APIVersion: \"\"},\n            ListMeta: {\n                SelfLink: \"\",\n                ResourceVersion: \"\",\n                Continue: \"\",\n                RemainingItemCount: nil,\n            },\n            Status: \"Failure\",\n            Message: \"namespaces \\\"iperf\\\" already exists\",\n            Reason: \"AlreadyExists\",\n            Details: {Name: \"iperf\", Group: \"\", Kind: \"namespaces\", UID: \"\", Causes: nil, RetryAfterSeconds: 0},\n            Code: 409,\n        },\n    }\n    namespaces \"iperf\" already exists\noccurred","time":{"start":1659447116000,"stop":1659447116608,"duration":608}},{"uid":"59f7f8c7e2799c5b","status":"failed","statusDetails":"Unexpected error:\n    <*errors.StatusError | 0xc000342280>: {\n        ErrStatus: {\n            TypeMeta: {Kind: \"\", APIVersion: \"\"},\n            ListMeta: {\n                SelfLink: \"\",\n                ResourceVersion: \"\",\n                Continue: \"\",\n                RemainingItemCount: nil,\n            },\n            Status: \"Failure\",\n            Message: \"object is being deleted: namespaces \\\"appns1\\\" already exists\",\n            Reason: \"AlreadyExists\",\n            Details: {Name: \"appns1\", Group: \"\", Kind: \"namespaces\", UID: \"\", Causes: nil, RetryAfterSeconds: 0},\n            Code: 409,\n        },\n    }\n    object is being deleted: namespaces \"appns1\" already exists\noccurred","time":{"start":1659448205000,"stop":1659448205283,"duration":283}},{"uid":"db29c0433201052c","status":"passed","time":{"start":1659448097000,"stop":1659448097616,"duration":616}},{"uid":"94df662a3c8d2b36","status":"passed","time":{"start":1659409279000,"stop":1659409279484,"duration":484}},{"uid":"f4c914ae1285db3f","status":"failed","statusDetails":"Unexpected error:\n    <*errors.StatusError | 0xc0009aca00>: {\n        ErrStatus: {\n            TypeMeta: {Kind: \"\", APIVersion: \"\"},\n            ListMeta: {\n                SelfLink: \"\",\n                ResourceVersion: \"\",\n                Continue: \"\",\n                RemainingItemCount: nil,\n            },\n            Status: \"Failure\",\n            Message: \"namespaces \\\"iperf\\\" already exists\",\n            Reason: \"AlreadyExists\",\n            Details: {Name: \"iperf\", Group: \"\", Kind: \"namespaces\", UID: \"\", Causes: nil, RetryAfterSeconds: 0},\n            Code: 409,\n        },\n    }\n    namespaces \"iperf\" already exists\noccurred","time":{"start":1659402555000,"stop":1659402555323,"duration":323}},{"uid":"981b2ddd264e7cec","status":"passed","time":{"start":1659291287000,"stop":1659291288144,"duration":1144}}]},"Worker Suite:Worker Suite#[It] Intracluster slice tests Iperf connectivity Verify Iperf traffic":{"statistic":{"failed":0,"broken":0,"skipped":30,"passed":5,"unknown":0,"total":35},"items":[{"uid":"93e4a76dfc51a352","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"5279f931074749bd","status":"passed","time":{"start":1660104898000,"stop":1660104967723,"duration":69723}},{"uid":"a60ed165405bfb9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"ac8fbb740fe2a6d8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}},{"uid":"ca3cdc17533b3532","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659982549000,"stop":1659982549000,"duration":0}},{"uid":"ff7453967a47588","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659970182000,"stop":1659970182000,"duration":0}},{"uid":"3444acdca0b6787","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659944495000,"stop":1659944495000,"duration":0}},{"uid":"9d4c37ca7aca3ad8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659874977000,"stop":1659874977000,"duration":0}},{"uid":"43bc1c6c4e266c89","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496050000,"stop":1659496050000,"duration":0}},{"uid":"67aac1a8e1dc2e42","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496083000,"stop":1659496083000,"duration":0}},{"uid":"74dd338d4d7d570b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"4a40569094ff9837","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659450916000,"stop":1659450916000,"duration":0}},{"uid":"b391898554e5bea7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"df2985074c9ee1d6","status":"passed","time":{"start":1659447298000,"stop":1659447368306,"duration":70306}},{"uid":"24fc39a6ff851b8d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447116000,"stop":1659447116000,"duration":0}},{"uid":"576e105b7bbdaf2c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448205000,"stop":1659448205000,"duration":0}},{"uid":"13c9a2bd1022ca13","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448097000,"stop":1659448097000,"duration":0}},{"uid":"a6b0ac86ff9e6287","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659409279000,"stop":1659409279000,"duration":0}},{"uid":"1cdb92d147e84b9c","status":"passed","time":{"start":1659402555000,"stop":1659402625579,"duration":70579}},{"uid":"9de784483625bd70","status":"passed","time":{"start":1659291287000,"stop":1659291356445,"duration":69445}}]},"Worker Suite:Worker Suite#[It] Intracluster slice tests Slice with netpol should reconcile netpol config":{"statistic":{"failed":6,"broken":0,"skipped":14,"passed":15,"unknown":0,"total":35},"items":[{"uid":"ad455162458977d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"ddbbf6a16b8f369a","status":"passed","time":{"start":1660104898000,"stop":1660104900319,"duration":2319}},{"uid":"964d7e47f9ad0048","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"dda066b66ff4e4e","status":"passed","time":{"start":1660047551000,"stop":1660047553273,"duration":2273}},{"uid":"4a6257fa541efb4f","status":"passed","time":{"start":1659982549000,"stop":1659982558004,"duration":9004}},{"uid":"c84e80603dc27b9c","status":"passed","time":{"start":1659970182000,"stop":1659970185573,"duration":3573}},{"uid":"4b15115c840bd5ce","status":"passed","time":{"start":1659944495000,"stop":1659944498821,"duration":3821}},{"uid":"be59dfbef116405d","status":"passed","time":{"start":1659874977000,"stop":1659874987540,"duration":10540}},{"uid":"ea8c0ba3980e3824","status":"failed","statusDetails":"\n\tError Trace:\tnamespace.go:15\n\t            \t\t\t\tt_intra_slice_test.go:98\n\t            \t\t\t\tsuite.go:596\n\t            \t\t\t\tasm_amd64.s:1581\n\tError:      \tReceived unexpected error:\n\t            \tnamespaces \"iperf\" already exists\n\tTest:       \tIntracluster slice tests Slice with netpol should reconcile netpol config\n","time":{"start":1659496050000,"stop":1659496097093,"duration":47093}},{"uid":"be343f482aa846fb","status":"failed","statusDetails":"\n\tError Trace:\tnamespace.go:15\n\t            \t\t\t\tt_intra_slice_test.go:98\n\t            \t\t\t\tsuite.go:596\n\t            \t\t\t\tasm_amd64.s:1581\n\tError:      \tReceived unexpected error:\n\t            \tnamespaces \"iperf\" already exists\n\tTest:       \tIntracluster slice tests Slice with netpol should reconcile netpol config\n","time":{"start":1659496083000,"stop":1659496089748,"duration":6748}},{"uid":"51d01dba53daab9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"f06401c869eecfb1","status":"passed","time":{"start":1659450916000,"stop":1659450919797,"duration":3797}},{"uid":"5b5da0a1cdfe5687","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"e797d8a6a47a327c","status":"passed","time":{"start":1659447298000,"stop":1659447319006,"duration":21006}},{"uid":"49fd959544321e7b","status":"passed","time":{"start":1659447116000,"stop":1659447119635,"duration":3635}},{"uid":"8c2d897cfb759a33","status":"passed","time":{"start":1659448205000,"stop":1659448207401,"duration":2401}},{"uid":"a57a261f0ce64b6e","status":"failed","statusDetails":"\n\tError Trace:\tnamespace.go:15\n\t            \t\t\t\tt_intra_slice_test.go:98\n\t            \t\t\t\tsuite.go:596\n\t            \t\t\t\tasm_amd64.s:1581\n\tError:      \tReceived unexpected error:\n\t            \tnamespaces \"iperf\" already exists\n\tTest:       \tIntracluster slice tests Slice with netpol should reconcile netpol config\n","time":{"start":1659448097000,"stop":1659448105833,"duration":8833}},{"uid":"fc890f83c2826889","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659409279000,"stop":1659409279000,"duration":0}},{"uid":"8ec6ba99e8e5d496","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659402555000,"stop":1659402555000,"duration":0}},{"uid":"8563d16e8b2d2fde","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659291287000,"stop":1659291287000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice creation scenarios: when slice red is installed install slice blue workers should have gateway pods from both slices":{"statistic":{"failed":0,"broken":0,"skipped":24,"passed":11,"unknown":0,"total":35},"items":[{"uid":"2b39c17db0aba308","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"34eb0385ebe89ef8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"f9fe66694047a9a7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"edc7efe3b07c09dc","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}},{"uid":"cc5d4e542e96e425","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659982549000,"stop":1659982549000,"duration":0}},{"uid":"7f37d7b482dd457","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659970182000,"stop":1659970182000,"duration":0}},{"uid":"8332a71818fe843b","status":"passed","time":{"start":1659944495000,"stop":1659944504383,"duration":9383}},{"uid":"fad9629d5a83c829","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659874977000,"stop":1659874977000,"duration":0}},{"uid":"1850f7250c447a17","status":"passed","time":{"start":1659496050000,"stop":1659496080822,"duration":30822}},{"uid":"273a69b09f814b","status":"passed","time":{"start":1659496083000,"stop":1659496105829,"duration":22829}},{"uid":"695fad9ae7bc120b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"b2155d2daa6d5b32","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659450916000,"stop":1659450916000,"duration":0}},{"uid":"1485d347117437c9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"a35c1e3b90aaeff6","status":"passed","time":{"start":1659447298000,"stop":1659447350643,"duration":52643}},{"uid":"6ab6fc46852a951f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447116000,"stop":1659447116000,"duration":0}},{"uid":"acb126f895196dca","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448205000,"stop":1659448205000,"duration":0}},{"uid":"3d0e084a65292b43","status":"passed","time":{"start":1659448097000,"stop":1659448111921,"duration":14921}},{"uid":"f477b2ef4943328f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659409279000,"stop":1659409279000,"duration":0}},{"uid":"e5b2f3e14a457311","status":"passed","time":{"start":1659402555000,"stop":1659402572030,"duration":17030}},{"uid":"f01f73f7061de038","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659291287000,"stop":1659291287000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Namespace reconcile test scenarios: Onboard label app ns on workers":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":35,"unknown":0,"total":35},"items":[{"uid":"92e75ee914ea46c9","status":"passed","time":{"start":1660113628000,"stop":1660113628644,"duration":644}},{"uid":"2a0eef263107f5d8","status":"passed","time":{"start":1660104898000,"stop":1660104898751,"duration":751}},{"uid":"c1326a6b9188664e","status":"passed","time":{"start":1660067259000,"stop":1660067260343,"duration":1343}},{"uid":"7cfb401f88ceeb0e","status":"passed","time":{"start":1660047551000,"stop":1660047552076,"duration":1076}},{"uid":"78389f531eabe6e2","status":"passed","time":{"start":1659982549000,"stop":1659982551207,"duration":2207}},{"uid":"e033e5f928b37871","status":"passed","time":{"start":1659970182000,"stop":1659970182878,"duration":878}},{"uid":"2ebea12dbb57daaf","status":"passed","time":{"start":1659944495000,"stop":1659944496942,"duration":1942}},{"uid":"38ef915aafe41c3a","status":"passed","time":{"start":1659874977000,"stop":1659874978507,"duration":1507}},{"uid":"9984faa860790168","status":"passed","time":{"start":1659496050000,"stop":1659496050690,"duration":690}},{"uid":"e9975b2c6430b06e","status":"passed","time":{"start":1659496083000,"stop":1659496083847,"duration":847}},{"uid":"838ca9c333cd6ef0","status":"passed","time":{"start":1659451501000,"stop":1659451501737,"duration":737}},{"uid":"f96fef4e3233c08a","status":"passed","time":{"start":1659450916000,"stop":1659450917648,"duration":1648}},{"uid":"ce31f053e0fba3b7","status":"passed","time":{"start":1659451524000,"stop":1659451525210,"duration":1210}},{"uid":"15770e8ef45c83ef","status":"passed","time":{"start":1659447298000,"stop":1659447298703,"duration":703}},{"uid":"545a7311c6090c29","status":"passed","time":{"start":1659447116000,"stop":1659447117520,"duration":1520}},{"uid":"e976408103243e1a","status":"passed","time":{"start":1659448205000,"stop":1659448205693,"duration":693}},{"uid":"8d9f78a6bcb063a9","status":"passed","time":{"start":1659448097000,"stop":1659448098677,"duration":1677}},{"uid":"93fa48e9e900c63b","status":"passed","time":{"start":1659409279000,"stop":1659409280201,"duration":1201}},{"uid":"293256103c55fa97","status":"passed","time":{"start":1659402555000,"stop":1659402555989,"duration":989}},{"uid":"4662b4df137a33d0","status":"passed","time":{"start":1659291287000,"stop":1659291287956,"duration":956}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests Iperf App connectivity should have vl3 router running":{"statistic":{"failed":20,"broken":0,"skipped":0,"passed":15,"unknown":0,"total":35},"items":[{"uid":"9c1ac4c625de9212","status":"passed","time":{"start":1660113628000,"stop":1660113628487,"duration":487}},{"uid":"8b394d22a22410ed","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 4\nto equal\n    <int>: 1","time":{"start":1660104898000,"stop":1660105081464,"duration":183464}},{"uid":"45c02c19e20732cd","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 2\nto equal\n    <int>: 1","time":{"start":1660067259000,"stop":1660067442133,"duration":183133}},{"uid":"397ec039985cdb94","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 2\nto equal\n    <int>: 1","time":{"start":1660047551000,"stop":1660047731889,"duration":180889}},{"uid":"31e3dd06a5376a65","status":"passed","time":{"start":1659982549000,"stop":1659982549652,"duration":652}},{"uid":"c9a9428c9bf70aab","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 2\nto equal\n    <int>: 1","time":{"start":1659970182000,"stop":1659970365167,"duration":183167}},{"uid":"c9b0b13fc07b9b93","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 2\nto equal\n    <int>: 1","time":{"start":1659944495000,"stop":1659944678278,"duration":183278}},{"uid":"fc3f38909d6dcc50","status":"passed","time":{"start":1659874977000,"stop":1659874977719,"duration":719}},{"uid":"a6acd4401d239c77","status":"passed","time":{"start":1659496050000,"stop":1659496050571,"duration":571}},{"uid":"4618911f61a7a8e","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 4\nto equal\n    <int>: 1","time":{"start":1659496083000,"stop":1659496266318,"duration":183318}},{"uid":"9111b1bffcd7e71a","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1659451501000,"stop":1659451685089,"duration":184089}},{"uid":"570745321da0801b","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 2\nto equal\n    <int>: 1","time":{"start":1659450916000,"stop":1659451104080,"duration":188080}},{"uid":"50d2afb671e492a","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1659451524000,"stop":1659451709724,"duration":185724}},{"uid":"43f4f381032348ff","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 2\nto equal\n    <int>: 1","time":{"start":1659447298000,"stop":1659447483021,"duration":185021}},{"uid":"480bf9ab0343e342","status":"passed","time":{"start":1659447116000,"stop":1659447116804,"duration":804}},{"uid":"fca79682723bfda7","status":"failed","statusDetails":"Unexpected error:\n    <*errors.StatusError | 0xc0007183c0>: {\n        ErrStatus: {\n            TypeMeta: {Kind: \"\", APIVersion: \"\"},\n            ListMeta: {\n                SelfLink: \"\",\n                ResourceVersion: \"\",\n                Continue: \"\",\n                RemainingItemCount: nil,\n            },\n            Status: \"Failure\",\n            Message: \"namespaces \\\"iperf\\\" already exists\",\n            Reason: \"AlreadyExists\",\n            Details: {Name: \"iperf\", Group: \"\", Kind: \"namespaces\", UID: \"\", Causes: nil, RetryAfterSeconds: 0},\n            Code: 409,\n        },\n    }\n    namespaces \"iperf\" already exists\noccurred","time":{"start":1659448205000,"stop":1659448206275,"duration":1275}},{"uid":"704831e55e966c30","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 3\nto equal\n    <int>: 1","time":{"start":1659448097000,"stop":1659448280299,"duration":183299}},{"uid":"a8e617aae9fe80dd","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1659409279000,"stop":1659409462736,"duration":183736}},{"uid":"b6e48c1574ab23d3","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 4\nto equal\n    <int>: 1","time":{"start":1659402555000,"stop":1659402738582,"duration":183582}},{"uid":"88c7af1dde0c6fcc","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 3\nto equal\n    <int>: 1","time":{"start":1659291287000,"stop":1659291470001,"duration":183001}}]},"Worker Suite:Worker Suite#[It] Slice Config Negative Scenario tests Slice Config Negative Scenario Update Should fail while updating sliceconfig with bandwithguaranteedkbps more than bandwidthceilingkbps in qosprofiledetailsnvalid ":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":35,"unknown":0,"total":35},"items":[{"uid":"fdb8860c9119d6c6","status":"passed","time":{"start":1660113628000,"stop":1660113628216,"duration":216}},{"uid":"7f217d9fae26e48d","status":"passed","time":{"start":1660104898000,"stop":1660104898230,"duration":230}},{"uid":"baddd58a1b48f1e3","status":"passed","time":{"start":1660067259000,"stop":1660067259302,"duration":302}},{"uid":"db127599586e5ed9","status":"passed","time":{"start":1660047551000,"stop":1660047551612,"duration":612}},{"uid":"31970597fea6bd34","status":"passed","time":{"start":1659982549000,"stop":1659982549844,"duration":844}},{"uid":"285ed495ec1158df","status":"passed","time":{"start":1659970182000,"stop":1659970182378,"duration":378}},{"uid":"9c3f0b47caa5fe25","status":"passed","time":{"start":1659944495000,"stop":1659944495549,"duration":549}},{"uid":"47091ca595e1de2d","status":"passed","time":{"start":1659874977000,"stop":1659874977406,"duration":406}},{"uid":"1289aea424ef4089","status":"passed","time":{"start":1659496050000,"stop":1659496050404,"duration":404}},{"uid":"33d7db9f8743e785","status":"passed","time":{"start":1659496083000,"stop":1659496083533,"duration":533}},{"uid":"cbdb89252fe0a4e6","status":"passed","time":{"start":1659451501000,"stop":1659451501800,"duration":800}},{"uid":"959934a4b45a4b1d","status":"passed","time":{"start":1659450916000,"stop":1659450916436,"duration":436}},{"uid":"23dc11f4e6270956","status":"passed","time":{"start":1659451524000,"stop":1659451524395,"duration":395}},{"uid":"f1e5ab2dfdbe7365","status":"passed","time":{"start":1659447298000,"stop":1659447298312,"duration":312}},{"uid":"b914e019a1e1d757","status":"passed","time":{"start":1659447116000,"stop":1659447116262,"duration":262}},{"uid":"d2a3480049a451e9","status":"passed","time":{"start":1659448205000,"stop":1659448205478,"duration":478}},{"uid":"9ba107789126166f","status":"passed","time":{"start":1659448097000,"stop":1659448097511,"duration":511}},{"uid":"ad423c8a82c17fa1","status":"passed","time":{"start":1659409279000,"stop":1659409279500,"duration":500}},{"uid":"e68deb8c277a90d8","status":"passed","time":{"start":1659402555000,"stop":1659402555482,"duration":482}},{"uid":"a33dd46004edc17","status":"passed","time":{"start":1659291287000,"stop":1659291287247,"duration":247}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Update Should update successfully when Project Applied while removing a service account name in Write users":{"statistic":{"failed":7,"broken":0,"skipped":0,"passed":79,"unknown":0,"total":86},"items":[{"uid":"62ded4bc1fc0a2fa","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0008103a8>: {\n        Underlying: <*exec.ExitError | 0xc000888fa0>{\n            ProcessState: {\n                pid: 6222,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 163153},\n                    Stime: {Sec: 0, Usec: 19896},\n                    Maxrss: 74364,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 6187,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13376,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 364,\n                    Nivcsw: 361,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The Project \"projectupdatetest\" is invalid: project namespace: Invalid value: \"kubeslice-projectupdatetest\": already exists\noccurred","time":{"start":1660145110000,"stop":1660145170244,"duration":60244}},{"uid":"5db9532dbc7b0683","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00066a270>: {\n        Underlying: <*exec.ExitError | 0xc00086e0a0>{\n            ProcessState: {\n                pid: 6326,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 158553},\n                    Stime: {Sec: 0, Usec: 50273},\n                    Maxrss: 80680,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 9301,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 13392,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 419,\n                    Nivcsw: 272,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The Project \"projectupdatetest\" is invalid: project namespace: Invalid value: \"kubeslice-projectupdatetest\": already exists\noccurred","time":{"start":1660142645000,"stop":1660142705263,"duration":60263}},{"uid":"b24b35314822605b","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000130588>: {\n        Underlying: <*exec.ExitError | 0xc0008fa040>{\n            ProcessState: {\n                pid: 6148,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 126910},\n                    Stime: {Sec: 0, Usec: 46149},\n                    Maxrss: 76640,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 9079,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13392,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 378,\n                    Nivcsw: 326,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The Project \"projectupdatetest\" is invalid: project namespace: Invalid value: \"kubeslice-projectupdatetest\": already exists\noccurred","time":{"start":1660141834000,"stop":1660141894225,"duration":60225}},{"uid":"d250d5e285aa9f3a","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000740150>: {\n        Underlying: <*exec.ExitError | 0xc0006703c0>{\n            ProcessState: {\n                pid: 6184,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 125430},\n                    Stime: {Sec: 0, Usec: 34841},\n                    Maxrss: 85848,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 11201,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13376,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 446,\n                    Nivcsw: 270,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The Project \"projectupdatetest\" is invalid: project namespace: Invalid value: \"kubeslice-projectupdatetest\": already exists\noccurred","time":{"start":1660138561000,"stop":1660138621216,"duration":60216}},{"uid":"ae6169af8465c35b","status":"passed","time":{"start":1660116744000,"stop":1660116749766,"duration":5766}},{"uid":"ff59e5f49026c2f","status":"passed","time":{"start":1660116144000,"stop":1660116148413,"duration":4413}},{"uid":"b95aa4019a2e3738","status":"passed","time":{"start":1660112860000,"stop":1660112864436,"duration":4436}},{"uid":"d38c61fde7f291f2","status":"passed","time":{"start":1660108970000,"stop":1660108974513,"duration":4513}},{"uid":"dc0117353809668a","status":"passed","time":{"start":1660104124000,"stop":1660104129690,"duration":5690}},{"uid":"ffa67a4db7b07016","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000353200>: {\n        Underlying: <*exec.ExitError | 0xc0006a50e0>{\n            ProcessState: {\n                pid: 6299,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 141717},\n                    Stime: {Sec: 0, Usec: 49792},\n                    Maxrss: 78080,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 10832,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13392,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 356,\n                    Nivcsw: 610,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The Project \"projectupdatetest\" is invalid: project namespace: Invalid value: \"kubeslice-projectupdatetest\": already exists\noccurred","time":{"start":1660104760000,"stop":1660104820277,"duration":60277}},{"uid":"1ee82e9ef47a0526","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00060f248>: {\n        Underlying: <*exec.ExitError | 0xc0001f44a0>{\n            ProcessState: {\n                pid: 6131,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 124084},\n                    Stime: {Sec: 0, Usec: 28019},\n                    Maxrss: 78924,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3431,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13376,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 437,\n                    Nivcsw: 334,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The Project \"projectupdatetest\" is invalid: project namespace: Invalid value: \"kubeslice-projectupdatetest\": already exists\noccurred","time":{"start":1660101116000,"stop":1660101176232,"duration":60232}},{"uid":"e24954aff8149a25","status":"passed","time":{"start":1660100168000,"stop":1660100173757,"duration":5757}},{"uid":"dacc7ad6ebced32e","status":"passed","time":{"start":1660066476000,"stop":1660066480459,"duration":4459}},{"uid":"77fb255fc3bf02d4","status":"passed","time":{"start":1660064121000,"stop":1660064125447,"duration":4447}},{"uid":"41d250106816d54b","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000440588>: {\n        Underlying: <*exec.ExitError | 0xc00069ce80>{\n            ProcessState: {\n                pid: 6207,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 142940},\n                    Stime: {Sec: 0, Usec: 54085},\n                    Maxrss: 71516,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 9785,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13392,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 360,\n                    Nivcsw: 433,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The Project \\\"projectupdatetest\\\" is invalid: project namespace: Invalid value: \\\"kubeslice-projectupdatetest\\\": already exists\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The Project \"projectupdatetest\" is invalid: project namespace: Invalid value: \"kubeslice-projectupdatetest\": already exists\noccurred","time":{"start":1660060680000,"stop":1660060740279,"duration":60279}},{"uid":"19c2c458ec48268a","status":"passed","time":{"start":1660052169000,"stop":1660052173992,"duration":4992}},{"uid":"5391cec9383b2874","status":"passed","time":{"start":1660050481000,"stop":1660050486621,"duration":5621}},{"uid":"1a1e38a756cfe3fa","status":"passed","time":{"start":1660046776000,"stop":1660046781390,"duration":5390}},{"uid":"8ddcc587451ff23f","status":"passed","time":{"start":1660046048000,"stop":1660046053380,"duration":5380}},{"uid":"f679648143f5b1e5","status":"passed","time":{"start":1660044532000,"stop":1660044537523,"duration":5523}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Update Should Update successfullyy when Project is Applied with valid service account name in Read users":{"statistic":{"failed":7,"broken":0,"skipped":0,"passed":79,"unknown":0,"total":86},"items":[{"uid":"c20f3633ed017e5f","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected an error to have occurred.  Got:\n    <nil>: nil","time":{"start":1660145110000,"stop":1660145172019,"duration":62019}},{"uid":"c4c488eba3ddd48e","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected an error to have occurred.  Got:\n    <nil>: nil","time":{"start":1660142645000,"stop":1660142707261,"duration":62261}},{"uid":"682ee4410b0d6668","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected an error to have occurred.  Got:\n    <nil>: nil","time":{"start":1660141834000,"stop":1660141896008,"duration":62008}},{"uid":"1c447c77637ef6e6","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected an error to have occurred.  Got:\n    <nil>: nil","time":{"start":1660138561000,"stop":1660138622974,"duration":61974}},{"uid":"ae8d514f9a12fb81","status":"passed","time":{"start":1660116744000,"stop":1660116753028,"duration":9028}},{"uid":"4bcbf8696886995a","status":"passed","time":{"start":1660116144000,"stop":1660116152875,"duration":8875}},{"uid":"4b0f8bc6af83da76","status":"passed","time":{"start":1660112860000,"stop":1660112867949,"duration":7949}},{"uid":"4f8a8fe43a7ad413","status":"passed","time":{"start":1660108970000,"stop":1660108978048,"duration":8048}},{"uid":"1c012b43145aacda","status":"passed","time":{"start":1660104124000,"stop":1660104132981,"duration":8981}},{"uid":"be7cad5cbb43baa0","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected an error to have occurred.  Got:\n    <nil>: nil","time":{"start":1660104760000,"stop":1660104822113,"duration":62113}},{"uid":"8fc4f5358d3132ee","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected an error to have occurred.  Got:\n    <nil>: nil","time":{"start":1660101116000,"stop":1660101177911,"duration":61911}},{"uid":"8298582378da8a9a","status":"passed","time":{"start":1660100168000,"stop":1660100177105,"duration":9105}},{"uid":"ba65f3e6dc6fe271","status":"passed","time":{"start":1660066476000,"stop":1660066483971,"duration":7971}},{"uid":"65782b1b6261076f","status":"passed","time":{"start":1660064121000,"stop":1660064129014,"duration":8014}},{"uid":"779a1354bf4bee19","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected an error to have occurred.  Got:\n    <nil>: nil","time":{"start":1660060680000,"stop":1660060742122,"duration":62122}},{"uid":"266de350eaeec64d","status":"passed","time":{"start":1660052169000,"stop":1660052178181,"duration":9181}},{"uid":"3647399786bbe04e","status":"passed","time":{"start":1660050481000,"stop":1660050489098,"duration":8098}},{"uid":"10a4347262c9a2a6","status":"passed","time":{"start":1660046776000,"stop":1660046784719,"duration":8719}},{"uid":"5fda55955463cc3b","status":"passed","time":{"start":1660046048000,"stop":1660046056020,"duration":8020}},{"uid":"47e98f5ddbe8e363","status":"passed","time":{"start":1660044532000,"stop":1660044540050,"duration":8050}}]},"Worker Suite:Worker Suite#[It] Attach/Deattach Scenario Attach cluster from slice  when Deattach cluster from slice  should have gateway pod running on attached cluster":{"statistic":{"failed":0,"broken":0,"skipped":23,"passed":3,"unknown":0,"total":26},"items":[{"uid":"555e31240e34c50e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"790d9b3d5f899d50","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"e235180fb4a2ffa7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"363616fddfb55827","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}},{"uid":"96d1b0dd0d5935bf","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659982549000,"stop":1659982549000,"duration":0}},{"uid":"e87263cf898fc369","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659970182000,"stop":1659970182000,"duration":0}},{"uid":"f482b73fcd6ee238","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659944495000,"stop":1659944495000,"duration":0}},{"uid":"549668cbae6f4d84","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659874977000,"stop":1659874977000,"duration":0}},{"uid":"e8d5cbdf96430536","status":"passed","time":{"start":1659496050000,"stop":1659496050092,"duration":92}},{"uid":"1a476fae25139fb","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496083000,"stop":1659496083000,"duration":0}},{"uid":"83c07334069f6936","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"7cfe380a4f59c443","status":"passed","time":{"start":1659450916000,"stop":1659450916111,"duration":111}},{"uid":"24a5b0e24eea21e6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"b431ad78de59c910","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447298000,"stop":1659447298000,"duration":0}},{"uid":"b50c2a230cd2654a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447116000,"stop":1659447116000,"duration":0}},{"uid":"b89ec0b725c3a4fe","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448205000,"stop":1659448205000,"duration":0}},{"uid":"9976ad1a96fa928b","status":"passed","time":{"start":1659448097000,"stop":1659448097125,"duration":125}},{"uid":"be5173c5cfe9b45c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659409279000,"stop":1659409279000,"duration":0}},{"uid":"d6f81dd4ef8483bc","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659402555000,"stop":1659402555000,"duration":0}},{"uid":"ca3a3c0ad21a7efe","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659291287000,"stop":1659291287000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Intracluster slice tests Iperf connectivity should onboard deployment in application Namespaces without slice annotations":{"statistic":{"failed":29,"broken":0,"skipped":0,"passed":6,"unknown":0,"total":35},"items":[{"uid":"eab693cd63959094","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 1\nto equal\n    <int>: 2","time":{"start":1660113628000,"stop":1660113817401,"duration":189401}},{"uid":"60d82e8838da507","status":"passed","time":{"start":1660104898000,"stop":1660104931357,"duration":33357}},{"uid":"a9fe513d1ac9c176","status":"failed","statusDetails":"Unexpected error:\n    <*errors.StatusError | 0xc0008fdcc0>: {\n        ErrStatus: {\n            TypeMeta: {Kind: \"\", APIVersion: \"\"},\n            ListMeta: {\n                SelfLink: \"\",\n                ResourceVersion: \"\",\n                Continue: \"\",\n                RemainingItemCount: nil,\n            },\n            Status: \"Failure\",\n            Message: \"namespaces \\\"appns1\\\" already exists\",\n            Reason: \"AlreadyExists\",\n            Details: {Name: \"appns1\", Group: \"\", Kind: \"namespaces\", UID: \"\", Causes: nil, RetryAfterSeconds: 0},\n            Code: 409,\n        },\n    }\n    namespaces \"appns1\" already exists\noccurred","time":{"start":1660067259000,"stop":1660067265261,"duration":6261}},{"uid":"6510f61f87f6472e","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 1\nto equal\n    <int>: 2","time":{"start":1660047551000,"stop":1660047741110,"duration":190110}},{"uid":"e2af5ba248631f5f","status":"failed","statusDetails":"Unexpected error:\n    <*errors.StatusError | 0xc0009460a0>: {\n        ErrStatus: {\n            TypeMeta: {Kind: \"\", APIVersion: \"\"},\n            ListMeta: {\n                SelfLink: \"\",\n                ResourceVersion: \"\",\n                Continue: \"\",\n                RemainingItemCount: nil,\n            },\n            Status: \"Failure\",\n            Message: \"namespaces \\\"appns1\\\" already exists\",\n            Reason: \"AlreadyExists\",\n            Details: {Name: \"appns1\", Group: \"\", Kind: \"namespaces\", UID: \"\", Causes: nil, RetryAfterSeconds: 0},\n            Code: 409,\n        },\n    }\n    namespaces \"appns1\" already exists\noccurred","time":{"start":1659982549000,"stop":1659982554162,"duration":5162}},{"uid":"76ae164694a735da","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 1\nto equal\n    <int>: 2","time":{"start":1659970182000,"stop":1659970371591,"duration":189591}},{"uid":"46e1425f925dab7","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 1\nto equal\n    <int>: 2","time":{"start":1659944495000,"stop":1659944685253,"duration":190253}},{"uid":"5f16298ecf0441a0","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 1\nto equal\n    <int>: 2","time":{"start":1659874977000,"stop":1659875166054,"duration":189054}},{"uid":"120f76f5576d0cb1","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 1\nto equal\n    <int>: 2","time":{"start":1659496050000,"stop":1659496239207,"duration":189207}},{"uid":"95cf27caac449ab3","status":"failed","statusDetails":"Unexpected error:\n    <*errors.StatusError | 0xc0000b1400>: {\n        ErrStatus: {\n            TypeMeta: {Kind: \"\", APIVersion: \"\"},\n            ListMeta: {\n                SelfLink: \"\",\n                ResourceVersion: \"\",\n                Continue: \"\",\n                RemainingItemCount: nil,\n            },\n            Status: \"Failure\",\n            Message: \"namespaces \\\"appns1\\\" already exists\",\n            Reason: \"AlreadyExists\",\n            Details: {Name: \"appns1\", Group: \"\", Kind: \"namespaces\", UID: \"\", Causes: nil, RetryAfterSeconds: 0},\n            Code: 409,\n        },\n    }\n    namespaces \"appns1\" already exists\noccurred","time":{"start":1659496083000,"stop":1659496089212,"duration":6212}},{"uid":"bbca4b55e0b9f6a0","status":"failed","statusDetails":"Unexpected error:\n    <*errors.StatusError | 0xc000624960>: {\n        ErrStatus: {\n            TypeMeta: {Kind: \"\", APIVersion: \"\"},\n            ListMeta: {\n                SelfLink: \"\",\n                ResourceVersion: \"\",\n                Continue: \"\",\n                RemainingItemCount: nil,\n            },\n            Status: \"Failure\",\n            Message: \"namespaces \\\"appns1\\\" already exists\",\n            Reason: \"AlreadyExists\",\n            Details: {Name: \"appns1\", Group: \"\", Kind: \"namespaces\", UID: \"\", Causes: nil, RetryAfterSeconds: 0},\n            Code: 409,\n        },\n    }\n    namespaces \"appns1\" already exists\noccurred","time":{"start":1659451501000,"stop":1659451507073,"duration":6073}},{"uid":"fd3780e463895c04","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 1\nto equal\n    <int>: 2","time":{"start":1659450916000,"stop":1659451105827,"duration":189827}},{"uid":"f28351a98a3fc559","status":"failed","statusDetails":"Unexpected error:\n    <*errors.StatusError | 0xc0006e2be0>: {\n        ErrStatus: {\n            TypeMeta: {Kind: \"\", APIVersion: \"\"},\n            ListMeta: {\n                SelfLink: \"\",\n                ResourceVersion: \"\",\n                Continue: \"\",\n                RemainingItemCount: nil,\n            },\n            Status: \"Failure\",\n            Message: \"namespaces \\\"appns1\\\" already exists\",\n            Reason: \"AlreadyExists\",\n            Details: {Name: \"appns1\", Group: \"\", Kind: \"namespaces\", UID: \"\", Causes: nil, RetryAfterSeconds: 0},\n            Code: 409,\n        },\n    }\n    namespaces \"appns1\" already exists\noccurred","time":{"start":1659451524000,"stop":1659451530205,"duration":6205}},{"uid":"5fda1d8097581a77","status":"passed","time":{"start":1659447298000,"stop":1659447320509,"duration":22509}},{"uid":"1656176ee08de5a0","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 2","time":{"start":1659447116000,"stop":1659447304906,"duration":188906}},{"uid":"4b15356ae35084d6","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 1\nto equal\n    <int>: 2","time":{"start":1659448205000,"stop":1659448394827,"duration":189827}},{"uid":"2fee0d54318175dd","status":"failed","statusDetails":"Unexpected error:\n    <*errors.StatusError | 0xc0009821e0>: {\n        ErrStatus: {\n            TypeMeta: {Kind: \"\", APIVersion: \"\"},\n            ListMeta: {\n                SelfLink: \"\",\n                ResourceVersion: \"\",\n                Continue: \"\",\n                RemainingItemCount: nil,\n            },\n            Status: \"Failure\",\n            Message: \"namespaces \\\"appns1\\\" already exists\",\n            Reason: \"AlreadyExists\",\n            Details: {Name: \"appns1\", Group: \"\", Kind: \"namespaces\", UID: \"\", Causes: nil, RetryAfterSeconds: 0},\n            Code: 409,\n        },\n    }\n    namespaces \"appns1\" already exists\noccurred","time":{"start":1659448097000,"stop":1659448103265,"duration":6265}},{"uid":"a8c0ed595d1d7ccb","status":"failed","statusDetails":"Unexpected error:\n    <*errors.StatusError | 0xc00042e640>: {\n        ErrStatus: {\n            TypeMeta: {Kind: \"\", APIVersion: \"\"},\n            ListMeta: {\n                SelfLink: \"\",\n                ResourceVersion: \"\",\n                Continue: \"\",\n                RemainingItemCount: nil,\n            },\n            Status: \"Failure\",\n            Message: \"namespaces \\\"appns1\\\" already exists\",\n            Reason: \"AlreadyExists\",\n            Details: {Name: \"appns1\", Group: \"\", Kind: \"namespaces\", UID: \"\", Causes: nil, RetryAfterSeconds: 0},\n            Code: 409,\n        },\n    }\n    namespaces \"appns1\" already exists\noccurred","time":{"start":1659409279000,"stop":1659409285383,"duration":6383}},{"uid":"df084123cf0eea01","status":"passed","time":{"start":1659402555000,"stop":1659402586838,"duration":31838}},{"uid":"1683da0422737d1","status":"passed","time":{"start":1659291287000,"stop":1659291309840,"duration":22840}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting vl3 pod Should restart vl3 pod":{"statistic":{"failed":0,"broken":0,"skipped":35,"passed":0,"unknown":0,"total":35},"items":[{"uid":"6d8449b7c8a56722","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"6e56ad5d7fd6f3fe","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"ae0e2b6adc6d0521","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"bec01fdc3a6c357a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}},{"uid":"ea7650c4c0ded86","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659982549000,"stop":1659982549000,"duration":0}},{"uid":"349df464973f84cd","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659970182000,"stop":1659970182000,"duration":0}},{"uid":"7aaa82744beeecee","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659944495000,"stop":1659944495000,"duration":0}},{"uid":"b2138379fac28fb","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659874977000,"stop":1659874977000,"duration":0}},{"uid":"250fcb183522d5e3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496050000,"stop":1659496050000,"duration":0}},{"uid":"28b6b3dd4f0cab5e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659496083000,"stop":1659496083000,"duration":0}},{"uid":"65eb609cab4f34ed","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"1bcee717a2b13273","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659450916000,"stop":1659450916000,"duration":0}},{"uid":"b77c86cf7d1efae6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"318665a126beea06","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447298000,"stop":1659447298000,"duration":0}},{"uid":"44200cf437485fb0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659447116000,"stop":1659447116000,"duration":0}},{"uid":"366748bc86b49eb6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448205000,"stop":1659448205000,"duration":0}},{"uid":"b8eff180e36c52f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659448097000,"stop":1659448097000,"duration":0}},{"uid":"970b7273c4579821","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659409279000,"stop":1659409279000,"duration":0}},{"uid":"5f34e0c74481921c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659402555000,"stop":1659402555000,"duration":0}},{"uid":"9932bec96ccb973c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659291287000,"stop":1659291287000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice Config Negative Scenario tests Slice Config Negative Scenario Creation Should fail while creating sliceconfig with bandwithguaranteedkbps more than bandwidthceilingkbps in qosprofiledetailsnvalid ":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":35,"unknown":0,"total":35},"items":[{"uid":"90a0e6b8d695258b","status":"passed","time":{"start":1660113628000,"stop":1660113628205,"duration":205}},{"uid":"55c390b09b40186d","status":"passed","time":{"start":1660104898000,"stop":1660104898200,"duration":200}},{"uid":"679987ed71045ea4","status":"passed","time":{"start":1660067259000,"stop":1660067259155,"duration":155}},{"uid":"7ed2d94e39a6cea1","status":"passed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}},{"uid":"4b770c5616a8c89a","status":"passed","time":{"start":1659982549000,"stop":1659982549613,"duration":613}},{"uid":"a25d7ba68bb69b85","status":"passed","time":{"start":1659970182000,"stop":1659970182000,"duration":0}},{"uid":"c814df820a004a88","status":"passed","time":{"start":1659944495000,"stop":1659944495000,"duration":0}},{"uid":"77a217487bebb8f8","status":"passed","time":{"start":1659874977000,"stop":1659874977265,"duration":265}},{"uid":"8ace5e607526ac43","status":"passed","time":{"start":1659496050000,"stop":1659496050000,"duration":0}},{"uid":"64b069d63825b8a2","status":"passed","time":{"start":1659496083000,"stop":1659496083000,"duration":0}},{"uid":"31e1bdde97820f9d","status":"passed","time":{"start":1659451501000,"stop":1659451501000,"duration":0}},{"uid":"9a6ab991f7ca653a","status":"passed","time":{"start":1659450916000,"stop":1659450916000,"duration":0}},{"uid":"fa9c9380ecc65db","status":"passed","time":{"start":1659451524000,"stop":1659451524000,"duration":0}},{"uid":"cf825e5c1afd3c31","status":"passed","time":{"start":1659447298000,"stop":1659447298000,"duration":0}},{"uid":"d9bc8a0fe4ed4154","status":"passed","time":{"start":1659447116000,"stop":1659447116000,"duration":0}},{"uid":"1ff28ec297cd02e4","status":"passed","time":{"start":1659448205000,"stop":1659448205258,"duration":258}},{"uid":"fb9b0c505c8173c8","status":"passed","time":{"start":1659448097000,"stop":1659448097000,"duration":0}},{"uid":"d6102b984599861d","status":"passed","time":{"start":1659409279000,"stop":1659409279194,"duration":194}},{"uid":"e2c8a80093c8264a","status":"passed","time":{"start":1659402555000,"stop":1659402555000,"duration":0}},{"uid":"2f7dc79864165e48","status":"passed","time":{"start":1659291287000,"stop":1659291287000,"duration":0}}]},"Hub Suite:Hub Suite#[It] Cluster CR tests Cluster CR validation Register worker cluster":{"statistic":{"failed":18,"broken":0,"skipped":0,"passed":71,"unknown":0,"total":89},"items":[{"uid":"371208622a8808b2","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0001fe030>: {\n        Underlying: <*exec.ExitError | 0xc0007d4000>{\n            ProcessState: {\n                pid: 6139,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 2, Usec: 291447},\n                    Stime: {Sec: 0, Usec: 149794},\n                    Maxrss: 83624,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 9364,\n                    Majflt: 10,\n                    Nswap: 0,\n                    Inblock: 1016,\n                    Oublock: 14424,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 4946,\n                    Nivcsw: 2749,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.4.1.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:165: [debug] Clearing discovery cache\",\n                        \"wait.go:48: [debug] beginning wait for 5 resources with timeout of 1m0s\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:310: [debug] Starting delete for \\\"kubeslice-preinstall\\\" ServiceAccount\",\n                        \"client.go:339: [debug] serviceaccounts \\\"kubeslice-preinstall\\\" not found\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:310: [debug] Starting delete for \\\"kubeslice-preinstall-role\\\" ClusterRole\",\n                        \"client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \\\"kubeslice-preinstall-role\\\" not found\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:310: [debug] Starting delete for \\\"kubeslice-preinstall-rolebinding\\\" ClusterRoleBinding\",\n                        \"client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \\\"kubeslice-preinstall-rolebinding\\\" not found\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:310: [debug] Starting delete for \\\"kubeslice-worker-preinstall-configmap\\\" ConfigMap\",\n                        \"client.go:339: [debug] configmaps \\\"kubeslice-worker-preinstall-configmap\\\" not found\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:310: [debug] Starting delete for \\\"kubeslice-worker-preinstall-job\\\" Job\",\n                        \"client.go:339: [debug] jobs.batch \\\"kubeslice-worker-preinstall-job\\\" not found\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:540: [debug] Watching for changes to Job kubeslice-worker-preinstall-job with timeout of 2m0s\",\n                        \"client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: ADDED\",\n                        \"client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 0, jobs failed: 0, jobs succeeded: 0\",\n                        \"client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\",\n                        \"client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\",\n                        \"client.go:568:...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.4.1.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:165: [debug] Clearing discovery cache\n    wait.go:48: [debug] beginning wait for 5 resources with timeout of 1m0s\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:310: [debug] Starting delete for \"kubeslice-preinstall\" ServiceAccount\n    client.go:339: [debug] serviceaccounts \"kubeslice-preinstall\" not found\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:310: [debug] Starting delete for \"kubeslice-preinstall-role\" ClusterRole\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-preinstall-role\" not found\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:310: [debug] Starting delete for \"kubeslice-preinstall-rolebinding\" ClusterRoleBinding\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-preinstall-rolebinding\" not found\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:310: [debug] Starting delete for \"kubeslice-worker-preinstall-configmap\" ConfigMap\n    client.go:339: [debug] configmaps \"kubeslice-worker-preinstall-configmap\" not found\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:310: [debug] Starting delete for \"kubeslice-worker-preinstall-job\" Job\n    client.go:339: [debug] jobs.batch \"kubeslice-worker-preinstall-job\" not found\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:540: [debug] Watching for changes to Job kubeslice-worker-preinstall-job with timeout of 2m0s\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: ADDED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 0, jobs failed: 0, jobs succeeded: 0\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 1, jobs succeeded: 0\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 2, jobs succeeded: 0\n    client.go:310: [debug] Starting delete for \"kubeslice-worker-preinstall-job\" Job\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:310: [debug] Starting delete for \"jaeger\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-svc\" Service\n    client.go:339: [debug] services \"jaeger\" not found\n    client.go:339: [debug] services \"kubeslice-webhook-service\" not found\n    client.go:339: [debug] services \"nsm-admission-webhook-svc\" not found\n    client.go:339: [debug] services \"kubeslice-dns\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:310: [debug] Starting delete for \"jaeger\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook\" Deployment\n    client.go:310: [debug] Starting delete for \"prefix-service\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:339: [debug] deployments.apps \"kubeslice-dns\" not found\n    client.go:339: [debug] deployments.apps \"kubeslice-operator\" not found\n    client.go:339: [debug] deployments.apps \"prefix-service\" not found\n    client.go:339: [debug] deployments.apps \"nsm-admission-webhook\" not found\n    client.go:339: [debug] deployments.apps \"jaeger\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsm-kernel-forwarder\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:339: [debug] daemonsets.apps \"nsmgr\" not found\n    client.go:339: [debug] daemonsets.apps \"nsm-kernel-forwarder\" not found\n    client.go:339: [debug] daemonsets.apps \"kubeslice-netop\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:339: [debug] rolebindings.rbac.authorization.k8s.io \"kubeslice-leader-election-rolebinding\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:339: [debug] roles.rbac.authorization.k8s.io \"kubeslice-leader-election-role\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-kubernetes-dashboard\" not found\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-proxy-rolebinding\" not found\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-dns-rolebinding\" not found\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"nsm-role-binding\" not found\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-manager-rolebinding\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"nsm-role\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-kubernetes-dashboard\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-proxy-role\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-dns-role\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-metrics-reader\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-manager-role\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"aggregate-network-services-view\" not found\n    client.go:310: [debug] Starting delete for \"networkservices.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkserviceendpoints.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservicemanagers.networkservicemesh.io\" CustomResourceDefinition\n    client.go:339: [debug] customresourcedefinitions.apiextensions.k8s.io \"networkserviceendpoints.networkservicemesh.io\" not found\n    client.go:339: [debug] customresourcedefinitions.apiextensions.k8s.io \"networkservicemanagers.networkservicemesh.io\" not found\n    client.go:339: [debug] customresourcedefinitions.apiextensions.k8s.io \"networkservices.networkservicemesh.io\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:339: [debug] configmaps \"kubeslice-manager-config\" not found\n    client.go:339: [debug] configmaps \"nsm-config\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:339: [debug] secrets \"nsm-admission-webhook-certs\" not found\n    client.go:339: [debug] secrets \"kubeslice-hub\" not found\n    client.go:339: [debug] secrets \"kubeslice-admission-webhook-certs\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:339: [debug] serviceaccounts \"nsc-acc\" not found\n    client.go:339: [debug] serviceaccounts \"kubeslice-kubernetes-dashboard\" not found\n    client.go:339: [debug] serviceaccounts \"vpn-gateway-client\" not found\n    client.go:339: [debug] serviceaccounts \"vpn-gateway-server\" not found\n    client.go:339: [debug] serviceaccounts \"forward-plane-acc\" not found\n    client.go:339: [debug] serviceaccounts \"nsmgr-acc\" not found\n    client.go:339: [debug] serviceaccounts \"slice-router\" not found\n    client.go:339: [debug] serviceaccounts \"kubeslice-dns\" not found\n    client.go:339: [debug] serviceaccounts \"nse-acc\" not found\n    client.go:339: [debug] serviceaccounts \"kubeslice-netop\" not found\n    client.go:339: [debug] serviceaccounts \"kubeslice-controller-manager\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-cfg\" MutatingWebhookConfiguration\n    client.go:339: [debug] mutatingwebhookconfigurations.admissionregistration.k8s.io \"nsm-admission-webhook-cfg\" not found\n    client.go:339: [debug] mutatingwebhookconfigurations.admissionregistration.k8s.io \"kubeslice-mutating-webhook-configuration\" not found\n    uninstall.go:144: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: failed pre-install: timed out waiting for the condition\n    helm.go:84: [debug] failed pre-install: timed out waiting for the condition\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:361\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1660145110000,"stop":1660145448159,"duration":338159}},{"uid":"877602e953817f43","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0005d64b0>: {\n        Underlying: <*exec.ExitError | 0xc00061d520>{\n            ProcessState: {\n                pid: 6242,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 1, Usec: 480502},\n                    Stime: {Sec: 0, Usec: 132762},\n                    Maxrss: 89276,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 6322,\n                    Majflt: 7,\n                    Nswap: 0,\n                    Inblock: 544,\n                    Oublock: 15144,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 2926,\n                    Nivcsw: 1813,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.4.1.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD serviceexports.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD serviceimports.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD slice.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD slicegateways.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD sliceresourcequotas.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"Error: INSTALLATION FAILED: create: failed to create: secrets \\\"sh.helm.release.v1.kubeslice-worker.v1\\\" is forbidden: unable to create new content in namespace kubeslice-system because it is being terminated\",\n                        \"helm.go:84: [debug] secrets \\\"sh.helm.release.v1.kubeslice-worker.v1\\\" is forbidden: unable to create new content in namespace kubeslice-system because it is being terminated\",\n                        \"create: failed to create\",\n                        \"helm.sh/helm/v3/pkg/storage/driver.(*Secrets).Create\",\n                        \"\\thelm.sh/helm/v3/pkg/storage/driver/secrets.go:164\",\n                        \"helm.sh/helm/v3/pkg/storage.(*Storage).Create\",\n                        \"\\thelm.sh/helm/v3/pkg/storage/storage.go:69\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:340\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:264\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:125\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.4.1.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD serviceexports.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD serviceimports.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD slice.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD slicegateways.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD sliceresourcequotas.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    Error: INSTALLATION FAILED: create: failed to create: secrets \"sh.helm.release.v1.kubeslice-worker.v1\" is forbidden: unable to create new content in namespace kubeslice-system because it is being terminated\n    helm.go:84: [debug] secrets \"sh.helm.release.v1.kubeslice-worker.v1\" is forbidden: unable to create new content in namespace kubeslice-system because it is being terminated\n    create: failed to create\n    helm.sh/helm/v3/pkg/storage/driver.(*Secrets).Create\n    \thelm.sh/helm/v3/pkg/storage/driver/secrets.go:164\n    helm.sh/helm/v3/pkg/storage.(*Storage).Create\n    \thelm.sh/helm/v3/pkg/storage/storage.go:69\n    helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\n    \thelm.sh/helm/v3/pkg/action/install.go:340\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:264\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:125\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1660142645000,"stop":1660142859368,"duration":214368}},{"uid":"4c24330e04624c13","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000400048>: {\n        Underlying: <*exec.ExitError | 0xc00067a000>{\n            ProcessState: {\n                pid: 5997,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 2, Usec: 140732},\n                    Stime: {Sec: 0, Usec: 112670},\n                    Maxrss: 82688,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 13904,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14424,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 5407,\n                    Nivcsw: 3520,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.4.1.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:165: [debug] Clearing discovery cache\",\n                        \"wait.go:48: [debug] beginning wait for 5 resources with timeout of 1m0s\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:310: [debug] Starting delete for \\\"kubeslice-preinstall\\\" ServiceAccount\",\n                        \"client.go:339: [debug] serviceaccounts \\\"kubeslice-preinstall\\\" not found\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:310: [debug] Starting delete for \\\"kubeslice-preinstall-role\\\" ClusterRole\",\n                        \"client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \\\"kubeslice-preinstall-role\\\" not found\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:310: [debug] Starting delete for \\\"kubeslice-preinstall-rolebinding\\\" ClusterRoleBinding\",\n                        \"client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \\\"kubeslice-preinstall-rolebinding\\\" not found\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:310: [debug] Starting delete for \\\"kubeslice-worker-preinstall-configmap\\\" ConfigMap\",\n                        \"client.go:339: [debug] configmaps \\\"kubeslice-worker-preinstall-configmap\\\" not found\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:310: [debug] Starting delete for \\\"kubeslice-worker-preinstall-job\\\" Job\",\n                        \"client.go:339: [debug] jobs.batch \\\"kubeslice-worker-preinstall-job\\\" not found\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:540: [debug] Watching for changes to Job kubeslice-worker-preinstall-job with timeout of 2m0s\",\n                        \"client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: ADDED\",\n                        \"client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 0, jobs failed: 0, jobs succe...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.4.1.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:165: [debug] Clearing discovery cache\n    wait.go:48: [debug] beginning wait for 5 resources with timeout of 1m0s\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:310: [debug] Starting delete for \"kubeslice-preinstall\" ServiceAccount\n    client.go:339: [debug] serviceaccounts \"kubeslice-preinstall\" not found\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:310: [debug] Starting delete for \"kubeslice-preinstall-role\" ClusterRole\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-preinstall-role\" not found\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:310: [debug] Starting delete for \"kubeslice-preinstall-rolebinding\" ClusterRoleBinding\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-preinstall-rolebinding\" not found\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:310: [debug] Starting delete for \"kubeslice-worker-preinstall-configmap\" ConfigMap\n    client.go:339: [debug] configmaps \"kubeslice-worker-preinstall-configmap\" not found\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:310: [debug] Starting delete for \"kubeslice-worker-preinstall-job\" Job\n    client.go:339: [debug] jobs.batch \"kubeslice-worker-preinstall-job\" not found\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:540: [debug] Watching for changes to Job kubeslice-worker-preinstall-job with timeout of 2m0s\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: ADDED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 0, jobs failed: 0, jobs succeeded: 0\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 1, jobs succeeded: 0\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 2, jobs succeeded: 0\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 3, jobs succeeded: 0\n    client.go:310: [debug] Starting delete for \"kubeslice-worker-preinstall-job\" Job\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:310: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"jaeger\" Service\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-svc\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:339: [debug] services \"nsm-admission-webhook-svc\" not found\n    client.go:339: [debug] services \"jaeger\" not found\n    client.go:339: [debug] services \"kubeslice-dns\" not found\n    client.go:339: [debug] services \"kubeslice-webhook-service\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook\" Deployment\n    client.go:310: [debug] Starting delete for \"jaeger\" Deployment\n    client.go:310: [debug] Starting delete for \"prefix-service\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:339: [debug] deployments.apps \"nsm-admission-webhook\" not found\n    client.go:339: [debug] deployments.apps \"jaeger\" not found\n    client.go:339: [debug] deployments.apps \"kubeslice-dns\" not found\n    client.go:339: [debug] deployments.apps \"kubeslice-operator\" not found\n    client.go:339: [debug] deployments.apps \"prefix-service\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsm-kernel-forwarder\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:339: [debug] daemonsets.apps \"nsm-kernel-forwarder\" not found\n    client.go:339: [debug] daemonsets.apps \"nsmgr\" not found\n    client.go:339: [debug] daemonsets.apps \"kubeslice-netop\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:339: [debug] rolebindings.rbac.authorization.k8s.io \"kubeslice-leader-election-rolebinding\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:339: [debug] roles.rbac.authorization.k8s.io \"kubeslice-leader-election-role\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-proxy-rolebinding\" not found\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-dns-rolebinding\" not found\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"nsm-role-binding\" not found\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-manager-rolebinding\" not found\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-kubernetes-dashboard\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRole\n    client.go:310: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-manager-role\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-proxy-role\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-dns-role\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-metrics-reader\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-kubernetes-dashboard\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"aggregate-network-services-view\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"nsm-role\" not found\n    client.go:310: [debug] Starting delete for \"networkservices.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkserviceendpoints.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservicemanagers.networkservicemesh.io\" CustomResourceDefinition\n    client.go:339: [debug] customresourcedefinitions.apiextensions.k8s.io \"networkserviceendpoints.networkservicemesh.io\" not found\n    client.go:339: [debug] customresourcedefinitions.apiextensions.k8s.io \"networkservicemanagers.networkservicemesh.io\" not found\n    client.go:339: [debug] customresourcedefinitions.apiextensions.k8s.io \"networkservices.networkservicemesh.io\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:339: [debug] configmaps \"nsm-config\" not found\n    client.go:339: [debug] configmaps \"kubeslice-manager-config\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-certs\" Secret\n    client.go:339: [debug] secrets \"kubeslice-hub\" not found\n    client.go:339: [debug] secrets \"kubeslice-admission-webhook-certs\" not found\n    client.go:339: [debug] secrets \"nsm-admission-webhook-certs\" not found\n    client.go:310: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:339: [debug] serviceaccounts \"kubeslice-netop\" not found\n    client.go:339: [debug] serviceaccounts \"forward-plane-acc\" not found\n    client.go:339: [debug] serviceaccounts \"slice-router\" not found\n    client.go:339: [debug] serviceaccounts \"nsmgr-acc\" not found\n    client.go:339: [debug] serviceaccounts \"nsc-acc\" not found\n    client.go:339: [debug] serviceaccounts \"vpn-gateway-client\" not found\n    client.go:339: [debug] serviceaccounts \"kubeslice-kubernetes-dashboard\" not found\n    client.go:339: [debug] serviceaccounts \"kubeslice-controller-manager\" not found\n    client.go:339: [debug] serviceaccounts \"vpn-gateway-server\" not found\n    client.go:339: [debug] serviceaccounts \"kubeslice-dns\" not found\n    client.go:339: [debug] serviceaccounts \"nse-acc\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-cfg\" MutatingWebhookConfiguration\n    client.go:339: [debug] mutatingwebhookconfigurations.admissionregistration.k8s.io \"kubeslice-mutating-webhook-configuration\" not found\n    client.go:339: [debug] mutatingwebhookconfigurations.admissionregistration.k8s.io \"nsm-admission-webhook-cfg\" not found\n    uninstall.go:144: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: failed pre-install: timed out waiting for the condition\n    helm.go:84: [debug] failed pre-install: timed out waiting for the condition\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:361\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1660141834000,"stop":1660142170192,"duration":336192}},{"uid":"2af8248029a6fed5","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000740bd0>: {\n        Underlying: <*exec.ExitError | 0xc00061cf40>{\n            ProcessState: {\n                pid: 6312,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 1, Usec: 160428},\n                    Stime: {Sec: 0, Usec: 133429},\n                    Maxrss: 84572,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 13077,\n                    Majflt: 8,\n                    Nswap: 0,\n                    Inblock: 688,\n                    Oublock: 15224,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 3056,\n                    Nivcsw: 1582,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.4.1.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD serviceexports.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD serviceimports.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD slice.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD slicegateways.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD sliceresourcequotas.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"Error: INSTALLATION FAILED: create: failed to create: secrets \\\"sh.helm.release.v1.kubeslice-worker.v1\\\" is forbidden: unable to create new content in namespace kubeslice-system because it is being terminated\",\n                        \"helm.go:84: [debug] secrets \\\"sh.helm.release.v1.kubeslice-worker.v1\\\" is forbidden: unable to create new content in namespace kubeslice-system because it is being terminated\",\n                        \"create: failed to create\",\n                        \"helm.sh/helm/v3/pkg/storage/driver.(*Secrets).Create\",\n                        \"\\thelm.sh/helm/v3/pkg/storage/driver/secrets.go:164\",\n                        \"helm.sh/helm/v3/pkg/storage.(*Storage).Create\",\n                        \"\\thelm.sh/helm/v3/pkg/storage/storage.go:69\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:340\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:264\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:125\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:97...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.4.1.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD serviceexports.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD serviceimports.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD slice.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD slicegateways.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD sliceresourcequotas.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    Error: INSTALLATION FAILED: create: failed to create: secrets \"sh.helm.release.v1.kubeslice-worker.v1\" is forbidden: unable to create new content in namespace kubeslice-system because it is being terminated\n    helm.go:84: [debug] secrets \"sh.helm.release.v1.kubeslice-worker.v1\" is forbidden: unable to create new content in namespace kubeslice-system because it is being terminated\n    create: failed to create\n    helm.sh/helm/v3/pkg/storage/driver.(*Secrets).Create\n    \thelm.sh/helm/v3/pkg/storage/driver/secrets.go:164\n    helm.sh/helm/v3/pkg/storage.(*Storage).Create\n    \thelm.sh/helm/v3/pkg/storage/storage.go:69\n    helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\n    \thelm.sh/helm/v3/pkg/action/install.go:340\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:264\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:125\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1660138561000,"stop":1660138775325,"duration":214325}},{"uid":"daccfca516876d55","status":"passed","time":{"start":1660116744000,"stop":1660116799227,"duration":55227}},{"uid":"62763440f0b50e7","status":"passed","time":{"start":1660116144000,"stop":1660116178357,"duration":34357}},{"uid":"4118ab0f7c2dd4bd","status":"passed","time":{"start":1660112860000,"stop":1660112892715,"duration":32715}},{"uid":"11b78aeb79d66809","status":"passed","time":{"start":1660108970000,"stop":1660109005865,"duration":35865}},{"uid":"e7e84dec6c76e0ed","status":"passed","time":{"start":1660104124000,"stop":1660104171515,"duration":47515}},{"uid":"4dc373f4267c235","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000130540>: {\n        Underlying: <*exec.ExitError | 0xc0006c0000>{\n            ProcessState: {\n                pid: 6031,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 1, Usec: 905814},\n                    Stime: {Sec: 0, Usec: 162449},\n                    Maxrss: 86316,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 15364,\n                    Majflt: 14,\n                    Nswap: 0,\n                    Inblock: 1840,\n                    Oublock: 13808,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 5297,\n                    Nivcsw: 2886,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.4.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:165: [debug] Clearing discovery cache\",\n                        \"wait.go:48: [debug] beginning wait for 5 resources with timeout of 1m0s\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:540: [debug] Watching for changes to Job kubeslice-worker-preinstall-job with timeout of 2m0s\",\n                        \"client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: ADDED\",\n                        \"client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 0, jobs failed: 0, jobs succeeded: 0\",\n                        \"client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\",\n                        \"client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\",\n                        \"client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\",\n                        \"client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 1, jobs succeeded: 0\",\n                        \"client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\",\n                        \"client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 2, jobs succeeded: 0\",\n                        \"client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\",\n                        \"client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 3, jobs succeeded: 0\",\n                        \"client.go:310: [debug] Starting delete for \\\"kubeslice-worker-preinstall-job\\\" Job\",\n                        \"install.go:441: [debug] Install failed a...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.4.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:165: [debug] Clearing discovery cache\n    wait.go:48: [debug] beginning wait for 5 resources with timeout of 1m0s\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:540: [debug] Watching for changes to Job kubeslice-worker-preinstall-job with timeout of 2m0s\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: ADDED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 0, jobs failed: 0, jobs succeeded: 0\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 1, jobs succeeded: 0\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 2, jobs succeeded: 0\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 3, jobs succeeded: 0\n    client.go:310: [debug] Starting delete for \"kubeslice-worker-preinstall-job\" Job\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:310: [debug] Starting delete for \"jaeger\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-svc\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:339: [debug] services \"jaeger\" not found\n    client.go:339: [debug] services \"nsm-admission-webhook-svc\" not found\n    client.go:339: [debug] services \"kubeslice-dns\" not found\n    client.go:339: [debug] services \"kubeslice-webhook-service\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:310: [debug] Starting delete for \"jaeger\" Deployment\n    client.go:310: [debug] Starting delete for \"prefix-service\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook\" Deployment\n    client.go:339: [debug] deployments.apps \"kubeslice-operator\" not found\n    client.go:339: [debug] deployments.apps \"prefix-service\" not found\n    client.go:339: [debug] deployments.apps \"jaeger\" not found\n    client.go:339: [debug] deployments.apps \"nsm-admission-webhook\" not found\n    client.go:339: [debug] deployments.apps \"kubeslice-dns\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsm-kernel-forwarder\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:339: [debug] daemonsets.apps \"nsm-kernel-forwarder\" not found\n    client.go:339: [debug] daemonsets.apps \"kubeslice-netop\" not found\n    client.go:339: [debug] daemonsets.apps \"nsmgr\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:339: [debug] rolebindings.rbac.authorization.k8s.io \"kubeslice-leader-election-rolebinding\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:339: [debug] roles.rbac.authorization.k8s.io \"kubeslice-leader-election-role\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-dns-rolebinding\" not found\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-kubernetes-dashboard\" not found\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-manager-rolebinding\" not found\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-proxy-rolebinding\" not found\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"nsm-role-binding\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRole\n    client.go:310: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-metrics-reader\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"nsm-role\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-manager-role\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-dns-role\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-kubernetes-dashboard\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"aggregate-network-services-view\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-proxy-role\" not found\n    client.go:310: [debug] Starting delete for \"networkserviceendpoints.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservicemanagers.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservices.networkservicemesh.io\" CustomResourceDefinition\n    client.go:339: [debug] customresourcedefinitions.apiextensions.k8s.io \"networkserviceendpoints.networkservicemesh.io\" not found\n    client.go:339: [debug] customresourcedefinitions.apiextensions.k8s.io \"networkservicemanagers.networkservicemesh.io\" not found\n    client.go:339: [debug] customresourcedefinitions.apiextensions.k8s.io \"networkservices.networkservicemesh.io\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:339: [debug] configmaps \"kubeslice-manager-config\" not found\n    client.go:339: [debug] configmaps \"nsm-config\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:339: [debug] secrets \"nsm-admission-webhook-certs\" not found\n    client.go:339: [debug] secrets \"kubeslice-admission-webhook-certs\" not found\n    client.go:339: [debug] secrets \"kubeslice-hub\" not found\n    client.go:310: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:339: [debug] serviceaccounts \"nse-acc\" not found\n    client.go:339: [debug] serviceaccounts \"slice-router\" not found\n    client.go:339: [debug] serviceaccounts \"kubeslice-netop\" not found\n    client.go:339: [debug] serviceaccounts \"kubeslice-kubernetes-dashboard\" not found\n    client.go:339: [debug] serviceaccounts \"forward-plane-acc\" not found\n    client.go:339: [debug] serviceaccounts \"vpn-gateway-client\" not found\n    client.go:339: [debug] serviceaccounts \"vpn-gateway-server\" not found\n    client.go:339: [debug] serviceaccounts \"kubeslice-dns\" not found\n    client.go:339: [debug] serviceaccounts \"nsmgr-acc\" not found\n    client.go:339: [debug] serviceaccounts \"kubeslice-controller-manager\" not found\n    client.go:339: [debug] serviceaccounts \"nsc-acc\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-cfg\" MutatingWebhookConfiguration\n    client.go:339: [debug] mutatingwebhookconfigurations.admissionregistration.k8s.io \"nsm-admission-webhook-cfg\" not found\n    client.go:339: [debug] mutatingwebhookconfigurations.admissionregistration.k8s.io \"kubeslice-mutating-webhook-configuration\" not found\n    uninstall.go:144: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: failed pre-install: timed out waiting for the condition\n    helm.go:84: [debug] failed pre-install: timed out waiting for the condition\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:361\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1660104760000,"stop":1660105097166,"duration":337166}},{"uid":"cb2694782b528a98","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0008895d8>: {\n        Underlying: <*exec.ExitError | 0xc000718840>{\n            ProcessState: {\n                pid: 6204,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 1, Usec: 468838},\n                    Stime: {Sec: 0, Usec: 135395},\n                    Maxrss: 91288,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 8151,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 15136,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 5197,\n                    Nivcsw: 2243,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.4.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD serviceexports.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD serviceimports.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD slice.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD slicegateways.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD sliceresourcequotas.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"Error: INSTALLATION FAILED: create: failed to create: secrets \\\"sh.helm.release.v1.kubeslice-worker.v1\\\" is forbidden: unable to create new content in namespace kubeslice-system because it is being terminated\",\n                        \"helm.go:84: [debug] secrets \\\"sh.helm.release.v1.kubeslice-worker.v1\\\" is forbidden: unable to create new content in namespace kubeslice-system because it is being terminated\",\n                        \"create: failed to create\",\n                        \"helm.sh/helm/v3/pkg/storage/driver.(*Secrets).Create\",\n                        \"\\thelm.sh/helm/v3/pkg/storage/driver/secrets.go:164\",\n                        \"helm.sh/helm/v3/pkg/storage.(*Storage).Create\",\n                        \"\\thelm.sh/helm/v3/pkg/storage/storage.go:69\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:340\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:264\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:125\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.4.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD serviceexports.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD serviceimports.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD slice.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD slicegateways.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD sliceresourcequotas.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    Error: INSTALLATION FAILED: create: failed to create: secrets \"sh.helm.release.v1.kubeslice-worker.v1\" is forbidden: unable to create new content in namespace kubeslice-system because it is being terminated\n    helm.go:84: [debug] secrets \"sh.helm.release.v1.kubeslice-worker.v1\" is forbidden: unable to create new content in namespace kubeslice-system because it is being terminated\n    create: failed to create\n    helm.sh/helm/v3/pkg/storage/driver.(*Secrets).Create\n    \thelm.sh/helm/v3/pkg/storage/driver/secrets.go:164\n    helm.sh/helm/v3/pkg/storage.(*Storage).Create\n    \thelm.sh/helm/v3/pkg/storage/storage.go:69\n    helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\n    \thelm.sh/helm/v3/pkg/action/install.go:340\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:264\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:125\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1660101116000,"stop":1660101331684,"duration":215684}},{"uid":"a9ab901c5658a897","status":"passed","time":{"start":1660100168000,"stop":1660100203588,"duration":35588}},{"uid":"5b10f4f3398a7e3c","status":"passed","time":{"start":1660066476000,"stop":1660066531763,"duration":55763}},{"uid":"f612c58e613effed","status":"passed","time":{"start":1660064121000,"stop":1660064154193,"duration":33193}},{"uid":"648854a1d6269abe","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0005f8018>: {\n        Underlying: <*exec.ExitError | 0xc0006d8000>{\n            ProcessState: {\n                pid: 5961,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 1, Usec: 954009},\n                    Stime: {Sec: 0, Usec: 150308},\n                    Maxrss: 91572,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 11597,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13808,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 4840,\n                    Nivcsw: 3453,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.4.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:165: [debug] Clearing discovery cache\",\n                        \"wait.go:48: [debug] beginning wait for 5 resources with timeout of 1m0s\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:540: [debug] Watching for changes to Job kubeslice-worker-preinstall-job with timeout of 2m0s\",\n                        \"client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: ADDED\",\n                        \"client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 0, jobs failed: 0, jobs succeeded: 0\",\n                        \"client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\",\n                        \"client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\",\n                        \"client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\",\n                        \"client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 1, jobs succeeded: 0\",\n                        \"client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\",\n                        \"client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 2, jobs succeeded: 0\",\n                        \"client.go:310: [debug] Starting delete for \\\"kubeslice-worker-preinstall-job\\\" Job\",\n                        \"install.go:441: [debug] Install failed and atomic is set, uninstalling release\",\n                        \"uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\",\n                        \"client.go:310: [debug] Starting delete for \\\"kubeslice-webhook-service\\\" Service\",\n            ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.4.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:165: [debug] Clearing discovery cache\n    wait.go:48: [debug] beginning wait for 5 resources with timeout of 1m0s\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:540: [debug] Watching for changes to Job kubeslice-worker-preinstall-job with timeout of 2m0s\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: ADDED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 0, jobs failed: 0, jobs succeeded: 0\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 1, jobs succeeded: 0\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 2, jobs succeeded: 0\n    client.go:310: [debug] Starting delete for \"kubeslice-worker-preinstall-job\" Job\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:310: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"jaeger\" Service\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-svc\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:339: [debug] services \"kubeslice-webhook-service\" not found\n    client.go:339: [debug] services \"jaeger\" not found\n    client.go:339: [debug] services \"kubeslice-dns\" not found\n    client.go:339: [debug] services \"nsm-admission-webhook-svc\" not found\n    client.go:310: [debug] Starting delete for \"jaeger\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook\" Deployment\n    client.go:310: [debug] Starting delete for \"prefix-service\" Deployment\n    client.go:339: [debug] deployments.apps \"jaeger\" not found\n    client.go:339: [debug] deployments.apps \"kubeslice-dns\" not found\n    client.go:339: [debug] deployments.apps \"prefix-service\" not found\n    client.go:339: [debug] deployments.apps \"nsm-admission-webhook\" not found\n    client.go:339: [debug] deployments.apps \"kubeslice-operator\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsm-kernel-forwarder\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:339: [debug] daemonsets.apps \"nsmgr\" not found\n    client.go:339: [debug] daemonsets.apps \"nsm-kernel-forwarder\" not found\n    client.go:339: [debug] daemonsets.apps \"kubeslice-netop\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:339: [debug] rolebindings.rbac.authorization.k8s.io \"kubeslice-leader-election-rolebinding\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:339: [debug] roles.rbac.authorization.k8s.io \"kubeslice-leader-election-role\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-dns-rolebinding\" not found\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-kubernetes-dashboard\" not found\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"nsm-role-binding\" not found\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-proxy-rolebinding\" not found\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-manager-rolebinding\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRole\n    client.go:310: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-manager-role\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"nsm-role\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-dns-role\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"aggregate-network-services-view\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-metrics-reader\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-kubernetes-dashboard\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-proxy-role\" not found\n    client.go:310: [debug] Starting delete for \"networkservices.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkserviceendpoints.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservicemanagers.networkservicemesh.io\" CustomResourceDefinition\n    client.go:339: [debug] customresourcedefinitions.apiextensions.k8s.io \"networkservices.networkservicemesh.io\" not found\n    client.go:339: [debug] customresourcedefinitions.apiextensions.k8s.io \"networkserviceendpoints.networkservicemesh.io\" not found\n    client.go:339: [debug] customresourcedefinitions.apiextensions.k8s.io \"networkservicemanagers.networkservicemesh.io\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:339: [debug] configmaps \"nsm-config\" not found\n    client.go:339: [debug] configmaps \"kubeslice-manager-config\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:339: [debug] secrets \"kubeslice-admission-webhook-certs\" not found\n    client.go:339: [debug] secrets \"nsm-admission-webhook-certs\" not found\n    client.go:339: [debug] secrets \"kubeslice-hub\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:339: [debug] serviceaccounts \"nse-acc\" not found\n    client.go:339: [debug] serviceaccounts \"vpn-gateway-client\" not found\n    client.go:339: [debug] serviceaccounts \"vpn-gateway-server\" not found\n    client.go:339: [debug] serviceaccounts \"nsc-acc\" not found\n    client.go:339: [debug] serviceaccounts \"kubeslice-netop\" not found\n    client.go:339: [debug] serviceaccounts \"kubeslice-dns\" not found\n    client.go:339: [debug] serviceaccounts \"kubeslice-controller-manager\" not found\n    client.go:339: [debug] serviceaccounts \"kubeslice-kubernetes-dashboard\" not found\n    client.go:339: [debug] serviceaccounts \"forward-plane-acc\" not found\n    client.go:339: [debug] serviceaccounts \"nsmgr-acc\" not found\n    client.go:339: [debug] serviceaccounts \"slice-router\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-cfg\" MutatingWebhookConfiguration\n    client.go:339: [debug] mutatingwebhookconfigurations.admissionregistration.k8s.io \"kubeslice-mutating-webhook-configuration\" not found\n    client.go:339: [debug] mutatingwebhookconfigurations.admissionregistration.k8s.io \"nsm-admission-webhook-cfg\" not found\n    uninstall.go:144: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: failed pre-install: timed out waiting for the condition\n    helm.go:84: [debug] failed pre-install: timed out waiting for the condition\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:361\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1660060680000,"stop":1660061017454,"duration":337454}},{"uid":"e5dd2e1cfa256b8","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000130540>: {\n        Underlying: <*exec.ExitError | 0xc0000ae000>{\n            ProcessState: {\n                pid: 6038,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 2, Usec: 896203},\n                    Stime: {Sec: 0, Usec: 412045},\n                    Maxrss: 84536,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 11861,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14408,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 26188,\n                    Nivcsw: 7189,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.4.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:165: [debug] Clearing discovery cache\",\n                        \"wait.go:48: [debug] beginning wait for 5 resources with timeout of 1m0s\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 47 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 47 resources with timeout of 2m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.4.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:165: [debug] Clearing discovery cache\n    wait.go:48: [debug] beginning wait for 5 resources with timeout of 1m0s\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 47 resource(s)\n    wait.go:48: [debug] beginning wait for 47 resources with timeout of 2m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:310: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-svc\" Service\n    client.go:310: [debug] Starting delete for \"jaeger\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:310: [debug] Starting delete for \"jaeger\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook\" Deployment\n    client.go:310: [debug] Starting delete for \"prefix-service\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsm-kernel-forwarder\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRole\n    client.go:310: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"networkservices.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkserviceendpoints.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservicemanagers.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-cfg\" MutatingWebhookConfiguration\n    uninstall.go:144: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1660052169000,"stop":1660052304704,"duration":135704}},{"uid":"21496224871698c6","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000669578>: {\n        Underlying: <*exec.ExitError | 0xc000817f80>{\n            ProcessState: {\n                pid: 6505,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 56683},\n                    Stime: {Sec: 0, Usec: 13080},\n                    Maxrss: 43272,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 1993,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 256,\n                    Nivcsw: 74,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"Error: INSTALLATION FAILED: looks like \\\"https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/\\\" is not a valid chart repository or cannot be reached: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/index.yaml : 404 Not Found\",\n                        \"helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/index.yaml : 404 Not Found\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\",\n                        \"helm.sh/helm/v3/pkg/repo.(*ChartRepository).DownloadIndexFile\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:128\",\n                        \"helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:252\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:719\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:190\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:125\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                        \"looks like \\\"https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/\\\" is not a valid chart repository or cannot be reached\",\n                        \"helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:254\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:719\",\n                        \"main....\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    Error: INSTALLATION FAILED: looks like \"https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/\" is not a valid chart repository or cannot be reached: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/index.yaml : 404 Not Found\n    helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/index.yaml : 404 Not Found\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\n    helm.sh/helm/v3/pkg/repo.(*ChartRepository).DownloadIndexFile\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:128\n    helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:252\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:719\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:190\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:125\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    looks like \"https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/\" is not a valid chart repository or cannot be reached\n    helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:254\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:719\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:190\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:125\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1660050481000,"stop":1660050489529,"duration":8529}},{"uid":"c9e0c80f86712963","status":"passed","time":{"start":1660046776000,"stop":1660046821738,"duration":45738}},{"uid":"206b4f0d1180f585","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000695bd8>: {\n        Underlying: <*exec.ExitError | 0xc00085a6a0>{\n            ProcessState: {\n                pid: 6058,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 42197},\n                    Stime: {Sec: 0, Usec: 18754},\n                    Maxrss: 40704,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4141,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 254,\n                    Nivcsw: 48,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"Error: INSTALLATION FAILED: looks like \\\"https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/\\\" is not a valid chart repository or cannot be reached: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/index.yaml : 404 Not Found\",\n                        \"helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/index.yaml : 404 Not Found\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\",\n                        \"helm.sh/helm/v3/pkg/repo.(*ChartRepository).DownloadIndexFile\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:128\",\n                        \"helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:252\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:719\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:190\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:125\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                        \"looks like \\\"https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/\\\" is not a valid chart repository or cannot be reached\",\n                        \"helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:254\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:719\",\n                        \"main....\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    Error: INSTALLATION FAILED: looks like \"https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/\" is not a valid chart repository or cannot be reached: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/index.yaml : 404 Not Found\n    helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/index.yaml : 404 Not Found\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\n    helm.sh/helm/v3/pkg/repo.(*ChartRepository).DownloadIndexFile\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:128\n    helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:252\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:719\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:190\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:125\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    looks like \"https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/\" is not a valid chart repository or cannot be reached\n    helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:254\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:719\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:190\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:125\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1660046048000,"stop":1660046056406,"duration":8406}},{"uid":"86c68e0fe83836b9","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000622b88>: {\n        Underlying: <*exec.ExitError | 0xc0006b5260>{\n            ProcessState: {\n                pid: 6177,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 49946},\n                    Stime: {Sec: 0, Usec: 4540},\n                    Maxrss: 43144,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 1977,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 142,\n                    Nivcsw: 51,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"Error: INSTALLATION FAILED: looks like \\\"https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/\\\" is not a valid chart repository or cannot be reached: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/index.yaml : 404 Not Found\",\n                        \"helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/index.yaml : 404 Not Found\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\",\n                        \"helm.sh/helm/v3/pkg/repo.(*ChartRepository).DownloadIndexFile\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:128\",\n                        \"helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:252\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:719\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:190\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:125\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                        \"looks like \\\"https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/\\\" is not a valid chart repository or cannot be reached\",\n                        \"helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:254\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:719\",\n                        \"main.r...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    Error: INSTALLATION FAILED: looks like \"https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/\" is not a valid chart repository or cannot be reached: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/index.yaml : 404 Not Found\n    helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/index.yaml : 404 Not Found\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\n    helm.sh/helm/v3/pkg/repo.(*ChartRepository).DownloadIndexFile\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:128\n    helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:252\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:719\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:190\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:125\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    looks like \"https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/\" is not a valid chart repository or cannot be reached\n    helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:254\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:719\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:190\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:125\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1660044532000,"stop":1660044540378,"duration":8378}}]},"Worker Suite:Worker Suite#[It] Slice Config Negative Scenario tests Slice Config Negative Scenario Creation Should fail while creating sliceconfig with invalid values on slice name":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":35,"unknown":0,"total":35},"items":[{"uid":"264aa8647beb9e93","status":"passed","time":{"start":1660113628000,"stop":1660113628773,"duration":773}},{"uid":"6e4fe1fbc6893916","status":"passed","time":{"start":1660104898000,"stop":1660104898923,"duration":923}},{"uid":"2a08273bcd85497f","status":"passed","time":{"start":1660067259000,"stop":1660067260202,"duration":1202}},{"uid":"f85ca57d0077a3c9","status":"passed","time":{"start":1660047551000,"stop":1660047551518,"duration":518}},{"uid":"3b64ba1098de671e","status":"passed","time":{"start":1659982549000,"stop":1659982549954,"duration":954}},{"uid":"3ed56ddde224430f","status":"passed","time":{"start":1659970182000,"stop":1659970183255,"duration":1255}},{"uid":"a93884af4c759219","status":"passed","time":{"start":1659944495000,"stop":1659944495527,"duration":527}},{"uid":"a154922038bc1f89","status":"passed","time":{"start":1659874977000,"stop":1659874977593,"duration":593}},{"uid":"8560c8faeae5056f","status":"passed","time":{"start":1659496050000,"stop":1659496050331,"duration":331}},{"uid":"e49752667e00b771","status":"passed","time":{"start":1659496083000,"stop":1659496083650,"duration":650}},{"uid":"42d43a09981e0141","status":"passed","time":{"start":1659451501000,"stop":1659451501930,"duration":930}},{"uid":"9d9ef9e3c0821f0a","status":"passed","time":{"start":1659450916000,"stop":1659450916846,"duration":846}},{"uid":"f7288fb1dfb03e1d","status":"passed","time":{"start":1659451524000,"stop":1659451525131,"duration":1131}},{"uid":"1858bdfdd3b593cc","status":"passed","time":{"start":1659447298000,"stop":1659447298785,"duration":785}},{"uid":"f0886a930a479a86","status":"passed","time":{"start":1659447116000,"stop":1659447117263,"duration":1263}},{"uid":"ecb322719b80293b","status":"passed","time":{"start":1659448205000,"stop":1659448205477,"duration":477}},{"uid":"9001a9c6cb9eb332","status":"passed","time":{"start":1659448097000,"stop":1659448097963,"duration":963}},{"uid":"11cf8a0532806c28","status":"passed","time":{"start":1659409279000,"stop":1659409279429,"duration":429}},{"uid":"a3af99af1dcacd7c","status":"passed","time":{"start":1659402555000,"stop":1659402555998,"duration":998}},{"uid":"e31abc56839d4514","status":"passed","time":{"start":1659291287000,"stop":1659291288025,"duration":1025}}]}}