{"uid":"91b1f7fc2ed65962","name":"[BeforeSuite]","historyId":"SliceHealth Suite:SliceHealth Suite#[BeforeSuite]","time":{"start":1711523331000,"stop":1711523331399,"duration":399},"status":"failed","statusMessage":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000ed38>: {\n        Underlying: <*exec.ExitError | 0xc0001164c0>{\n            ProcessState: {\n                pid: 7541,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 124139},\n                    Stime: {Sec: 0, Usec: 20689},\n                    Maxrss: 46732,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 1915,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 176,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 538,\n                    Nivcsw: 845,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:214: [debug] Original chart version: \\\"\\\"\",\n                        \"Error: INSTALLATION FAILED: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/nexus/kubeslice-controller-1.2.1.tgz : 404 Not Found\",\n                        \"helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/nexus/kubeslice-controller-1.2.1.tgz : 404 Not Found\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\",\n                        \"helm.sh/helm/v3/pkg/downloader.(*ChartDownloader).DownloadTo\",\n                        \"\\thelm.sh/helm/v3/pkg/downloader/chart_downloader.go:100\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:805\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:226\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:152\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.8.0/command.go:983\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.8.0/command.go:1115\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.8.0/command.go:1039\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:267\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1650\",\n                        \"INSTALLATION FAILED\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:154\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.8.0/command.go:983\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.8.0/command.go:1115\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.8.0/command.go:1039\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"ru...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:214: [debug] Original chart version: \"\"\n    Error: INSTALLATION FAILED: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/nexus/kubeslice-controller-1.2.1.tgz : 404 Not Found\n    helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/nexus/kubeslice-controller-1.2.1.tgz : 404 Not Found\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\n    helm.sh/helm/v3/pkg/downloader.(*ChartDownloader).DownloadTo\n    \thelm.sh/helm/v3/pkg/downloader/chart_downloader.go:100\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:805\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:226\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:152\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:983\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:1115\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:1039\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:267\n    runtime.goexit\n    \truntime/asm_amd64.s:1650\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:154\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:983\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:1115\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:1039\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:267\n    runtime.goexit\n    \truntime/asm_amd64.s:1650\noccurred","statusTrace":"/e2e/tests/slicehealth/setup_test.go:69\ngithub.com/kubeslice/kubeslice-e2e-automation/tests/slicehealth_test.glob..func1()\n\t/e2e/tests/slicehealth/setup_test.go:69 +0x803","flaky":false,"newFailed":false,"newBroken":false,"newPassed":false,"retriesCount":0,"retriesStatusChange":false,"beforeStages":[],"testStage":{"steps":[{"name":"","time":{},"steps":[],"attachments":[],"parameters":[],"stepsCount":0,"attachmentsCount":0,"shouldDisplayMessage":false,"hasContent":false},{"name":"Report Entries:","time":{},"steps":[],"attachments":[],"parameters":[],"stepsCount":0,"attachmentsCount":0,"shouldDisplayMessage":false,"hasContent":false},{"name":"By Step","time":{},"steps":[],"attachments":[],"parameters":[],"stepsCount":0,"attachmentsCount":0,"shouldDisplayMessage":false,"hasContent":false},{"name":"/e2e/tests/slicehealth/setup_test.go:33","time":{},"steps":[],"attachments":[],"parameters":[],"stepsCount":0,"attachmentsCount":0,"shouldDisplayMessage":false,"hasContent":false},{"name":"2024-03-27T07:08:51.213467278Z","time":{},"steps":[],"attachments":[],"parameters":[],"stepsCount":0,"attachmentsCount":0,"shouldDisplayMessage":false,"hasContent":false},{"name":"&{Text:Find controller endpoint; k config view -o jsonpath=\"{.clusters[?(@.name==\"kind-controller\")].cluster.server}\" Duration:0s}","time":{},"steps":[],"attachments":[],"parameters":[],"stepsCount":0,"attachmentsCount":0,"shouldDisplayMessage":false,"hasContent":false},{"name":"--","time":{},"steps":[],"attachments":[],"parameters":[],"stepsCount":0,"attachmentsCount":0,"shouldDisplayMessage":false,"hasContent":false},{"name":"By Step","time":{},"steps":[],"attachments":[],"parameters":[],"stepsCount":0,"attachmentsCount":0,"shouldDisplayMessage":false,"hasContent":false},{"name":"/e2e/tests/slicehealth/setup_test.go:56","time":{},"steps":[],"attachments":[],"parameters":[],"stepsCount":0,"attachmentsCount":0,"shouldDisplayMessage":false,"hasContent":false},{"name":"2024-03-27T07:08:51.370682423Z","time":{},"steps":[],"attachments":[],"parameters":[],"stepsCount":0,"attachmentsCount":0,"shouldDisplayMessage":false,"hasContent":false},{"name":"&{Text:installing hub on kind-controller Duration:0s}","time":{},"steps":[],"attachments":[],"parameters":[],"stepsCount":0,"attachmentsCount":0,"shouldDisplayMessage":false,"hasContent":false},{"name":"--","time":{},"steps":[],"attachments":[],"parameters":[],"stepsCount":0,"attachmentsCount":0,"shouldDisplayMessage":false,"hasContent":false},{"name":"By Step","time":{},"steps":[],"attachments":[],"parameters":[],"stepsCount":0,"attachmentsCount":0,"shouldDisplayMessage":false,"hasContent":false},{"name":"/e2e/tests/slicehealth/setup_test.go:63","time":{},"steps":[],"attachments":[],"parameters":[],"stepsCount":0,"attachmentsCount":0,"shouldDisplayMessage":false,"hasContent":false},{"name":"2024-03-27T07:08:51.371083811Z","time":{},"steps":[],"attachments":[],"parameters":[],"stepsCount":0,"attachmentsCount":0,"shouldDisplayMessage":false,"hasContent":false},{"name":"&{Text:Installing kubeslice-controller chart using helm Duration:0s}","time":{},"steps":[],"attachments":[],"parameters":[],"stepsCount":0,"attachmentsCount":0,"shouldDisplayMessage":false,"hasContent":false}],"attachments":[],"parameters":[],"stepsCount":16,"attachmentsCount":0,"shouldDisplayMessage":false,"hasContent":true},"afterStages":[],"labels":[{"name":"resultFormat","value":"junit"},{"name":"suite","value":"SliceHealth Suite"},{"name":"testClass","value":"SliceHealth Suite"},{"name":"package","value":"SliceHealth Suite"}],"parameters":[],"links":[],"hidden":false,"retry":false,"extra":{"severity":"normal","retries":[],"categories":[{"name":"Product defects","matchedStatuses":[],"flaky":false}],"history":{"statistic":{"failed":10,"broken":0,"skipped":0,"passed":2,"unknown":0,"total":12},"items":[{"uid":"2d861b4e589dd812","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0000c6d20>: {\n        Underlying: <*exec.ExitError | 0xc0000ea400>{\n            ProcessState: {\n                pid: 7888,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 74072},\n                    Stime: {Sec: 0, Usec: 13071},\n                    Maxrss: 48848,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2797,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 176,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 554,\n                    Nivcsw: 141,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:214: [debug] Original chart version: \\\"\\\"\",\n                        \"Error: INSTALLATION FAILED: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/nexus/kubeslice-controller-1.2.1.tgz : 404 Not Found\",\n                        \"helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/nexus/kubeslice-controller-1.2.1.tgz : 404 Not Found\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\",\n                        \"helm.sh/helm/v3/pkg/downloader.(*ChartDownloader).DownloadTo\",\n                        \"\\thelm.sh/helm/v3/pkg/downloader/chart_downloader.go:100\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:805\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:226\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:152\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.8.0/command.go:983\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.8.0/command.go:1115\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.8.0/command.go:1039\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:267\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1650\",\n                        \"INSTALLATION FAILED\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:154\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.8.0/command.go:983\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.8.0/command.go:1115\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.8.0/command.go:1039\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"run...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:214: [debug] Original chart version: \"\"\n    Error: INSTALLATION FAILED: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/nexus/kubeslice-controller-1.2.1.tgz : 404 Not Found\n    helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/nexus/kubeslice-controller-1.2.1.tgz : 404 Not Found\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\n    helm.sh/helm/v3/pkg/downloader.(*ChartDownloader).DownloadTo\n    \thelm.sh/helm/v3/pkg/downloader/chart_downloader.go:100\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:805\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:226\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:152\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:983\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:1115\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:1039\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:267\n    runtime.goexit\n    \truntime/asm_amd64.s:1650\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:154\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:983\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:1115\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:1039\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:267\n    runtime.goexit\n    \truntime/asm_amd64.s:1650\noccurred","time":{"start":1711007884000,"stop":1711007884175,"duration":175}},{"uid":"430b7c8d375602ec","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000ed50>: {\n        Underlying: <*exec.ExitError | 0xc0005982a0>{\n            ProcessState: {\n                pid: 7683,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 158579},\n                    Stime: {Sec: 0, Usec: 25815},\n                    Maxrss: 49252,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 1855,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 176,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 574,\n                    Nivcsw: 934,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:214: [debug] Original chart version: \\\"\\\"\",\n                        \"Error: INSTALLATION FAILED: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/nexus/kubeslice-controller-1.2.1.tgz : 404 Not Found\",\n                        \"helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/nexus/kubeslice-controller-1.2.1.tgz : 404 Not Found\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\",\n                        \"helm.sh/helm/v3/pkg/downloader.(*ChartDownloader).DownloadTo\",\n                        \"\\thelm.sh/helm/v3/pkg/downloader/chart_downloader.go:100\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:805\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:226\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:152\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.8.0/command.go:983\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.8.0/command.go:1115\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.8.0/command.go:1039\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:267\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1650\",\n                        \"INSTALLATION FAILED\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:154\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.8.0/command.go:983\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.8.0/command.go:1115\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.8.0/command.go:1039\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"ru...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:214: [debug] Original chart version: \"\"\n    Error: INSTALLATION FAILED: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/nexus/kubeslice-controller-1.2.1.tgz : 404 Not Found\n    helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/nexus/kubeslice-controller-1.2.1.tgz : 404 Not Found\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\n    helm.sh/helm/v3/pkg/downloader.(*ChartDownloader).DownloadTo\n    \thelm.sh/helm/v3/pkg/downloader/chart_downloader.go:100\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:805\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:226\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:152\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:983\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:1115\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:1039\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:267\n    runtime.goexit\n    \truntime/asm_amd64.s:1650\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:154\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:983\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:1115\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:1039\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:267\n    runtime.goexit\n    \truntime/asm_amd64.s:1650\noccurred","time":{"start":1710850775000,"stop":1710850775662,"duration":662}},{"uid":"599f02e45767393f","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00012a300>: {\n        Underlying: <*exec.ExitError | 0xc000046460>{\n            ProcessState: {\n                pid: 7875,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 78079},\n                    Stime: {Sec: 0, Usec: 0},\n                    Maxrss: 47068,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 1826,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 176,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 609,\n                    Nivcsw: 137,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:214: [debug] Original chart version: \\\"\\\"\",\n                        \"Error: INSTALLATION FAILED: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/nexus/kubeslice-controller-1.2.1.tgz : 404 Not Found\",\n                        \"helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/nexus/kubeslice-controller-1.2.1.tgz : 404 Not Found\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\",\n                        \"helm.sh/helm/v3/pkg/downloader.(*ChartDownloader).DownloadTo\",\n                        \"\\thelm.sh/helm/v3/pkg/downloader/chart_downloader.go:100\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:805\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:226\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:152\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.8.0/command.go:983\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.8.0/command.go:1115\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.8.0/command.go:1039\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:267\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1650\",\n                        \"INSTALLATION FAILED\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:154\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.8.0/command.go:983\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.8.0/command.go:1115\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.8.0/command.go:1039\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:214: [debug] Original chart version: \"\"\n    Error: INSTALLATION FAILED: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/nexus/kubeslice-controller-1.2.1.tgz : 404 Not Found\n    helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/nexus/kubeslice-controller-1.2.1.tgz : 404 Not Found\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\n    helm.sh/helm/v3/pkg/downloader.(*ChartDownloader).DownloadTo\n    \thelm.sh/helm/v3/pkg/downloader/chart_downloader.go:100\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:805\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:226\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:152\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:983\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:1115\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:1039\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:267\n    runtime.goexit\n    \truntime/asm_amd64.s:1650\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:154\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:983\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:1115\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:1039\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:267\n    runtime.goexit\n    \truntime/asm_amd64.s:1650\noccurred","time":{"start":1710825518000,"stop":1710825518117,"duration":117}},{"uid":"af36be1112d47f6a","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00082a018>: {\n        Underlying: <*exec.ExitError | 0xc0004fc000>{\n            ProcessState: {\n                pid: 9523,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 8, Usec: 259715},\n                    Stime: {Sec: 0, Usec: 737888},\n                    Maxrss: 124944,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 37064,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 776,\n                    Oublock: 21592,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 57565,\n                    Nivcsw: 11017,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:214: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:231: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-1.2.1.tgz\",\n                        \"\",\n                        \"client.go:142: [debug] creating 1 resource(s)\",\n                        \"install.go:168: [debug] CRD serviceexports.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:142: [debug] creating 1 resource(s)\",\n                        \"install.go:168: [debug] CRD serviceimports.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:142: [debug] creating 1 resource(s)\",\n                        \"install.go:168: [debug] CRD slicegateways.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:142: [debug] creating 1 resource(s)\",\n                        \"install.go:168: [debug] CRD slices.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:142: [debug] creating 1 resource(s)\",\n                        \"install.go:168: [debug] CRD clusterfederatedtrustdomains.spire.spiffe.io is already present. Skipping.\",\n                        \"client.go:142: [debug] creating 1 resource(s)\",\n                        \"install.go:168: [debug] CRD clusterspiffeids.spire.spiffe.io is already present. Skipping.\",\n                        \"client.go:142: [debug] creating 1 resource(s)\",\n                        \"install.go:168: [debug] CRD networkservices.networkservicemesh.io is already present. Skipping.\",\n                        \"client.go:142: [debug] creating 1 resource(s)\",\n                        \"install.go:168: [debug] CRD networkserviceendpoints.networkservicemesh.io is already present. Skipping.\",\n                        \"client.go:142: [debug] creating 1 resource(s)\",\n                        \"client.go:486: [debug] Starting delete for \\\"kubeslice-install-crds\\\" ServiceAccount\",\n                        \"client.go:490: [debug] Ignoring delete failure for \\\"kubeslice-install-crds\\\" /v1, Kind=ServiceAccount: serviceaccounts \\\"kubeslice-install-crds\\\" not found\",\n                        \"wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\",\n                        \"client.go:142: [debug] creating 1 resource(s)\",\n                        \"client.go:486: [debug] Starting delete for \\\"kubeslice-install-crds\\\" ConfigMap\",\n                        \"client.go:490: [debug] Ignoring delete failure for \\\"kubeslice-install-crds\\\" /v1, Kind=ConfigMap: configmaps \\\"kubeslice-install-crds\\\" not found\",\n                        \"wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeo...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:214: [debug] Original chart version: \"\"\n    install.go:231: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-1.2.1.tgz\n    \n    client.go:142: [debug] creating 1 resource(s)\n    install.go:168: [debug] CRD serviceexports.networking.kubeslice.io is already present. Skipping.\n    client.go:142: [debug] creating 1 resource(s)\n    install.go:168: [debug] CRD serviceimports.networking.kubeslice.io is already present. Skipping.\n    client.go:142: [debug] creating 1 resource(s)\n    install.go:168: [debug] CRD slicegateways.networking.kubeslice.io is already present. Skipping.\n    client.go:142: [debug] creating 1 resource(s)\n    install.go:168: [debug] CRD slices.networking.kubeslice.io is already present. Skipping.\n    client.go:142: [debug] creating 1 resource(s)\n    install.go:168: [debug] CRD clusterfederatedtrustdomains.spire.spiffe.io is already present. Skipping.\n    client.go:142: [debug] creating 1 resource(s)\n    install.go:168: [debug] CRD clusterspiffeids.spire.spiffe.io is already present. Skipping.\n    client.go:142: [debug] creating 1 resource(s)\n    install.go:168: [debug] CRD networkservices.networkservicemesh.io is already present. Skipping.\n    client.go:142: [debug] creating 1 resource(s)\n    install.go:168: [debug] CRD networkserviceendpoints.networkservicemesh.io is already present. Skipping.\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"kubeslice-install-crds\" ServiceAccount\n    client.go:490: [debug] Ignoring delete failure for \"kubeslice-install-crds\" /v1, Kind=ServiceAccount: serviceaccounts \"kubeslice-install-crds\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"kubeslice-install-crds\" ConfigMap\n    client.go:490: [debug] Ignoring delete failure for \"kubeslice-install-crds\" /v1, Kind=ConfigMap: configmaps \"kubeslice-install-crds\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"kubeslice-install-crds\" ClusterRole\n    client.go:490: [debug] Ignoring delete failure for \"kubeslice-install-crds\" rbac.authorization.k8s.io/v1, Kind=ClusterRole: clusterroles.rbac.authorization.k8s.io \"kubeslice-install-crds\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"kubeslice-install-crds\" ClusterRoleBinding\n    client.go:490: [debug] Ignoring delete failure for \"kubeslice-install-crds\" rbac.authorization.k8s.io/v1, Kind=ClusterRoleBinding: clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-install-crds\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:490: [debug] Ignoring delete failure for \"nsm-config\" /v1, Kind=ConfigMap: configmaps \"nsm-config\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"nsm-crd-install\" ConfigMap\n    client.go:490: [debug] Ignoring delete failure for \"nsm-crd-install\" /v1, Kind=ConfigMap: configmaps \"nsm-crd-install\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"nsm-install-crds\" ServiceAccount\n    client.go:490: [debug] Ignoring delete failure for \"nsm-install-crds\" /v1, Kind=ServiceAccount: serviceaccounts \"nsm-install-crds\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"nsm-install-crds\" ClusterRole\n    client.go:490: [debug] Ignoring delete failure for \"nsm-install-crds\" rbac.authorization.k8s.io/v1, Kind=ClusterRole: clusterroles.rbac.authorization.k8s.io \"nsm-install-crds\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"nsm-install-crds\" ClusterRoleBinding\n    client.go:490: [debug] Ignoring delete failure for \"nsm-install-crds\" rbac.authorization.k8s.io/v1, Kind=ClusterRoleBinding: clusterrolebindings.rbac.authorization.k8s.io \"nsm-install-crds\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"spire-clusterid-cr-install\" ConfigMap\n    client.go:490: [debug] Ignoring delete failure for \"spire-clusterid-cr-install\" /v1, Kind=ConfigMap: configmaps \"spire-clusterid-cr-install\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"spire-crd-install\" ConfigMap\n    client.go:490: [debug] Ignoring delete failure for \"spire-crd-install\" /v1, Kind=ConfigMap: configmaps \"spire-crd-install\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"spire-install-clusterid-cr\" ServiceAccount\n    client.go:490: [debug] Ignoring delete failure for \"spire-install-clusterid-cr\" /v1, Kind=ServiceAccount: serviceaccounts \"spire-install-clusterid-cr\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"spire-install-clusterid-cr\" ClusterRole\n    client.go:490: [debug] Ignoring delete failure for \"spire-install-clusterid-cr\" rbac.authorization.k8s.io/v1, Kind=ClusterRole: clusterroles.rbac.authorization.k8s.io \"spire-install-clusterid-cr\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"spire-install-clusterid-cr\" ClusterRoleBinding\n    client.go:490: [debug] Ignoring delete failure for \"spire-install-clusterid-cr\" rbac.authorization.k8s.io/v1, Kind=ClusterRoleBinding: clusterrolebindings.rbac.authorization.k8s.io \"spire-install-clusterid-cr\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"spire-install-crds\" ServiceAccount\n    client.go:490: [debug] Ignoring delete failure for \"spire-install-crds\" /v1, Kind=ServiceAccount: serviceaccounts \"spire-install-crds\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"spire-install-crds\" ClusterRole\n    client.go:490: [debug] Ignoring delete failure for \"spire-install-crds\" rbac.authorization.k8s.io/v1, Kind=ClusterRole: clusterroles.rbac.authorization.k8s.io \"spire-install-crds\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"spire-install-crds\" ClusterRoleBinding\n    client.go:490: [debug] Ignoring delete failure for \"spire-install-crds\" rbac.authorization.k8s.io/v1, Kind=ClusterRoleBinding: clusterrolebindings.rbac.authorization.k8s.io \"spire-install-crds\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"kubeslice-install-crds\" Job\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:712: [debug] Watching for changes to Job kubeslice-install-crds with timeout of 10m0s\n    client.go:740: [debug] Add/Modify event for kubeslice-install-crds: ADDED\n    client.go:779: [debug] kubeslice-install-crds: Jobs active: 0, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for kubeslice-install-crds: MODIFIED\n    client.go:779: [debug] kubeslice-install-crds: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for kubeslice-install-crds: MODIFIED\n    client.go:779: [debug] kubeslice-install-crds: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for kubeslice-install-crds: MODIFIED\n    client.go:779: [debug] kubeslice-install-crds: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for kubeslice-install-crds: MODIFIED\n    client.go:486: [debug] Starting delete for \"nsm-install-crds\" Job\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:712: [debug] Watching for changes to Job nsm-install-crds with timeout of 10m0s\n    client.go:740: [debug] Add/Modify event for nsm-install-crds: ADDED\n    client.go:779: [debug] nsm-install-crds: Jobs active: 0, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for nsm-install-crds: MODIFIED\n    client.go:779: [debug] nsm-install-crds: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for nsm-install-crds: MODIFIED\n    client.go:779: [debug] nsm-install-crds: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for nsm-install-crds: MODIFIED\n    client.go:779: [debug] nsm-install-crds: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for nsm-install-crds: MODIFIED\n    client.go:486: [debug] Starting delete for \"spire-install-crds\" Job\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:712: [debug] Watching for changes to Job spire-install-crds with timeout of 10m0s\n    client.go:740: [debug] Add/Modify event for spire-install-crds: ADDED\n    client.go:779: [debug] spire-install-crds: Jobs active: 0, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for spire-install-crds: MODIFIED\n    client.go:779: [debug] spire-install-crds: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for spire-install-crds: MODIFIED\n    client.go:779: [debug] spire-install-crds: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for spire-install-crds: MODIFIED\n    client.go:779: [debug] spire-install-crds: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for spire-install-crds: MODIFIED\n    client.go:486: [debug] Starting delete for \"spire-install-clusterid-cr\" Job\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:712: [debug] Watching for changes to Job spire-install-clusterid-cr with timeout of 10m0s\n    client.go:740: [debug] Add/Modify event for spire-install-clusterid-cr: ADDED\n    client.go:779: [debug] spire-install-clusterid-cr: Jobs active: 0, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for spire-install-clusterid-cr: MODIFIED\n    client.go:779: [debug] spire-install-clusterid-cr: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for spire-install-clusterid-cr: MODIFIED\n    client.go:779: [debug] spire-install-clusterid-cr: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for spire-install-clusterid-cr: MODIFIED\n    client.go:779: [debug] spire-install-clusterid-cr: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for spire-install-clusterid-cr: MODIFIED\n    client.go:486: [debug] Starting delete for \"kubeslice-install-crds\" ServiceAccount\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"kubeslice-install-crds\" ConfigMap\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"kubeslice-install-crds\" ClusterRole\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"kubeslice-install-crds\" ClusterRoleBinding\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"nsm-crd-install\" ConfigMap\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"nsm-install-crds\" ServiceAccount\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"nsm-install-crds\" ClusterRole\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"nsm-install-crds\" ClusterRoleBinding\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"spire-clusterid-cr-install\" ConfigMap\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"spire-crd-install\" ConfigMap\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"spire-install-clusterid-cr\" ServiceAccount\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"spire-install-clusterid-cr\" ClusterRole\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"spire-install-clusterid-cr\" ClusterRoleBinding\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"spire-install-crds\" ServiceAccount\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"spire-install-crds\" ClusterRole\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"spire-install-crds\" ClusterRoleBinding\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:393: [debug] checking 74 resources for changes\n    client.go:414: [debug] Created a new PriorityClass called \"nsm-webhook-high-priority\" in \n    \n    client.go:414: [debug] Created a new Namespace called \"spire\" in \n    \n    client.go:414: [debug] Created a new ServiceAccount called \"admission-webhook-sa\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new ServiceAccount called \"nse-acc\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new ServiceAccount called \"nsc-acc\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new ServiceAccount called \"nsmgr-acc\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new ServiceAccount called \"forward-plane-acc\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new ServiceAccount called \"spiffe-csi-driver\" in spire\n    \n    client.go:414: [debug] Created a new ServiceAccount called \"spire-server\" in spire\n    \n    client.go:414: [debug] Created a new ServiceAccount called \"spire-agent\" in spire\n    \n    client.go:414: [debug] Created a new ServiceAccount called \"kubeslice-kubernetes-dashboard\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new ServiceAccount called \"kubeslice-dns\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new ServiceAccount called \"kubeslice-gateway-edge\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new ServiceAccount called \"kubeslice-netop\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new ServiceAccount called \"vpn-gateway-server\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new ServiceAccount called \"vpn-gateway-client\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new ServiceAccount called \"slice-router\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new ServiceAccount called \"kubeslice-controller-manager\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new Secret called \"kubeslice-kubernetes-dashboard-creds\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new Secret called \"kubeslice-hub\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new Secret called \"kubeslice-admission-webhook-certs\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new ConfigMap called \"spire-bundle\" in spire\n    \n    client.go:414: [debug] Created a new ConfigMap called \"spire-controller-manager-config\" in spire\n    \n    client.go:414: [debug] Created a new ConfigMap called \"spire-server\" in spire\n    \n    client.go:414: [debug] Created a new ConfigMap called \"spire-agent\" in spire\n    \n    client.go:414: [debug] Created a new ConfigMap called \"nsmgr-cm\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new ConfigMap called \"kubeslice-manager-config\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new ConfigMap called \"kubeslice-worker-event-schema-conf\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new ClusterRole called \"admission-webhook-role\" in \n    \n    client.go:414: [debug] Created a new ClusterRole called \"nsm-role\" in \n    \n    client.go:414: [debug] Created a new ClusterRole called \"aggregate-network-services-view\" in \n    \n    client.go:414: [debug] Created a new ClusterRole called \"manager-role\" in \n    \n    client.go:414: [debug] Created a new ClusterRole called \"spire-server-trust-role\" in \n    \n    client.go:414: [debug] Created a new ClusterRole called \"spire-agent-cluster-role\" in \n    \n    client.go:414: [debug] Created a new ClusterRole called \"kubeslice-kubernetes-dashboard\" in \n    \n    client.go:414: [debug] Created a new ClusterRole called \"kubeslice-dns-role\" in \n    \n    client.go:414: [debug] Created a new ClusterRole called \"kubeslice-manager-role\" in \n    \n    client.go:414: [debug] Created a new ClusterRole called \"kubeslice-metrics-reader\" in \n    \n    client.go:414: [debug] Created a new ClusterRole called \"kubeslice-proxy-role\" in \n    \n    client.go:414: [debug] Created a new ClusterRoleBinding called \"admission-webhook-binding\" in \n    \n    client.go:414: [debug] Created a new ClusterRoleBinding called \"nsm-role-binding\" in \n    \n    client.go:414: [debug] Created a new ClusterRoleBinding called \"manager-rolebinding\" in \n    \n    client.go:414: [debug] Created a new ClusterRoleBinding called \"spire-server-trust-role-binding\" in \n    \n    client.go:414: [debug] Created a new ClusterRoleBinding called \"spire-agent-cluster-role-binding\" in \n    \n    client.go:414: [debug] Created a new ClusterRoleBinding called \"kubeslice-kubernetes-dashboard\" in \n    \n    client.go:414: [debug] Created a new ClusterRoleBinding called \"kubeslice-dns-rolebinding\" in \n    \n    client.go:414: [debug] Created a new ClusterRoleBinding called \"kubeslice-manager-rolebinding\" in \n    \n    client.go:414: [debug] Created a new ClusterRoleBinding called \"kubeslice-proxy-rolebinding\" in \n    \n    client.go:414: [debug] Created a new Role called \"leader-election-role\" in spire\n    \n    client.go:414: [debug] Created a new Role called \"spire-server-role\" in spire\n    \n    client.go:414: [debug] Created a new Role called \"kubeslice-leader-election-role\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new RoleBinding called \"leader-election-rolebinding\" in spire\n    \n    client.go:414: [debug] Created a new RoleBinding called \"spire-server-role-binding\" in spire\n    \n    client.go:414: [debug] Created a new RoleBinding called \"kubeslice-leader-election-rolebinding\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new Service called \"admission-webhook-svc\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new Service called \"spire-server\" in spire\n    \n    client.go:414: [debug] Created a new Service called \"spire-controller-manager-webhook-service\" in spire\n    \n    client.go:414: [debug] Created a new Service called \"nsmgr\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new Service called \"registry\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new Service called \"kubeslice-dns\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new Service called \"kubeslice-webhook-service\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new DaemonSet called \"spiffe-csi-driver\" in spire\n    \n    client.go:414: [debug] Created a new DaemonSet called \"spire-agent\" in spire\n    \n    client.go:414: [debug] Created a new DaemonSet called \"forwarder-kernel\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new DaemonSet called \"nsmgr\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new DaemonSet called \"kubeslice-netop\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new Deployment called \"nsm-admission-webhook-k8s\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new Deployment called \"registry-k8s\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new Deployment called \"kubeslice-dns\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new Deployment called \"kubeslice-operator\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new StatefulSet called \"spire-server\" in spire\n    \n    client.go:684: [debug] Looks like there are no changes for CSIDriver \"csi.spiffe.io\"\n    client.go:414: [debug] Created a new MutatingWebhookConfiguration called \"kubeslice-mutating-webhook-configuration\" in \n    \n    client.go:414: [debug] Created a new ValidatingWebhookConfiguration called \"spire-controller-manager-webhook\" in \n    \n    wait.go:48: [debug] beginning wait for 74 resources with timeout of 10m0s\n    ready.go:336: [debug] DaemonSet is not ready: spire/spiffe-csi-driver. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spiffe-csi-driver. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spiffe-csi-driver. 1 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    install.go:488: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:102: [debug] uninstall: Deleting kubeslice-worker\n    client.go:486: [debug] Starting delete for \"nsm-delete-webhooks\" ClusterRole\n    client.go:490: [debug] Ignoring delete failure for \"nsm-delete-webhooks\" rbac.authorization.k8s.io/v1, Kind=ClusterRole: clusterroles.rbac.authorization.k8s.io \"nsm-delete-webhooks\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"nsm-delete-webhooks\" ClusterRoleBinding\n    client.go:490: [debug] Ignoring delete failure for \"nsm-delete-webhooks\" rbac.authorization.k8s.io/v1, Kind=ClusterRoleBinding: clusterrolebindings.rbac.authorization.k8s.io \"nsm-delete-webhooks\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"kubeslice-cleanup\" ServiceAccount\n    client.go:490: [debug] Ignoring delete failure for \"kubeslice-cleanup\" /v1, Kind=ServiceAccount: serviceaccounts \"kubeslice-cleanup\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"kubeslice-cleanup\" ClusterRole\n    client.go:490: [debug] Ignoring delete failure for \"kubeslice-cleanup\" rbac.authorization.k8s.io/v1, Kind=ClusterRole: clusterroles.rbac.authorization.k8s.io \"kubeslice-cleanup\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"kubeslice-cleanup\" ClusterRoleBinding\n    client.go:490: [debug] Ignoring delete failure for \"kubeslice-cleanup\" rbac.authorization.k8s.io/v1, Kind=ClusterRoleBinding: clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-cleanup\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"nsm-delete-webhooks\" ServiceAccount\n    client.go:490: [debug] Ignoring delete failure for \"nsm-delete-webhooks\" /v1, Kind=ServiceAccount: serviceaccounts \"nsm-delete-webhooks\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"nsm-delete-webhooks\" ConfigMap\n    client.go:490: [debug] Ignoring delete failure for \"nsm-delete-webhooks\" /v1, Kind=ConfigMap: configmaps \"nsm-delete-webhooks\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"kubeslice-cleanup\" Job\n    client.go:490: [debug] Ignoring delete failure for \"kubeslice-cleanup\" batch/v1, Kind=Job: jobs.batch \"kubeslice-cleanup\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:712: [debug] Watching for changes to Job kubeslice-cleanup with timeout of 10m0s\n    client.go:740: [debug] Add/Modify event for kubeslice-cleanup: ADDED\n    client.go:779: [debug] kubeslice-cleanup: Jobs active: 0, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for kubeslice-cleanup: MODIFIED\n    client.go:779: [debug] kubeslice-cleanup: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for kubeslice-cleanup: MODIFIED\n    client.go:486: [debug] Starting delete for \"nsm-delete-webhooks\" Job\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:712: [debug] Watching for changes to Job nsm-delete-webhooks with timeout of 10m0s\n    client.go:740: [debug] Add/Modify event for nsm-delete-webhooks: ADDED\n    client.go:779: [debug] nsm-delete-webhooks: Jobs active: 0, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for nsm-delete-webhooks: MODIFIED\n    client.go:779: [debug] nsm-delete-webhooks: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for nsm-delete-webhooks: MODIFIED\n    client.go:486: [debug] Starting delete for \"nsm-delete-webhooks\" ClusterRole\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"nsm-delete-webhooks\" ClusterRoleBinding\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"kubeslice-cleanup\" ServiceAccount\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"kubeslice-cleanup\" ClusterRole\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"kubeslice-cleanup\" ClusterRoleBinding\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"nsm-delete-webhooks\" ServiceAccount\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"nsm-delete-webhooks\" ConfigMap\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"kubeslice-cleanup\" Job\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    uninstall.go:248: [debug] uninstall: given cascade value: , defaulting to delete propagation background\n    client.go:486: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:486: [debug] Starting delete for \"admission-webhook-svc\" Service\n    client.go:486: [debug] Starting delete for \"spire-server\" Service\n    client.go:486: [debug] Starting delete for \"spire-controller-manager-webhook-service\" Service\n    client.go:486: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:486: [debug] Starting delete for \"nsmgr\" Service\n    client.go:486: [debug] Starting delete for \"registry\" Service\n    client.go:486: [debug] Starting delete for \"spire-server\" StatefulSet\n    client.go:486: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:486: [debug] Starting delete for \"nsm-admission-webhook-k8s\" Deployment\n    client.go:486: [debug] Starting delete for \"registry-k8s\" Deployment\n    client.go:486: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:486: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:486: [debug] Starting delete for \"spiffe-csi-driver\" DaemonSet\n    client.go:486: [debug] Starting delete for \"spire-agent\" DaemonSet\n    client.go:486: [debug] Starting delete for \"forwarder-kernel\" DaemonSet\n    client.go:486: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:486: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:486: [debug] Starting delete for \"leader-election-rolebinding\" RoleBinding\n    client.go:486: [debug] Starting delete for \"spire-server-role-binding\" RoleBinding\n    client.go:486: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:486: [debug] Starting delete for \"leader-election-role\" Role\n    client.go:486: [debug] Starting delete for \"spire-server-role\" Role\n    client.go:486: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:486: [debug] Starting delete for \"admission-webhook-binding\" ClusterRoleBinding\n    client.go:486: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:486: [debug] Starting delete for \"manager-rolebinding\" ClusterRoleBinding\n    client.go:486: [debug] Starting delete for \"spire-server-trust-role-binding\" ClusterRoleBinding\n    client.go:486: [debug] Starting delete for \"spire-agent-cluster-role-binding\" ClusterRoleBinding\n    client.go:486: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRoleBinding\n    client.go:486: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:486: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:486: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:486: [debug] Starting delete for \"spire-server-trust-role\" ClusterRole\n    client.go:486: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:486: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:486: [debug] Starting delete for \"spire-agent-cluster-role\" ClusterRole\n    client.go:486: [debug] Starting delete for \"admission-webhook-role\" ClusterRole\n    client.go:486: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRole\n    client.go:486: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:486: [debug] Starting delete for \"manager-role\" ClusterRole\n    client.go:486: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:486: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:486: [debug] Starting delete for \"kubeslice-worker-event-schema-conf\" ConfigMap\n    client.go:486: [debug] Starting delete for \"spire-bundle\" ConfigMap\n    client.go:486: [debug] Starting delete for \"spire-controller-manager-config\" ConfigMap\n    client.go:486: [debug] Starting delete for \"spire-server\" ConfigMap\n    client.go:486: [debug] Starting delete for \"spire-agent\" ConfigMap\n    client.go:486: [debug] Starting delete for \"nsmgr-cm\" ConfigMap\n    client.go:486: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:486: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:486: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:486: [debug] Starting delete for \"kubeslice-kubernetes-dashboard-creds\" Secret\n    client.go:486: [debug] Starting delete for \"spire-agent\" ServiceAccount\n    client.go:486: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ServiceAccount\n    client.go:486: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:486: [debug] Starting delete for \"kubeslice-gateway-edge\" ServiceAccount\n    client.go:486: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:486: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:486: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:486: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:486: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:486: [debug] Starting delete for \"admission-webhook-sa\" ServiceAccount\n    client.go:486: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:486: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:486: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:486: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:486: [debug] Starting delete for \"spiffe-csi-driver\" ServiceAccount\n    client.go:486: [debug] Starting delete for \"spire-server\" ServiceAccount\n    client.go:486: [debug] Starting delete for \"spire\" Namespace\n    client.go:486: [debug] Starting delete for \"nsm-webhook-high-priority\" PriorityClass\n    client.go:486: [debug] Starting delete for \"csi.spiffe.io\" CSIDriver\n    client.go:486: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:486: [debug] Starting delete for \"spire-controller-manager-webhook\" ValidatingWebhookConfiguration\n    client.go:486: [debug] Starting delete for \"kubeslice-postdelete-job\" ClusterRole\n    client.go:490: [debug] Ignoring delete failure for \"kubeslice-postdelete-job\" rbac.authorization.k8s.io/v1, Kind=ClusterRole: clusterroles.rbac.authorization.k8s.io \"kubeslice-postdelete-job\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"kubeslice-postdelete-job\" ClusterRoleBinding\n    client.go:490: [debug] Ignoring delete failure for \"kubeslice-postdelete-job\" rbac.authorization.k8s.io/v1, Kind=ClusterRoleBinding: clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-postdelete-job\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"kubeslice-postdelete-job\" ServiceAccount\n    client.go:490: [debug] Ignoring delete failure for \"kubeslice-postdelete-job\" /v1, Kind=ServiceAccount: serviceaccounts \"kubeslice-postdelete-job\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"kubeslice-postdelete-job\" ConfigMap\n    client.go:490: [debug] Ignoring delete failure for \"kubeslice-postdelete-job\" /v1, Kind=ConfigMap: configmaps \"kubeslice-postdelete-job\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"kubeslice-postdelete-job\" Job\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:712: [debug] Watching for changes to Job kubeslice-postdelete-job with timeout of 10m0s\n    client.go:740: [debug] Add/Modify event for kubeslice-postdelete-job: ADDED\n    client.go:779: [debug] kubeslice-postdelete-job: Jobs active: 0, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for kubeslice-postdelete-job: MODIFIED\n    client.go:779: [debug] kubeslice-postdelete-job: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for kubeslice-postdelete-job: MODIFIED\n    client.go:779: [debug] kubeslice-postdelete-job: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for kubeslice-postdelete-job: MODIFIED\n    client.go:486: [debug] Starting delete for \"kubeslice-postdelete-job\" ClusterRole\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"kubeslice-postdelete-job\" ClusterRoleBinding\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"kubeslice-postdelete-job\" ServiceAccount\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"kubeslice-postdelete-job\" ConfigMap\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    uninstall.go:155: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: client rate limiter Wait returned an error: context deadline exceeded\n    helm.go:84: [debug] client rate limiter Wait returned an error: context deadline exceeded\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:496\n    helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\n    \thelm.sh/helm/v3/pkg/action/install.go:394\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:306\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:152\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:983\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:1115\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:1039\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:267\n    runtime.goexit\n    \truntime/asm_amd64.s:1650\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:154\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:983\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:1115\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:1039\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:267\n    runtime.goexit\n    \truntime/asm_amd64.s:1650\noccurred","time":{"start":1710226523000,"stop":1710227184086,"duration":661086}},{"uid":"6b543d6ebc369ddd","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0003cc0a8>: {\n        Underlying: <*exec.ExitError | 0xc0008ca000>{\n            ProcessState: {\n                pid: 9320,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 8, Usec: 344463},\n                    Stime: {Sec: 0, Usec: 757822},\n                    Maxrss: 121136,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 32632,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 776,\n                    Oublock: 21592,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 56341,\n                    Nivcsw: 11672,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:214: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:231: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-1.2.1.tgz\",\n                        \"\",\n                        \"client.go:142: [debug] creating 1 resource(s)\",\n                        \"install.go:168: [debug] CRD serviceexports.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:142: [debug] creating 1 resource(s)\",\n                        \"install.go:168: [debug] CRD serviceimports.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:142: [debug] creating 1 resource(s)\",\n                        \"install.go:168: [debug] CRD slicegateways.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:142: [debug] creating 1 resource(s)\",\n                        \"install.go:168: [debug] CRD slices.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:142: [debug] creating 1 resource(s)\",\n                        \"install.go:168: [debug] CRD networkservices.networkservicemesh.io is already present. Skipping.\",\n                        \"client.go:142: [debug] creating 1 resource(s)\",\n                        \"install.go:168: [debug] CRD networkserviceendpoints.networkservicemesh.io is already present. Skipping.\",\n                        \"client.go:142: [debug] creating 1 resource(s)\",\n                        \"install.go:168: [debug] CRD clusterfederatedtrustdomains.spire.spiffe.io is already present. Skipping.\",\n                        \"client.go:142: [debug] creating 1 resource(s)\",\n                        \"install.go:168: [debug] CRD clusterspiffeids.spire.spiffe.io is already present. Skipping.\",\n                        \"client.go:142: [debug] creating 1 resource(s)\",\n                        \"client.go:486: [debug] Starting delete for \\\"kubeslice-install-crds\\\" ServiceAccount\",\n                        \"client.go:490: [debug] Ignoring delete failure for \\\"kubeslice-install-crds\\\" /v1, Kind=ServiceAccount: serviceaccounts \\\"kubeslice-install-crds\\\" not found\",\n                        \"wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\",\n                        \"client.go:142: [debug] creating 1 resource(s)\",\n                        \"client.go:486: [debug] Starting delete for \\\"kubeslice-install-crds\\\" ConfigMap\",\n                        \"client.go:490: [debug] Ignoring delete failure for \\\"kubeslice-install-crds\\\" /v1, Kind=ConfigMap: configmaps \\\"kubeslice-install-crds\\\" not found\",\n                        \"wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeo...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:214: [debug] Original chart version: \"\"\n    install.go:231: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-1.2.1.tgz\n    \n    client.go:142: [debug] creating 1 resource(s)\n    install.go:168: [debug] CRD serviceexports.networking.kubeslice.io is already present. Skipping.\n    client.go:142: [debug] creating 1 resource(s)\n    install.go:168: [debug] CRD serviceimports.networking.kubeslice.io is already present. Skipping.\n    client.go:142: [debug] creating 1 resource(s)\n    install.go:168: [debug] CRD slicegateways.networking.kubeslice.io is already present. Skipping.\n    client.go:142: [debug] creating 1 resource(s)\n    install.go:168: [debug] CRD slices.networking.kubeslice.io is already present. Skipping.\n    client.go:142: [debug] creating 1 resource(s)\n    install.go:168: [debug] CRD networkservices.networkservicemesh.io is already present. Skipping.\n    client.go:142: [debug] creating 1 resource(s)\n    install.go:168: [debug] CRD networkserviceendpoints.networkservicemesh.io is already present. Skipping.\n    client.go:142: [debug] creating 1 resource(s)\n    install.go:168: [debug] CRD clusterfederatedtrustdomains.spire.spiffe.io is already present. Skipping.\n    client.go:142: [debug] creating 1 resource(s)\n    install.go:168: [debug] CRD clusterspiffeids.spire.spiffe.io is already present. Skipping.\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"kubeslice-install-crds\" ServiceAccount\n    client.go:490: [debug] Ignoring delete failure for \"kubeslice-install-crds\" /v1, Kind=ServiceAccount: serviceaccounts \"kubeslice-install-crds\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"kubeslice-install-crds\" ConfigMap\n    client.go:490: [debug] Ignoring delete failure for \"kubeslice-install-crds\" /v1, Kind=ConfigMap: configmaps \"kubeslice-install-crds\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"kubeslice-install-crds\" ClusterRole\n    client.go:490: [debug] Ignoring delete failure for \"kubeslice-install-crds\" rbac.authorization.k8s.io/v1, Kind=ClusterRole: clusterroles.rbac.authorization.k8s.io \"kubeslice-install-crds\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"kubeslice-install-crds\" ClusterRoleBinding\n    client.go:490: [debug] Ignoring delete failure for \"kubeslice-install-crds\" rbac.authorization.k8s.io/v1, Kind=ClusterRoleBinding: clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-install-crds\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:490: [debug] Ignoring delete failure for \"nsm-config\" /v1, Kind=ConfigMap: configmaps \"nsm-config\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"nsm-crd-install\" ConfigMap\n    client.go:490: [debug] Ignoring delete failure for \"nsm-crd-install\" /v1, Kind=ConfigMap: configmaps \"nsm-crd-install\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"nsm-install-crds\" ServiceAccount\n    client.go:490: [debug] Ignoring delete failure for \"nsm-install-crds\" /v1, Kind=ServiceAccount: serviceaccounts \"nsm-install-crds\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"nsm-install-crds\" ClusterRole\n    client.go:490: [debug] Ignoring delete failure for \"nsm-install-crds\" rbac.authorization.k8s.io/v1, Kind=ClusterRole: clusterroles.rbac.authorization.k8s.io \"nsm-install-crds\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"nsm-install-crds\" ClusterRoleBinding\n    client.go:490: [debug] Ignoring delete failure for \"nsm-install-crds\" rbac.authorization.k8s.io/v1, Kind=ClusterRoleBinding: clusterrolebindings.rbac.authorization.k8s.io \"nsm-install-crds\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"spire-clusterid-cr-install\" ConfigMap\n    client.go:490: [debug] Ignoring delete failure for \"spire-clusterid-cr-install\" /v1, Kind=ConfigMap: configmaps \"spire-clusterid-cr-install\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"spire-crd-install\" ConfigMap\n    client.go:490: [debug] Ignoring delete failure for \"spire-crd-install\" /v1, Kind=ConfigMap: configmaps \"spire-crd-install\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"spire-install-clusterid-cr\" ServiceAccount\n    client.go:490: [debug] Ignoring delete failure for \"spire-install-clusterid-cr\" /v1, Kind=ServiceAccount: serviceaccounts \"spire-install-clusterid-cr\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"spire-install-clusterid-cr\" ClusterRole\n    client.go:490: [debug] Ignoring delete failure for \"spire-install-clusterid-cr\" rbac.authorization.k8s.io/v1, Kind=ClusterRole: clusterroles.rbac.authorization.k8s.io \"spire-install-clusterid-cr\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"spire-install-clusterid-cr\" ClusterRoleBinding\n    client.go:490: [debug] Ignoring delete failure for \"spire-install-clusterid-cr\" rbac.authorization.k8s.io/v1, Kind=ClusterRoleBinding: clusterrolebindings.rbac.authorization.k8s.io \"spire-install-clusterid-cr\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"spire-install-crds\" ServiceAccount\n    client.go:490: [debug] Ignoring delete failure for \"spire-install-crds\" /v1, Kind=ServiceAccount: serviceaccounts \"spire-install-crds\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"spire-install-crds\" ClusterRole\n    client.go:490: [debug] Ignoring delete failure for \"spire-install-crds\" rbac.authorization.k8s.io/v1, Kind=ClusterRole: clusterroles.rbac.authorization.k8s.io \"spire-install-crds\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"spire-install-crds\" ClusterRoleBinding\n    client.go:490: [debug] Ignoring delete failure for \"spire-install-crds\" rbac.authorization.k8s.io/v1, Kind=ClusterRoleBinding: clusterrolebindings.rbac.authorization.k8s.io \"spire-install-crds\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"kubeslice-install-crds\" Job\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:712: [debug] Watching for changes to Job kubeslice-install-crds with timeout of 10m0s\n    client.go:740: [debug] Add/Modify event for kubeslice-install-crds: ADDED\n    client.go:779: [debug] kubeslice-install-crds: Jobs active: 0, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for kubeslice-install-crds: MODIFIED\n    client.go:779: [debug] kubeslice-install-crds: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for kubeslice-install-crds: MODIFIED\n    client.go:779: [debug] kubeslice-install-crds: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for kubeslice-install-crds: MODIFIED\n    client.go:779: [debug] kubeslice-install-crds: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for kubeslice-install-crds: MODIFIED\n    client.go:486: [debug] Starting delete for \"nsm-install-crds\" Job\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:712: [debug] Watching for changes to Job nsm-install-crds with timeout of 10m0s\n    client.go:740: [debug] Add/Modify event for nsm-install-crds: ADDED\n    client.go:779: [debug] nsm-install-crds: Jobs active: 0, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for nsm-install-crds: MODIFIED\n    client.go:779: [debug] nsm-install-crds: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for nsm-install-crds: MODIFIED\n    client.go:486: [debug] Starting delete for \"spire-install-crds\" Job\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:712: [debug] Watching for changes to Job spire-install-crds with timeout of 10m0s\n    client.go:740: [debug] Add/Modify event for spire-install-crds: ADDED\n    client.go:779: [debug] spire-install-crds: Jobs active: 0, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for spire-install-crds: MODIFIED\n    client.go:779: [debug] spire-install-crds: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for spire-install-crds: MODIFIED\n    client.go:779: [debug] spire-install-crds: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for spire-install-crds: MODIFIED\n    client.go:779: [debug] spire-install-crds: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for spire-install-crds: MODIFIED\n    client.go:486: [debug] Starting delete for \"spire-install-clusterid-cr\" Job\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:712: [debug] Watching for changes to Job spire-install-clusterid-cr with timeout of 10m0s\n    client.go:740: [debug] Add/Modify event for spire-install-clusterid-cr: ADDED\n    client.go:779: [debug] spire-install-clusterid-cr: Jobs active: 0, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for spire-install-clusterid-cr: MODIFIED\n    client.go:779: [debug] spire-install-clusterid-cr: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for spire-install-clusterid-cr: MODIFIED\n    client.go:779: [debug] spire-install-clusterid-cr: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for spire-install-clusterid-cr: MODIFIED\n    client.go:779: [debug] spire-install-clusterid-cr: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for spire-install-clusterid-cr: MODIFIED\n    client.go:486: [debug] Starting delete for \"kubeslice-install-crds\" ServiceAccount\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"kubeslice-install-crds\" ConfigMap\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"kubeslice-install-crds\" ClusterRole\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"kubeslice-install-crds\" ClusterRoleBinding\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"nsm-crd-install\" ConfigMap\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"nsm-install-crds\" ServiceAccount\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"nsm-install-crds\" ClusterRole\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"nsm-install-crds\" ClusterRoleBinding\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"spire-clusterid-cr-install\" ConfigMap\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"spire-crd-install\" ConfigMap\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"spire-install-clusterid-cr\" ServiceAccount\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"spire-install-clusterid-cr\" ClusterRole\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"spire-install-clusterid-cr\" ClusterRoleBinding\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"spire-install-crds\" ServiceAccount\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"spire-install-crds\" ClusterRole\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"spire-install-crds\" ClusterRoleBinding\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:393: [debug] checking 74 resources for changes\n    client.go:414: [debug] Created a new PriorityClass called \"nsm-webhook-high-priority\" in \n    \n    client.go:414: [debug] Created a new Namespace called \"spire\" in \n    \n    client.go:414: [debug] Created a new ServiceAccount called \"admission-webhook-sa\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new ServiceAccount called \"nse-acc\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new ServiceAccount called \"nsc-acc\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new ServiceAccount called \"nsmgr-acc\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new ServiceAccount called \"forward-plane-acc\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new ServiceAccount called \"spiffe-csi-driver\" in spire\n    \n    client.go:414: [debug] Created a new ServiceAccount called \"spire-server\" in spire\n    \n    client.go:414: [debug] Created a new ServiceAccount called \"spire-agent\" in spire\n    \n    client.go:414: [debug] Created a new ServiceAccount called \"kubeslice-kubernetes-dashboard\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new ServiceAccount called \"kubeslice-dns\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new ServiceAccount called \"kubeslice-gateway-edge\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new ServiceAccount called \"kubeslice-netop\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new ServiceAccount called \"vpn-gateway-server\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new ServiceAccount called \"vpn-gateway-client\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new ServiceAccount called \"slice-router\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new ServiceAccount called \"kubeslice-controller-manager\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new Secret called \"kubeslice-kubernetes-dashboard-creds\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new Secret called \"kubeslice-hub\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new Secret called \"kubeslice-admission-webhook-certs\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new ConfigMap called \"spire-bundle\" in spire\n    \n    client.go:414: [debug] Created a new ConfigMap called \"spire-controller-manager-config\" in spire\n    \n    client.go:414: [debug] Created a new ConfigMap called \"spire-server\" in spire\n    \n    client.go:414: [debug] Created a new ConfigMap called \"spire-agent\" in spire\n    \n    client.go:414: [debug] Created a new ConfigMap called \"nsmgr-cm\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new ConfigMap called \"kubeslice-manager-config\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new ConfigMap called \"kubeslice-worker-event-schema-conf\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new ClusterRole called \"admission-webhook-role\" in \n    \n    client.go:414: [debug] Created a new ClusterRole called \"nsm-role\" in \n    \n    client.go:414: [debug] Created a new ClusterRole called \"aggregate-network-services-view\" in \n    \n    client.go:414: [debug] Created a new ClusterRole called \"manager-role\" in \n    \n    client.go:414: [debug] Created a new ClusterRole called \"spire-server-trust-role\" in \n    \n    client.go:414: [debug] Created a new ClusterRole called \"spire-agent-cluster-role\" in \n    \n    client.go:414: [debug] Created a new ClusterRole called \"kubeslice-kubernetes-dashboard\" in \n    \n    client.go:414: [debug] Created a new ClusterRole called \"kubeslice-dns-role\" in \n    \n    client.go:414: [debug] Created a new ClusterRole called \"kubeslice-manager-role\" in \n    \n    client.go:414: [debug] Created a new ClusterRole called \"kubeslice-metrics-reader\" in \n    \n    client.go:414: [debug] Created a new ClusterRole called \"kubeslice-proxy-role\" in \n    \n    client.go:414: [debug] Created a new ClusterRoleBinding called \"admission-webhook-binding\" in \n    \n    client.go:414: [debug] Created a new ClusterRoleBinding called \"nsm-role-binding\" in \n    \n    client.go:414: [debug] Created a new ClusterRoleBinding called \"manager-rolebinding\" in \n    \n    client.go:414: [debug] Created a new ClusterRoleBinding called \"spire-server-trust-role-binding\" in \n    \n    client.go:414: [debug] Created a new ClusterRoleBinding called \"spire-agent-cluster-role-binding\" in \n    \n    client.go:414: [debug] Created a new ClusterRoleBinding called \"kubeslice-kubernetes-dashboard\" in \n    \n    client.go:414: [debug] Created a new ClusterRoleBinding called \"kubeslice-dns-rolebinding\" in \n    \n    client.go:414: [debug] Created a new ClusterRoleBinding called \"kubeslice-manager-rolebinding\" in \n    \n    client.go:414: [debug] Created a new ClusterRoleBinding called \"kubeslice-proxy-rolebinding\" in \n    \n    client.go:414: [debug] Created a new Role called \"leader-election-role\" in spire\n    \n    client.go:414: [debug] Created a new Role called \"spire-server-role\" in spire\n    \n    client.go:414: [debug] Created a new Role called \"kubeslice-leader-election-role\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new RoleBinding called \"leader-election-rolebinding\" in spire\n    \n    client.go:414: [debug] Created a new RoleBinding called \"spire-server-role-binding\" in spire\n    \n    client.go:414: [debug] Created a new RoleBinding called \"kubeslice-leader-election-rolebinding\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new Service called \"admission-webhook-svc\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new Service called \"spire-server\" in spire\n    \n    client.go:414: [debug] Created a new Service called \"spire-controller-manager-webhook-service\" in spire\n    \n    client.go:414: [debug] Created a new Service called \"nsmgr\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new Service called \"registry\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new Service called \"kubeslice-dns\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new Service called \"kubeslice-webhook-service\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new DaemonSet called \"spiffe-csi-driver\" in spire\n    \n    client.go:414: [debug] Created a new DaemonSet called \"spire-agent\" in spire\n    \n    client.go:414: [debug] Created a new DaemonSet called \"forwarder-kernel\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new DaemonSet called \"nsmgr\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new DaemonSet called \"kubeslice-netop\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new Deployment called \"nsm-admission-webhook-k8s\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new Deployment called \"registry-k8s\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new Deployment called \"kubeslice-dns\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new Deployment called \"kubeslice-operator\" in kubeslice-system\n    \n    client.go:414: [debug] Created a new StatefulSet called \"spire-server\" in spire\n    \n    client.go:684: [debug] Looks like there are no changes for CSIDriver \"csi.spiffe.io\"\n    client.go:414: [debug] Created a new MutatingWebhookConfiguration called \"kubeslice-mutating-webhook-configuration\" in \n    \n    client.go:414: [debug] Created a new ValidatingWebhookConfiguration called \"spire-controller-manager-webhook\" in \n    \n    wait.go:48: [debug] beginning wait for 74 resources with timeout of 10m0s\n    ready.go:336: [debug] DaemonSet is not ready: spire/spiffe-csi-driver. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spiffe-csi-driver. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spiffe-csi-driver. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    ready.go:336: [debug] DaemonSet is not ready: spire/spire-agent. 0 out of 2 expected pods are ready\n    install.go:488: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:102: [debug] uninstall: Deleting kubeslice-worker\n    client.go:486: [debug] Starting delete for \"nsm-delete-webhooks\" ClusterRole\n    client.go:490: [debug] Ignoring delete failure for \"nsm-delete-webhooks\" rbac.authorization.k8s.io/v1, Kind=ClusterRole: clusterroles.rbac.authorization.k8s.io \"nsm-delete-webhooks\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"nsm-delete-webhooks\" ClusterRoleBinding\n    client.go:490: [debug] Ignoring delete failure for \"nsm-delete-webhooks\" rbac.authorization.k8s.io/v1, Kind=ClusterRoleBinding: clusterrolebindings.rbac.authorization.k8s.io \"nsm-delete-webhooks\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"kubeslice-cleanup\" ServiceAccount\n    client.go:490: [debug] Ignoring delete failure for \"kubeslice-cleanup\" /v1, Kind=ServiceAccount: serviceaccounts \"kubeslice-cleanup\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"kubeslice-cleanup\" ClusterRole\n    client.go:490: [debug] Ignoring delete failure for \"kubeslice-cleanup\" rbac.authorization.k8s.io/v1, Kind=ClusterRole: clusterroles.rbac.authorization.k8s.io \"kubeslice-cleanup\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"kubeslice-cleanup\" ClusterRoleBinding\n    client.go:490: [debug] Ignoring delete failure for \"kubeslice-cleanup\" rbac.authorization.k8s.io/v1, Kind=ClusterRoleBinding: clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-cleanup\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"nsm-delete-webhooks\" ServiceAccount\n    client.go:490: [debug] Ignoring delete failure for \"nsm-delete-webhooks\" /v1, Kind=ServiceAccount: serviceaccounts \"nsm-delete-webhooks\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"nsm-delete-webhooks\" ConfigMap\n    client.go:490: [debug] Ignoring delete failure for \"nsm-delete-webhooks\" /v1, Kind=ConfigMap: configmaps \"nsm-delete-webhooks\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"kubeslice-cleanup\" Job\n    client.go:490: [debug] Ignoring delete failure for \"kubeslice-cleanup\" batch/v1, Kind=Job: jobs.batch \"kubeslice-cleanup\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:712: [debug] Watching for changes to Job kubeslice-cleanup with timeout of 10m0s\n    client.go:740: [debug] Add/Modify event for kubeslice-cleanup: ADDED\n    client.go:779: [debug] kubeslice-cleanup: Jobs active: 0, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for kubeslice-cleanup: MODIFIED\n    client.go:779: [debug] kubeslice-cleanup: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for kubeslice-cleanup: MODIFIED\n    client.go:486: [debug] Starting delete for \"nsm-delete-webhooks\" Job\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:712: [debug] Watching for changes to Job nsm-delete-webhooks with timeout of 10m0s\n    client.go:740: [debug] Add/Modify event for nsm-delete-webhooks: ADDED\n    client.go:779: [debug] nsm-delete-webhooks: Jobs active: 0, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for nsm-delete-webhooks: MODIFIED\n    client.go:779: [debug] nsm-delete-webhooks: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for nsm-delete-webhooks: MODIFIED\n    client.go:486: [debug] Starting delete for \"nsm-delete-webhooks\" ClusterRole\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"nsm-delete-webhooks\" ClusterRoleBinding\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"kubeslice-cleanup\" ServiceAccount\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"kubeslice-cleanup\" ClusterRole\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"kubeslice-cleanup\" ClusterRoleBinding\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"nsm-delete-webhooks\" ServiceAccount\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"nsm-delete-webhooks\" ConfigMap\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"kubeslice-cleanup\" Job\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    uninstall.go:248: [debug] uninstall: given cascade value: , defaulting to delete propagation background\n    client.go:486: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:486: [debug] Starting delete for \"admission-webhook-svc\" Service\n    client.go:486: [debug] Starting delete for \"registry\" Service\n    client.go:486: [debug] Starting delete for \"nsmgr\" Service\n    client.go:486: [debug] Starting delete for \"spire-controller-manager-webhook-service\" Service\n    client.go:486: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:486: [debug] Starting delete for \"spire-server\" Service\n    client.go:486: [debug] Starting delete for \"spire-server\" StatefulSet\n    client.go:486: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:486: [debug] Starting delete for \"nsm-admission-webhook-k8s\" Deployment\n    client.go:486: [debug] Starting delete for \"registry-k8s\" Deployment\n    client.go:486: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:486: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:486: [debug] Starting delete for \"spiffe-csi-driver\" DaemonSet\n    client.go:486: [debug] Starting delete for \"spire-agent\" DaemonSet\n    client.go:486: [debug] Starting delete for \"forwarder-kernel\" DaemonSet\n    client.go:486: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:486: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:486: [debug] Starting delete for \"leader-election-rolebinding\" RoleBinding\n    client.go:486: [debug] Starting delete for \"spire-server-role-binding\" RoleBinding\n    client.go:486: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:486: [debug] Starting delete for \"leader-election-role\" Role\n    client.go:486: [debug] Starting delete for \"spire-server-role\" Role\n    client.go:486: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:486: [debug] Starting delete for \"admission-webhook-binding\" ClusterRoleBinding\n    client.go:486: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:486: [debug] Starting delete for \"manager-rolebinding\" ClusterRoleBinding\n    client.go:486: [debug] Starting delete for \"spire-server-trust-role-binding\" ClusterRoleBinding\n    client.go:486: [debug] Starting delete for \"spire-agent-cluster-role-binding\" ClusterRoleBinding\n    client.go:486: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRoleBinding\n    client.go:486: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:486: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:486: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:486: [debug] Starting delete for \"admission-webhook-role\" ClusterRole\n    client.go:486: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:486: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:486: [debug] Starting delete for \"manager-role\" ClusterRole\n    client.go:486: [debug] Starting delete for \"spire-server-trust-role\" ClusterRole\n    client.go:486: [debug] Starting delete for \"spire-agent-cluster-role\" ClusterRole\n    client.go:486: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:486: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRole\n    client.go:486: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:486: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:486: [debug] Starting delete for \"kubeslice-worker-event-schema-conf\" ConfigMap\n    client.go:486: [debug] Starting delete for \"spire-bundle\" ConfigMap\n    client.go:486: [debug] Starting delete for \"spire-controller-manager-config\" ConfigMap\n    client.go:486: [debug] Starting delete for \"spire-server\" ConfigMap\n    client.go:486: [debug] Starting delete for \"nsmgr-cm\" ConfigMap\n    client.go:486: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:486: [debug] Starting delete for \"spire-agent\" ConfigMap\n    client.go:486: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:486: [debug] Starting delete for \"kubeslice-kubernetes-dashboard-creds\" Secret\n    client.go:486: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:486: [debug] Starting delete for \"spire-agent\" ServiceAccount\n    client.go:486: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ServiceAccount\n    client.go:486: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:486: [debug] Starting delete for \"kubeslice-gateway-edge\" ServiceAccount\n    client.go:486: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:486: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:486: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:486: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:486: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:486: [debug] Starting delete for \"admission-webhook-sa\" ServiceAccount\n    client.go:486: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:486: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:486: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:486: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:486: [debug] Starting delete for \"spiffe-csi-driver\" ServiceAccount\n    client.go:486: [debug] Starting delete for \"spire-server\" ServiceAccount\n    client.go:486: [debug] Starting delete for \"spire\" Namespace\n    client.go:486: [debug] Starting delete for \"nsm-webhook-high-priority\" PriorityClass\n    client.go:486: [debug] Starting delete for \"csi.spiffe.io\" CSIDriver\n    client.go:486: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:486: [debug] Starting delete for \"spire-controller-manager-webhook\" ValidatingWebhookConfiguration\n    client.go:486: [debug] Starting delete for \"kubeslice-postdelete-job\" ClusterRole\n    client.go:490: [debug] Ignoring delete failure for \"kubeslice-postdelete-job\" rbac.authorization.k8s.io/v1, Kind=ClusterRole: clusterroles.rbac.authorization.k8s.io \"kubeslice-postdelete-job\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"kubeslice-postdelete-job\" ClusterRoleBinding\n    client.go:490: [debug] Ignoring delete failure for \"kubeslice-postdelete-job\" rbac.authorization.k8s.io/v1, Kind=ClusterRoleBinding: clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-postdelete-job\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"kubeslice-postdelete-job\" ServiceAccount\n    client.go:490: [debug] Ignoring delete failure for \"kubeslice-postdelete-job\" /v1, Kind=ServiceAccount: serviceaccounts \"kubeslice-postdelete-job\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"kubeslice-postdelete-job\" ConfigMap\n    client.go:490: [debug] Ignoring delete failure for \"kubeslice-postdelete-job\" /v1, Kind=ConfigMap: configmaps \"kubeslice-postdelete-job\" not found\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:486: [debug] Starting delete for \"kubeslice-postdelete-job\" Job\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:142: [debug] creating 1 resource(s)\n    client.go:712: [debug] Watching for changes to Job kubeslice-postdelete-job with timeout of 10m0s\n    client.go:740: [debug] Add/Modify event for kubeslice-postdelete-job: ADDED\n    client.go:779: [debug] kubeslice-postdelete-job: Jobs active: 0, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for kubeslice-postdelete-job: MODIFIED\n    client.go:779: [debug] kubeslice-postdelete-job: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:740: [debug] Add/Modify event for kubeslice-postdelete-job: MODIFIED\n    client.go:486: [debug] Starting delete for \"kubeslice-postdelete-job\" ClusterRole\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"kubeslice-postdelete-job\" ClusterRoleBinding\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"kubeslice-postdelete-job\" ServiceAccount\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    client.go:486: [debug] Starting delete for \"kubeslice-postdelete-job\" ConfigMap\n    wait.go:66: [debug] beginning wait for 1 resources to be deleted with timeout of 10m0s\n    uninstall.go:155: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: client rate limiter Wait returned an error: context deadline exceeded\n    helm.go:84: [debug] client rate limiter Wait returned an error: context deadline exceeded\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:496\n    helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\n    \thelm.sh/helm/v3/pkg/action/install.go:394\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:306\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:152\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:983\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:1115\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:1039\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:267\n    runtime.goexit\n    \truntime/asm_amd64.s:1650\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:154\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:983\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:1115\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.8.0/command.go:1039\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:267\n    runtime.goexit\n    \truntime/asm_amd64.s:1650\noccurred","time":{"start":1709885462000,"stop":1709886121435,"duration":659435}},{"uid":"b22f5254a6dcd7aa","status":"passed","time":{"start":1709723127000,"stop":1709723320067,"duration":193067}},{"uid":"4bb7ded7f53c95be","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000e480>: {\n        Underlying: <*exec.ExitError | 0xc0004f8240>{\n            ProcessState: {\n                pid: 6479,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 112083},\n                    Stime: {Sec: 0, Usec: 20378},\n                    Maxrss: 50008,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2469,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 176,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 652,\n                    Nivcsw: 448,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:200: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:217: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-1.1.2.tgz\",\n                        \"\",\n                        \"Error: INSTALLATION FAILED: cannot re-use a name that is still in use\",\n                        \"helm.go:84: [debug] cannot re-use a name that is still in use\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).availableName\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:517\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:227\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:287\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:145\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.6.1/command.go:916\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.6.1/command.go:1044\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.6.1/command.go:968\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:250\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1598\",\n                        \"INSTALLATION FAILED\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:147\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.6.1/command.go:916\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.6.1/command.go:1044\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.6.1/command.go:968\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:250\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1598\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is in...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:200: [debug] Original chart version: \"\"\n    install.go:217: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-1.1.2.tgz\n    \n    Error: INSTALLATION FAILED: cannot re-use a name that is still in use\n    helm.go:84: [debug] cannot re-use a name that is still in use\n    helm.sh/helm/v3/pkg/action.(*Install).availableName\n    \thelm.sh/helm/v3/pkg/action/install.go:517\n    helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\n    \thelm.sh/helm/v3/pkg/action/install.go:227\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:287\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:145\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:916\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:1044\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:968\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1598\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:147\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:916\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:1044\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:968\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1598\noccurred","time":{"start":1695807042000,"stop":1695807042621,"duration":621}},{"uid":"1f38bf34b0805f26","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000014e88>: {\n        Underlying: <*exec.ExitError | 0xc000476540>{\n            ProcessState: {\n                pid: 6405,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 89072},\n                    Stime: {Sec: 0, Usec: 32390},\n                    Maxrss: 48120,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2149,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 168,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 542,\n                    Nivcsw: 277,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:200: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:217: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-1.2.0.tgz\",\n                        \"\",\n                        \"Error: INSTALLATION FAILED: cannot re-use a name that is still in use\",\n                        \"helm.go:84: [debug] cannot re-use a name that is still in use\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).availableName\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:512\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:222\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:286\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:145\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.6.1/command.go:916\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.6.1/command.go:1044\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.6.1/command.go:968\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:250\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1598\",\n                        \"INSTALLATION FAILED\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:147\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.6.1/command.go:916\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.6.1/command.go:1044\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.6.1/command.go:968\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:250\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1598\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is ins...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:200: [debug] Original chart version: \"\"\n    install.go:217: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-1.2.0.tgz\n    \n    Error: INSTALLATION FAILED: cannot re-use a name that is still in use\n    helm.go:84: [debug] cannot re-use a name that is still in use\n    helm.sh/helm/v3/pkg/action.(*Install).availableName\n    \thelm.sh/helm/v3/pkg/action/install.go:512\n    helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\n    \thelm.sh/helm/v3/pkg/action/install.go:222\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:286\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:145\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:916\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:1044\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:968\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1598\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:147\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:916\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:1044\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:968\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1598\noccurred","time":{"start":1689751363000,"stop":1689751363398,"duration":398}},{"uid":"627308c2e51d2e47","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000128d50>: {\n        Underlying: <*exec.ExitError | 0xc0003fbf60>{\n            ProcessState: {\n                pid: 6379,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 103836},\n                    Stime: {Sec: 0, Usec: 23962},\n                    Maxrss: 48128,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2302,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 168,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 517,\n                    Nivcsw: 278,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:200: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:217: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-1.2.0.tgz\",\n                        \"\",\n                        \"Error: INSTALLATION FAILED: cannot re-use a name that is still in use\",\n                        \"helm.go:84: [debug] cannot re-use a name that is still in use\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).availableName\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:512\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:222\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:286\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:145\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.6.1/command.go:916\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.6.1/command.go:1044\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.6.1/command.go:968\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:250\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1598\",\n                        \"INSTALLATION FAILED\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:147\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.6.1/command.go:916\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.6.1/command.go:1044\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.6.1/command.go:968\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:250\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1598\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is in...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:200: [debug] Original chart version: \"\"\n    install.go:217: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-1.2.0.tgz\n    \n    Error: INSTALLATION FAILED: cannot re-use a name that is still in use\n    helm.go:84: [debug] cannot re-use a name that is still in use\n    helm.sh/helm/v3/pkg/action.(*Install).availableName\n    \thelm.sh/helm/v3/pkg/action/install.go:512\n    helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\n    \thelm.sh/helm/v3/pkg/action/install.go:222\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:286\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:145\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:916\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:1044\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:968\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1598\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:147\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:916\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:1044\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:968\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1598\noccurred","time":{"start":1688628155000,"stop":1688628155543,"duration":543}},{"uid":"ca2ade5904afb48e","status":"passed","time":{"start":1688026084000,"stop":1688026304701,"duration":220701}},{"uid":"61d09ccb032bdb72","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004a4558>: {\n        Underlying: <*exec.ExitError | 0xc0004b01a0>{\n            ProcessState: {\n                pid: 6319,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 69266},\n                    Stime: {Sec: 0, Usec: 4329},\n                    Maxrss: 43880,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2030,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 351,\n                    Nivcsw: 293,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: error executing jsonpath \\\"\\\\\\\"{.clusters[?(@.name==\\\\\\\"kind-controller\\\\\\\")].cluster.server}\\\\\\\"\\\": Error executing template: <nil> is not array or slice and cannot be filtered. Printing more information for debugging the template:\",\n                        \"\\ttemplate was:\",\n                        \"\\t\\t\\\"{.clusters[?(@.name==\\\"kind-controller\\\")].cluster.server}\\\"\",\n                        \"\\tobject given to jsonpath engine was:\",\n                        \"\\t\\tmap[string]interface {}{\\\"apiVersion\\\":\\\"v1\\\", \\\"clusters\\\":interface {}(nil), \\\"contexts\\\":interface {}(nil), \\\"current-context\\\":\\\"\\\", \\\"kind\\\":\\\"Config\\\", \\\"preferences\\\":map[string]interface {}{}, \\\"users\\\":interface {}(nil)}\",\n                        \"\",\n                        \"\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: error executing jsonpath \\\"\\\\\\\"{.clusters[?(@.name==\\\\\\\"kind-controller\\\\\\\")].cluster.server}\\\\\\\"\\\": Error executing template: <nil> is not array or slice and cannot be filtered. Printing more information for debugging the template:\",\n                    \"\\ttemplate was:\",\n                    \"\\t\\t\\\"{.clusters[?(@.name==\\\"kind-controller\\\")].cluster.server}\\\"\",\n                    \"\\tobject given to jsonpath engine was:\",\n                    \"\\t\\tmap[string]interface {}{\\\"apiVersion\\\":\\\"v1\\\", \\\"clusters\\\":interface {}(nil), \\\"contexts\\\":interface {}(nil), \\\"current-context\\\":\\\"\\\", \\\"kind\\\":\\\"Config\\\", \\\"preferences\\\":map[string]interface {}{}, \\\"users\\\":interface {}(nil)}\",\n                    \"\",\n                    \"\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: error executing jsonpath \\\"\\\\\\\"{.clusters[?(@.name==\\\\\\\"kind-controller\\\\\\\")].cluster.server}\\\\\\\"\\\": Error executing template: <nil> is not array or slice and cannot be filtered. Printing more information for debugging the template:\",\n                        \"\\ttemplate was:\",\n                        \"\\t\\t\\\"{.clusters[?(@.name==\\\"kind-controller\\\")].cluster.server}\\\"\",\n                        \"\\tobject given to jsonpath engine was:\",\n                        \"\\t\\tmap[string]interface {}{\\\"apiVersion\\\":\\\"v1\\\", \\\"clusters\\\":interface {}(nil), \\\"contexts\\\":interface {}(nil), \\\"current-context\\\":\\\"\\\", \\\"kind\\\":\\\"Config\\\", \\\"preferences\\\":map[string]interface {}{}, \\\"users\\\":interface {}(nil)}\",\n                        \"\",\n                        \"\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: error executing jsonpath \\\"\\\\\\\"{.clusters[?(@.name==\\\\\\\"kind-controller\\\\\\\")].cluster.server}\\\\\\\"\\\": Error executing template: <nil> is not array or slice and cannot be filtered. Printing more information for debugging the template:\",\n                    \"\\ttemplate was:\",\n                    \"\\t\\t\\\"{.clusters[?(@...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; error: error executing jsonpath \"\\\"{.clusters[?(@.name==\\\"kind-controller\\\")].cluster.server}\\\"\": Error executing template: <nil> is not array or slice and cannot be filtered. Printing more information for debugging the template:\n    \ttemplate was:\n    \t\t\"{.clusters[?(@.name==\"kind-controller\")].cluster.server}\"\n    \tobject given to jsonpath engine was:\n    \t\tmap[string]interface {}{\"apiVersion\":\"v1\", \"clusters\":interface {}(nil), \"contexts\":interface {}(nil), \"current-context\":\"\", \"kind\":\"Config\", \"preferences\":map[string]interface {}{}, \"users\":interface {}(nil)}\n    \n    \noccurred","time":{"start":1688018435000,"stop":1688018435086,"duration":86}}]},"tags":[]},"source":"91b1f7fc2ed65962.json","parameterValues":[]}