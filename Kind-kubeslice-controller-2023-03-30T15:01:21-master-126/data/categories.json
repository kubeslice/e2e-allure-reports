{"uid":"4b4757e66a1912dae1a509f688f20b0f","children":[{"name":"Product defects","children":[{"name":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000383128>: {\n        Underlying: <*exec.ExitError | 0xc000841860>{\n            ProcessState: {\n                pid: 6991,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 1, Usec: 488271},\n                    Stime: {Sec: 0, Usec: 171111},\n                    Maxrss: 87804,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3450,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13472,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 2585,\n                    Nivcsw: 3016,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:194: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:211: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.7.1.tgz\",\n                        \"\",\n                        \"client.go:133: [debug] creating 1 resource(s)\",\n                        \"install.go:154: [debug] CRD serviceexports.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:133: [debug] creating 1 resource(s)\",\n                        \"install.go:154: [debug] CRD serviceimports.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:133: [debug] creating 1 resource(s)\",\n                        \"install.go:154: [debug] CRD slicegateways.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:133: [debug] creating 1 resource(s)\",\n                        \"install.go:154: [debug] CRD slices.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:133: [debug] creating 1 resource(s)\",\n                        \"install.go:154: [debug] CRD networkservices.networkservicemesh.io is already present. Skipping.\",\n                        \"client.go:133: [debug] creating 1 resource(s)\",\n                        \"install.go:154: [debug] CRD networkserviceendpoints.networkservicemesh.io is already present. Skipping.\",\n                        \"client.go:133: [debug] creating 1 resource(s)\",\n                        \"install.go:154: [debug] CRD spiffeids.spiffeid.spiffe.io is already present. Skipping.\",\n                        \"client.go:133: [debug] creating 1 resource(s)\",\n                        \"Error: INSTALLATION FAILED: create: failed to create: secrets \\\"sh.helm.release.v1.kubeslice-worker.v1\\\" is forbidden: unable to create new content in namespace kubeslice-system because it is being terminated\",\n                        \"helm.go:84: [debug] secrets \\\"sh.helm.release.v1.kubeslice-worker.v1\\\" is forbidden: unable to create new content in namespace kubeslice-system because it is being terminated\",\n                        \"create: failed to create\",\n                        \"helm.sh/helm/v3/pkg/storage/driver.(*Secrets).Create\",\n                        \"\\thelm.sh/helm/v3/pkg/storage/driver/secrets.go:164\",\n                        \"helm.sh/helm/v3/pkg/storage.(*Storage).Create\",\n                        \"\\thelm.sh/helm/v3/pkg/storage/storage.go:69\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:343\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:280\",\n                     ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:194: [debug] Original chart version: \"\"\n    install.go:211: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.7.1.tgz\n    \n    client.go:133: [debug] creating 1 resource(s)\n    install.go:154: [debug] CRD serviceexports.networking.kubeslice.io is already present. Skipping.\n    client.go:133: [debug] creating 1 resource(s)\n    install.go:154: [debug] CRD serviceimports.networking.kubeslice.io is already present. Skipping.\n    client.go:133: [debug] creating 1 resource(s)\n    install.go:154: [debug] CRD slicegateways.networking.kubeslice.io is already present. Skipping.\n    client.go:133: [debug] creating 1 resource(s)\n    install.go:154: [debug] CRD slices.networking.kubeslice.io is already present. Skipping.\n    client.go:133: [debug] creating 1 resource(s)\n    install.go:154: [debug] CRD networkservices.networkservicemesh.io is already present. Skipping.\n    client.go:133: [debug] creating 1 resource(s)\n    install.go:154: [debug] CRD networkserviceendpoints.networkservicemesh.io is already present. Skipping.\n    client.go:133: [debug] creating 1 resource(s)\n    install.go:154: [debug] CRD spiffeids.spiffeid.spiffe.io is already present. Skipping.\n    client.go:133: [debug] creating 1 resource(s)\n    Error: INSTALLATION FAILED: create: failed to create: secrets \"sh.helm.release.v1.kubeslice-worker.v1\" is forbidden: unable to create new content in namespace kubeslice-system because it is being terminated\n    helm.go:84: [debug] secrets \"sh.helm.release.v1.kubeslice-worker.v1\" is forbidden: unable to create new content in namespace kubeslice-system because it is being terminated\n    create: failed to create\n    helm.sh/helm/v3/pkg/storage/driver.(*Secrets).Create\n    \thelm.sh/helm/v3/pkg/storage/driver/secrets.go:164\n    helm.sh/helm/v3/pkg/storage.(*Storage).Create\n    \thelm.sh/helm/v3/pkg/storage/storage.go:69\n    helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\n    \thelm.sh/helm/v3/pkg/action/install.go:343\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:280\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:139\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:916\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:1044\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:968\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:141\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:916\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:1044\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:968\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\noccurred","children":[{"name":"[It] Cluster CR tests Cluster CR validation Register worker cluster (with minimal values in cluster CR)","uid":"5bd3a8c7b1c63aad","parentUid":"70b137d8d1c0ec536e29c5792ea2d985","status":"failed","time":{"start":1680187794000,"stop":1680187804711,"duration":10711},"flaky":false,"newFailed":false,"newPassed":false,"newBroken":false,"retriesCount":0,"retriesStatusChange":false,"parameters":[]}],"uid":"70b137d8d1c0ec536e29c5792ea2d985"},{"name":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004ba150>: {\n        Underlying: <*exec.ExitError | 0xc000896000>{\n            ProcessState: {\n                pid: 7046,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 6, Usec: 121677},\n                    Stime: {Sec: 0, Usec: 875160},\n                    Maxrss: 91868,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 8814,\n                    Majflt: 2,\n                    Nswap: 0,\n                    Inblock: 344,\n                    Oublock: 13472,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 59438,\n                    Nivcsw: 18318,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:194: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:211: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.7.1.tgz\",\n                        \"\",\n                        \"client.go:133: [debug] creating 1 resource(s)\",\n                        \"install.go:154: [debug] CRD serviceexports.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:133: [debug] creating 1 resource(s)\",\n                        \"install.go:154: [debug] CRD serviceimports.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:133: [debug] creating 1 resource(s)\",\n                        \"install.go:154: [debug] CRD slicegateways.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:133: [debug] creating 1 resource(s)\",\n                        \"install.go:154: [debug] CRD slices.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:133: [debug] creating 1 resource(s)\",\n                        \"install.go:154: [debug] CRD networkservices.networkservicemesh.io is already present. Skipping.\",\n                        \"client.go:133: [debug] creating 1 resource(s)\",\n                        \"install.go:154: [debug] CRD networkserviceendpoints.networkservicemesh.io is already present. Skipping.\",\n                        \"client.go:133: [debug] creating 1 resource(s)\",\n                        \"install.go:154: [debug] CRD spiffeids.spiffeid.spiffe.io is already present. Skipping.\",\n                        \"client.go:133: [debug] creating 1 resource(s)\",\n                        \"client.go:477: [debug] Starting delete for \\\"kubeslice-install-crds\\\" ServiceAccount\",\n                        \"client.go:481: [debug] Ignoring delete failure for \\\"kubeslice-install-crds\\\" /v1, Kind=ServiceAccount: serviceaccounts \\\"kubeslice-install-crds\\\" not found\",\n                        \"client.go:133: [debug] creating 1 resource(s)\",\n                        \"client.go:477: [debug] Starting delete for \\\"kubeslice-install-crds\\\" ConfigMap\",\n                        \"client.go:481: [debug] Ignoring delete failure for \\\"kubeslice-install-crds\\\" /v1, Kind=ConfigMap: configmaps \\\"kubeslice-install-crds\\\" not found\",\n                        \"client.go:133: [debug] creating 1 resource(s)\",\n                        \"client.go:477: [debug] Starting delete for \\\"kubeslice-install-crds\\\" ClusterRole\",\n                        \"client.go:481: [debug] Ignoring delete failure for \\\"kubeslice-install-crds\\\" rbac.authorization.k8s.io/v1, Kind=ClusterRole: clusterroles.rbac.authorization.k8s.io \\\"kubeslice-install-crds\\\" not found\",\n            ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:194: [debug] Original chart version: \"\"\n    install.go:211: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.7.1.tgz\n    \n    client.go:133: [debug] creating 1 resource(s)\n    install.go:154: [debug] CRD serviceexports.networking.kubeslice.io is already present. Skipping.\n    client.go:133: [debug] creating 1 resource(s)\n    install.go:154: [debug] CRD serviceimports.networking.kubeslice.io is already present. Skipping.\n    client.go:133: [debug] creating 1 resource(s)\n    install.go:154: [debug] CRD slicegateways.networking.kubeslice.io is already present. Skipping.\n    client.go:133: [debug] creating 1 resource(s)\n    install.go:154: [debug] CRD slices.networking.kubeslice.io is already present. Skipping.\n    client.go:133: [debug] creating 1 resource(s)\n    install.go:154: [debug] CRD networkservices.networkservicemesh.io is already present. Skipping.\n    client.go:133: [debug] creating 1 resource(s)\n    install.go:154: [debug] CRD networkserviceendpoints.networkservicemesh.io is already present. Skipping.\n    client.go:133: [debug] creating 1 resource(s)\n    install.go:154: [debug] CRD spiffeids.spiffeid.spiffe.io is already present. Skipping.\n    client.go:133: [debug] creating 1 resource(s)\n    client.go:477: [debug] Starting delete for \"kubeslice-install-crds\" ServiceAccount\n    client.go:481: [debug] Ignoring delete failure for \"kubeslice-install-crds\" /v1, Kind=ServiceAccount: serviceaccounts \"kubeslice-install-crds\" not found\n    client.go:133: [debug] creating 1 resource(s)\n    client.go:477: [debug] Starting delete for \"kubeslice-install-crds\" ConfigMap\n    client.go:481: [debug] Ignoring delete failure for \"kubeslice-install-crds\" /v1, Kind=ConfigMap: configmaps \"kubeslice-install-crds\" not found\n    client.go:133: [debug] creating 1 resource(s)\n    client.go:477: [debug] Starting delete for \"kubeslice-install-crds\" ClusterRole\n    client.go:481: [debug] Ignoring delete failure for \"kubeslice-install-crds\" rbac.authorization.k8s.io/v1, Kind=ClusterRole: clusterroles.rbac.authorization.k8s.io \"kubeslice-install-crds\" not found\n    client.go:133: [debug] creating 1 resource(s)\n    client.go:477: [debug] Starting delete for \"kubeslice-install-crds\" ClusterRoleBinding\n    client.go:481: [debug] Ignoring delete failure for \"kubeslice-install-crds\" rbac.authorization.k8s.io/v1, Kind=ClusterRoleBinding: clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-install-crds\" not found\n    client.go:133: [debug] creating 1 resource(s)\n    client.go:477: [debug] Starting delete for \"nsm-crd-install\" ConfigMap\n    client.go:481: [debug] Ignoring delete failure for \"nsm-crd-install\" /v1, Kind=ConfigMap: configmaps \"nsm-crd-install\" not found\n    client.go:133: [debug] creating 1 resource(s)\n    client.go:477: [debug] Starting delete for \"nsm-install-crds\" ServiceAccount\n    client.go:481: [debug] Ignoring delete failure for \"nsm-install-crds\" /v1, Kind=ServiceAccount: serviceaccounts \"nsm-install-crds\" not found\n    client.go:133: [debug] creating 1 resource(s)\n    client.go:477: [debug] Starting delete for \"nsm-install-crds\" ClusterRole\n    client.go:481: [debug] Ignoring delete failure for \"nsm-install-crds\" rbac.authorization.k8s.io/v1, Kind=ClusterRole: clusterroles.rbac.authorization.k8s.io \"nsm-install-crds\" not found\n    client.go:133: [debug] creating 1 resource(s)\n    client.go:477: [debug] Starting delete for \"nsm-install-crds\" ClusterRoleBinding\n    client.go:481: [debug] Ignoring delete failure for \"nsm-install-crds\" rbac.authorization.k8s.io/v1, Kind=ClusterRoleBinding: clusterrolebindings.rbac.authorization.k8s.io \"nsm-install-crds\" not found\n    client.go:133: [debug] creating 1 resource(s)\n    client.go:477: [debug] Starting delete for \"kubeslice-install-crds\" Job\n    client.go:481: [debug] Ignoring delete failure for \"kubeslice-install-crds\" batch/v1, Kind=Job: jobs.batch \"kubeslice-install-crds\" not found\n    client.go:133: [debug] creating 1 resource(s)\n    client.go:703: [debug] Watching for changes to Job kubeslice-install-crds with timeout of 5m0s\n    client.go:731: [debug] Add/Modify event for kubeslice-install-crds: ADDED\n    client.go:770: [debug] kubeslice-install-crds: Jobs active: 0, jobs failed: 0, jobs succeeded: 0\n    client.go:731: [debug] Add/Modify event for kubeslice-install-crds: MODIFIED\n    client.go:770: [debug] kubeslice-install-crds: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:731: [debug] Add/Modify event for kubeslice-install-crds: MODIFIED\n    client.go:477: [debug] Starting delete for \"nsm-install-crds\" Job\n    client.go:481: [debug] Ignoring delete failure for \"nsm-install-crds\" batch/v1, Kind=Job: jobs.batch \"nsm-install-crds\" not found\n    client.go:133: [debug] creating 1 resource(s)\n    client.go:703: [debug] Watching for changes to Job nsm-install-crds with timeout of 5m0s\n    client.go:731: [debug] Add/Modify event for nsm-install-crds: ADDED\n    client.go:770: [debug] nsm-install-crds: Jobs active: 0, jobs failed: 0, jobs succeeded: 0\n    client.go:731: [debug] Add/Modify event for nsm-install-crds: MODIFIED\n    client.go:770: [debug] nsm-install-crds: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:731: [debug] Add/Modify event for nsm-install-crds: MODIFIED\n    client.go:477: [debug] Starting delete for \"kubeslice-install-crds\" ServiceAccount\n    client.go:477: [debug] Starting delete for \"kubeslice-install-crds\" ConfigMap\n    client.go:477: [debug] Starting delete for \"kubeslice-install-crds\" ClusterRole\n    client.go:477: [debug] Starting delete for \"kubeslice-install-crds\" ClusterRoleBinding\n    client.go:477: [debug] Starting delete for \"nsm-crd-install\" ConfigMap\n    client.go:477: [debug] Starting delete for \"nsm-install-crds\" ServiceAccount\n    client.go:477: [debug] Starting delete for \"nsm-install-crds\" ClusterRole\n    client.go:477: [debug] Starting delete for \"nsm-install-crds\" ClusterRoleBinding\n    client.go:133: [debug] creating 64 resource(s)\n    wait.go:48: [debug] beginning wait for 64 resources with timeout of 5m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook-k8s. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook-k8s. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook-k8s. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook-k8s. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-dns. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    install.go:444: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:477: [debug] Starting delete for \"nsm-delete-webhooks\" ClusterRole\n    client.go:481: [debug] Ignoring delete failure for \"nsm-delete-webhooks\" rbac.authorization.k8s.io/v1, Kind=ClusterRole: clusterroles.rbac.authorization.k8s.io \"nsm-delete-webhooks\" not found\n    client.go:133: [debug] creating 1 resource(s)\n    client.go:477: [debug] Starting delete for \"nsm-delete-webhooks\" ClusterRoleBinding\n    client.go:481: [debug] Ignoring delete failure for \"nsm-delete-webhooks\" rbac.authorization.k8s.io/v1, Kind=ClusterRoleBinding: clusterrolebindings.rbac.authorization.k8s.io \"nsm-delete-webhooks\" not found\n    client.go:133: [debug] creating 1 resource(s)\n    client.go:477: [debug] Starting delete for \"kubeslice-cleanup\" ServiceAccount\n    client.go:481: [debug] Ignoring delete failure for \"kubeslice-cleanup\" /v1, Kind=ServiceAccount: serviceaccounts \"kubeslice-cleanup\" not found\n    client.go:133: [debug] creating 1 resource(s)\n    client.go:477: [debug] Starting delete for \"kubeslice-cleanup\" ClusterRole\n    client.go:481: [debug] Ignoring delete failure for \"kubeslice-cleanup\" rbac.authorization.k8s.io/v1, Kind=ClusterRole: clusterroles.rbac.authorization.k8s.io \"kubeslice-cleanup\" not found\n    client.go:133: [debug] creating 1 resource(s)\n    client.go:477: [debug] Starting delete for \"kubeslice-cleanup\" ClusterRoleBinding\n    client.go:481: [debug] Ignoring delete failure for \"kubeslice-cleanup\" rbac.authorization.k8s.io/v1, Kind=ClusterRoleBinding: clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-cleanup\" not found\n    client.go:133: [debug] creating 1 resource(s)\n    client.go:477: [debug] Starting delete for \"nsm-delete-webhooks\" ServiceAccount\n    client.go:481: [debug] Ignoring delete failure for \"nsm-delete-webhooks\" /v1, Kind=ServiceAccount: serviceaccounts \"nsm-delete-webhooks\" not found\n    client.go:133: [debug] creating 1 resource(s)\n    client.go:477: [debug] Starting delete for \"nsm-delete-webhooks\" ConfigMap\n    client.go:481: [debug] Ignoring delete failure for \"nsm-delete-webhooks\" /v1, Kind=ConfigMap: configmaps \"nsm-delete-webhooks\" not found\n    client.go:133: [debug] creating 1 resource(s)\n    client.go:477: [debug] Starting delete for \"kubeslice-cleanup\" Job\n    client.go:481: [debug] Ignoring delete failure for \"kubeslice-cleanup\" batch/v1, Kind=Job: jobs.batch \"kubeslice-cleanup\" not found\n    client.go:133: [debug] creating 1 resource(s)\n    client.go:703: [debug] Watching for changes to Job kubeslice-cleanup with timeout of 5m0s\n    client.go:731: [debug] Add/Modify event for kubeslice-cleanup: ADDED\n    client.go:770: [debug] kubeslice-cleanup: Jobs active: 0, jobs failed: 0, jobs succeeded: 0\n    client.go:731: [debug] Add/Modify event for kubeslice-cleanup: MODIFIED\n    client.go:770: [debug] kubeslice-cleanup: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:731: [debug] Add/Modify event for kubeslice-cleanup: MODIFIED\n    client.go:477: [debug] Starting delete for \"nsm-delete-webhooks\" Job\n    client.go:481: [debug] Ignoring delete failure for \"nsm-delete-webhooks\" batch/v1, Kind=Job: jobs.batch \"nsm-delete-webhooks\" not found\n    client.go:133: [debug] creating 1 resource(s)\n    client.go:703: [debug] Watching for changes to Job nsm-delete-webhooks with timeout of 5m0s\n    client.go:731: [debug] Add/Modify event for nsm-delete-webhooks: ADDED\n    client.go:770: [debug] nsm-delete-webhooks: Jobs active: 0, jobs failed: 0, jobs succeeded: 0\n    client.go:731: [debug] Add/Modify event for nsm-delete-webhooks: MODIFIED\n    client.go:770: [debug] nsm-delete-webhooks: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:731: [debug] Add/Modify event for nsm-delete-webhooks: MODIFIED\n    client.go:477: [debug] Starting delete for \"nsm-delete-webhooks\" ClusterRole\n    client.go:477: [debug] Starting delete for \"nsm-delete-webhooks\" ClusterRoleBinding\n    client.go:477: [debug] Starting delete for \"kubeslice-cleanup\" ServiceAccount\n    client.go:477: [debug] Starting delete for \"kubeslice-cleanup\" ClusterRole\n    client.go:477: [debug] Starting delete for \"kubeslice-cleanup\" ClusterRoleBinding\n    client.go:477: [debug] Starting delete for \"nsm-delete-webhooks\" ServiceAccount\n    client.go:477: [debug] Starting delete for \"nsm-delete-webhooks\" ConfigMap\n    client.go:477: [debug] Starting delete for \"kubeslice-cleanup\" Job\n    client.go:477: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:477: [debug] Starting delete for \"admission-webhook-svc\" Service\n    client.go:477: [debug] Starting delete for \"k8s-workload-registrar\" Service\n    client.go:477: [debug] Starting delete for \"registry\" Service\n    client.go:477: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:477: [debug] Starting delete for \"spire-server\" Service\n    client.go:477: [debug] Starting delete for \"spire-server\" StatefulSet\n    client.go:477: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:477: [debug] Starting delete for \"nsm-admission-webhook-k8s\" Deployment\n    client.go:477: [debug] Starting delete for \"registry-k8s\" Deployment\n    client.go:477: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:477: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:477: [debug] Starting delete for \"forwarder-kernel\" DaemonSet\n    client.go:477: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:477: [debug] Starting delete for \"spire-agent\" DaemonSet\n    client.go:477: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:477: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:477: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:477: [debug] Starting delete for \"admission-webhook-binding\" ClusterRoleBinding\n    client.go:477: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:477: [debug] Starting delete for \"spire-agent-cluster-role-binding\" ClusterRoleBinding\n    client.go:477: [debug] Starting delete for \"k8s-workload-registrar-role-binding\" ClusterRoleBinding\n    client.go:477: [debug] Starting delete for \"spire-server-trust-role-binding\" ClusterRoleBinding\n    client.go:477: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRoleBinding\n    client.go:477: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:477: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:477: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:477: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:477: [debug] Starting delete for \"admission-webhook-role\" ClusterRole\n    client.go:477: [debug] Starting delete for \"spire-server-trust-role\" ClusterRole\n    client.go:477: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:477: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:477: [debug] Starting delete for \"spire-agent-cluster-role\" ClusterRole\n    client.go:477: [debug] Starting delete for \"k8s-workload-registrar-role\" ClusterRole\n    client.go:477: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRole\n    client.go:477: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:477: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:477: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:477: [debug] Starting delete for \"k8s-workload-registrar\" ConfigMap\n    client.go:477: [debug] Starting delete for \"spire-agent\" ConfigMap\n    client.go:477: [debug] Starting delete for \"spire-bundle\" ConfigMap\n    client.go:477: [debug] Starting delete for \"spire-server\" ConfigMap\n    client.go:477: [debug] Starting delete for \"kubeslice-kubernetes-dashboard-creds\" Secret\n    client.go:477: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:477: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:477: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:477: [debug] Starting delete for \"admission-webhook-sa\" ServiceAccount\n    client.go:477: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:477: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:477: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:477: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:477: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:477: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:477: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:477: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:477: [debug] Starting delete for \"spire-server\" ServiceAccount\n    client.go:477: [debug] Starting delete for \"spire-agent\" ServiceAccount\n    client.go:477: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:477: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ServiceAccount\n    client.go:477: [debug] Starting delete for \"spire\" Namespace\n    client.go:477: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:477: [debug] Starting delete for \"nsm-webhook-high-priority\" PriorityClass\n    client.go:477: [debug] Starting delete for \"k8s-workload-registrar\" ValidatingWebhookConfiguration\n    client.go:477: [debug] Starting delete for \"kubeslice-postdelete-job\" ClusterRole\n    client.go:481: [debug] Ignoring delete failure for \"kubeslice-postdelete-job\" rbac.authorization.k8s.io/v1, Kind=ClusterRole: clusterroles.rbac.authorization.k8s.io \"kubeslice-postdelete-job\" not found\n    client.go:133: [debug] creating 1 resource(s)\n    client.go:477: [debug] Starting delete for \"kubeslice-postdelete-job\" ClusterRoleBinding\n    client.go:481: [debug] Ignoring delete failure for \"kubeslice-postdelete-job\" rbac.authorization.k8s.io/v1, Kind=ClusterRoleBinding: clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-postdelete-job\" not found\n    client.go:133: [debug] creating 1 resource(s)\n    client.go:477: [debug] Starting delete for \"kubeslice-postdelete-job\" ServiceAccount\n    client.go:481: [debug] Ignoring delete failure for \"kubeslice-postdelete-job\" /v1, Kind=ServiceAccount: serviceaccounts \"kubeslice-postdelete-job\" not found\n    client.go:133: [debug] creating 1 resource(s)\n    client.go:477: [debug] Starting delete for \"kubeslice-postdelete-job\" ConfigMap\n    client.go:481: [debug] Ignoring delete failure for \"kubeslice-postdelete-job\" /v1, Kind=ConfigMap: configmaps \"kubeslice-postdelete-job\" not found\n    client.go:133: [debug] creating 1 resource(s)\n    client.go:477: [debug] Starting delete for \"kubeslice-postdelete-job\" Job\n    client.go:481: [debug] Ignoring delete failure for \"kubeslice-postdelete-job\" batch/v1, Kind=Job: jobs.batch \"kubeslice-postdelete-job\" not found\n    client.go:133: [debug] creating 1 resource(s)\n    client.go:703: [debug] Watching for changes to Job kubeslice-postdelete-job with timeout of 5m0s\n    client.go:731: [debug] Add/Modify event for kubeslice-postdelete-job: ADDED\n    client.go:770: [debug] kubeslice-postdelete-job: Jobs active: 0, jobs failed: 0, jobs succeeded: 0\n    client.go:731: [debug] Add/Modify event for kubeslice-postdelete-job: MODIFIED\n    client.go:770: [debug] kubeslice-postdelete-job: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:731: [debug] Add/Modify event for kubeslice-postdelete-job: MODIFIED\n    client.go:477: [debug] Starting delete for \"kubeslice-postdelete-job\" ClusterRole\n    client.go:477: [debug] Starting delete for \"kubeslice-postdelete-job\" ClusterRoleBinding\n    client.go:477: [debug] Starting delete for \"kubeslice-postdelete-job\" ServiceAccount\n    client.go:477: [debug] Starting delete for \"kubeslice-postdelete-job\" ConfigMap\n    uninstall.go:148: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:452\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:436\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:392\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:141\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:916\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:1044\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:968\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\noccurred","children":[{"name":"[BeforeSuite]","uid":"f2f5eb62efd7b712","parentUid":"45c257f19318b3c16d121ba27ba9b521","status":"failed","time":{"start":1680188118000,"stop":1680188464065,"duration":346065},"flaky":false,"newFailed":false,"newPassed":false,"newBroken":false,"retriesCount":0,"retriesStatusChange":false,"parameters":[]}],"uid":"45c257f19318b3c16d121ba27ba9b521"}],"uid":"8fb3a91ba5aaf9de24cc8a92edc82b5d"}],"name":"categories"}