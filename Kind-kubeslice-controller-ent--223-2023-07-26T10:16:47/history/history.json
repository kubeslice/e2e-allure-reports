{"Hub Suite:Hub Suite#[It] Project and Cluster Events Test Events get recorded for cluster creation and deletion Events get recorded for cluster creation Validating events for cluster role binding creation":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"7e6fbb774b4090f5","status":"passed","time":{"start":1688022039000,"stop":1688022043461,"duration":4461}}]},"Hub Suite:Hub Suite#[It] Project and Cluster Events Test Events get recorded for project creation and deletion Events get recorded for project creation Validating events for role binding creation":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"e4e20991e629a9eb","status":"passed","time":{"start":1688022039000,"stop":1688022045250,"duration":6250}}]},"Intracluster Suite:Intracluster Suite#[It] Intracluster slice tests Slice with netpol should have vl3 router running":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"5abd7a6e040f698d","status":"passed","time":{"start":1688025155000,"stop":1688025160301,"duration":5301}}]},"Hub Suite:Hub Suite#[It] Project and Cluster Events Test Events get recorded for project creation and deletion Events get recorded for project creation Validating events for namespace creation":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"2e9abc5384f69bc7","status":"passed","time":{"start":1688022039000,"stop":1688022040287,"duration":1287}}]},"SliceHealth Suite:SliceHealth Suite#[It] Verify gw redundancy wrt health check when slice is installed should eventually have slicehealth status normal":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"e2272610a02e3389","status":"passed","time":{"start":1688026084000,"stop":1688026206536,"duration":122536}}]},"Hub Suite:Hub Suite#[It] Cluster negative tests Worker Cluster Registration with Wrong token":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"46630b4f5c462de0","status":"passed","time":{"start":1688022039000,"stop":1688022040359,"duration":1359}}]},"Intracluster Suite:Intracluster Suite#[It] Intracluster slice tests Iperf connectivity should have vl3 router running":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"b63200323e7576b3","status":"passed","time":{"start":1688025155000,"stop":1688025155006,"duration":6}}]},"Hub Suite:Hub Suite#[It] Project and Cluster Events Test Events get recorded for project creation and deletion Events get recorded for project creation Validating events for cluster role creation":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"f776d2c86c0d0f74","status":"passed","time":{"start":1688022039000,"stop":1688022039075,"duration":75}}]},"SliceHealth Suite:SliceHealth Suite#[It] Slice Health Check tests Test Slice Health Check Should not have slice ingress status when ingress is unavailable":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":0,"unknown":0,"total":1},"items":[{"uid":"56d2f5707dae7974","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1688026084000,"stop":1688026084000,"duration":0}}]},"Hub Suite:Hub Suite#[It] Project and Cluster Events Test Events get recorded for cluster creation and deletion Events get recorded for cluster creation Validating events for cluster service account secret creation":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"ec16e01047a2bf56","status":"passed","time":{"start":1688022039000,"stop":1688022039077,"duration":77}}]},"Hub Suite:Hub Suite#[It] Cluster CR tests Cluster CR validation Verify NodeIPs are updated in cluster CR after worker installation":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"2fbf708ee1369e53","status":"passed","time":{"start":1688022039000,"stop":1688022683620,"duration":644620}}]},"Hub Suite:Hub Suite#[BeforeSuite]":{"statistic":{"failed":2,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":3},"items":[{"uid":"e772e0f7ef6c311","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000498528>: {\n        Underlying: <*exec.ExitError | 0xc000116000>{\n            ProcessState: {\n                pid: 6165,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 3, Usec: 965017},\n                    Stime: {Sec: 0, Usec: 692834},\n                    Maxrss: 93440,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 8942,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 2248,\n                    Oublock: 17384,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 43008,\n                    Nivcsw: 13396,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:200: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:217: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-1.2.0.tgz\",\n                        \"\",\n                        \"client.go:134: [debug] creating 1 resource(s)\",\n                        \"client.go:134: [debug] creating 29 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 29 resources with timeout of 5m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                    ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:200: [debug] Original chart version: \"\"\n    install.go:217: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-1.2.0.tgz\n    \n    client.go:134: [debug] creating 1 resource(s)\n    client.go:134: [debug] creating 29 resource(s)\n    wait.go:48: [debug] beginning wait for 29 resources with timeout of 5m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    install.go:470: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:97: [debug] uninstall: Deleting kubeslice-controller\n    client.go:134: [debug] creating 1 resource(s)\n    client.go:706: [debug] Watching for changes to Job kubeslice-controller-cleanup with timeout of 5m0s\n    client.go:734: [debug] Add/Modify event for kubeslice-controller-cleanup: ADDED\n    client.go:773: [debug] kubeslice-controller-cleanup: Jobs active: 0, jobs failed: 0, jobs succeeded: 0\n    client.go:734: [debug] Add/Modify event for kubeslice-controller-cleanup: MODIFIED\n    client.go:773: [debug] kubeslice-controller-cleanup: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:478: [debug] Starting delete for \"kubeslice-controller-cleanup\" Job\n    Error: INSTALLATION FAILED: an error occurred while uninstalling the release. original install error: context deadline exceeded: 1 error occurred:\n    \t* timed out waiting for the condition\n    \n    \n    helm.go:84: [debug] 1 error occurred:\n    \t* timed out waiting for the condition\n    \n    \n    an error occurred while uninstalling the release. original install error: context deadline exceeded\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:476\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:462\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:418\n    runtime.goexit\n    \truntime/asm_amd64.s:1598\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:147\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:916\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:1044\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:968\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1598\noccurred","time":{"start":1689750156000,"stop":1689750760905,"duration":604905}},{"uid":"e6c71bdd9c8771fa","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004a4060>: {\n        Underlying: <*exec.ExitError | 0xc000078020>{\n            ProcessState: {\n                pid: 6147,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 3, Usec: 485829},\n                    Stime: {Sec: 0, Usec: 501499},\n                    Maxrss: 82224,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 5270,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 1088,\n                    Oublock: 11584,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 41386,\n                    Nivcsw: 11774,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:200: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:217: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-1.2.0.tgz\",\n                        \"\",\n                        \"client.go:134: [debug] creating 1 resource(s)\",\n                        \"client.go:134: [debug] creating 29 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 29 resources with timeout of 5m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                    ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:200: [debug] Original chart version: \"\"\n    install.go:217: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-1.2.0.tgz\n    \n    client.go:134: [debug] creating 1 resource(s)\n    client.go:134: [debug] creating 29 resource(s)\n    wait.go:48: [debug] beginning wait for 29 resources with timeout of 5m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    install.go:470: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:97: [debug] uninstall: Deleting kubeslice-controller\n    client.go:134: [debug] creating 1 resource(s)\n    client.go:706: [debug] Watching for changes to Job kubeslice-controller-cleanup with timeout of 5m0s\n    client.go:734: [debug] Add/Modify event for kubeslice-controller-cleanup: ADDED\n    client.go:773: [debug] kubeslice-controller-cleanup: Jobs active: 0, jobs failed: 0, jobs succeeded: 0\n    client.go:734: [debug] Add/Modify event for kubeslice-controller-cleanup: MODIFIED\n    client.go:773: [debug] kubeslice-controller-cleanup: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:478: [debug] Starting delete for \"kubeslice-controller-cleanup\" Job\n    Error: INSTALLATION FAILED: an error occurred while uninstalling the release. original install error: context deadline exceeded: 1 error occurred:\n    \t* timed out waiting for the condition\n    \n    \n    helm.go:84: [debug] 1 error occurred:\n    \t* timed out waiting for the condition\n    \n    \n    an error occurred while uninstalling the release. original install error: context deadline exceeded\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:476\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:462\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:418\n    runtime.goexit\n    \truntime/asm_amd64.s:1598\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:147\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:916\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:1044\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:968\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1598\noccurred","time":{"start":1688626949000,"stop":1688627552804,"duration":603804}},{"uid":"5948718af3daf0e9","status":"passed","time":{"start":1688022039000,"stop":1688022053084,"duration":14084}}]},"Hub Suite:Hub Suite#[It] Project and Cluster Events Test Events get recorded for cluster creation and deletion Events get recorded for cluster deletion Validating events for cluster role binding deletion":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"722939c78fbaeb3","status":"passed","time":{"start":1688022039000,"stop":1688022043255,"duration":4255}}]},"Intracluster Suite:Intracluster Suite#[BeforeSuite]":{"statistic":{"failed":2,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":3},"items":[{"uid":"c229893beaa509e8","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004b1350>: {\n        Underlying: <*exec.ExitError | 0xc00013a2a0>{\n            ProcessState: {\n                pid: 6358,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 97224},\n                    Stime: {Sec: 0, Usec: 28357},\n                    Maxrss: 50484,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2337,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 168,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 541,\n                    Nivcsw: 344,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:200: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:217: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-1.2.0.tgz\",\n                        \"\",\n                        \"Error: INSTALLATION FAILED: cannot re-use a name that is still in use\",\n                        \"helm.go:84: [debug] cannot re-use a name that is still in use\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).availableName\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:512\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:222\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:286\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:145\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.6.1/command.go:916\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.6.1/command.go:1044\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.6.1/command.go:968\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:250\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1598\",\n                        \"INSTALLATION FAILED\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:147\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.6.1/command.go:916\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.6.1/command.go:1044\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.6.1/command.go:968\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:250\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1598\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is ins...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:200: [debug] Original chart version: \"\"\n    install.go:217: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-1.2.0.tgz\n    \n    Error: INSTALLATION FAILED: cannot re-use a name that is still in use\n    helm.go:84: [debug] cannot re-use a name that is still in use\n    helm.sh/helm/v3/pkg/action.(*Install).availableName\n    \thelm.sh/helm/v3/pkg/action/install.go:512\n    helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\n    \thelm.sh/helm/v3/pkg/action/install.go:222\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:286\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:145\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:916\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:1044\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:968\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1598\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:147\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:916\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:1044\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:968\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1598\noccurred","time":{"start":1689751062000,"stop":1689751062403,"duration":403}},{"uid":"8cd686f2467a5451","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000f098>: {\n        Underlying: <*exec.ExitError | 0xc00049e880>{\n            ProcessState: {\n                pid: 6336,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 103164},\n                    Stime: {Sec: 0, Usec: 19104},\n                    Maxrss: 48060,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2379,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 168,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 609,\n                    Nivcsw: 335,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:200: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:217: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-1.2.0.tgz\",\n                        \"\",\n                        \"Error: INSTALLATION FAILED: cannot re-use a name that is still in use\",\n                        \"helm.go:84: [debug] cannot re-use a name that is still in use\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).availableName\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:512\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:222\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:286\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:145\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.6.1/command.go:916\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.6.1/command.go:1044\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.6.1/command.go:968\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:250\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1598\",\n                        \"INSTALLATION FAILED\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:147\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.6.1/command.go:916\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.6.1/command.go:1044\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.6.1/command.go:968\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:250\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1598\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is in...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:200: [debug] Original chart version: \"\"\n    install.go:217: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-1.2.0.tgz\n    \n    Error: INSTALLATION FAILED: cannot re-use a name that is still in use\n    helm.go:84: [debug] cannot re-use a name that is still in use\n    helm.sh/helm/v3/pkg/action.(*Install).availableName\n    \thelm.sh/helm/v3/pkg/action/install.go:512\n    helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\n    \thelm.sh/helm/v3/pkg/action/install.go:222\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:286\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:145\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:916\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:1044\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:968\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1598\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:147\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:916\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:1044\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:968\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1598\noccurred","time":{"start":1688627854000,"stop":1688627854455,"duration":455}},{"uid":"7e4c5bfa62c44c00","status":"passed","time":{"start":1688025155000,"stop":1688025328626,"duration":173626}}]},"Hub Suite:Hub Suite#[It] Cluster health check tests Cluster health check - without istio Has health status normal":{"statistic":{"failed":1,"broken":0,"skipped":0,"passed":0,"unknown":0,"total":1},"items":[{"uid":"b0c3eea5e4f82850","status":"failed","statusDetails":"Timed out after 120.000s.\nExpected\n    <bool>: false\nto be true","time":{"start":1688022039000,"stop":1688022812843,"duration":773843}}]},"SliceHealth Suite:SliceHealth Suite#[It] Slice Health Check tests Test Slice Health Check Should have slice egress unavailable when egress is unavailable":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":0,"unknown":0,"total":1},"items":[{"uid":"d7f680e6614d9a1d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1688026084000,"stop":1688026084000,"duration":0}}]},"Hub Suite:Hub Suite#[It] Project and Cluster Events Test Events get recorded for project creation and deletion Events get recorded for project deletion Validating events for namespace deletion":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"f5edc60a22f99f6d","status":"passed","time":{"start":1688022039000,"stop":1688022039066,"duration":66}}]},"Intracluster Suite:Intracluster Suite#[It] Intracluster slice tests Iperf connectivity Verify Iperf traffic":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"7f4c01e3b8611ef1","status":"passed","time":{"start":1688025155000,"stop":1688025223538,"duration":68538}}]},"Hub Suite:Hub Suite#[It] Cluster negative tests Worker Cluster Registration with Wrong ca.cert":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"d2046d516febc28d","status":"passed","time":{"start":1688022039000,"stop":1688022040200,"duration":1200}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Creation Should fail while creating service accounts as combination of special characters":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"15ac33d822fe389e","status":"passed","time":{"start":1688022039000,"stop":1688022039271,"duration":271}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Creation Should fail while creating Project with project name as combination of special characters":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"96b3318a8936baad","status":"passed","time":{"start":1688022039000,"stop":1688022039250,"duration":250}}]},"Intracluster Suite:Intracluster Suite#[It] Intracluster slice tests Slice with netpol should create & label app ns with kubeslice label":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"407908df22066657","status":"passed","time":{"start":1688025155000,"stop":1688025155111,"duration":111}}]},"Hub Suite:Hub Suite#[It] Project and Cluster Events Test Events get recorded for project creation and deletion Events get recorded for project creation Validating events for role creation":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"edeac29fbdf55e7f","status":"passed","time":{"start":1688022039000,"stop":1688022039128,"duration":128}}]},"SliceHealth Suite:SliceHealth Suite#[It] Slice Health Check tests Test Slice Health Check Should have slice slicerouter status Error when slicerouter is unavailable":{"statistic":{"failed":1,"broken":0,"skipped":0,"passed":0,"unknown":0,"total":1},"items":[{"uid":"6e010f9729d521a","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00080c018>: {\n        Underlying: <*exec.ExitError | 0xc0003d4000>{\n            ProcessState: {\n                pid: 8860,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 1, Usec: 328430},\n                    Stime: {Sec: 0, Usec: 227981},\n                    Maxrss: 92140,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 22839,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 16368,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 11478,\n                    Nivcsw: 4670,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:200: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:217: [debug] CHART PATH: /root/.cache/helm/repository/istio-discovery-1.16.0.tgz\",\n                        \"\",\n                        \"client.go:134: [debug] creating 1 resource(s)\",\n                        \"client.go:134: [debug] creating 24 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 24 resources with timeout of 5m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:200: [debug] Original chart version: \"\"\n    install.go:217: [debug] CHART PATH: /root/.cache/helm/repository/istio-discovery-1.16.0.tgz\n    \n    client.go:134: [debug] creating 1 resource(s)\n    client.go:134: [debug] creating 24 resource(s)\n    wait.go:48: [debug] beginning wait for 24 resources with timeout of 5m0s\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: istio-system/istiod. 0 out of 1 expected pods are ready\n    install.go:470: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:97: [debug] uninstall: Deleting istiod\n    uninstall.go:243: [debug] uninstall: given cascade value: , defaulting to delete propagation background\n    client.go:478: [debug] Starting delete for \"istiod\" Service\n    client.go:478: [debug] Starting delete for \"istiod\" HorizontalPodAutoscaler\n    client.go:478: [debug] Starting delete for \"istiod\" Deployment\n    client.go:478: [debug] Starting delete for \"istiod\" RoleBinding\n    client.go:478: [debug] Starting delete for \"istiod\" Role\n    client.go:478: [debug] Starting delete for \"istio-reader-clusterrole-istio-system\" ClusterRoleBinding\n    client.go:478: [debug] Starting delete for \"istiod-clusterrole-istio-system\" ClusterRoleBinding\n    client.go:478: [debug] Starting delete for \"istiod-gateway-controller-istio-system\" ClusterRoleBinding\n    client.go:478: [debug] Starting delete for \"istio-reader-clusterrole-istio-system\" ClusterRole\n    client.go:478: [debug] Starting delete for \"istiod-clusterrole-istio-system\" ClusterRole\n    client.go:478: [debug] Starting delete for \"istiod-gateway-controller-istio-system\" ClusterRole\n    client.go:478: [debug] Starting delete for \"istio-sidecar-injector\" ConfigMap\n    client.go:478: [debug] Starting delete for \"istio\" ConfigMap\n    client.go:478: [debug] Starting delete for \"istiod\" ServiceAccount\n    client.go:478: [debug] Starting delete for \"istiod\" PodDisruptionBudget\n    client.go:478: [debug] Starting delete for \"tcp-stats-filter-1.16\" EnvoyFilter\n    client.go:478: [debug] Starting delete for \"stats-filter-1.13\" EnvoyFilter\n    client.go:478: [debug] Starting delete for \"tcp-stats-filter-1.13\" EnvoyFilter\n    client.go:478: [debug] Starting delete for \"stats-filter-1.14\" EnvoyFilter\n    client.go:478: [debug] Starting delete for \"tcp-stats-filter-1.14\" EnvoyFilter\n    client.go:478: [debug] Starting delete for \"stats-filter-1.15\" EnvoyFilter\n    client.go:478: [debug] Starting delete for \"tcp-stats-filter-1.15\" EnvoyFilter\n    client.go:478: [debug] Starting delete for \"stats-filter-1.16\" EnvoyFilter\n    client.go:478: [debug] Starting delete for \"istio-sidecar-injector\" MutatingWebhookConfiguration\n    uninstall.go:150: [debug] purge requested for istiod\n    Error: INSTALLATION FAILED: release istiod failed, and has been uninstalled due to atomic being set: context deadline exceeded\n    helm.go:84: [debug] context deadline exceeded\n    release istiod failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:478\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:462\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:418\n    runtime.goexit\n    \truntime/asm_amd64.s:1598\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:147\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:916\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:1044\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:968\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1598\noccurred","time":{"start":1688026084000,"stop":1688026400126,"duration":316126}}]},"SliceHealth Suite:SliceHealth Suite#[It] Slice Health Check tests Test Slice Health Check Should have slice health status Normal in controller slice CR":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":0,"unknown":0,"total":1},"items":[{"uid":"75fbb8615c4d5c73","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1688026084000,"stop":1688026084000,"duration":0}}]},"Hub Suite:Hub Suite#[It] Project and Cluster Events Test Events get recorded for project creation and deletion Events get recorded for project creation Validating events for service account secret creation":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"1bf1fdfe76c65847","status":"passed","time":{"start":1688022039000,"stop":1688022039085,"duration":85}}]},"Hub Suite:Hub Suite#[It] Cluster CR tests Cluster CR validation Register worker cluster (with minimal values in cluster CR)":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"2d26f60d8b34d04a","status":"passed","time":{"start":1688022039000,"stop":1688022682877,"duration":643877}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Update Should fail when Project is applied with service account name as blank in Write users":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"7e12b059c021b5b0","status":"passed","time":{"start":1688022039000,"stop":1688022043894,"duration":4894}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Deletion Should fail when deleting a project that does not exist":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"c9838ac3f5a19546","status":"passed","time":{"start":1688022039000,"stop":1688022046764,"duration":7764}}]},"SliceHealth Suite:SliceHealth Suite#[It] Slice Health Check tests Test Slice Health Check Should have component status Normal when services are available":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":0,"unknown":0,"total":1},"items":[{"uid":"d497d0c2ec0af0f9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1688026084000,"stop":1688026084000,"duration":0}}]},"Intracluster Suite:Intracluster Suite#[It] Intracluster slice tests Iperf connectivity should create all application namespaces":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"c92d3a7459ec60a9","status":"passed","time":{"start":1688025155000,"stop":1688025157434,"duration":2434}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Creation Should create Multiple Projects in controller using valid manifest":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"d5ab998bd8be6bac","status":"passed","time":{"start":1688022039000,"stop":1688022046883,"duration":7883}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Update Should fail when Project is Applied with service account name as combination of special characters in Write users":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"1861f732ec019a8c","status":"passed","time":{"start":1688022039000,"stop":1688022044879,"duration":5879}}]},"Intracluster Suite:Intracluster Suite#[AfterSuite]":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":3,"unknown":0,"total":3},"items":[{"uid":"2cb9a9f4c124fa69","status":"passed","time":{"start":1689751062000,"stop":1689751362625,"duration":300625}},{"uid":"3e6772abec027ef8","status":"passed","time":{"start":1688627854000,"stop":1688628154532,"duration":300532}},{"uid":"98e58dbfa523cd34","status":"passed","time":{"start":1688025155000,"stop":1688025787097,"duration":632097}}]},"Intracluster Suite:Intracluster Suite#[It] Intracluster slice tests Slice with netpol should reconcile netpol in app NS":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"67fdae79f6020848","status":"passed","time":{"start":1688025155000,"stop":1688025155138,"duration":138}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Deletion Should Delete an existing project successfully":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"c185478d50b7a6cd","status":"passed","time":{"start":1688022039000,"stop":1688022043737,"duration":4737}}]},"Hub Suite:Hub Suite#[It] Project and Cluster Events Test Events get recorded for cluster creation and deletion Events get recorded for cluster deletion Validating events for cluster service account deletion":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"55a0a54c49bfec58","status":"passed","time":{"start":1688022039000,"stop":1688022039065,"duration":65}}]},"Hub Suite:Hub Suite#[It] Cluster negative tests Worker Cluster Registration with Wrong clustername":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"42655503d36c6d8e","status":"passed","time":{"start":1688022039000,"stop":1688022046816,"duration":7816}}]},"Worker Suite:Worker Suite#[AfterSuite]":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"504dbb660704473e","status":"passed","time":{"start":1688018439000,"stop":1688018439339,"duration":339}}]},"SliceHealth Suite:SliceHealth Suite#[It] Verify gw redundancy wrt health check when both gateways are down should have tunnel status in error state":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":0,"unknown":0,"total":1},"items":[{"uid":"b0b095a62705926c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1688026084000,"stop":1688026084000,"duration":0}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Update Should fail when Project is Applied with service account name as blank in Read users":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"9fcdc3beeff34617","status":"passed","time":{"start":1688022039000,"stop":1688022043868,"duration":4868}}]},"SliceHealth Suite:SliceHealth Suite#[BeforeSuite]":{"statistic":{"failed":3,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":4},"items":[{"uid":"1f38bf34b0805f26","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000014e88>: {\n        Underlying: <*exec.ExitError | 0xc000476540>{\n            ProcessState: {\n                pid: 6405,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 89072},\n                    Stime: {Sec: 0, Usec: 32390},\n                    Maxrss: 48120,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2149,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 168,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 542,\n                    Nivcsw: 277,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:200: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:217: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-1.2.0.tgz\",\n                        \"\",\n                        \"Error: INSTALLATION FAILED: cannot re-use a name that is still in use\",\n                        \"helm.go:84: [debug] cannot re-use a name that is still in use\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).availableName\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:512\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:222\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:286\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:145\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.6.1/command.go:916\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.6.1/command.go:1044\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.6.1/command.go:968\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:250\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1598\",\n                        \"INSTALLATION FAILED\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:147\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.6.1/command.go:916\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.6.1/command.go:1044\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.6.1/command.go:968\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:250\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1598\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is ins...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:200: [debug] Original chart version: \"\"\n    install.go:217: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-1.2.0.tgz\n    \n    Error: INSTALLATION FAILED: cannot re-use a name that is still in use\n    helm.go:84: [debug] cannot re-use a name that is still in use\n    helm.sh/helm/v3/pkg/action.(*Install).availableName\n    \thelm.sh/helm/v3/pkg/action/install.go:512\n    helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\n    \thelm.sh/helm/v3/pkg/action/install.go:222\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:286\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:145\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:916\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:1044\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:968\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1598\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:147\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:916\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:1044\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:968\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1598\noccurred","time":{"start":1689751363000,"stop":1689751363398,"duration":398}},{"uid":"627308c2e51d2e47","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000128d50>: {\n        Underlying: <*exec.ExitError | 0xc0003fbf60>{\n            ProcessState: {\n                pid: 6379,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 103836},\n                    Stime: {Sec: 0, Usec: 23962},\n                    Maxrss: 48128,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2302,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 168,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 517,\n                    Nivcsw: 278,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:200: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:217: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-1.2.0.tgz\",\n                        \"\",\n                        \"Error: INSTALLATION FAILED: cannot re-use a name that is still in use\",\n                        \"helm.go:84: [debug] cannot re-use a name that is still in use\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).availableName\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:512\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:222\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:286\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:145\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.6.1/command.go:916\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.6.1/command.go:1044\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.6.1/command.go:968\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:250\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1598\",\n                        \"INSTALLATION FAILED\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:147\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.6.1/command.go:916\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.6.1/command.go:1044\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.6.1/command.go:968\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:250\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1598\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is in...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:200: [debug] Original chart version: \"\"\n    install.go:217: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-1.2.0.tgz\n    \n    Error: INSTALLATION FAILED: cannot re-use a name that is still in use\n    helm.go:84: [debug] cannot re-use a name that is still in use\n    helm.sh/helm/v3/pkg/action.(*Install).availableName\n    \thelm.sh/helm/v3/pkg/action/install.go:512\n    helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\n    \thelm.sh/helm/v3/pkg/action/install.go:222\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:286\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:145\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:916\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:1044\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:968\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1598\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:147\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:916\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:1044\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.6.1/command.go:968\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1598\noccurred","time":{"start":1688628155000,"stop":1688628155543,"duration":543}},{"uid":"ca2ade5904afb48e","status":"passed","time":{"start":1688026084000,"stop":1688026304701,"duration":220701}},{"uid":"61d09ccb032bdb72","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004a4558>: {\n        Underlying: <*exec.ExitError | 0xc0004b01a0>{\n            ProcessState: {\n                pid: 6319,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 69266},\n                    Stime: {Sec: 0, Usec: 4329},\n                    Maxrss: 43880,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2030,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 351,\n                    Nivcsw: 293,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: error executing jsonpath \\\"\\\\\\\"{.clusters[?(@.name==\\\\\\\"kind-controller\\\\\\\")].cluster.server}\\\\\\\"\\\": Error executing template: <nil> is not array or slice and cannot be filtered. Printing more information for debugging the template:\",\n                        \"\\ttemplate was:\",\n                        \"\\t\\t\\\"{.clusters[?(@.name==\\\"kind-controller\\\")].cluster.server}\\\"\",\n                        \"\\tobject given to jsonpath engine was:\",\n                        \"\\t\\tmap[string]interface {}{\\\"apiVersion\\\":\\\"v1\\\", \\\"clusters\\\":interface {}(nil), \\\"contexts\\\":interface {}(nil), \\\"current-context\\\":\\\"\\\", \\\"kind\\\":\\\"Config\\\", \\\"preferences\\\":map[string]interface {}{}, \\\"users\\\":interface {}(nil)}\",\n                        \"\",\n                        \"\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: error executing jsonpath \\\"\\\\\\\"{.clusters[?(@.name==\\\\\\\"kind-controller\\\\\\\")].cluster.server}\\\\\\\"\\\": Error executing template: <nil> is not array or slice and cannot be filtered. Printing more information for debugging the template:\",\n                    \"\\ttemplate was:\",\n                    \"\\t\\t\\\"{.clusters[?(@.name==\\\"kind-controller\\\")].cluster.server}\\\"\",\n                    \"\\tobject given to jsonpath engine was:\",\n                    \"\\t\\tmap[string]interface {}{\\\"apiVersion\\\":\\\"v1\\\", \\\"clusters\\\":interface {}(nil), \\\"contexts\\\":interface {}(nil), \\\"current-context\\\":\\\"\\\", \\\"kind\\\":\\\"Config\\\", \\\"preferences\\\":map[string]interface {}{}, \\\"users\\\":interface {}(nil)}\",\n                    \"\",\n                    \"\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: error executing jsonpath \\\"\\\\\\\"{.clusters[?(@.name==\\\\\\\"kind-controller\\\\\\\")].cluster.server}\\\\\\\"\\\": Error executing template: <nil> is not array or slice and cannot be filtered. Printing more information for debugging the template:\",\n                        \"\\ttemplate was:\",\n                        \"\\t\\t\\\"{.clusters[?(@.name==\\\"kind-controller\\\")].cluster.server}\\\"\",\n                        \"\\tobject given to jsonpath engine was:\",\n                        \"\\t\\tmap[string]interface {}{\\\"apiVersion\\\":\\\"v1\\\", \\\"clusters\\\":interface {}(nil), \\\"contexts\\\":interface {}(nil), \\\"current-context\\\":\\\"\\\", \\\"kind\\\":\\\"Config\\\", \\\"preferences\\\":map[string]interface {}{}, \\\"users\\\":interface {}(nil)}\",\n                        \"\",\n                        \"\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: error executing jsonpath \\\"\\\\\\\"{.clusters[?(@.name==\\\\\\\"kind-controller\\\\\\\")].cluster.server}\\\\\\\"\\\": Error executing template: <nil> is not array or slice and cannot be filtered. Printing more information for debugging the template:\",\n                    \"\\ttemplate was:\",\n                    \"\\t\\t\\\"{.clusters[?(@...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; error: error executing jsonpath \"\\\"{.clusters[?(@.name==\\\"kind-controller\\\")].cluster.server}\\\"\": Error executing template: <nil> is not array or slice and cannot be filtered. Printing more information for debugging the template:\n    \ttemplate was:\n    \t\t\"{.clusters[?(@.name==\"kind-controller\")].cluster.server}\"\n    \tobject given to jsonpath engine was:\n    \t\tmap[string]interface {}{\"apiVersion\":\"v1\", \"clusters\":interface {}(nil), \"contexts\":interface {}(nil), \"current-context\":\"\", \"kind\":\"Config\", \"preferences\":map[string]interface {}{}, \"users\":interface {}(nil)}\n    \n    \noccurred","time":{"start":1688018435000,"stop":1688018435086,"duration":86}}]},"SliceHealth Suite:SliceHealth Suite#[It] Verify gw redundancy wrt health check when atleast one gateway is down should have gw health in warning state":{"statistic":{"failed":1,"broken":0,"skipped":0,"passed":0,"unknown":0,"total":1},"items":[{"uid":"2bfefdbdf57455f6","status":"failed","statusDetails":"Timed out after 720.005s.\nExpected\n    <bool>: false\nto be true","time":{"start":1688026084000,"stop":1688026807757,"duration":723757}}]},"Controller Suite:Controller Suite#[AfterSuite]":{"statistic":{"failed":2,"broken":0,"skipped":0,"passed":0,"unknown":0,"total":2},"items":[{"uid":"cdcc37c6ecf1f84f","status":"failed","statusDetails":"Timed out after 500.017s.\nExpected\n    <bool>: false\nto be true","time":{"start":1689758771000,"stop":1689759271018,"duration":500018}},{"uid":"4b58394970be3af6","status":"failed","statusDetails":"Timed out after 500.000s.\nExpected\n    <bool>: false\nto be true","time":{"start":1689755137000,"stop":1689755637001,"duration":500001}}]},"SliceHealth Suite:SliceHealth Suite#[It] Verify gw redundancy wrt health check when both gateways are up should have gw in normal state":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":0,"unknown":0,"total":1},"items":[{"uid":"17abb4d4e3be499e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1688026084000,"stop":1688026084000,"duration":0}}]},"Intracluster Suite:Intracluster Suite#[It] Intracluster slice tests Slice with netpol should remove network policy resource from application namespace once the isolationEnabled is set to false":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"7bdf9383408e6153","status":"passed","time":{"start":1688025155000,"stop":1688025164148,"duration":9148}}]},"SliceHealth Suite:SliceHealth Suite#[It] Verify gw redundancy wrt health check when both gateways are up should have tunnel status in normal state":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":0,"unknown":0,"total":1},"items":[{"uid":"ec999425d007f8f1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1688026084000,"stop":1688026084000,"duration":0}}]},"Intracluster Suite:Intracluster Suite#[It] Intracluster slice tests Iperf connectivity should onboard deployment in application Namespaces without slice annotations":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"f345adf86187f0f9","status":"passed","time":{"start":1688025155000,"stop":1688025185414,"duration":30414}}]},"Intracluster Suite:Intracluster Suite#[It] Intracluster slice tests Slice with netpol should reconcile netpol config":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"961fc174052b2654","status":"passed","time":{"start":1688025155000,"stop":1688025155205,"duration":205}}]},"Hub Suite:Hub Suite#[It] Project and Cluster Events Test Events get recorded for cluster creation and deletion Events get recorded for cluster deletion Validating events for cluster deletion":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"1d961e876d295d6a","status":"passed","time":{"start":1688022039000,"stop":1688022040711,"duration":1711}}]},"Hub Suite:Hub Suite#[It] Cluster negative tests Worker Cluster Registration with Wrong endpoint":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"4b5e4d54fe849d4d","status":"passed","time":{"start":1688022039000,"stop":1688022184601,"duration":145601}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Creation Should update Project while applying valid manifest with existing Project name":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"1133a4bc9de0e0a4","status":"passed","time":{"start":1688022039000,"stop":1688022043764,"duration":4764}}]},"Worker Suite:Worker Suite#[BeforeSuite]":{"statistic":{"failed":1,"broken":0,"skipped":0,"passed":0,"unknown":0,"total":1},"items":[{"uid":"3fcd4274e0a85d49","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0001f1e60>: {\n        Underlying: <*exec.ExitError | 0xc000462b00>{\n            ProcessState: {\n                pid: 6392,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 42165},\n                    Stime: {Sec: 0, Usec: 16866},\n                    Maxrss: 43264,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 1507,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 189,\n                    Nivcsw: 39,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: error executing jsonpath \\\"\\\\\\\"{.clusters[?(@.name==\\\\\\\"kind-controller\\\\\\\")].cluster.server}\\\\\\\"\\\": Error executing template: <nil> is not array or slice and cannot be filtered. Printing more information for debugging the template:\",\n                        \"\\ttemplate was:\",\n                        \"\\t\\t\\\"{.clusters[?(@.name==\\\"kind-controller\\\")].cluster.server}\\\"\",\n                        \"\\tobject given to jsonpath engine was:\",\n                        \"\\t\\tmap[string]interface {}{\\\"apiVersion\\\":\\\"v1\\\", \\\"clusters\\\":interface {}(nil), \\\"contexts\\\":interface {}(nil), \\\"current-context\\\":\\\"\\\", \\\"kind\\\":\\\"Config\\\", \\\"preferences\\\":map[string]interface {}{}, \\\"users\\\":interface {}(nil)}\",\n                        \"\",\n                        \"\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: error executing jsonpath \\\"\\\\\\\"{.clusters[?(@.name==\\\\\\\"kind-controller\\\\\\\")].cluster.server}\\\\\\\"\\\": Error executing template: <nil> is not array or slice and cannot be filtered. Printing more information for debugging the template:\",\n                    \"\\ttemplate was:\",\n                    \"\\t\\t\\\"{.clusters[?(@.name==\\\"kind-controller\\\")].cluster.server}\\\"\",\n                    \"\\tobject given to jsonpath engine was:\",\n                    \"\\t\\tmap[string]interface {}{\\\"apiVersion\\\":\\\"v1\\\", \\\"clusters\\\":interface {}(nil), \\\"contexts\\\":interface {}(nil), \\\"current-context\\\":\\\"\\\", \\\"kind\\\":\\\"Config\\\", \\\"preferences\\\":map[string]interface {}{}, \\\"users\\\":interface {}(nil)}\",\n                    \"\",\n                    \"\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: error executing jsonpath \\\"\\\\\\\"{.clusters[?(@.name==\\\\\\\"kind-controller\\\\\\\")].cluster.server}\\\\\\\"\\\": Error executing template: <nil> is not array or slice and cannot be filtered. Printing more information for debugging the template:\",\n                        \"\\ttemplate was:\",\n                        \"\\t\\t\\\"{.clusters[?(@.name==\\\"kind-controller\\\")].cluster.server}\\\"\",\n                        \"\\tobject given to jsonpath engine was:\",\n                        \"\\t\\tmap[string]interface {}{\\\"apiVersion\\\":\\\"v1\\\", \\\"clusters\\\":interface {}(nil), \\\"contexts\\\":interface {}(nil), \\\"current-context\\\":\\\"\\\", \\\"kind\\\":\\\"Config\\\", \\\"preferences\\\":map[string]interface {}{}, \\\"users\\\":interface {}(nil)}\",\n                        \"\",\n                        \"\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: error executing jsonpath \\\"\\\\\\\"{.clusters[?(@.name==\\\\\\\"kind-controller\\\\\\\")].cluster.server}\\\\\\\"\\\": Error executing template: <nil> is not array or slice and cannot be filtered. Printing more information for debugging the template:\",\n                    \"\\ttemplate was:\",\n                    \"\\t\\t\\\"{.clusters[?(@...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; error: error executing jsonpath \"\\\"{.clusters[?(@.name==\\\"kind-controller\\\")].cluster.server}\\\"\": Error executing template: <nil> is not array or slice and cannot be filtered. Printing more information for debugging the template:\n    \ttemplate was:\n    \t\t\"{.clusters[?(@.name==\"kind-controller\")].cluster.server}\"\n    \tobject given to jsonpath engine was:\n    \t\tmap[string]interface {}{\"apiVersion\":\"v1\", \"clusters\":interface {}(nil), \"contexts\":interface {}(nil), \"current-context\":\"\", \"kind\":\"Config\", \"preferences\":map[string]interface {}{}, \"users\":interface {}(nil)}\n    \n    \noccurred","time":{"start":1688018439000,"stop":1688018439058,"duration":58}}]},"Hub Suite:Hub Suite#[It] Project and Cluster Events Test Events get recorded for cluster creation and deletion Events get recorded for cluster creation Validating events for cluster service account creation":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"d078a813a0ab6580","status":"passed","time":{"start":1688022039000,"stop":1688022040519,"duration":1519}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Update Should update successfully when Project applied with valid service account name in Write users":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"863e7fd9c4f182ef","status":"passed","time":{"start":1688022039000,"stop":1688022044245,"duration":5245}}]},"Hub Suite:Hub Suite#[It] Cluster health check tests Cluster health check - without istio should contain node IP & cni subnet status information":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":0,"unknown":0,"total":1},"items":[{"uid":"a9b7672b02dc4b87","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1688022039000,"stop":1688022039000,"duration":0}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Update Should update successfully when Project Applied while removing a service account name in Read users":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"dcf349d50124e020","status":"passed","time":{"start":1688022039000,"stop":1688022043750,"duration":4750}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Update Should fail when Project is Applied with service account name as combination of special characters in Read users":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"fce4b1cc82ddb816","status":"passed","time":{"start":1688022039000,"stop":1688022043868,"duration":4868}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Creation Should create Project while using valid manifest":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"2d64d088b0950f1f","status":"passed","time":{"start":1688022039000,"stop":1688022047590,"duration":8590}}]},"SliceHealth Suite:SliceHealth Suite#[It] Slice Health Check tests Test Slice Health Check Should have slicegateway in Error state when both slicegateways are unavailable":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":0,"unknown":0,"total":1},"items":[{"uid":"e1872fd168cbec5a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1688026084000,"stop":1688026084000,"duration":0}}]},"Intracluster Suite:Intracluster Suite#[It] Intracluster slice tests Iperf connectivity Verify Connectivity between iperf-server and iperf-client":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"2fe1a9b41bfc6310","status":"passed","time":{"start":1688025155000,"stop":1688025161670,"duration":6670}}]},"SliceHealth Suite:SliceHealth Suite#[AfterSuite]":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":4,"unknown":0,"total":4},"items":[{"uid":"e2c5662393ec82b2","status":"passed","time":{"start":1689751363000,"stop":1689751663695,"duration":300695}},{"uid":"1b5884fae2692ec5","status":"passed","time":{"start":1688628155000,"stop":1688628455676,"duration":300676}},{"uid":"11093e3bbac876a3","status":"passed","time":{"start":1688026084000,"stop":1688027359364,"duration":1275364}},{"uid":"64d1f57a0920b48d","status":"passed","time":{"start":1688018435000,"stop":1688018435448,"duration":448}}]},"SliceHealth Suite:SliceHealth Suite#[It] Verify gw redundancy wrt health check when atleast one gateway is down should have tunnel status in warning state":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":0,"unknown":0,"total":1},"items":[{"uid":"7f6ac5d60b0552b7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1688026084000,"stop":1688026084000,"duration":0}}]},"Hub Suite:Hub Suite#[It] Project and Cluster Events Test Events get recorded for project creation and deletion Events get recorded for project creation Validating events for service account creation":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"84eceefdc700e89d","status":"passed","time":{"start":1688022039000,"stop":1688022039075,"duration":75}}]},"Controller Suite:Controller Suite#[BeforeSuite]":{"statistic":{"failed":2,"broken":0,"skipped":0,"passed":0,"unknown":0,"total":2},"items":[{"uid":"85e12f821cb61827","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000496168>: {\n        Underlying: <*exec.ExitError | 0xc0004ac120>{\n            ProcessState: {\n                pid: 5957,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 54926},\n                    Stime: {Sec: 0, Usec: 11769},\n                    Maxrss: 42128,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 1518,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 251,\n                    Nivcsw: 200,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: error executing jsonpath \\\"\\\\\\\"{.clusters[?(@.name==\\\\\\\"kind-controller\\\\\\\")].cluster.server}\\\\\\\"\\\": Error executing template: <nil> is not array or slice and cannot be filtered. Printing more information for debugging the template:\",\n                        \"\\ttemplate was:\",\n                        \"\\t\\t\\\"{.clusters[?(@.name==\\\"kind-controller\\\")].cluster.server}\\\"\",\n                        \"\\tobject given to jsonpath engine was:\",\n                        \"\\t\\tmap[string]interface {}{\\\"apiVersion\\\":\\\"v1\\\", \\\"clusters\\\":interface {}(nil), \\\"contexts\\\":interface {}(nil), \\\"current-context\\\":\\\"\\\", \\\"kind\\\":\\\"Config\\\", \\\"preferences\\\":map[string]interface {}{}, \\\"users\\\":interface {}(nil)}\",\n                        \"\",\n                        \"\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: error executing jsonpath \\\"\\\\\\\"{.clusters[?(@.name==\\\\\\\"kind-controller\\\\\\\")].cluster.server}\\\\\\\"\\\": Error executing template: <nil> is not array or slice and cannot be filtered. Printing more information for debugging the template:\",\n                    \"\\ttemplate was:\",\n                    \"\\t\\t\\\"{.clusters[?(@.name==\\\"kind-controller\\\")].cluster.server}\\\"\",\n                    \"\\tobject given to jsonpath engine was:\",\n                    \"\\t\\tmap[string]interface {}{\\\"apiVersion\\\":\\\"v1\\\", \\\"clusters\\\":interface {}(nil), \\\"contexts\\\":interface {}(nil), \\\"current-context\\\":\\\"\\\", \\\"kind\\\":\\\"Config\\\", \\\"preferences\\\":map[string]interface {}{}, \\\"users\\\":interface {}(nil)}\",\n                    \"\",\n                    \"\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: error executing jsonpath \\\"\\\\\\\"{.clusters[?(@.name==\\\\\\\"kind-controller\\\\\\\")].cluster.server}\\\\\\\"\\\": Error executing template: <nil> is not array or slice and cannot be filtered. Printing more information for debugging the template:\",\n                        \"\\ttemplate was:\",\n                        \"\\t\\t\\\"{.clusters[?(@.name==\\\"kind-controller\\\")].cluster.server}\\\"\",\n                        \"\\tobject given to jsonpath engine was:\",\n                        \"\\t\\tmap[string]interface {}{\\\"apiVersion\\\":\\\"v1\\\", \\\"clusters\\\":interface {}(nil), \\\"contexts\\\":interface {}(nil), \\\"current-context\\\":\\\"\\\", \\\"kind\\\":\\\"Config\\\", \\\"preferences\\\":map[string]interface {}{}, \\\"users\\\":interface {}(nil)}\",\n                        \"\",\n                        \"\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: error executing jsonpath \\\"\\\\\\\"{.clusters[?(@.name==\\\\\\\"kind-controller\\\\\\\")].cluster.server}\\\\\\\"\\\": Error executing template: <nil> is not array or slice and cannot be filtered. Printing more information for debugging the template:\",\n                    \"\\ttemplate was:\",\n                    \"\\t\\t\\\"{.clusters[?(...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; error: error executing jsonpath \"\\\"{.clusters[?(@.name==\\\"kind-controller\\\")].cluster.server}\\\"\": Error executing template: <nil> is not array or slice and cannot be filtered. Printing more information for debugging the template:\n    \ttemplate was:\n    \t\t\"{.clusters[?(@.name==\"kind-controller\")].cluster.server}\"\n    \tobject given to jsonpath engine was:\n    \t\tmap[string]interface {}{\"apiVersion\":\"v1\", \"clusters\":interface {}(nil), \"contexts\":interface {}(nil), \"current-context\":\"\", \"kind\":\"Config\", \"preferences\":map[string]interface {}{}, \"users\":interface {}(nil)}\n    \n    \noccurred","time":{"start":1689758771000,"stop":1689758771075,"duration":75}},{"uid":"14893ac9a70379c8","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000463368>: {\n        Underlying: <*exec.ExitError | 0xc0003db720>{\n            ProcessState: {\n                pid: 5944,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 39703},\n                    Stime: {Sec: 0, Usec: 11911},\n                    Maxrss: 42048,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2006,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 222,\n                    Nivcsw: 64,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: error executing jsonpath \\\"\\\\\\\"{.clusters[?(@.name==\\\\\\\"kind-controller\\\\\\\")].cluster.server}\\\\\\\"\\\": Error executing template: <nil> is not array or slice and cannot be filtered. Printing more information for debugging the template:\",\n                        \"\\ttemplate was:\",\n                        \"\\t\\t\\\"{.clusters[?(@.name==\\\"kind-controller\\\")].cluster.server}\\\"\",\n                        \"\\tobject given to jsonpath engine was:\",\n                        \"\\t\\tmap[string]interface {}{\\\"apiVersion\\\":\\\"v1\\\", \\\"clusters\\\":interface {}(nil), \\\"contexts\\\":interface {}(nil), \\\"current-context\\\":\\\"\\\", \\\"kind\\\":\\\"Config\\\", \\\"preferences\\\":map[string]interface {}{}, \\\"users\\\":interface {}(nil)}\",\n                        \"\",\n                        \"\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: error executing jsonpath \\\"\\\\\\\"{.clusters[?(@.name==\\\\\\\"kind-controller\\\\\\\")].cluster.server}\\\\\\\"\\\": Error executing template: <nil> is not array or slice and cannot be filtered. Printing more information for debugging the template:\",\n                    \"\\ttemplate was:\",\n                    \"\\t\\t\\\"{.clusters[?(@.name==\\\"kind-controller\\\")].cluster.server}\\\"\",\n                    \"\\tobject given to jsonpath engine was:\",\n                    \"\\t\\tmap[string]interface {}{\\\"apiVersion\\\":\\\"v1\\\", \\\"clusters\\\":interface {}(nil), \\\"contexts\\\":interface {}(nil), \\\"current-context\\\":\\\"\\\", \\\"kind\\\":\\\"Config\\\", \\\"preferences\\\":map[string]interface {}{}, \\\"users\\\":interface {}(nil)}\",\n                    \"\",\n                    \"\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: error executing jsonpath \\\"\\\\\\\"{.clusters[?(@.name==\\\\\\\"kind-controller\\\\\\\")].cluster.server}\\\\\\\"\\\": Error executing template: <nil> is not array or slice and cannot be filtered. Printing more information for debugging the template:\",\n                        \"\\ttemplate was:\",\n                        \"\\t\\t\\\"{.clusters[?(@.name==\\\"kind-controller\\\")].cluster.server}\\\"\",\n                        \"\\tobject given to jsonpath engine was:\",\n                        \"\\t\\tmap[string]interface {}{\\\"apiVersion\\\":\\\"v1\\\", \\\"clusters\\\":interface {}(nil), \\\"contexts\\\":interface {}(nil), \\\"current-context\\\":\\\"\\\", \\\"kind\\\":\\\"Config\\\", \\\"preferences\\\":map[string]interface {}{}, \\\"users\\\":interface {}(nil)}\",\n                        \"\",\n                        \"\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: error executing jsonpath \\\"\\\\\\\"{.clusters[?(@.name==\\\\\\\"kind-controller\\\\\\\")].cluster.server}\\\\\\\"\\\": Error executing template: <nil> is not array or slice and cannot be filtered. Printing more information for debugging the template:\",\n                    \"\\ttemplate was:\",\n                    \"\\t\\t\\\"{.clusters[?(@...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; error: error executing jsonpath \"\\\"{.clusters[?(@.name==\\\"kind-controller\\\")].cluster.server}\\\"\": Error executing template: <nil> is not array or slice and cannot be filtered. Printing more information for debugging the template:\n    \ttemplate was:\n    \t\t\"{.clusters[?(@.name==\"kind-controller\")].cluster.server}\"\n    \tobject given to jsonpath engine was:\n    \t\tmap[string]interface {}{\"apiVersion\":\"v1\", \"clusters\":interface {}(nil), \"contexts\":interface {}(nil), \"current-context\":\"\", \"kind\":\"Config\", \"preferences\":map[string]interface {}{}, \"users\":interface {}(nil)}\n    \n    \noccurred","time":{"start":1689755137000,"stop":1689755137061,"duration":61}}]},"Hub Suite:Hub Suite#[AfterSuite]":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":3,"unknown":0,"total":3},"items":[{"uid":"11d49aa167c5eda3","status":"passed","time":{"start":1689750156000,"stop":1689750457095,"duration":301095}},{"uid":"a882604afc922415","status":"passed","time":{"start":1688626949000,"stop":1688627250054,"duration":301054}},{"uid":"bceb9263bdb76971","status":"passed","time":{"start":1688022039000,"stop":1688022041714,"duration":2714}}]},"Hub Suite:Hub Suite#[It] Cluster auto deregister validation tests Cluster auto deregister Creates cluster secrets":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"ea0e3430c423df0d","status":"passed","time":{"start":1688022039000,"stop":1688022165816,"duration":126816}}]},"Hub Suite:Hub Suite#[It] Cluster CR tests Cluster CR validation Creates cluster secrets":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"69cbafba9535dfd1","status":"passed","time":{"start":1688022039000,"stop":1688022042600,"duration":3600}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Update Should update successfully when Project Applied while removing a service account name in Write users":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"498044a4e2996b8a","status":"passed","time":{"start":1688022039000,"stop":1688022044696,"duration":5696}}]},"Hub Suite:Hub Suite#[It] Cluster health check tests Cluster health check - without istio Updates health status when dns is not working":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":0,"unknown":0,"total":1},"items":[{"uid":"3b10059df4284c38","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1688022039000,"stop":1688022039000,"duration":0}}]},"Hub Suite:Hub Suite#[It] Project and Cluster Events Test Events get recorded for project creation and deletion Events get recorded for project deletion Validating events for project deletion":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"ff2582d0c39b16eb","status":"passed","time":{"start":1688022039000,"stop":1688022043472,"duration":4472}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Creation Should fail while creating Project with  project name as blank":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"2a9831abbaf03a03","status":"passed","time":{"start":1688022039000,"stop":1688022039236,"duration":236}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Update Should Update successfully when Project is Applied with valid service account name in Read users":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"16b1ec889e7da2cd","status":"passed","time":{"start":1688022039000,"stop":1688022047393,"duration":8393}}]},"Hub Suite:Hub Suite#[It] Cluster CR tests Cluster CR validation Register worker cluster":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"428bdf1bca524381","status":"passed","time":{"start":1688022039000,"stop":1688022686534,"duration":647534}}]},"SliceHealth Suite:SliceHealth Suite#[It] Verify gw redundancy wrt health check when both gateways are down should have gw in error state":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":0,"unknown":0,"total":1},"items":[{"uid":"6fc4174623ac6d9c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1688026084000,"stop":1688026084000,"duration":0}}]}}