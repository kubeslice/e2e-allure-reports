{"Worker Suite:Worker Suite#[It] Slice Config Negative Scenario tests Slice Config Negative Scenario Update Should fail while updating sliceconfig with invalid clusters":{"statistic":{"failed":0,"broken":0,"skipped":14,"passed":42,"unknown":0,"total":56},"items":[{"uid":"8c0c4de3b572694","status":"passed","time":{"start":1664094053000,"stop":1664094053463,"duration":463}},{"uid":"2ec8e8399497263a","status":"passed","time":{"start":1663843963000,"stop":1663843963618,"duration":618}},{"uid":"875854a1d441f1ab","status":"passed","time":{"start":1663829327000,"stop":1663829327540,"duration":540}},{"uid":"5b9e4e1f5572dad4","status":"passed","time":{"start":1663343132000,"stop":1663343132616,"duration":616}},{"uid":"3948f8cc3c55f140","status":"passed","time":{"start":1663342947000,"stop":1663342947526,"duration":526}},{"uid":"13852af469deaca6","status":"passed","time":{"start":1663044723000,"stop":1663044723443,"duration":443}},{"uid":"45dea2f773d31f12","status":"skipped","statusDetails":"skipped","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"b2edaebc986239fa","status":"passed","time":{"start":1662348908000,"stop":1662348908558,"duration":558}},{"uid":"2a31082fe0d5e535","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"23b9f719d57ac655","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"7a5647d17e0b5bcb","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"b9b379f4416b5d59","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"f34754622a0b6abc","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"b537556db97f6321","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"de078acf3821df6d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"5a354b8d9b3fbc3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"ecece9e3a4a054e4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"46651dd944544c23","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"a1fcb0e25a7117c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"657df4a50e7fe701","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}}]},"650bae1143f83c38b4918b41d1f54ae9":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"3039c369a16c66bf","status":"passed","time":{"start":1663842155329,"stop":1663842178251,"duration":22922}}]},"417b4913fe2ac77aec5434c7a18f543e":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"b9e987cd7e35fa86","status":"passed","time":{"start":1663838529491,"stop":1663838551529,"duration":22038}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting worker-operator pod Check ping between iperf-server and iperf-client after worker-operator pod restart":{"statistic":{"failed":0,"broken":0,"skipped":56,"passed":0,"unknown":0,"total":56},"items":[{"uid":"a9886342b2ce1dc0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1664094053000,"stop":1664094053000,"duration":0}},{"uid":"d89cc46fae8bf964","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663843963000,"stop":1663843963000,"duration":0}},{"uid":"3e317b6a9b21eb0c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663829327000,"stop":1663829327000,"duration":0}},{"uid":"25f34dffc4fc4040","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663343132000,"stop":1663343132000,"duration":0}},{"uid":"45a2657421b4eeed","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663342947000,"stop":1663342947000,"duration":0}},{"uid":"28c3e32ce483dd53","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"4ac7ebb2c9f20f3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"2251bd97a582a2d9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662348908000,"stop":1662348908000,"duration":0}},{"uid":"923ed96ab662e4c4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"49a9906fd903410e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"69b83bd378cb9215","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"f902726b3d94801d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"720408ddc6745501","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"1fcad47399f6f8c1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"876447118287556b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"6e44cd66bd515231","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"b2ae4122e2d79852","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"83c8f056812fe9f2","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"a4b18b22b1de744d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"dd95e720072bb5ba","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}}]},"bdf08c8858b0bd4840a4541ea4c6233a":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"afb3f6a592e19389","status":"passed","time":{"start":1663841261807,"stop":1663841275477,"duration":13670}}]},"6ae8a5d614c80588c194ed25759bde2b":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"bf37f0feeb60eb36","status":"passed","time":{"start":1663842878500,"stop":1663842927040,"duration":48540}}]},"9ef7db3deae08deb0fbd2b005a52d632":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"90a6d29e098ae380","status":"passed","time":{"start":1663840977862,"stop":1663840993532,"duration":15670}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting vl3 pod Check ping between iperf-server and iperf-client after vl3 pod restart":{"statistic":{"failed":1,"broken":0,"skipped":55,"passed":0,"unknown":0,"total":56},"items":[{"uid":"2155fc9e57bef05d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1664094053000,"stop":1664094053000,"duration":0}},{"uid":"fdfa2dc0282a8bea","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663843963000,"stop":1663843963000,"duration":0}},{"uid":"68f852606957566b","status":"failed","statusDetails":"Timed out after 240.001s.\nExpected\n    <bool>: false\nto be true","time":{"start":1663829327000,"stop":1663829577368,"duration":250368}},{"uid":"4c9efd5df2407314","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663343132000,"stop":1663343132000,"duration":0}},{"uid":"c5e0778bf87eb0c8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663342947000,"stop":1663342947000,"duration":0}},{"uid":"f505cb950b4c7dde","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"aacab27b5d1e76ac","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"dc477ac1860562ef","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662348908000,"stop":1662348908000,"duration":0}},{"uid":"59ffecf5ba3eb0fc","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"e20bfccadd2af6c8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"4ab01c6c4960b411","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"a9321f81cd35af60","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"559110264e8bcfea","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"2f252e80b9e4f527","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"bca6a30e9f22bc29","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"2f7684c8fab6cb68","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"9a4ba359b3243e0e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"ace3bab5c7d41de5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"8e342b171cb4e98f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"65957c53148a92f2","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}}]},"bcc917aa2239ac4df31ce29d8e628340":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"958d48338a6b4ba4","status":"passed","time":{"start":1663840217424,"stop":1663840239406,"duration":21982}}]},"b5618b8202cb21926c27643deaf90e77":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"11241361353cef30","status":"passed","time":{"start":1663841302992,"stop":1663841319635,"duration":16643}}]},"26642511d2fc9a5c17d13db2ee0ee31a":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"47f609ca27d17ae6","status":"passed","time":{"start":1663842592160,"stop":1663842635150,"duration":42990}}]},"Worker Suite:Worker Suite#[It] Hub netpol scenarios: Slice Configs Netpol scenarios Should create slice for valid namespace and valid clusters in applicationNamespaces of sliceconfigs manifest":{"statistic":{"failed":13,"broken":0,"skipped":1,"passed":42,"unknown":0,"total":56},"items":[{"uid":"4584c080417d6712","status":"passed","time":{"start":1664094053000,"stop":1664094063960,"duration":10960}},{"uid":"b201ae6a6e5e9b9d","status":"passed","time":{"start":1663843963000,"stop":1663843974619,"duration":11619}},{"uid":"25d7e302bae05f2d","status":"passed","time":{"start":1663829327000,"stop":1663829337541,"duration":10541}},{"uid":"e65d1f8f9a5c652a","status":"passed","time":{"start":1663343132000,"stop":1663343142437,"duration":10437}},{"uid":"ab10c7626add97af","status":"passed","time":{"start":1663342947000,"stop":1663342957512,"duration":10512}},{"uid":"c74214d8e8a11343","status":"skipped","statusDetails":"skipped","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"16fc69378bea4996","status":"passed","time":{"start":1663044069000,"stop":1663044079698,"duration":10698}},{"uid":"3f594a5a7689de26","status":"passed","time":{"start":1662348908000,"stop":1662348918504,"duration":10504}},{"uid":"8770428b79a3720a","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000468048>: {\n        Underlying: <*exec.ExitError | 0xc000708040>{\n            ProcessState: {\n                pid: 7148,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 245000},\n                    Stime: {Sec: 0, Usec: 73889},\n                    Maxrss: 83972,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 6636,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 393,\n                    Nivcsw: 455,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"fireredslice\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\noccurred","time":{"start":1661844205000,"stop":1661844205290,"duration":290}},{"uid":"70b6d88d01a62d61","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004bfcc8>: {\n        Underlying: <*exec.ExitError | 0xc000658fc0>{\n            ProcessState: {\n                pid: 7298,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 183810},\n                    Stime: {Sec: 0, Usec: 40846},\n                    Maxrss: 85124,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 5461,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 309,\n                    Nivcsw: 549,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"fireredslice\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661754870000,"stop":1661754870248,"duration":248}},{"uid":"830da7dbfa65ed7f","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000ed08>: {\n        Underlying: <*exec.ExitError | 0xc0007b06e0>{\n            ProcessState: {\n                pid: 7131,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 170259},\n                    Stime: {Sec: 0, Usec: 46434},\n                    Maxrss: 87412,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4166,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 345,\n                    Nivcsw: 521,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"fireredslice\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661611962000,"stop":1661611962258,"duration":258}},{"uid":"4ed4f0488ece8e10","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00090cc90>: {\n        Underlying: <*exec.ExitError | 0xc00096a6e0>{\n            ProcessState: {\n                pid: 7232,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 196262},\n                    Stime: {Sec: 0, Usec: 53526},\n                    Maxrss: 80964,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 5010,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 447,\n                    Nivcsw: 846,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"fireredslice\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661585077000,"stop":1661585077302,"duration":302}},{"uid":"3410fee95ffc57cc","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00013b7b8>: {\n        Underlying: <*exec.ExitError | 0xc000697c20>{\n            ProcessState: {\n                pid: 6013,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 222498},\n                    Stime: {Sec: 0, Usec: 27324},\n                    Maxrss: 81192,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3698,\n                    Majflt: 8,\n                    Nswap: 0,\n                    Inblock: 1736,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 480,\n                    Nivcsw: 510,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"fireredslice\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661584447000,"stop":1661584447239,"duration":239}},{"uid":"f6e37700b8473264","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0006d28a0>: {\n        Underlying: <*exec.ExitError | 0xc0003f02a0>{\n            ProcessState: {\n                pid: 7303,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 179564},\n                    Stime: {Sec: 0, Usec: 36729},\n                    Maxrss: 84344,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2913,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 399,\n                    Nivcsw: 549,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"fireredslice\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661580045000,"stop":1661580045224,"duration":224}},{"uid":"daff666b8e978b98","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0000bfe48>: {\n        Underlying: <*exec.ExitError | 0xc00051d320>{\n            ProcessState: {\n                pid: 7230,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 228929},\n                    Stime: {Sec: 0, Usec: 57232},\n                    Maxrss: 95092,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4156,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 399,\n                    Nivcsw: 333,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"fireredslice\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661578505000,"stop":1661578505295,"duration":295}},{"uid":"f1f1c741ba4bbb05","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0003d2d38>: {\n        Underlying: <*exec.ExitError | 0xc0006b7b00>{\n            ProcessState: {\n                pid: 7331,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 270476},\n                    Stime: {Sec: 0, Usec: 35798},\n                    Maxrss: 91452,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 6170,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 342,\n                    Nivcsw: 427,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"fireredslice\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661572479000,"stop":1661572479279,"duration":279}},{"uid":"a8726f27b2ffb762","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0006eb380>: {\n        Underlying: <*exec.ExitError | 0xc0007e26c0>{\n            ProcessState: {\n                pid: 7111,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 184641},\n                    Stime: {Sec: 0, Usec: 34620},\n                    Maxrss: 79596,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 6028,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 508,\n                    Nivcsw: 294,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"fireredslice\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660832281000,"stop":1660832281229,"duration":229}},{"uid":"3637107585add953","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0007f13b0>: {\n        Underlying: <*exec.ExitError | 0xc0006623a0>{\n            ProcessState: {\n                pid: 7029,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 163457},\n                    Stime: {Sec: 0, Usec: 66161},\n                    Maxrss: 85640,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 7899,\n                    Majflt: 1,\n                    Nswap: 0,\n                    Inblock: 128,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 441,\n                    Nivcsw: 329,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"fireredslice\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660808797000,"stop":1660808797220,"duration":220}},{"uid":"ed59468a101d0f76","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00066d5d8>: {\n        Underlying: <*exec.ExitError | 0xc000917b20>{\n            ProcessState: {\n                pid: 7232,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 136938},\n                    Stime: {Sec: 0, Usec: 41842},\n                    Maxrss: 88508,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 5791,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 322,\n                    Nivcsw: 296,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"fireredslice\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660299182000,"stop":1660299182176,"duration":176}},{"uid":"2fd8ed7ea4ab8a5e","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0000cf9e0>: {\n        Underlying: <*exec.ExitError | 0xc00078bf40>{\n            ProcessState: {\n                pid: 7185,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 110522},\n                    Stime: {Sec: 0, Usec: 55261},\n                    Maxrss: 84484,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3732,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 271,\n                    Nivcsw: 231,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"fireredslice\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660295740000,"stop":1660295740156,"duration":156}}]},"471e28c4d1e4371505695fe97672319":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"dca717ceb0d92ce6","status":"passed","time":{"start":1663838997983,"stop":1663839007350,"duration":9367}}]},"Istio Suite:Istio Suite#[It] Bookinfo tests Bookinfo Scenario 3 without egress and with ingress Should verify if Ping works successfully from Productpage to details endpoint":{"statistic":{"failed":7,"broken":0,"skipped":58,"passed":7,"unknown":0,"total":72},"items":[{"uid":"9eb74995c243dbbf","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1664246558000,"stop":1664246558000,"duration":0}},{"uid":"df42889714ac894e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1664162065000,"stop":1664162065000,"duration":0}},{"uid":"5b1ecd86bd33b9b5","status":"passed","time":{"start":1664093039000,"stop":1664093098721,"duration":59721}},{"uid":"8b7ea83e3ac69c72","status":"passed","time":{"start":1663842833000,"stop":1663842894820,"duration":61820}},{"uid":"76ec2a8aa0a69b24","status":"passed","time":{"start":1663828380000,"stop":1663828439840,"duration":59840}},{"uid":"81b0cd363efbca53","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663769301000,"stop":1663769301000,"duration":0}},{"uid":"6b3ee84ed2aee912","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663671043000,"stop":1663671043000,"duration":0}},{"uid":"7caae059bcb8b18b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663666667000,"stop":1663666667000,"duration":0}},{"uid":"bc74e416c8d1c528","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663666522000,"stop":1663666522000,"duration":0}},{"uid":"80e2ca163dde9a0e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663585263000,"stop":1663585263000,"duration":0}},{"uid":"73ac5634de3a151d","status":"failed","statusDetails":"Unexpected non-nil/non-zero argument at index 1:\n\t<*shell.ErrWithCmdOutput>: &shell.ErrWithCmdOutput{Underlying:(*exec.ExitError)(0xc000843ec0), Output:(*shell.output)(0xc0004e7c08)}","time":{"start":1663342266000,"stop":1663342333201,"duration":67201}},{"uid":"31517ff1e0b2684a","status":"failed","statusDetails":"Unexpected non-nil/non-zero argument at index 1:\n\t<*shell.ErrWithCmdOutput>: &shell.ErrWithCmdOutput{Underlying:(*exec.ExitError)(0xc0005fbc00), Output:(*shell.output)(0xc000633d40)}","time":{"start":1663341990000,"stop":1663342057119,"duration":67119}},{"uid":"d49c2ee17e1612c2","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663315204000,"stop":1663315204000,"duration":0}},{"uid":"77e27bc77ee7cfe2","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663312558000,"stop":1663312558000,"duration":0}},{"uid":"ba9e08df3df606ca","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663055013000,"stop":1663055013000,"duration":0}},{"uid":"f7e50f9fd5b5058","status":"failed","statusDetails":"Unexpected non-nil/non-zero argument at index 1:\n\t<*shell.ErrWithCmdOutput>: &shell.ErrWithCmdOutput{Underlying:(*exec.ExitError)(0xc000395c40), Output:(*shell.output)(0xc000490b58)}","time":{"start":1663043831000,"stop":1663043897534,"duration":66534}},{"uid":"708190d7c2db8755","status":"failed","statusDetails":"Unexpected non-nil/non-zero argument at index 1:\n\t<*shell.ErrWithCmdOutput>: &shell.ErrWithCmdOutput{Underlying:(*exec.ExitError)(0xc0005e2660), Output:(*shell.output)(0xc0006c4450)}","time":{"start":1663043075000,"stop":1663043142569,"duration":67569}},{"uid":"b68f9024cd4a6053","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663040338000,"stop":1663040338000,"duration":0}},{"uid":"6a335db38bd597bb","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662819574000,"stop":1662819574000,"duration":0}},{"uid":"db4f1f5a4e4bb248","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662795310000,"stop":1662795310000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice Config Negative Scenario tests Slice Config Negative Scenario Creation Should fail while creating sliceconfig with invalid clusters":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":55,"unknown":0,"total":56},"items":[{"uid":"aff09152eaa839e2","status":"passed","time":{"start":1664094053000,"stop":1664094053233,"duration":233}},{"uid":"28dd2fd7b1856893","status":"passed","time":{"start":1663843963000,"stop":1663843963422,"duration":422}},{"uid":"8c4f5c81b0288b29","status":"passed","time":{"start":1663829327000,"stop":1663829327399,"duration":399}},{"uid":"649e3efb5a624b95","status":"passed","time":{"start":1663343132000,"stop":1663343132379,"duration":379}},{"uid":"454687bc6d556086","status":"passed","time":{"start":1663342947000,"stop":1663342947251,"duration":251}},{"uid":"503aa401e6a3399d","status":"passed","time":{"start":1663044723000,"stop":1663044723239,"duration":239}},{"uid":"98f2dc08ca2b4f76","status":"skipped","statusDetails":"skipped","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"f8847491d7582a17","status":"passed","time":{"start":1662348908000,"stop":1662348908238,"duration":238}},{"uid":"231f32628976e3d5","status":"passed","time":{"start":1661844205000,"stop":1661844205284,"duration":284}},{"uid":"56d1561dfa0b72d4","status":"passed","time":{"start":1661754870000,"stop":1661754870230,"duration":230}},{"uid":"a16adc13a7cc28a3","status":"passed","time":{"start":1661611962000,"stop":1661611962271,"duration":271}},{"uid":"2ef6ce84ec5bbed9","status":"passed","time":{"start":1661585077000,"stop":1661585077261,"duration":261}},{"uid":"77046303fe151842","status":"passed","time":{"start":1661584447000,"stop":1661584447240,"duration":240}},{"uid":"a468aabadf132e7a","status":"passed","time":{"start":1661580045000,"stop":1661580045259,"duration":259}},{"uid":"2c3fad03a8a08bb1","status":"passed","time":{"start":1661578505000,"stop":1661578505298,"duration":298}},{"uid":"9615f8008c9bca00","status":"passed","time":{"start":1661572479000,"stop":1661572479278,"duration":278}},{"uid":"f6df31f7c57c3084","status":"passed","time":{"start":1660832281000,"stop":1660832281227,"duration":227}},{"uid":"1c6fd09974e0dd56","status":"passed","time":{"start":1660808797000,"stop":1660808797210,"duration":210}},{"uid":"fd58156bdd2eeda9","status":"passed","time":{"start":1660299182000,"stop":1660299182184,"duration":184}},{"uid":"2a85415c006c6122","status":"passed","time":{"start":1660295740000,"stop":1660295740187,"duration":187}}]},"fd62e4f55836eca21788e40535d34c3":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"a2366711f7cf253d","status":"passed","time":{"start":1663840630396,"stop":1663840642841,"duration":12445}}]},"fc482db0a03fe41116b7cb9b3e22d798":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"5c659e2ac17f8733","status":"passed","time":{"start":1663840081132,"stop":1663840092157,"duration":11025}}]},"Worker Suite:Worker Suite#[It] Intracluster slice tests Slice with netpol should remove network policy resource from application namespace once the isolationEnabled is set to false":{"statistic":{"failed":1,"broken":0,"skipped":39,"passed":16,"unknown":0,"total":56},"items":[{"uid":"6b1228cda284c520","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1664094053000,"stop":1664094053000,"duration":0}},{"uid":"d4f28ec98e69d60b","status":"passed","time":{"start":1663843963000,"stop":1663843970329,"duration":7329}},{"uid":"2e0baca4f7a39e7b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663829327000,"stop":1663829327000,"duration":0}},{"uid":"faf69dfc29a8f34d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663343132000,"stop":1663343132000,"duration":0}},{"uid":"975df74869d12f04","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663342947000,"stop":1663342947000,"duration":0}},{"uid":"8c393c519eab89f6","status":"skipped","statusDetails":"skipped","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"3cd59e898b9101d5","status":"passed","time":{"start":1663044069000,"stop":1663044078287,"duration":9287}},{"uid":"a84b851cccb1a4f8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662348908000,"stop":1662348908000,"duration":0}},{"uid":"feda9d1d72df3201","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"62129a23520222a8","status":"skipped","statusDetails":"skipped - Spec skipped because Skip() was called in BeforeAll","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"6ab576ccf1dd5409","status":"skipped","statusDetails":"skipped - Spec skipped because Skip() was called in BeforeAll","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"18fd6314ea755aae","status":"skipped","statusDetails":"skipped - Spec skipped because Skip() was called in BeforeAll","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"88b74ac5cb908b36","status":"skipped","statusDetails":"skipped - Spec skipped because Skip() was called in BeforeAll","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"ea10db5d245fdad4","status":"skipped","statusDetails":"skipped - Spec skipped because Skip() was called in BeforeAll","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"a91109949847603a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"b1627f18254eb1b3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"62485e10b5a8bbfd","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"7384bf6b99d34c2f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"db0298cc8d1fe4b6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"43c67000a11a2e4c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests Iperf App connectivity Verify Connectivity between iperf-server and iperf-client":{"statistic":{"failed":0,"broken":0,"skipped":48,"passed":8,"unknown":0,"total":56},"items":[{"uid":"cf57e7e8a83e96ff","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1664094053000,"stop":1664094053000,"duration":0}},{"uid":"38014db1df58135a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663843963000,"stop":1663843963000,"duration":0}},{"uid":"3ee4c72416cecbf9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663829327000,"stop":1663829327000,"duration":0}},{"uid":"84deb502ebdc549d","status":"passed","time":{"start":1663343132000,"stop":1663343157242,"duration":25242}},{"uid":"ec095014ff78a712","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663342947000,"stop":1663342947000,"duration":0}},{"uid":"29b66b9bf9bad3b3","status":"skipped","statusDetails":"skipped","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"4d47a848aac3d58d","status":"skipped","statusDetails":"skipped","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"18e2d2cc6e6ea028","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662348908000,"stop":1662348908000,"duration":0}},{"uid":"fae5d12f16cd3652","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"b8a8e0ce96f0487e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"f48912ef2ee99c0f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"ef2a27827a94d3b1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"d810e546e6225e38","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"7be46fc6a929e7bd","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"a3cdb33def2d44ef","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"fc8ab61f11a7ffa0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"e61c29d4c68c191e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"b2dd8250a0c30e23","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"ccb1ebe6dc37989","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"53f69498bc56e46a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}}]},"Istio Suite:Istio Suite#[It] Bookinfo tests Bookinfo Scenario 2 with egress and without ingress Should verify if Ping works successfully from Productpage to details endpoint":{"statistic":{"failed":9,"broken":0,"skipped":57,"passed":6,"unknown":0,"total":72},"items":[{"uid":"2bc566e94637fb","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1664246558000,"stop":1664246558000,"duration":0}},{"uid":"a7806ac33df55d22","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1664162065000,"stop":1664162065000,"duration":0}},{"uid":"56a29c2ecb46b1e0","status":"failed","statusDetails":"Unexpected non-nil/non-zero argument at index 1:\n\t<*shell.ErrWithCmdOutput>: &shell.ErrWithCmdOutput{Underlying:(*exec.ExitError)(0xc0006a3a00), Output:(*shell.output)(0xc0005ef920)}","time":{"start":1664093039000,"stop":1664093105525,"duration":66525}},{"uid":"57942d07d584640f","status":"failed","statusDetails":"Unexpected non-nil/non-zero argument at index 1:\n\t<*shell.ErrWithCmdOutput>: &shell.ErrWithCmdOutput{Underlying:(*exec.ExitError)(0xc000644600), Output:(*shell.output)(0xc00045df98)}","time":{"start":1663842833000,"stop":1663842901976,"duration":68976}},{"uid":"55322386cc397e52","status":"failed","statusDetails":"Unexpected non-nil/non-zero argument at index 1:\n\t<*shell.ErrWithCmdOutput>: &shell.ErrWithCmdOutput{Underlying:(*exec.ExitError)(0xc00047b580), Output:(*shell.output)(0xc00000e2d0)}","time":{"start":1663828380000,"stop":1663828447078,"duration":67078}},{"uid":"4a3f33b0359a52ac","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663769301000,"stop":1663769301000,"duration":0}},{"uid":"d65954637c9c47d4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663671043000,"stop":1663671043000,"duration":0}},{"uid":"b22004bdca4be5e0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663666667000,"stop":1663666667000,"duration":0}},{"uid":"db831506b4caea1e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663666522000,"stop":1663666522000,"duration":0}},{"uid":"d9ff842c35089cf4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663585263000,"stop":1663585263000,"duration":0}},{"uid":"8899b45efb9ebfbe","status":"passed","time":{"start":1663342266000,"stop":1663342323829,"duration":57829}},{"uid":"b74be8b2a8d76785","status":"failed","statusDetails":"Unexpected non-nil/non-zero argument at index 1:\n\t<*shell.ErrWithCmdOutput>: &shell.ErrWithCmdOutput{Underlying:(*exec.ExitError)(0xc00099b2c0), Output:(*shell.output)(0xc0009b54b8)}","time":{"start":1663341990000,"stop":1663342056910,"duration":66910}},{"uid":"d9504f38a5ea1dee","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663315204000,"stop":1663315204000,"duration":0}},{"uid":"416d9fb04720fdd6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663312558000,"stop":1663312558000,"duration":0}},{"uid":"7ccce6ec60fd74c3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663055013000,"stop":1663055013000,"duration":0}},{"uid":"5a21dee37df4e98c","status":"failed","statusDetails":"Unexpected non-nil/non-zero argument at index 1:\n\t<*shell.ErrWithCmdOutput>: &shell.ErrWithCmdOutput{Underlying:(*exec.ExitError)(0xc0006524a0), Output:(*shell.output)(0xc00088af30)}","time":{"start":1663043831000,"stop":1663043898138,"duration":67138}},{"uid":"327fdddf7fd8f36f","status":"failed","statusDetails":"Unexpected non-nil/non-zero argument at index 1:\n\t<*shell.ErrWithCmdOutput>: &shell.ErrWithCmdOutput{Underlying:(*exec.ExitError)(0xc00038a560), Output:(*shell.output)(0xc00000f290)}","time":{"start":1663043075000,"stop":1663043144017,"duration":69017}},{"uid":"dfb2d5e38f87df36","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663040338000,"stop":1663040338000,"duration":0}},{"uid":"59735254a5a81684","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662819574000,"stop":1662819574000,"duration":0}},{"uid":"1bbb9a677c1b2e0e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662795310000,"stop":1662795310000,"duration":0}}]},"Istio Suite:Istio Suite#[It] Bookinfo tests Bookinfo Scenario 4 with egress and ingress Should install Details, Reviews, Ratings and Serviceexport on worker cluster-2":{"statistic":{"failed":0,"broken":0,"skipped":60,"passed":12,"unknown":0,"total":72},"items":[{"uid":"179fcfa2344acdaa","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1664246558000,"stop":1664246558000,"duration":0}},{"uid":"bde4baa18baf523f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1664162065000,"stop":1664162065000,"duration":0}},{"uid":"33744484134b0de1","status":"passed","time":{"start":1664093039000,"stop":1664093070371,"duration":31371}},{"uid":"cc769546636b82eb","status":"passed","time":{"start":1663842833000,"stop":1663842867669,"duration":34669}},{"uid":"9d5b0971b7d02a3c","status":"passed","time":{"start":1663828380000,"stop":1663828412000,"duration":32000}},{"uid":"628de735af8be517","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663769301000,"stop":1663769301000,"duration":0}},{"uid":"67f9e54ab04e5e61","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663671043000,"stop":1663671043000,"duration":0}},{"uid":"4eda68f1cd59fb3e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663666667000,"stop":1663666667000,"duration":0}},{"uid":"a2b81b526558c07b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663666522000,"stop":1663666522000,"duration":0}},{"uid":"f273a495b7e162e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663585263000,"stop":1663585263000,"duration":0}},{"uid":"8c2f72ee537206bb","status":"passed","time":{"start":1663342266000,"stop":1663342332922,"duration":66922}},{"uid":"a5ad92cdba84a4f","status":"passed","time":{"start":1663341990000,"stop":1663342020126,"duration":30126}},{"uid":"bd1c937f42993266","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663315204000,"stop":1663315204000,"duration":0}},{"uid":"c83cc3805bf20565","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663312558000,"stop":1663312558000,"duration":0}},{"uid":"c75614e29f058bfb","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663055013000,"stop":1663055013000,"duration":0}},{"uid":"ecef0b6ecfc27a75","status":"passed","time":{"start":1663043831000,"stop":1663043897538,"duration":66538}},{"uid":"991f3c9565593854","status":"passed","time":{"start":1663043075000,"stop":1663043155370,"duration":80370}},{"uid":"5c35609bb4a5925d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663040338000,"stop":1663040338000,"duration":0}},{"uid":"e341a76f02d611cc","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662819574000,"stop":1662819574000,"duration":0}},{"uid":"efd3b30d8efbce42","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662795310000,"stop":1662795310000,"duration":0}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests Iperf App connectivity should have gateway pod running":{"statistic":{"failed":2,"broken":0,"skipped":34,"passed":20,"unknown":0,"total":56},"items":[{"uid":"401348978c87e23b","status":"passed","time":{"start":1664094053000,"stop":1664094053057,"duration":57}},{"uid":"fffd17400b7f87f","status":"passed","time":{"start":1663843963000,"stop":1663843963104,"duration":104}},{"uid":"95930e11f7f20edc","status":"passed","time":{"start":1663829327000,"stop":1663829327040,"duration":40}},{"uid":"ad31a00d663db2ed","status":"passed","time":{"start":1663343132000,"stop":1663343300655,"duration":168655}},{"uid":"18a2b577a548529b","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1663342947000,"stop":1663343129699,"duration":182699}},{"uid":"34bb4ac9b34dd92d","status":"skipped","statusDetails":"skipped","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"21938c37dba0ed11","status":"passed","time":{"start":1663044069000,"stop":1663044069070,"duration":70}},{"uid":"a4fb98d97ddd2b1b","status":"passed","time":{"start":1662348908000,"stop":1662348908076,"duration":76}},{"uid":"aef73c86444d0311","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"8c40594824340462","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"3fe05c6618b3fdb9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"9a2cb6accaac43b1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"5c5e8c2c989340c1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"1faa7a0077e2ec95","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"f7eb01f5623ae202","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"5e242b3a2c4ee07f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"8cbbb3185ae4282c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"ca752e72eaa1ba83","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"70a7d27a99b2b60b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"3eeb746fa3a2651","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}}]},"c75425eb7b2cc996cd7107f6de63a826":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"db727e45fc841097","status":"passed","time":{"start":1663838551583,"stop":1663838560677,"duration":9094}}]},"Worker Suite:Worker Suite#[It] Slice Config Negative Scenario tests Slice Config Negative Scenario Creation Should fail while creating sliceconfig with with invalid DSCP class in qosprofiledetails":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":55,"unknown":0,"total":56},"items":[{"uid":"647f3a79679c2dea","status":"passed","time":{"start":1664094053000,"stop":1664094053379,"duration":379}},{"uid":"fd17104822b05ee","status":"passed","time":{"start":1663843963000,"stop":1663843963595,"duration":595}},{"uid":"4cc0aa6f6b77afe0","status":"passed","time":{"start":1663829327000,"stop":1663829327454,"duration":454}},{"uid":"fecb0a51d2279158","status":"passed","time":{"start":1663343132000,"stop":1663343132591,"duration":591}},{"uid":"b6ce68bf80cd69b1","status":"passed","time":{"start":1663342947000,"stop":1663342947319,"duration":319}},{"uid":"5e1737d0d13c93ea","status":"passed","time":{"start":1663044723000,"stop":1663044723338,"duration":338}},{"uid":"f2110e6a2eea2ed5","status":"skipped","statusDetails":"skipped","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"1104d3dd41d60e76","status":"passed","time":{"start":1662348908000,"stop":1662348908315,"duration":315}},{"uid":"af6e7e4e8814319c","status":"passed","time":{"start":1661844205000,"stop":1661844205383,"duration":383}},{"uid":"dacbda61d8c4a71f","status":"passed","time":{"start":1661754870000,"stop":1661754870260,"duration":260}},{"uid":"de203be1d7ef11a3","status":"passed","time":{"start":1661611962000,"stop":1661611962418,"duration":418}},{"uid":"20de9d1baf0c9638","status":"passed","time":{"start":1661585077000,"stop":1661585077301,"duration":301}},{"uid":"4d48ea733163a031","status":"passed","time":{"start":1661584447000,"stop":1661584447328,"duration":328}},{"uid":"c2293b4f43f1a99e","status":"passed","time":{"start":1661580045000,"stop":1661580045305,"duration":305}},{"uid":"b975d8a237252a11","status":"passed","time":{"start":1661578505000,"stop":1661578505350,"duration":350}},{"uid":"e61ba8e5ecc71d10","status":"passed","time":{"start":1661572479000,"stop":1661572479377,"duration":377}},{"uid":"dfd1b2b6dbbf63f6","status":"passed","time":{"start":1660832281000,"stop":1660832281320,"duration":320}},{"uid":"cd51e04c4119c18b","status":"passed","time":{"start":1660808797000,"stop":1660808797285,"duration":285}},{"uid":"5df03f75f30172d4","status":"passed","time":{"start":1660299182000,"stop":1660299182244,"duration":244}},{"uid":"6bc63fee86082900","status":"passed","time":{"start":1660295740000,"stop":1660295740239,"duration":239}}]},"9f28a38778690c46934c0b9c75e76c0d":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"a1a154f56e6c4cc3","status":"passed","time":{"start":1663840140146,"stop":1663840151451,"duration":11305}}]},"54b42dbaa702498b68e8264f16963ce0":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"a6f6a7eb73001d69","status":"passed","time":{"start":1663843711228,"stop":1663843805055,"duration":93827}}]},"Worker Suite:Worker Suite#[It] Slice creation scenarios: when slice red is installed install slice blue workers should have vl3 routers from both slices":{"statistic":{"failed":1,"broken":0,"skipped":37,"passed":18,"unknown":0,"total":56},"items":[{"uid":"431a423a970f8a34","status":"passed","time":{"start":1664094053000,"stop":1664094055062,"duration":2062}},{"uid":"cfa0571a405ab1c5","status":"passed","time":{"start":1663843963000,"stop":1663843967122,"duration":4122}},{"uid":"a43bff9a0a7395ea","status":"passed","time":{"start":1663829327000,"stop":1663829328039,"duration":1039}},{"uid":"bdbb9adc7f91a3ed","status":"passed","time":{"start":1663343132000,"stop":1663343135147,"duration":3147}},{"uid":"66a2bf3fdfa7ef98","status":"passed","time":{"start":1663342947000,"stop":1663342949063,"duration":2063}},{"uid":"826a455eaa5f5621","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"a35b18eec1a32c16","status":"passed","time":{"start":1663044069000,"stop":1663044072121,"duration":3121}},{"uid":"4a7d6badbfafc152","status":"passed","time":{"start":1662348908000,"stop":1662348911131,"duration":3131}},{"uid":"609e5605c9a70b66","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"71482a91ff625ecd","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"111e92d2ef60a9cd","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"d1ee453a6614a6d8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"cfa47d3b8ea4e4a4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"52e015bc0afdef9f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"60389ccaccd1a34e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"6624b2dd9231b57d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"c78e5b225a17f576","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"1d16c0e27c4410cb","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"8629b3ff1550e495","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"97f1ae79f85deec5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Update Should fail when Project is applied with service account name as blank in Write users":{"statistic":{"failed":26,"broken":0,"skipped":0,"passed":190,"unknown":0,"total":216},"items":[{"uid":"27dda9c9b9937ba5","status":"passed","time":{"start":1664275319000,"stop":1664275323914,"duration":4914}},{"uid":"c58e889fa9a4c545","status":"passed","time":{"start":1664245203000,"stop":1664245208046,"duration":5046}},{"uid":"d3106af79ae1b20f","status":"passed","time":{"start":1664160732000,"stop":1664160736854,"duration":4854}},{"uid":"85e411f1c9cba087","status":"passed","time":{"start":1664091725000,"stop":1664091730864,"duration":5864}},{"uid":"6e6dc4e6476e01d8","status":"passed","time":{"start":1663841627000,"stop":1663841632146,"duration":5146}},{"uid":"b8e40b5ab27cb489","status":"passed","time":{"start":1663827045000,"stop":1663827050908,"duration":5908}},{"uid":"4c005d8479c24cff","status":"passed","time":{"start":1663767974000,"stop":1663767978872,"duration":4872}},{"uid":"83ce1a2f63f31e67","status":"passed","time":{"start":1663669677000,"stop":1663669681900,"duration":4900}},{"uid":"551a0cb01dab947f","status":"passed","time":{"start":1663665323000,"stop":1663665328065,"duration":5065}},{"uid":"9ba1659c6fa2e89b","status":"passed","time":{"start":1663665170000,"stop":1663665174983,"duration":4983}},{"uid":"3438603b2a453493","status":"passed","time":{"start":1663659067000,"stop":1663659072944,"duration":5944}},{"uid":"88dcb07610584806","status":"passed","time":{"start":1663656809000,"stop":1663656813923,"duration":4923}},{"uid":"897f6a002930fdae","status":"passed","time":{"start":1663583877000,"stop":1663583882106,"duration":5106}},{"uid":"e06d449a6653f94d","status":"passed","time":{"start":1663340950000,"stop":1663340954892,"duration":4892}},{"uid":"90e25f1ba33f2bc1","status":"passed","time":{"start":1663340650000,"stop":1663340655892,"duration":5892}},{"uid":"654131c1667e45a3","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <bool>: false\nto be true","time":{"start":1663308473000,"stop":1663308594385,"duration":121385}},{"uid":"d5c3d2614a9b09fa","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000824978>: {\n        Underlying: <*exec.ExitError | 0xc000751b20>{\n            ProcessState: {\n                pid: 6255,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 203402},\n                    Stime: {Sec: 0, Usec: 30702},\n                    Maxrss: 84132,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3516,\n                    Majflt: 2,\n                    Nswap: 0,\n                    Inblock: 360,\n                    Oublock: 13344,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 399,\n                    Nivcsw: 254,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20applied%20with%20service%20account%20name%20as%20blank%20in%20Write%20users3897995411\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.26.65:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20applied%20with%20service%20account%20name%20as%20blank%20in%20Write%20users3897995411\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.26.65:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20applied%20with%20service%20account%20name%20as%20blank%20in%20Write%20users3897995411\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.26.65:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20applied%20with%20service%20account%20name%20as%20blank%20in%20Write%20users3897995411\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.26.65:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20applied%20with%20service%20account%20name%20as%20blank%20in%20Write%20users3897995411\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.26.65:443: connect: connection refused\noccurred","time":{"start":1663304356000,"stop":1663304356315,"duration":315}},{"uid":"bc30a9d03f89875f","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00080b890>: {\n        Underlying: <*exec.ExitError | 0xc00085b4c0>{\n            ProcessState: {\n                pid: 7587,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 156092},\n                    Stime: {Sec: 0, Usec: 48028},\n                    Maxrss: 88572,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 7548,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 362,\n                    Nivcsw: 304,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20applied%20with%20service%20account%20name%20as%20blank%20in%20Write%20users1565178828\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.179.221:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20applied%20with%20service%20account%20name%20as%20blank%20in%20Write%20users1565178828\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.179.221:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20applied%20with%20service%20account%20name%20as%20blank%20in%20Write%20users1565178828\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.179.221:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20applied%20with%20service%20account%20name%20as%20blank%20in%20Write%20users1565178828\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.179.221:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20applied%20with%20service%20account%20name%20as%20blank%20in%20Write%20users1565178828\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.179.221:443: connect: connection refused\noccurred","time":{"start":1663302519000,"stop":1663302519267,"duration":267}},{"uid":"7267aa2c31a1090d","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00038f920>: {\n        Underlying: <*exec.ExitError | 0xc0003e3fa0>{\n            ProcessState: {\n                pid: 7226,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 185848},\n                    Stime: {Sec: 0, Usec: 57524},\n                    Maxrss: 87368,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3921,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13344,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 268,\n                    Nivcsw: 171,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20applied%20with%20service%20account%20name%20as%20blank%20in%20Write%20users1362596297\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.55.244:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20applied%20with%20service%20account%20name%20as%20blank%20in%20Write%20users1362596297\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.55.244:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20applied%20with%20service%20account%20name%20as%20blank%20in%20Write%20users1362596297\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.55.244:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20applied%20with%20service%20account%20name%20as%20blank%20in%20Write%20users1362596297\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.55.244:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20applied%20with%20service%20account%20name%20as%20blank%20in%20Write%20users1362596297\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.55.244:443: connect: connection refused\noccurred","time":{"start":1663264756000,"stop":1663264756314,"duration":314}},{"uid":"62d20d3bfd7a958","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00084a9d8>: {\n        Underlying: <*exec.ExitError | 0xc0005a24c0>{\n            ProcessState: {\n                pid: 7267,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 200793},\n                    Stime: {Sec: 0, Usec: 45076},\n                    Maxrss: 81808,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3705,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13344,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 384,\n                    Nivcsw: 257,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20applied%20with%20service%20account%20name%20as%20blank%20in%20Write%20users877498945\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.221.241:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20applied%20with%20service%20account%20name%20as%20blank%20in%20Write%20users877498945\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.221.241:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20applied%20with%20service%20account%20name%20as%20blank%20in%20Write%20users877498945\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.221.241:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20applied%20with%20service%20account%20name%20as%20blank%20in%20Write%20users877498945\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.221.241:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20applied%20with%20service%20account%20name%20as%20blank%20in%20Write%20users877498945\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.221.241:443: connect: connection refused\noccurred","time":{"start":1663228190000,"stop":1663228190281,"duration":281}}]},"cb52420756dd2307f39e260d201ab579":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"8caaf777cf15b6ea","status":"passed","time":{"start":1663840053482,"stop":1663840069171,"duration":15689}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Deletion Should fail when deleting a project that does not exist":{"statistic":{"failed":24,"broken":0,"skipped":0,"passed":192,"unknown":0,"total":216},"items":[{"uid":"8da5b4325980d709","status":"passed","time":{"start":1664275319000,"stop":1664275326803,"duration":7803}},{"uid":"3625608bf744dd52","status":"passed","time":{"start":1664245203000,"stop":1664245211745,"duration":8745}},{"uid":"8a65fe49621ddb8a","status":"passed","time":{"start":1664160732000,"stop":1664160740531,"duration":8531}},{"uid":"ae61cf68ea57b5f6","status":"passed","time":{"start":1664091725000,"stop":1664091733444,"duration":8444}},{"uid":"fc7d929a5d801e57","status":"passed","time":{"start":1663841627000,"stop":1663841635918,"duration":8918}},{"uid":"db08bf60e8777a18","status":"passed","time":{"start":1663827045000,"stop":1663827053450,"duration":8450}},{"uid":"19bf5bfc46b5a3e7","status":"passed","time":{"start":1663767974000,"stop":1663767989751,"duration":15751}},{"uid":"437f95df28e03e97","status":"passed","time":{"start":1663669677000,"stop":1663669685480,"duration":8480}},{"uid":"565ab66ef83a1231","status":"passed","time":{"start":1663665323000,"stop":1663665330993,"duration":7993}},{"uid":"63e4a4346fd3571f","status":"passed","time":{"start":1663665170000,"stop":1663665177814,"duration":7814}},{"uid":"2ff7e005bb3cce0d","status":"passed","time":{"start":1663659067000,"stop":1663659083764,"duration":16764}},{"uid":"fbc567f62ae17cf7","status":"passed","time":{"start":1663656809000,"stop":1663656816844,"duration":7844}},{"uid":"fdf15d041ed873ef","status":"passed","time":{"start":1663583877000,"stop":1663583884951,"duration":7951}},{"uid":"f799ef1c71bd361","status":"passed","time":{"start":1663340950000,"stop":1663340965520,"duration":15520}},{"uid":"8836df2b3fa9af34","status":"passed","time":{"start":1663340650000,"stop":1663340658426,"duration":8426}},{"uid":"cddf42063f96b175","status":"failed","statusDetails":"Timed out after 60.073s.\nExpected\n    <bool>: false\nto be true","time":{"start":1663308473000,"stop":1663308547847,"duration":74847}},{"uid":"c841d369798f0f82","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000824ae0>: {\n        Underlying: <*exec.ExitError | 0xc000472100>{\n            ProcessState: {\n                pid: 6265,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 194627},\n                    Stime: {Sec: 0, Usec: 34345},\n                    Maxrss: 84840,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3135,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13344,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 296,\n                    Nivcsw: 271,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Deletion%20Should%20fail%20when%20deleting%20a%20project%20that%20does%20not%20exist1795824715\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.26.65:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Deletion%20Should%20fail%20when%20deleting%20a%20project%20that%20does%20not%20exist1795824715\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.26.65:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Deletion%20Should%20fail%20when%20deleting%20a%20project%20that%20does%20not%20exist1795824715\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.26.65:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Deletion%20Should%20fail%20when%20deleting%20a%20project%20that%20does%20not%20exist1795824715\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.26.65:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Deletion%20Should%20fail%20when%20deleting%20a%20project%20that%20does%20not%20exist1795824715\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.26.65:443: connect: connection refused\noccurred","time":{"start":1663304356000,"stop":1663304356285,"duration":285}},{"uid":"23f2515900d6b6b","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00080a708>: {\n        Underlying: <*exec.ExitError | 0xc00079f500>{\n            ProcessState: {\n                pid: 7596,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 171567},\n                    Stime: {Sec: 0, Usec: 46791},\n                    Maxrss: 74068,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4401,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 368,\n                    Nivcsw: 290,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Deletion%20Should%20fail%20when%20deleting%20a%20project%20that%20does%20not%20exist2219834650\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.179.221:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Deletion%20Should%20fail%20when%20deleting%20a%20project%20that%20does%20not%20exist2219834650\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.179.221:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Deletion%20Should%20fail%20when%20deleting%20a%20project%20that%20does%20not%20exist2219834650\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.179.221:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Deletion%20Should%20fail%20when%20deleting%20a%20project%20that%20does%20not%20exist2219834650\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.179.221:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Deletion%20Should%20fail%20when%20deleting%20a%20project%20that%20does%20not%20exist2219834650\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.179.221:443: connect: connection refused\noccurred","time":{"start":1663302519000,"stop":1663302519254,"duration":254}},{"uid":"c1e08662a9dde6c1","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <bool>: false\nto be true","time":{"start":1663264756000,"stop":1663264832574,"duration":76574}},{"uid":"a6f168f87f0624fe","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <bool>: false\nto be true","time":{"start":1663228190000,"stop":1663228258345,"duration":68345}}]},"f9f4c00f1e1ec49abb73692ba86f0a96":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"6341ce177b216a23","status":"passed","time":{"start":1663839021084,"stop":1663839051532,"duration":30448}}]},"Worker Suite:Worker Suite#[It] Hub netpol scenarios: Slice Configs Netpol scenarios Should successfully deploy slice for valid namespace and valid clusters in allowedNamespaces of sliceconfigs manifest":{"statistic":{"failed":9,"broken":0,"skipped":14,"passed":33,"unknown":0,"total":56},"items":[{"uid":"355b2ee9d314f190","status":"passed","time":{"start":1664094053000,"stop":1664094053496,"duration":496}},{"uid":"9a2b5480a43dfcd7","status":"passed","time":{"start":1663843963000,"stop":1663843963659,"duration":659}},{"uid":"965cc5e828e5d629","status":"passed","time":{"start":1663829327000,"stop":1663829327533,"duration":533}},{"uid":"718ef4aec3857955","status":"passed","time":{"start":1663343132000,"stop":1663343132407,"duration":407}},{"uid":"24206bb851ece85e","status":"passed","time":{"start":1663342947000,"stop":1663342947563,"duration":563}},{"uid":"528fbd531717aadd","status":"skipped","statusDetails":"skipped","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"c7a0e30610d4f762","status":"passed","time":{"start":1663044069000,"stop":1663044069783,"duration":783}},{"uid":"180178e2c5d1e752","status":"passed","time":{"start":1662348908000,"stop":1662348909101,"duration":1101}},{"uid":"66660a5e2a5eba5d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"5669a6e840dcf9e1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"de97ff44621bde80","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"a15a8fd746b67a09","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"358e25373fbebb63","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"48c7e95ac2ae349a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"c3bda7178d792c8d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"8820f3af6ec50079","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"49271970c0f4e59a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"78d9515ea7494cc9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"bda88e86652e4d4c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"400ce18d643c9e5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Update Should fail when Project is Applied with service account name as combination of special characters in Write users":{"statistic":{"failed":26,"broken":0,"skipped":0,"passed":190,"unknown":0,"total":216},"items":[{"uid":"f6babb17d3f3823","status":"passed","time":{"start":1664275319000,"stop":1664275323892,"duration":4892}},{"uid":"7c99dcbbbc0d27ba","status":"passed","time":{"start":1664245203000,"stop":1664245208028,"duration":5028}},{"uid":"15d0799edf231403","status":"passed","time":{"start":1664160732000,"stop":1664160736863,"duration":4863}},{"uid":"cd691db49c10e7d1","status":"passed","time":{"start":1664091725000,"stop":1664091729837,"duration":4837}},{"uid":"c1b9608b8bc75616","status":"passed","time":{"start":1663841627000,"stop":1663841633141,"duration":6141}},{"uid":"82e4339ac2fe541f","status":"passed","time":{"start":1663827045000,"stop":1663827049900,"duration":4900}},{"uid":"39e35704bc0b1e0d","status":"passed","time":{"start":1663767974000,"stop":1663767978875,"duration":4875}},{"uid":"b7133a9c7a79e0f0","status":"passed","time":{"start":1663669677000,"stop":1663669681876,"duration":4876}},{"uid":"aa9d839c6924d291","status":"passed","time":{"start":1663665323000,"stop":1663665329107,"duration":6107}},{"uid":"9390a105529ae2d9","status":"passed","time":{"start":1663665170000,"stop":1663665176015,"duration":6015}},{"uid":"2bc4bb4ca0618e44","status":"passed","time":{"start":1663659067000,"stop":1663659071957,"duration":4957}},{"uid":"e8a4352d7d864e70","status":"passed","time":{"start":1663656809000,"stop":1663656814994,"duration":5994}},{"uid":"9e79890de5314975","status":"passed","time":{"start":1663583877000,"stop":1663583882207,"duration":5207}},{"uid":"98d291cb5d2ddd8d","status":"passed","time":{"start":1663340950000,"stop":1663340955836,"duration":5836}},{"uid":"4d927fb0cd1f3d88","status":"passed","time":{"start":1663340650000,"stop":1663340654918,"duration":4918}},{"uid":"69318627c6ad3bc","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <bool>: false\nto be true","time":{"start":1663308473000,"stop":1663308539777,"duration":66777}},{"uid":"3432f9535dd9cfac","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0003a4678>: {\n        Underlying: <*exec.ExitError | 0xc0007d17c0>{\n            ProcessState: {\n                pid: 6246,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 183720},\n                    Stime: {Sec: 0, Usec: 45930},\n                    Maxrss: 81348,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2956,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13344,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 250,\n                    Nivcsw: 456,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Write%20users2393763096\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.26.65:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Write%20users2393763096\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.26.65:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Write%20users2393763096\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.26.65:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Write%20users2393763096\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.26.65:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Write%20users2393763096\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.26.65:443: connect: connection refused\noccurred","time":{"start":1663304356000,"stop":1663304356281,"duration":281}},{"uid":"815fae78a61aaa3c","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000642018>: {\n        Underlying: <*exec.ExitError | 0xc0007570e0>{\n            ProcessState: {\n                pid: 7578,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 159867},\n                    Stime: {Sec: 0, Usec: 40991},\n                    Maxrss: 83136,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 5541,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 422,\n                    Nivcsw: 241,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Write%20users2731519535\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.179.221:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Write%20users2731519535\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.179.221:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Write%20users2731519535\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.179.221:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Write%20users2731519535\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.179.221:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Write%20users2731519535\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.179.221:443: connect: connection refused\noccurred","time":{"start":1663302519000,"stop":1663302519258,"duration":258}},{"uid":"69fd77284fc1e4d2","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00038e780>: {\n        Underlying: <*exec.ExitError | 0xc0007ee140>{\n            ProcessState: {\n                pid: 7217,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 215507},\n                    Stime: {Sec: 0, Usec: 43101},\n                    Maxrss: 84584,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4388,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13344,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 524,\n                    Nivcsw: 465,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Write%20users1902255276\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.55.244:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Write%20users1902255276\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.55.244:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Write%20users1902255276\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.55.244:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Write%20users1902255276\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.55.244:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Write%20users1902255276\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.55.244:443: connect: connection refused\noccurred","time":{"start":1663264756000,"stop":1663264756300,"duration":300}},{"uid":"b87e3c81ace2eb9","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00084a6c0>: {\n        Underlying: <*exec.ExitError | 0xc0003dda00>{\n            ProcessState: {\n                pid: 7258,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 162376},\n                    Stime: {Sec: 0, Usec: 52772},\n                    Maxrss: 85352,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3277,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13344,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 319,\n                    Nivcsw: 248,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Write%20users4075903970\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.221.241:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Write%20users4075903970\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.221.241:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Write%20users4075903970\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.221.241:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Write%20users4075903970\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.221.241:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Write%20users4075903970\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.221.241:443: connect: connection refused\noccurred","time":{"start":1663228190000,"stop":1663228190288,"duration":288}}]},"29fad7d4b367ba294a72bedd326378e4":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"34d00d7deb6941c0","status":"passed","time":{"start":1663841720004,"stop":1663841742750,"duration":22746}}]},"b510c86f9cadcf5c4ef6c4fe66e6e2ea":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"d28da6475a284d0","status":"passed","time":{"start":1663839995575,"stop":1663840004631,"duration":9056}}]},"ce4f8505772fa840a116d8242cc53f66":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"3200b776bec7d271","status":"passed","time":{"start":1663838575979,"stop":1663838588706,"duration":12727}}]},"54f34edb8f9e63d34643bc7a9d633870":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"972586736624f9cc","status":"passed","time":{"start":1663841608964,"stop":1663841636682,"duration":27718}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting nsm-kernel-forwarder pod Iperf cleanup":{"statistic":{"failed":0,"broken":0,"skipped":49,"passed":0,"unknown":0,"total":49},"items":[{"uid":"da1d6dbb87696050","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662348908000,"stop":1662348908000,"duration":0}},{"uid":"8fec16d306a0857f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"a3532def6f931738","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"f576622de0198209","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"3b3ed979748cc93f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"578a0569b89d5be9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"64f13ff87aaa8a65","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"f28514e4e1d2d49f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"a1e45a0e4b3c3fd5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"5e74417ff37a92ee","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"b5f1e5596a991cf5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"44569541dcf147a3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"11fda8d81cec246a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}},{"uid":"f731ca1e4707a20d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660293657000,"stop":1660293657000,"duration":0}},{"uid":"f44600f22877f25","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660113628000,"stop":1660113628000,"duration":0}},{"uid":"52f42cba52cf9c10","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660104898000,"stop":1660104898000,"duration":0}},{"uid":"c69073c6064b5f56","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660067259000,"stop":1660067259000,"duration":0}},{"uid":"5c14f502ae6b7d0f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660047551000,"stop":1660047551000,"duration":0}},{"uid":"76aba77214a9e69f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659982549000,"stop":1659982549000,"duration":0}},{"uid":"8dc20c5bb402487f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659970182000,"stop":1659970182000,"duration":0}}]},"481ffbc2e98016d7b444019ecade352":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"9ab5f7912d2caeea","status":"passed","time":{"start":1663840889694,"stop":1663840910645,"duration":20951}}]},"Empty Suite:Empty Suite#[AfterSuite]":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":248,"unknown":0,"total":248},"items":[{"uid":"fd840052461ce085","status":"passed","time":{"start":1664282572000,"stop":1664282572000,"duration":0}},{"uid":"4180ee1bb4bc25fd","status":"passed","time":{"start":1664279225000,"stop":1664279225000,"duration":0}},{"uid":"d0878bd160f97e4e","status":"passed","time":{"start":1664277805000,"stop":1664277805000,"duration":0}},{"uid":"7f5c1ba90f19e686","status":"passed","time":{"start":1664275170000,"stop":1664275170000,"duration":0}},{"uid":"a815c898dd03966d","status":"passed","time":{"start":1664275319000,"stop":1664275319000,"duration":0}},{"uid":"9b722c69fe964a34","status":"passed","time":{"start":1664269661000,"stop":1664269661000,"duration":0}},{"uid":"9b5e1938a36acb02","status":"passed","time":{"start":1664261300000,"stop":1664261300000,"duration":0}},{"uid":"f5ce1b832a60a7de","status":"passed","time":{"start":1664245049000,"stop":1664245049000,"duration":0}},{"uid":"613420d285d1a4c0","status":"passed","time":{"start":1664218010000,"stop":1664218010000,"duration":0}},{"uid":"a8bebf5f3c4f9f19","status":"passed","time":{"start":1664160583000,"stop":1664160583000,"duration":0}},{"uid":"3738f020df027e9f","status":"passed","time":{"start":1664105948000,"stop":1664105948000,"duration":0}},{"uid":"8157c7c73b08e125","status":"passed","time":{"start":1664103959000,"stop":1664103959000,"duration":0}},{"uid":"53b2deac9ec0378a","status":"passed","time":{"start":1664091567000,"stop":1664091567000,"duration":0}},{"uid":"554b68eb0685f58a","status":"passed","time":{"start":1663932902000,"stop":1663932902000,"duration":0}},{"uid":"1cba2ab460589188","status":"passed","time":{"start":1663917718000,"stop":1663917718000,"duration":0}},{"uid":"49c80d6a6fa7378","status":"passed","time":{"start":1663912384000,"stop":1663912384000,"duration":0}},{"uid":"1d37e22066d70dcc","status":"passed","time":{"start":1663841452000,"stop":1663841452000,"duration":0}},{"uid":"d72498b9365ddd6","status":"passed","time":{"start":1663838862000,"stop":1663838862000,"duration":0}},{"uid":"d8881a2089271d8d","status":"passed","time":{"start":1663838333000,"stop":1663838333000,"duration":0}},{"uid":"7ca80dd860650d70","status":"passed","time":{"start":1663836341000,"stop":1663836341000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice Config Negative Scenario tests Slice Config Negative Scenario Update Should update while deploying sliceconfig with existing slice name":{"statistic":{"failed":13,"broken":0,"skipped":1,"passed":42,"unknown":0,"total":56},"items":[{"uid":"aa6be68aa0def492","status":"passed","time":{"start":1664094053000,"stop":1664094053677,"duration":677}},{"uid":"b40cebd2d3e154a5","status":"passed","time":{"start":1663843963000,"stop":1663843964009,"duration":1009}},{"uid":"987ce4a18ddbf3e4","status":"passed","time":{"start":1663829327000,"stop":1663829327766,"duration":766}},{"uid":"3c669e783329ee54","status":"passed","time":{"start":1663343132000,"stop":1663343133028,"duration":1028}},{"uid":"6f40a102f5f23b5b","status":"passed","time":{"start":1663342947000,"stop":1663342947832,"duration":832}},{"uid":"4745c44492f4d0f8","status":"passed","time":{"start":1663044723000,"stop":1663044723721,"duration":721}},{"uid":"bb6ca02c8e23b72f","status":"skipped","statusDetails":"skipped","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"32cddebe910da9ed","status":"passed","time":{"start":1662348908000,"stop":1662348908804,"duration":804}},{"uid":"2f5b64ea780b3837","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000ecf0>: {\n        Underlying: <*exec.ExitError | 0xc00067e6a0>{\n            ProcessState: {\n                pid: 7128,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 229406},\n                    Stime: {Sec: 0, Usec: 75078},\n                    Maxrss: 86248,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 8201,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 439,\n                    Nivcsw: 359,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"fireredslice\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\noccurred","time":{"start":1661844205000,"stop":1661844205407,"duration":407}},{"uid":"5d4ae9cd1dcaef6f","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004bef00>: {\n        Underlying: <*exec.ExitError | 0xc000659700>{\n            ProcessState: {\n                pid: 7265,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 178046},\n                    Stime: {Sec: 0, Usec: 33124},\n                    Maxrss: 86976,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3437,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 367,\n                    Nivcsw: 390,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"fireredslice\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661754870000,"stop":1661754870266,"duration":266}},{"uid":"ffed4af211a6727b","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0006f0ca8>: {\n        Underlying: <*exec.ExitError | 0xc0006a8d20>{\n            ProcessState: {\n                pid: 7083,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 214348},\n                    Stime: {Sec: 0, Usec: 65953},\n                    Maxrss: 74460,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2742,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 424,\n                    Nivcsw: 816,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"fireredslice\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661611962000,"stop":1661611962442,"duration":442}},{"uid":"db4728768904462","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00090d998>: {\n        Underlying: <*exec.ExitError | 0xc000479420>{\n            ProcessState: {\n                pid: 7291,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 188629},\n                    Stime: {Sec: 0, Usec: 26947},\n                    Maxrss: 83616,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 5007,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 374,\n                    Nivcsw: 414,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"fireredslice\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661585077000,"stop":1661585077270,"duration":270}},{"uid":"ad1ae62b1c879ca3","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00013bc38>: {\n        Underlying: <*exec.ExitError | 0xc0006ab9c0>{\n            ProcessState: {\n                pid: 6228,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 185338},\n                    Stime: {Sec: 0, Usec: 65640},\n                    Maxrss: 84236,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4067,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 515,\n                    Nivcsw: 556,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"fireredslice\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661584447000,"stop":1661584447433,"duration":433}},{"uid":"8b18cc2dd6ddd8c1","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0006d2468>: {\n        Underlying: <*exec.ExitError | 0xc000074c60>{\n            ProcessState: {\n                pid: 7277,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 159232},\n                    Stime: {Sec: 0, Usec: 51750},\n                    Maxrss: 84320,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3710,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 278,\n                    Nivcsw: 476,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"fireredslice\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661580045000,"stop":1661580045271,"duration":271}},{"uid":"c0eed808589e7219","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0007793f8>: {\n        Underlying: <*exec.ExitError | 0xc000075900>{\n            ProcessState: {\n                pid: 7268,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 236804},\n                    Stime: {Sec: 0, Usec: 42286},\n                    Maxrss: 94616,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3820,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 418,\n                    Nivcsw: 240,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"fireredslice\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661578505000,"stop":1661578505349,"duration":349}},{"uid":"d5ecffbdb9c9e181","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0008a3320>: {\n        Underlying: <*exec.ExitError | 0xc0003a6740>{\n            ProcessState: {\n                pid: 7297,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 212930},\n                    Stime: {Sec: 0, Usec: 70976},\n                    Maxrss: 90156,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3854,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 409,\n                    Nivcsw: 348,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"fireredslice\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661572479000,"stop":1661572479336,"duration":336}},{"uid":"98f600ff7bac110d","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0006eafc0>: {\n        Underlying: <*exec.ExitError | 0xc0008c9f80>{\n            ProcessState: {\n                pid: 7305,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 171516},\n                    Stime: {Sec: 0, Usec: 47440},\n                    Maxrss: 82808,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4687,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 395,\n                    Nivcsw: 346,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"fireredslice\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660832281000,"stop":1660832281316,"duration":316}},{"uid":"98e8309932e9a69d","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000989a70>: {\n        Underlying: <*exec.ExitError | 0xc000711960>{\n            ProcessState: {\n                pid: 7197,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 157560},\n                    Stime: {Sec: 0, Usec: 24240},\n                    Maxrss: 85672,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3933,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 490,\n                    Nivcsw: 221,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"fireredslice\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660808797000,"stop":1660808797263,"duration":263}},{"uid":"cc42e741da377427","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00066c150>: {\n        Underlying: <*exec.ExitError | 0xc0007e03a0>{\n            ProcessState: {\n                pid: 7100,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 148423},\n                    Stime: {Sec: 0, Usec: 42406},\n                    Maxrss: 79908,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 5968,\n                    Majflt: 1,\n                    Nswap: 0,\n                    Inblock: 256,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 463,\n                    Nivcsw: 352,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"fireredslice\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660299182000,"stop":1660299182252,"duration":252}},{"uid":"1f9f92f7f1e1c193","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00013b410>: {\n        Underlying: <*exec.ExitError | 0xc0008192c0>{\n            ProcessState: {\n                pid: 7141,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 149363},\n                    Stime: {Sec: 0, Usec: 35144},\n                    Maxrss: 80492,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 5329,\n                    Majflt: 3,\n                    Nswap: 0,\n                    Inblock: 648,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 494,\n                    Nivcsw: 204,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"fireredslice\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660295740000,"stop":1660295740251,"duration":251}}]},"c139608c621ab7d86b32ec4b9529bf14":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"2c8a90e9cce4e8c0","status":"passed","time":{"start":1663838746190,"stop":1663838760845,"duration":14655}}]},"9576d9434fb857412e37854686274a1c":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"4f65d2970986c5f7","status":"passed","time":{"start":1663839051533,"stop":1663839058007,"duration":6474}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests Iperf App connectivity Should install iperf-server on server cluster":{"statistic":{"failed":0,"broken":0,"skipped":48,"passed":8,"unknown":0,"total":56},"items":[{"uid":"a8d6bcf1989c85e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1664094053000,"stop":1664094053000,"duration":0}},{"uid":"ea2a28ca2fba3ba5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663843963000,"stop":1663843963000,"duration":0}},{"uid":"958b9fb9f4d67c6d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663829327000,"stop":1663829327000,"duration":0}},{"uid":"287f8eec07ba32b5","status":"passed","time":{"start":1663343132000,"stop":1663343143992,"duration":11992}},{"uid":"cfd970c5615c7cd8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663342947000,"stop":1663342947000,"duration":0}},{"uid":"e44f9f93340c8290","status":"skipped","statusDetails":"skipped","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"29142957967f879d","status":"skipped","statusDetails":"skipped","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"3f0a21ea5ddea009","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662348908000,"stop":1662348908000,"duration":0}},{"uid":"ffb4c9e037f8ea2d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"a15066b5f8011c70","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"a777065c1d5dab8b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"cbc3f0cd28e96bfa","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"956af9a92fe2a27","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"8bf227ff49255b1b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"df599ec35a9a24f3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"5e2801a4267faab5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"7721835a6b01244c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"6d5da9b9ff2f0faf","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"2a7fce59b0c9ebd5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"bbb3aa694ee8e679","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests slice with netpol should remove network policy resource from application namespace once the isolationEnabled is set to false":{"statistic":{"failed":0,"broken":3,"skipped":40,"passed":13,"unknown":0,"total":56},"items":[{"uid":"ac4533a8926e982f","status":"passed","time":{"start":1664094053000,"stop":1664094068422,"duration":15422}},{"uid":"25db3eac50850771","status":"passed","time":{"start":1663843963000,"stop":1663843977040,"duration":14040}},{"uid":"28882e36469f4ea1","status":"passed","time":{"start":1663829327000,"stop":1663829342533,"duration":15533}},{"uid":"7535736dc15703f5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663343132000,"stop":1663343132000,"duration":0}},{"uid":"a6c9be0be61bbf0c","status":"passed","time":{"start":1663342947000,"stop":1663342963056,"duration":16056}},{"uid":"764c8ebb37a46f0c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"891661ce0562d5a","status":"passed","time":{"start":1663044069000,"stop":1663044085248,"duration":16248}},{"uid":"b654d765334f487f","status":"passed","time":{"start":1662348908000,"stop":1662348921419,"duration":13419}},{"uid":"903e8b36b7837756","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"18aac04a797e5a82","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"2e593b4d6954a0e0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"57bdae6a663bdfed","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"fb08291a59dd19a1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"dd8a67e62d0d5e71","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"28c76fe3861e742","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"a71e9ac144237f81","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"9a5f5c84baefa5e4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"fc298b6286f3394a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"49819d8bc146893","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"5f78d7a5d06f9620","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}}]},"9ef73bc37e2f6648278e0ca72db06049":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"36d4651d81b6ff31","status":"passed","time":{"start":1663839387856,"stop":1663839400955,"duration":13099}}]},"bacdcda0478aef0109a01e50fc6c768a":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"5df159bfc81df9f3","status":"passed","time":{"start":1663843384621,"stop":1663843424913,"duration":40292}}]},"ef99b5b2afee8f2db66ea7ac17ebbc8d":{"statistic":{"failed":1,"broken":0,"skipped":0,"passed":0,"unknown":0,"total":1},"items":[{"uid":"8b24e73bffab48e0","status":"failed","statusDetails":"Timed out retrying after 30000ms: expected '<table.MuiTable-root.MuiTable-stickyHeader.sc-eXlEPa.gQjEgO.css-lkld8c>' to have text 'slice-automation-460', but the text was 'Cluster  NameSlice GatewayAPP Podspreprod-worker-1slice-automation-460-preprod-worker-1-preprod-worker-20preprod-worker-2slice-automation-460-preprod-worker-2-preprod-worker-10'","time":{"start":1663842315917,"stop":1663842507582,"duration":191665}}]},"Worker Suite:Worker Suite#[It] Slice validation Slice with LOW QOS should have gateway pod running":{"statistic":{"failed":1,"broken":0,"skipped":2,"passed":6,"unknown":0,"total":9},"items":[{"uid":"37109e6a1fcf8e95","status":"passed","time":{"start":1659168739000,"stop":1659168769082,"duration":30082}},{"uid":"174f63e4d95b0be8","status":"passed","time":{"start":1659164084000,"stop":1659164109099,"duration":25099}},{"uid":"a62f8935b4aa5bb8","status":"passed","time":{"start":1659160188000,"stop":1659160208045,"duration":20045}},{"uid":"b0ef192b3b3a9205","status":"passed","time":{"start":1659119724000,"stop":1659119749068,"duration":25068}},{"uid":"8e90a32bd8a8113b","status":"passed","time":{"start":1659116511000,"stop":1659116511012,"duration":12}},{"uid":"f41770fc4484150","status":"passed","time":{"start":1659109470000,"stop":1659109470028,"duration":28}},{"uid":"8ad77caf63959cf7","status":"failed","statusDetails":"Timed out after 180.004s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1659106836000,"stop":1659107016285,"duration":180285}},{"uid":"17f1dcc7e00985cc","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659101522000,"stop":1659101522000,"duration":0}},{"uid":"6423839b465e949a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659082816000,"stop":1659082816000,"duration":0}}]},"1849819f9055fb66ed51b1d697fd42a6":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"ee54990dfdad8ed4","status":"passed","time":{"start":1663838731539,"stop":1663838746189,"duration":14650}}]},"33dab19bb5f54b910c50ff02f151c73a":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"52f975921b41786d","status":"passed","time":{"start":1663841914225,"stop":1663841949285,"duration":35060}}]},"84ebb533001ff055fc79cec3f34a2dfc":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"b687052cbbc2627f","status":"passed","time":{"start":1663840408258,"stop":1663840421768,"duration":13510}}]},"Hub Suite:Hub Suite#[It] Cluster negative tests Worker Cluster Registration with Wrong endpoint":{"statistic":{"failed":37,"broken":0,"skipped":0,"passed":182,"unknown":0,"total":219},"items":[{"uid":"f0e914506294a06b","status":"failed","statusDetails":"Timed out after 120.000s.\nExpected an error to have occurred.  Got:\n    <nil>: nil","time":{"start":1664275319000,"stop":1664275958079,"duration":639079}},{"uid":"3f96352a10f2cbfd","status":"passed","time":{"start":1664245203000,"stop":1664245518600,"duration":315600}},{"uid":"a2296c167073b5d1","status":"passed","time":{"start":1664160732000,"stop":1664161053171,"duration":321171}},{"uid":"d0bef82ae46d5a1e","status":"passed","time":{"start":1664091725000,"stop":1664092044144,"duration":319144}},{"uid":"e15d5e88456bd443","status":"passed","time":{"start":1663841627000,"stop":1663841944202,"duration":317202}},{"uid":"ce50f031ca915141","status":"passed","time":{"start":1663827045000,"stop":1663827360014,"duration":315014}},{"uid":"6517dc3c4b6ab24d","status":"passed","time":{"start":1663767974000,"stop":1663768293638,"duration":319638}},{"uid":"1ef358b47ddf135a","status":"passed","time":{"start":1663669677000,"stop":1663669997169,"duration":320169}},{"uid":"e0d0afbdd3835b98","status":"passed","time":{"start":1663665323000,"stop":1663665641414,"duration":318414}},{"uid":"92c754784ca9e852","status":"passed","time":{"start":1663665170000,"stop":1663665484903,"duration":314903}},{"uid":"42e16c5d56d040","status":"passed","time":{"start":1663659067000,"stop":1663659383377,"duration":316377}},{"uid":"c15b09e25067979","status":"passed","time":{"start":1663656809000,"stop":1663657126439,"duration":317439}},{"uid":"8cb4876d522f90cb","status":"passed","time":{"start":1663583877000,"stop":1663584192884,"duration":315884}},{"uid":"9d31f822754b6834","status":"passed","time":{"start":1663340950000,"stop":1663341265066,"duration":315066}},{"uid":"3ee69db4112786ad","status":"passed","time":{"start":1663340650000,"stop":1663340965263,"duration":315263}},{"uid":"f229a9f2a4a88858","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000e6a8>: {\n        Underlying: <*exec.ExitError | 0xc00071a380>{\n            ProcessState: {\n                pid: 11827,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 216057},\n                    Stime: {Sec: 0, Usec: 61730},\n                    Maxrss: 72364,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 8160,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 395,\n                    Nivcsw: 435,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Cluster%20negative%20tests%20Worker%20Cluster%20Registration%20with%20Wrong%20endpoint1069816193\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.135.179:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Cluster%20negative%20tests%20Worker%20Cluster%20Registration%20with%20Wrong%20endpoint1069816193\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.135.179:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Cluster%20negative%20tests%20Worker%20Cluster%20Registration%20with%20Wrong%20endpoint1069816193\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.135.179:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Cluster%20negative%20tests%20Worker%20Cluster%20Registration%20with%20Wrong%20endpoint1069816193\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.135.179:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Cluster%20negative%20tests%20Worker%20Cluster%20Registration%20with%20Wrong%20endpoint1069816193\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.135.179:443: connect: connection refused\noccurred","time":{"start":1663308473000,"stop":1663308473544,"duration":544}},{"uid":"f1a23ccb17f90e9f","status":"passed","time":{"start":1663304356000,"stop":1663304671595,"duration":315595}},{"uid":"462a74c93552ab8e","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00080b008>: {\n        Underlying: <*exec.ExitError | 0xc0006a9e20>{\n            ProcessState: {\n                pid: 7624,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 171104},\n                    Stime: {Sec: 0, Usec: 34220},\n                    Maxrss: 86064,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 5731,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 223,\n                    Nivcsw: 295,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Cluster%20negative%20tests%20Worker%20Cluster%20Registration%20with%20Wrong%20endpoint920474558\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.179.221:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Cluster%20negative%20tests%20Worker%20Cluster%20Registration%20with%20Wrong%20endpoint920474558\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.179.221:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Cluster%20negative%20tests%20Worker%20Cluster%20Registration%20with%20Wrong%20endpoint920474558\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.179.221:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Cluster%20negative%20tests%20Worker%20Cluster%20Registration%20with%20Wrong%20endpoint920474558\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.179.221:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Cluster%20negative%20tests%20Worker%20Cluster%20Registration%20with%20Wrong%20endpoint920474558\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.179.221:443: connect: connection refused\noccurred","time":{"start":1663302519000,"stop":1663302519402,"duration":402}},{"uid":"af20ea2b43c7e286","status":"passed","time":{"start":1663264756000,"stop":1663265071673,"duration":315673}},{"uid":"4664d271bfd1b601","status":"passed","time":{"start":1663228190000,"stop":1663228508055,"duration":318055}}]},"a5a24448fa241624f11db4d89db11dd4":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"c457d6c72e53cbad","status":"passed","time":{"start":1663840347196,"stop":1663840356878,"duration":9682}}]},"Worker Suite:Worker Suite#[It] Hub netpol scenarios: Slice Configs Netpol scenarios Should successfully deploy for valid namespace creating clusters with * in both applicationNamespace and allowedNamespace":{"statistic":{"failed":0,"broken":0,"skipped":23,"passed":33,"unknown":0,"total":56},"items":[{"uid":"d5935e562919f97a","status":"passed","time":{"start":1664094053000,"stop":1664094053558,"duration":558}},{"uid":"63001d4321d36f4f","status":"passed","time":{"start":1663843963000,"stop":1663843963705,"duration":705}},{"uid":"18fe286ff89b0d12","status":"passed","time":{"start":1663829327000,"stop":1663829327743,"duration":743}},{"uid":"bca61b7313d9e240","status":"passed","time":{"start":1663343132000,"stop":1663343132682,"duration":682}},{"uid":"c1fe223be9f1436a","status":"passed","time":{"start":1663342947000,"stop":1663342947540,"duration":540}},{"uid":"55242c1d27494a23","status":"skipped","statusDetails":"skipped","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"33ed6559abdb3aa8","status":"passed","time":{"start":1663044069000,"stop":1663044069772,"duration":772}},{"uid":"21d3a1888f7e823d","status":"passed","time":{"start":1662348908000,"stop":1662348908567,"duration":567}},{"uid":"659f799daa406a53","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"58712e30c9891d54","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"ee6e6a594d2bd85c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"69e6ed6ac5a41c4c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"b60923dd357bad16","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"e923c2b2035a0f8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"8992cf6c6e9c8b63","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"d64f481d3204b2b8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"5e6aa19f41097bc2","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"4bce15fce59422e2","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"944b083da0bff0ae","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"3e52cce1f61a860b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Creation Should update Project while applying valid manifest with existing Project name":{"statistic":{"failed":24,"broken":0,"skipped":0,"passed":195,"unknown":0,"total":219},"items":[{"uid":"cee12b5d797018ab","status":"passed","time":{"start":1664275319000,"stop":1664275323757,"duration":4757}},{"uid":"87c05460aeae8254","status":"passed","time":{"start":1664245203000,"stop":1664245207782,"duration":4782}},{"uid":"c76681e63c0c5669","status":"passed","time":{"start":1664160732000,"stop":1664160736635,"duration":4635}},{"uid":"ce9d69ff3176fabc","status":"passed","time":{"start":1664091725000,"stop":1664091729689,"duration":4689}},{"uid":"6af99f3e7a40368e","status":"passed","time":{"start":1663841627000,"stop":1663841631874,"duration":4874}},{"uid":"5dc56ebef3b0d6a5","status":"passed","time":{"start":1663827045000,"stop":1663827049734,"duration":4734}},{"uid":"95103e943e498b34","status":"passed","time":{"start":1663767974000,"stop":1663767978678,"duration":4678}},{"uid":"2850c751f47fa9db","status":"passed","time":{"start":1663669677000,"stop":1663669681727,"duration":4727}},{"uid":"1c5e2ed4e08a4c1e","status":"passed","time":{"start":1663665323000,"stop":1663665327888,"duration":4888}},{"uid":"693466d67aeaae05","status":"passed","time":{"start":1663665170000,"stop":1663665174754,"duration":4754}},{"uid":"a59401dea345eb31","status":"passed","time":{"start":1663659067000,"stop":1663659071796,"duration":4796}},{"uid":"484547d53f3ff609","status":"passed","time":{"start":1663656809000,"stop":1663656813761,"duration":4761}},{"uid":"90983152076cb3de","status":"passed","time":{"start":1663583877000,"stop":1663583881895,"duration":4895}},{"uid":"9f909d0b2e336450","status":"passed","time":{"start":1663340950000,"stop":1663340954627,"duration":4627}},{"uid":"8d7b3e044f340acc","status":"passed","time":{"start":1663340650000,"stop":1663340654674,"duration":4674}},{"uid":"c54f7cf2e7c6eca6","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <bool>: false\nto be true","time":{"start":1663308473000,"stop":1663308540634,"duration":67634}},{"uid":"b0c6d25d96ea3e97","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0006ef170>: {\n        Underlying: <*exec.ExitError | 0xc00064a2e0>{\n            ProcessState: {\n                pid: 6141,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 173867},\n                    Stime: {Sec: 0, Usec: 69546},\n                    Maxrss: 84536,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2863,\n                    Majflt: 2,\n                    Nswap: 0,\n                    Inblock: 256,\n                    Oublock: 13344,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 415,\n                    Nivcsw: 361,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20update%20Project%20while%20applying%20valid%20manifest%20with%20existing%20Project%20name3321080709\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.26.65:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20update%20Project%20while%20applying%20valid%20manifest%20with%20existing%20Project%20name3321080709\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.26.65:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20update%20Project%20while%20applying%20valid%20manifest%20with%20existing%20Project%20name3321080709\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.26.65:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20update%20Project%20while%20applying%20valid%20manifest%20with%20existing%20Project%20name3321080709\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.26.65:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20update%20Project%20while%20applying%20valid%20manifest%20with%20existing%20Project%20name3321080709\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.26.65:443: connect: connection refused\noccurred","time":{"start":1663304356000,"stop":1663304356297,"duration":297}},{"uid":"f1a07fb032ef7766","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <bool>: false\nto be true","time":{"start":1663302519000,"stop":1663302640307,"duration":121307}},{"uid":"1dcd50db4658826","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0006d1860>: {\n        Underlying: <*exec.ExitError | 0xc0004a6b00>{\n            ProcessState: {\n                pid: 7111,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 206559},\n                    Stime: {Sec: 0, Usec: 46768},\n                    Maxrss: 84844,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3676,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13344,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 402,\n                    Nivcsw: 366,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20update%20Project%20while%20applying%20valid%20manifest%20with%20existing%20Project%20name4074151073\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.55.244:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20update%20Project%20while%20applying%20valid%20manifest%20with%20existing%20Project%20name4074151073\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.55.244:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20update%20Project%20while%20applying%20valid%20manifest%20with%20existing%20Project%20name4074151073\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.55.244:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20update%20Project%20while%20applying%20valid%20manifest%20with%20existing%20Project%20name4074151073\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.55.244:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20update%20Project%20while%20applying%20valid%20manifest%20with%20existing%20Project%20name4074151073\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.55.244:443: connect: connection refused\noccurred","time":{"start":1663264756000,"stop":1663264756298,"duration":298}},{"uid":"582286091c9c9a4d","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00084b8c0>: {\n        Underlying: <*exec.ExitError | 0xc000664460>{\n            ProcessState: {\n                pid: 7157,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 188516},\n                    Stime: {Sec: 0, Usec: 40981},\n                    Maxrss: 85688,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3449,\n                    Majflt: 1,\n                    Nswap: 0,\n                    Inblock: 256,\n                    Oublock: 13344,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 582,\n                    Nivcsw: 402,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20update%20Project%20while%20applying%20valid%20manifest%20with%20existing%20Project%20name1012151360\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.221.241:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20update%20Project%20while%20applying%20valid%20manifest%20with%20existing%20Project%20name1012151360\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.221.241:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20update%20Project%20while%20applying%20valid%20manifest%20with%20existing%20Project%20name1012151360\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.221.241:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20update%20Project%20while%20applying%20valid%20manifest%20with%20existing%20Project%20name1012151360\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.221.241:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20update%20Project%20while%20applying%20valid%20manifest%20with%20existing%20Project%20name1012151360\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.221.241:443: connect: connection refused\noccurred","time":{"start":1663228190000,"stop":1663228190289,"duration":289}}]},"Worker Suite:Worker Suite#[BeforeSuite]":{"statistic":{"failed":272,"broken":4,"skipped":0,"passed":56,"unknown":0,"total":332},"items":[{"uid":"89b63a6c3134ea31","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00009cc00>: {\n        Underlying: <*exec.ExitError | 0xc00009a600>{\n            ProcessState: {\n                pid: 6148,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 131178},\n                    Stime: {Sec: 0, Usec: 28695},\n                    Maxrss: 53908,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2734,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 24,\n                    Oublock: 360,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 809,\n                    Nivcsw: 276,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"install.go:178: [debug] Original chart version: \\\"v1.8.0\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\",\n                        \"\",\n                        \"Error: INSTALLATION FAILED: cannot re-use a name that is still in use\",\n                        \"helm.go:84: [debug] cannot re-use a name that is still in use\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).availableName\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:483\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:197\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:264\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:125\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                        \"INSTALLATION FAILED\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:127\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"install.go:178: [debug] Original chart version: \\\"v1.8.0\\\"\",\n                    \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\",\n                    \"\",\n                    \"Error: INSTALLATION FAILED: cannot re-use a name that is still in use\",\n                    \"helm.go:84: [debug] cannot re-use a name that is still in...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; install.go:178: [debug] Original chart version: \"v1.8.0\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\n    \n    Error: INSTALLATION FAILED: cannot re-use a name that is still in use\n    helm.go:84: [debug] cannot re-use a name that is still in use\n    helm.sh/helm/v3/pkg/action.(*Install).availableName\n    \thelm.sh/helm/v3/pkg/action/install.go:483\n    helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\n    \thelm.sh/helm/v3/pkg/action/install.go:197\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:264\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:125\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1664283136000,"stop":1664283136443,"duration":443}},{"uid":"cca550e110a035a0","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00034e540>: {\n        Underlying: <*exec.ExitError | 0xc0003a4ac0>{\n            ProcessState: {\n                pid: 6153,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 113642},\n                    Stime: {Sec: 0, Usec: 39064},\n                    Maxrss: 54112,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3183,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 24,\n                    Oublock: 360,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 770,\n                    Nivcsw: 250,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"install.go:178: [debug] Original chart version: \\\"v1.8.0\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\",\n                        \"\",\n                        \"Error: INSTALLATION FAILED: cannot re-use a name that is still in use\",\n                        \"helm.go:84: [debug] cannot re-use a name that is still in use\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).availableName\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:483\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:197\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:264\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:125\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                        \"INSTALLATION FAILED\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:127\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"install.go:178: [debug] Original chart version: \\\"v1.8.0\\\"\",\n                    \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\",\n                    \"\",\n                    \"Error: INSTALLATION FAILED: cannot re-use a name that is still in use\",\n                    \"helm.go:84: [debug] cannot re-use a name that is still in...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; install.go:178: [debug] Original chart version: \"v1.8.0\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\n    \n    Error: INSTALLATION FAILED: cannot re-use a name that is still in use\n    helm.go:84: [debug] cannot re-use a name that is still in use\n    helm.sh/helm/v3/pkg/action.(*Install).availableName\n    \thelm.sh/helm/v3/pkg/action/install.go:483\n    helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\n    \thelm.sh/helm/v3/pkg/action/install.go:197\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:264\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:125\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1664279812000,"stop":1664279812369,"duration":369}},{"uid":"ce0d74fdc34dd05c","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004aa7f8>: {\n        Underlying: <*exec.ExitError | 0xc000075ce0>{\n            ProcessState: {\n                pid: 6185,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 121442},\n                    Stime: {Sec: 0, Usec: 23504},\n                    Maxrss: 52484,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2215,\n                    Majflt: 1,\n                    Nswap: 0,\n                    Inblock: 152,\n                    Oublock: 360,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 849,\n                    Nivcsw: 285,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"v1.8.0\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\",\n                        \"\",\n                        \"Error: INSTALLATION FAILED: cannot re-use a name that is still in use\",\n                        \"helm.go:84: [debug] cannot re-use a name that is still in use\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).availableName\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:483\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:197\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:264\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:125\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                        \"INSTALLATION FAILED\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:127\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is ins...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"v1.8.0\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\n    \n    Error: INSTALLATION FAILED: cannot re-use a name that is still in use\n    helm.go:84: [debug] cannot re-use a name that is still in use\n    helm.sh/helm/v3/pkg/action.(*Install).availableName\n    \thelm.sh/helm/v3/pkg/action/install.go:483\n    helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\n    \thelm.sh/helm/v3/pkg/action/install.go:197\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:264\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:125\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1664278354000,"stop":1664278354432,"duration":432}},{"uid":"fa7968c7b5701cc9","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000f248>: {\n        Underlying: <*exec.ExitError | 0xc0003a7900>{\n            ProcessState: {\n                pid: 6177,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 159634},\n                    Stime: {Sec: 0, Usec: 12602},\n                    Maxrss: 54140,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3132,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 24,\n                    Oublock: 360,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 567,\n                    Nivcsw: 486,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"v1.8.0\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\",\n                        \"\",\n                        \"Error: INSTALLATION FAILED: cannot re-use a name that is still in use\",\n                        \"helm.go:84: [debug] cannot re-use a name that is still in use\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).availableName\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:483\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:197\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:264\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:125\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                        \"INSTALLATION FAILED\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:127\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is inse...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"v1.8.0\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\n    \n    Error: INSTALLATION FAILED: cannot re-use a name that is still in use\n    helm.go:84: [debug] cannot re-use a name that is still in use\n    helm.sh/helm/v3/pkg/action.(*Install).availableName\n    \thelm.sh/helm/v3/pkg/action/install.go:483\n    helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\n    \thelm.sh/helm/v3/pkg/action/install.go:197\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:264\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:125\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1664275871000,"stop":1664275871518,"duration":518}},{"uid":"481b2d7a1429ac91","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000364b70>: {\n        Underlying: <*exec.ExitError | 0xc000437080>{\n            ProcessState: {\n                pid: 6091,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 156233},\n                    Stime: {Sec: 0, Usec: 15623},\n                    Maxrss: 52068,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2194,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 360,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 480,\n                    Nivcsw: 207,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"v1.8.0\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\",\n                        \"\",\n                        \"Error: INSTALLATION FAILED: cannot re-use a name that is still in use\",\n                        \"helm.go:84: [debug] cannot re-use a name that is still in use\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).availableName\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:483\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:197\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:264\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:125\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                        \"INSTALLATION FAILED\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:127\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insec...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"v1.8.0\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\n    \n    Error: INSTALLATION FAILED: cannot re-use a name that is still in use\n    helm.go:84: [debug] cannot re-use a name that is still in use\n    helm.sh/helm/v3/pkg/action.(*Install).availableName\n    \thelm.sh/helm/v3/pkg/action/install.go:483\n    helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\n    \thelm.sh/helm/v3/pkg/action/install.go:197\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:264\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:125\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1664271148000,"stop":1664271148320,"duration":320}},{"uid":"48f591183ed93e9d","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00042f170>: {\n        Underlying: <*exec.ExitError | 0xc00038ba00>{\n            ProcessState: {\n                pid: 6190,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 117831},\n                    Stime: {Sec: 0, Usec: 23566},\n                    Maxrss: 54392,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2187,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 360,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 662,\n                    Nivcsw: 236,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"v1.8.0\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\",\n                        \"\",\n                        \"Error: INSTALLATION FAILED: cannot re-use a name that is still in use\",\n                        \"helm.go:84: [debug] cannot re-use a name that is still in use\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).availableName\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:483\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:197\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:264\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:125\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                        \"INSTALLATION FAILED\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:127\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insec...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"v1.8.0\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\n    \n    Error: INSTALLATION FAILED: cannot re-use a name that is still in use\n    helm.go:84: [debug] cannot re-use a name that is still in use\n    helm.sh/helm/v3/pkg/action.(*Install).availableName\n    \thelm.sh/helm/v3/pkg/action/install.go:483\n    helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\n    \thelm.sh/helm/v3/pkg/action/install.go:197\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:264\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:125\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1664261838000,"stop":1664261838419,"duration":419}},{"uid":"440a57e698990769","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000433998>: {\n        Underlying: <*exec.ExitError | 0xc00030b840>{\n            ProcessState: {\n                pid: 7082,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 140558},\n                    Stime: {Sec: 0, Usec: 46852},\n                    Maxrss: 54492,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2816,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 552,\n                    Oublock: 360,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 861,\n                    Nivcsw: 342,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"v1.8.0\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\",\n                        \"\",\n                        \"Error: INSTALLATION FAILED: cannot re-use a name that is still in use\",\n                        \"helm.go:84: [debug] cannot re-use a name that is still in use\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).availableName\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:483\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:197\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:264\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:125\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                        \"INSTALLATION FAILED\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:127\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is ins...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"v1.8.0\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\n    \n    Error: INSTALLATION FAILED: cannot re-use a name that is still in use\n    helm.go:84: [debug] cannot re-use a name that is still in use\n    helm.sh/helm/v3/pkg/action.(*Install).availableName\n    \thelm.sh/helm/v3/pkg/action/install.go:483\n    helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\n    \thelm.sh/helm/v3/pkg/action/install.go:197\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:264\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:125\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1664247752000,"stop":1664247752453,"duration":453}},{"uid":"2a7c238d32474584","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0000a7470>: {\n        Underlying: <*exec.ExitError | 0xc000406a40>{\n            ProcessState: {\n                pid: 6184,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 140604},\n                    Stime: {Sec: 0, Usec: 25564},\n                    Maxrss: 51920,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2837,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 360,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 942,\n                    Nivcsw: 214,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"v1.8.0\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\",\n                        \"\",\n                        \"Error: INSTALLATION FAILED: cannot re-use a name that is still in use\",\n                        \"helm.go:84: [debug] cannot re-use a name that is still in use\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).availableName\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:483\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:197\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:264\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:125\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                        \"INSTALLATION FAILED\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:127\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insec...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"v1.8.0\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\n    \n    Error: INSTALLATION FAILED: cannot re-use a name that is still in use\n    helm.go:84: [debug] cannot re-use a name that is still in use\n    helm.sh/helm/v3/pkg/action.(*Install).availableName\n    \thelm.sh/helm/v3/pkg/action/install.go:483\n    helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\n    \thelm.sh/helm/v3/pkg/action/install.go:197\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:264\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:125\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1664218591000,"stop":1664218591697,"duration":697}},{"uid":"58a57cd2af1f2cca","status":"broken","statusDetails":"Error response from daemon: failed to create shim: OCI runtime create failed: runc create failed: unable to start container process: error during container init: error mounting \"/e2e/ui-automation/cypress.json\" to rootfs at \"/build/cypress.json\": mount /e2e/ui-automation/cypress.json:/build/cypress.json (via /proc/self/fd/6), flags: 0x5000: not a directory: unknown: Are you trying to mount a directory onto a file (or vice-versa)? Check if the specified host path exists and is the expected type","time":{"start":1664196101000,"stop":1664196257944,"duration":156944}},{"uid":"cc25b8880545eee5","status":"broken","statusDetails":"Error response from daemon: failed to create shim: OCI runtime create failed: runc create failed: unable to start container process: error during container init: error mounting \"/e2e/ui-automation/cypress.json\" to rootfs at \"/build/cypress.json\": mount /e2e/ui-automation/cypress.json:/build/cypress.json (via /proc/self/fd/6), flags: 0x5000: not a directory: unknown: Are you trying to mount a directory onto a file (or vice-versa)? Check if the specified host path exists and is the expected type","time":{"start":1664192951000,"stop":1664193117984,"duration":166984}},{"uid":"4cb15bd223a1d783","status":"broken","statusDetails":"Error response from daemon: failed to create shim: OCI runtime create failed: runc create failed: unable to start container process: error during container init: error mounting \"/e2e/ui-automation/cypress.json\" to rootfs at \"/build/cypress.json\": mount /e2e/ui-automation/cypress.json:/build/cypress.json (via /proc/self/fd/6), flags: 0x5000: not a directory: unknown: Are you trying to mount a directory onto a file (or vice-versa)? Check if the specified host path exists and is the expected type","time":{"start":1664191201000,"stop":1664191359175,"duration":158175}},{"uid":"ad308ce379313b84","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00018c078>: {\n        Underlying: <*exec.ExitError | 0xc00007c040>{\n            ProcessState: {\n                pid: 7066,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 112371},\n                    Stime: {Sec: 0, Usec: 38748},\n                    Maxrss: 51420,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2723,\n                    Majflt: 2,\n                    Nswap: 0,\n                    Inblock: 48,\n                    Oublock: 360,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 860,\n                    Nivcsw: 486,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"v1.8.0\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\",\n                        \"\",\n                        \"Error: INSTALLATION FAILED: cannot re-use a name that is still in use\",\n                        \"helm.go:84: [debug] cannot re-use a name that is still in use\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).availableName\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:483\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:197\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:264\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:125\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                        \"INSTALLATION FAILED\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:127\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is inse...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"v1.8.0\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\n    \n    Error: INSTALLATION FAILED: cannot re-use a name that is still in use\n    helm.go:84: [debug] cannot re-use a name that is still in use\n    helm.sh/helm/v3/pkg/action.(*Install).availableName\n    \thelm.sh/helm/v3/pkg/action/install.go:483\n    helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\n    \thelm.sh/helm/v3/pkg/action/install.go:197\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:264\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:125\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1664163199000,"stop":1664163199472,"duration":472}},{"uid":"a92f9d636e8585f0","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004be1b0>: {\n        Underlying: <*exec.ExitError | 0xc00014c840>{\n            ProcessState: {\n                pid: 6164,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 185216},\n                    Stime: {Sec: 0, Usec: 30239},\n                    Maxrss: 52136,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2748,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 360,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 695,\n                    Nivcsw: 223,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"v1.8.0\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\",\n                        \"\",\n                        \"Error: INSTALLATION FAILED: cannot re-use a name that is still in use\",\n                        \"helm.go:84: [debug] cannot re-use a name that is still in use\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).availableName\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:483\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:197\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:264\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:125\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                        \"INSTALLATION FAILED\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:127\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insec...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"v1.8.0\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\n    \n    Error: INSTALLATION FAILED: cannot re-use a name that is still in use\n    helm.go:84: [debug] cannot re-use a name that is still in use\n    helm.sh/helm/v3/pkg/action.(*Install).availableName\n    \thelm.sh/helm/v3/pkg/action/install.go:483\n    helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\n    \thelm.sh/helm/v3/pkg/action/install.go:197\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:264\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:125\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1664106542000,"stop":1664106542565,"duration":565}},{"uid":"a96a9bb0caf0877a","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0001c2198>: {\n        Underlying: <*exec.ExitError | 0xc0004c4560>{\n            ProcessState: {\n                pid: 6169,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 136225},\n                    Stime: {Sec: 0, Usec: 27943},\n                    Maxrss: 56460,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2748,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 360,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 779,\n                    Nivcsw: 603,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"v1.8.0\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\",\n                        \"\",\n                        \"Error: INSTALLATION FAILED: cannot re-use a name that is still in use\",\n                        \"helm.go:84: [debug] cannot re-use a name that is still in use\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).availableName\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:483\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:197\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:264\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:125\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                        \"INSTALLATION FAILED\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:127\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insec...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"v1.8.0\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\n    \n    Error: INSTALLATION FAILED: cannot re-use a name that is still in use\n    helm.go:84: [debug] cannot re-use a name that is still in use\n    helm.sh/helm/v3/pkg/action.(*Install).availableName\n    \thelm.sh/helm/v3/pkg/action/install.go:483\n    helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\n    \thelm.sh/helm/v3/pkg/action/install.go:197\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:264\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:125\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1664104522000,"stop":1664104522394,"duration":394}},{"uid":"890fb3c19f626545","status":"passed","time":{"start":1664094053000,"stop":1664094215033,"duration":162033}},{"uid":"bdaf89ca04e4a19e","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0005c28d0>: {\n        Underlying: <*exec.ExitError | 0xc00039f7e0>{\n            ProcessState: {\n                pid: 6126,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 185700},\n                    Stime: {Sec: 0, Usec: 39793},\n                    Maxrss: 52196,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2799,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 360,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 865,\n                    Nivcsw: 618,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"v1.8.0\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\",\n                        \"\",\n                        \"Error: INSTALLATION FAILED: cannot re-use a name that is still in use\",\n                        \"helm.go:84: [debug] cannot re-use a name that is still in use\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).availableName\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:483\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:197\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:264\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:125\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                        \"INSTALLATION FAILED\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:127\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insec...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"v1.8.0\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\n    \n    Error: INSTALLATION FAILED: cannot re-use a name that is still in use\n    helm.go:84: [debug] cannot re-use a name that is still in use\n    helm.sh/helm/v3/pkg/action.(*Install).availableName\n    \thelm.sh/helm/v3/pkg/action/install.go:483\n    helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\n    \thelm.sh/helm/v3/pkg/action/install.go:197\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:264\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:125\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1663933497000,"stop":1663933497815,"duration":815}},{"uid":"e28dad7077015f65","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000e360>: {\n        Underlying: <*exec.ExitError | 0xc0004d65c0>{\n            ProcessState: {\n                pid: 6111,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 126806},\n                    Stime: {Sec: 0, Usec: 18115},\n                    Maxrss: 51736,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3710,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 360,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 791,\n                    Nivcsw: 237,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"v1.8.0\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\",\n                        \"\",\n                        \"Error: INSTALLATION FAILED: cannot re-use a name that is still in use\",\n                        \"helm.go:84: [debug] cannot re-use a name that is still in use\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).availableName\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:483\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:197\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:264\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:125\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                        \"INSTALLATION FAILED\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:127\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insec...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"v1.8.0\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\n    \n    Error: INSTALLATION FAILED: cannot re-use a name that is still in use\n    helm.go:84: [debug] cannot re-use a name that is still in use\n    helm.sh/helm/v3/pkg/action.(*Install).availableName\n    \thelm.sh/helm/v3/pkg/action/install.go:483\n    helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\n    \thelm.sh/helm/v3/pkg/action/install.go:197\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:264\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:125\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1663919193000,"stop":1663919193430,"duration":430}},{"uid":"14e246b9ff01bf9","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0003f4c48>: {\n        Underlying: <*exec.ExitError | 0xc00038c200>{\n            ProcessState: {\n                pid: 6154,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 129776},\n                    Stime: {Sec: 0, Usec: 21044},\n                    Maxrss: 54048,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3655,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 360,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 721,\n                    Nivcsw: 479,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"v1.8.0\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\",\n                        \"\",\n                        \"Error: INSTALLATION FAILED: cannot re-use a name that is still in use\",\n                        \"helm.go:84: [debug] cannot re-use a name that is still in use\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).availableName\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:483\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:197\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:264\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:125\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                        \"INSTALLATION FAILED\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:127\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insec...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"v1.8.0\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\n    \n    Error: INSTALLATION FAILED: cannot re-use a name that is still in use\n    helm.go:84: [debug] cannot re-use a name that is still in use\n    helm.sh/helm/v3/pkg/action.(*Install).availableName\n    \thelm.sh/helm/v3/pkg/action/install.go:483\n    helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\n    \thelm.sh/helm/v3/pkg/action/install.go:197\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:264\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:125\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1663912943000,"stop":1663912943328,"duration":328}},{"uid":"ad11a2a4943448aa","status":"passed","time":{"start":1663843963000,"stop":1663844129826,"duration":166826}},{"uid":"cd191509cf4c81ad","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00048a180>: {\n        Underlying: <*exec.ExitError | 0xc000496060>{\n            ProcessState: {\n                pid: 6139,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 124564},\n                    Stime: {Sec: 0, Usec: 39336},\n                    Maxrss: 52316,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2679,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 24,\n                    Oublock: 360,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 608,\n                    Nivcsw: 636,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"v1.8.0\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\",\n                        \"\",\n                        \"Error: INSTALLATION FAILED: cannot re-use a name that is still in use\",\n                        \"helm.go:84: [debug] cannot re-use a name that is still in use\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).availableName\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:483\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:197\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:264\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:125\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                        \"INSTALLATION FAILED\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:127\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is inse...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"v1.8.0\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\n    \n    Error: INSTALLATION FAILED: cannot re-use a name that is still in use\n    helm.go:84: [debug] cannot re-use a name that is still in use\n    helm.sh/helm/v3/pkg/action.(*Install).availableName\n    \thelm.sh/helm/v3/pkg/action/install.go:483\n    helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\n    \thelm.sh/helm/v3/pkg/action/install.go:197\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:264\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:125\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1663839375000,"stop":1663839375554,"duration":554}}]},"b04c53e3b55ecb158ecc0472d5af5e4a":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"e383a9b661b4c33e","status":"passed","time":{"start":1663839594284,"stop":1663839602432,"duration":8148}}]},"6ccd8eb7f60bc5f6a81fb73388e426ce":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"b2cd140045d714b7","status":"passed","time":{"start":1663840470756,"stop":1663840484430,"duration":13674}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting iperf-server pod Check ping between iperf-server and iperf-client after iperf-server pod restart":{"statistic":{"failed":0,"broken":0,"skipped":56,"passed":0,"unknown":0,"total":56},"items":[{"uid":"25287069779bcbbd","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1664094053000,"stop":1664094053000,"duration":0}},{"uid":"fba88d426781e010","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663843963000,"stop":1663843963000,"duration":0}},{"uid":"b4199f3c0f884a2b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663829327000,"stop":1663829327000,"duration":0}},{"uid":"a5e50902678a9d7d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663343132000,"stop":1663343132000,"duration":0}},{"uid":"ab570416858cc338","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663342947000,"stop":1663342947000,"duration":0}},{"uid":"932bb9fea60aa1b0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"ca6a4cf44e46bfdd","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"96e188ed94ec6662","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662348908000,"stop":1662348908000,"duration":0}},{"uid":"6e14fa104477a13f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"e9d84da102b21840","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"5e517711eba9a066","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"50cf073fe40dd4e3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"c6a1d444f638bb5e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"c24d795e2eb5e1e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"c0a52a0ed2e2cdab","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"a7dba4b37f4ad822","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"19d3b4bc6782301e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"d7b8b124d5be2f76","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"c118fdb8c9f167cc","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"27834db9efcc7feb","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice creation scenarios: when slice red is installed workers should have gateway pod running":{"statistic":{"failed":1,"broken":0,"skipped":36,"passed":19,"unknown":0,"total":56},"items":[{"uid":"70a4f5896bb1357f","status":"passed","time":{"start":1664094053000,"stop":1664094063044,"duration":10044}},{"uid":"495bcd92336fe693","status":"passed","time":{"start":1663843963000,"stop":1663844129806,"duration":166806}},{"uid":"16ff6d7c89296df7","status":"passed","time":{"start":1663829327000,"stop":1663829491543,"duration":164543}},{"uid":"ae21e637c9f5de22","status":"passed","time":{"start":1663343132000,"stop":1663343214296,"duration":82296}},{"uid":"ed2c1496f45dc256","status":"passed","time":{"start":1663342947000,"stop":1663342989159,"duration":42159}},{"uid":"c9147c7363a92094","status":"failed","statusDetails":"Timed out after 120.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1663044723000,"stop":1663044843182,"duration":120182}},{"uid":"8fd563b7b6a740a","status":"passed","time":{"start":1663044069000,"stop":1663044111267,"duration":42267}},{"uid":"a07659fcdbf12d0d","status":"passed","time":{"start":1662348908000,"stop":1662348920055,"duration":12055}},{"uid":"2b98e4cfea151cf2","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"92a39eba28f3088c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"72afaad3fd8faab5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"acb02b8e924a8e78","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"d7ed5bfc413dafff","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"297ff7cc320f47a5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"2d20dbb0b59a19d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"1c7e63ba47a26ac9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"bb22c02fa61a64ad","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"6917e7d4f39e53eb","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"ef34e1212f4a0ae2","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"c8346067fdbf0ab1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}}]},"11b52e134f2ecffd8e1311d34b1558f4":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"ab577e69bdca5900","status":"passed","time":{"start":1663838760845,"stop":1663838776949,"duration":16104}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Update Should update successfully when Project Applied while removing a service account name in Read users":{"statistic":{"failed":26,"broken":0,"skipped":0,"passed":190,"unknown":0,"total":216},"items":[{"uid":"b9690ca8e35f16a4","status":"passed","time":{"start":1664275319000,"stop":1664275324829,"duration":5829}},{"uid":"236558cdc1e4db32","status":"passed","time":{"start":1664245203000,"stop":1664245208144,"duration":5144}},{"uid":"d03ab30ef1a2b866","status":"passed","time":{"start":1664160732000,"stop":1664160737651,"duration":5651}},{"uid":"fa8be7214466ec92","status":"passed","time":{"start":1664091725000,"stop":1664091730624,"duration":5624}},{"uid":"290f8a4a33f9f1b6","status":"passed","time":{"start":1663841627000,"stop":1663841632183,"duration":5183}},{"uid":"51766c096eac5dc0","status":"passed","time":{"start":1663827045000,"stop":1663827049686,"duration":4686}},{"uid":"fa39d95fa7fc3e8","status":"passed","time":{"start":1663767974000,"stop":1663767979773,"duration":5773}},{"uid":"fb18df0e17f3540b","status":"passed","time":{"start":1663669677000,"stop":1663669681729,"duration":4729}},{"uid":"5bbbf9646f6c9510","status":"passed","time":{"start":1663665323000,"stop":1663665328241,"duration":5241}},{"uid":"d19d48a0eea98cd5","status":"passed","time":{"start":1663665170000,"stop":1663665175954,"duration":5954}},{"uid":"c355ffb69d020447","status":"passed","time":{"start":1663659067000,"stop":1663659073011,"duration":6011}},{"uid":"913fb39b6babfca4","status":"passed","time":{"start":1663656809000,"stop":1663656814889,"duration":5889}},{"uid":"27ffa603899bd1b3","status":"passed","time":{"start":1663583877000,"stop":1663583882313,"duration":5313}},{"uid":"f31839abc2069c20","status":"passed","time":{"start":1663340950000,"stop":1663340954644,"duration":4644}},{"uid":"f04b7d1ff661214d","status":"passed","time":{"start":1663340650000,"stop":1663340654702,"duration":4702}},{"uid":"871a3da4f8da79f2","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <bool>: false\nto be true","time":{"start":1663308473000,"stop":1663308594408,"duration":121408}},{"uid":"5b6a7d23f6c644e7","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0006efb60>: {\n        Underlying: <*exec.ExitError | 0xc000695240>{\n            ProcessState: {\n                pid: 6207,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 201477},\n                    Stime: {Sec: 0, Usec: 51357},\n                    Maxrss: 75616,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3216,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13344,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 303,\n                    Nivcsw: 525,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Read%20users2671031166\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.26.65:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Read%20users2671031166\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.26.65:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Read%20users2671031166\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.26.65:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Read%20users2671031166\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.26.65:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Read%20users2671031166\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.26.65:443: connect: connection refused\noccurred","time":{"start":1663304356000,"stop":1663304356309,"duration":309}},{"uid":"b21bc9127f34b40b","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00080a150>: {\n        Underlying: <*exec.ExitError | 0xc0003e4e00>{\n            ProcessState: {\n                pid: 7542,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 170077},\n                    Stime: {Sec: 0, Usec: 31642},\n                    Maxrss: 81056,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 5637,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 441,\n                    Nivcsw: 237,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Read%20users2453288014\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.179.221:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Read%20users2453288014\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.179.221:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Read%20users2453288014\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.179.221:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Read%20users2453288014\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.179.221:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Read%20users2453288014\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.179.221:443: connect: connection refused\noccurred","time":{"start":1663302519000,"stop":1663302519249,"duration":249}},{"uid":"91c2095bb565e4f6","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000639470>: {\n        Underlying: <*exec.ExitError | 0xc00045ce60>{\n            ProcessState: {\n                pid: 7177,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 237719},\n                    Stime: {Sec: 0, Usec: 53281},\n                    Maxrss: 87072,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2931,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13344,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 361,\n                    Nivcsw: 400,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Read%20users722703850\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.55.244:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Read%20users722703850\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.55.244:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Read%20users722703850\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.55.244:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Read%20users722703850\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.55.244:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Read%20users722703850\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.55.244:443: connect: connection refused\noccurred","time":{"start":1663264756000,"stop":1663264756363,"duration":363}},{"uid":"13688414a3e5dc10","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00084a990>: {\n        Underlying: <*exec.ExitError | 0xc0000d8c60>{\n            ProcessState: {\n                pid: 7220,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 178303},\n                    Stime: {Sec: 0, Usec: 39622},\n                    Maxrss: 83976,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3467,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13344,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 419,\n                    Nivcsw: 379,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Read%20users3479231624\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.221.241:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Read%20users3479231624\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.221.241:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Read%20users3479231624\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.221.241:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Read%20users3479231624\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.221.241:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Read%20users3479231624\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.221.241:443: connect: connection refused\noccurred","time":{"start":1663228190000,"stop":1663228190289,"duration":289}}]},"5ba697b5110fefc800ec20bae33ddb9b":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"dc10a78d6d660be7","status":"passed","time":{"start":1663842280495,"stop":1663842315915,"duration":35420}}]},"Hub Suite:Hub Suite#[It]  QosProfile tests QosProfile delete validation Delete qosProfile positive scenario":{"statistic":{"failed":0,"broken":0,"skipped":149,"passed":0,"unknown":0,"total":149},"items":[{"uid":"71ed8524107522ad","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661501311000,"stop":1661501311000,"duration":0}},{"uid":"5d2f453a753cb5cf","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661434220000,"stop":1661434220000,"duration":0}},{"uid":"a1032a03c5fc5b59","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661356056000,"stop":1661356056000,"duration":0}},{"uid":"b8e1a3ba1adf7262","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661352461000,"stop":1661352461000,"duration":0}},{"uid":"febce3a75402692","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661352207000,"stop":1661352207000,"duration":0}},{"uid":"11e568aeae2815ae","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661352028000,"stop":1661352028000,"duration":0}},{"uid":"eb9a8e0c6b7d6a13","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661351263000,"stop":1661351263000,"duration":0}},{"uid":"75c5e6a0143b901a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661188463000,"stop":1661188463000,"duration":0}},{"uid":"c5add898cdb05dd9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661183038000,"stop":1661183038000,"duration":0}},{"uid":"131a33865450a862","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661180055000,"stop":1661180055000,"duration":0}},{"uid":"c9e3352d3a7e669a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661152474000,"stop":1661152474000,"duration":0}},{"uid":"192acab30bedbfca","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661083598000,"stop":1661083598000,"duration":0}},{"uid":"a814297a4bc804b8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661070332000,"stop":1661070332000,"duration":0}},{"uid":"f138849f9725f609","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661066603000,"stop":1661066603000,"duration":0}},{"uid":"8b0e13afaca99cdd","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661018182000,"stop":1661018182000,"duration":0}},{"uid":"4b4af5b0dd430667","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661014620000,"stop":1661014620000,"duration":0}},{"uid":"90eee801da00d03b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661011831000,"stop":1661011831000,"duration":0}},{"uid":"35925f2160cbbee6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661011780000,"stop":1661011780000,"duration":0}},{"uid":"b6751e2b7c2fa29d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661009684000,"stop":1661009684000,"duration":0}},{"uid":"e72cecce8cbfb6d1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661007831000,"stop":1661007831000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Hub netpol scenarios: Slice Configs Netpol scenarios Should fail while Deleting Slice without removing the namespaceIsolationProfile part in slice configs":{"statistic":{"failed":0,"broken":0,"skipped":23,"passed":33,"unknown":0,"total":56},"items":[{"uid":"fc1037d3662e5d27","status":"passed","time":{"start":1664094053000,"stop":1664094053102,"duration":102}},{"uid":"8b235dd46309d5ec","status":"passed","time":{"start":1663843963000,"stop":1663843963184,"duration":184}},{"uid":"93a5b6e903c1d4e4","status":"passed","time":{"start":1663829327000,"stop":1663829327105,"duration":105}},{"uid":"76946beb2bed76b2","status":"passed","time":{"start":1663343132000,"stop":1663343132179,"duration":179}},{"uid":"9789da75574cdc68","status":"passed","time":{"start":1663342947000,"stop":1663342947107,"duration":107}},{"uid":"693d3e81bd14451e","status":"skipped","statusDetails":"skipped","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"257b2653a9989bd7","status":"passed","time":{"start":1663044069000,"stop":1663044069106,"duration":106}},{"uid":"b5974d9250d5c884","status":"passed","time":{"start":1662348908000,"stop":1662348908123,"duration":123}},{"uid":"191222d065916334","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"faf21b4d922e3f82","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"e56eec13b14cf185","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"fa2fd007e1777b41","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"bdedd5ec49fdf518","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"f0588c2e6e35520d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"c01c104adafd18ee","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"efb6c41437ff0bf0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"881e23aecd17e1bc","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"4d35a03346a35521","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"d7affabc70874cf5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"952c17cd76c850bd","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}}]},"68c0b7aed4d773c3a2340da4c6626d5e":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"728f8d43e0941b1b","status":"passed","time":{"start":1663838911244,"stop":1663838922260,"duration":11016}}]},"dc806ab2d182e90e8cd0ef1eb151381":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"e01fbed6b3095377","status":"passed","time":{"start":1663843336763,"stop":1663843384620,"duration":47857}}]},"6b7af1ab6f9a4225156bede1ddde542e":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"c5f585b78108e1a5","status":"passed","time":{"start":1663843528255,"stop":1663843547163,"duration":18908}}]},"Worker Suite:Worker Suite#[It] Slice deletion tests: Slice is installed & validated Slice object is deleted from hub should remove vl3 router from spoke":{"statistic":{"failed":41,"broken":0,"skipped":1,"passed":14,"unknown":0,"total":56},"items":[{"uid":"aa6fab6a405cc674","status":"passed","time":{"start":1664094053000,"stop":1664094080105,"duration":27105}},{"uid":"2f6a6ca59bbcb73c","status":"passed","time":{"start":1663843963000,"stop":1663844054556,"duration":91556}},{"uid":"34181311f1ebf73e","status":"passed","time":{"start":1663829327000,"stop":1663829375187,"duration":48187}},{"uid":"9632672b78035e49","status":"failed","statusDetails":"Timed out after 120.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1663343132000,"stop":1663343257082,"duration":125082}},{"uid":"709328a3c45fd35","status":"passed","time":{"start":1663342947000,"stop":1663343037091,"duration":90091}},{"uid":"a93cc1cff872ee9c","status":"skipped","statusDetails":"skipped","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"72e8a7cc0bcb4080","status":"passed","time":{"start":1663044069000,"stop":1663044162253,"duration":93253}},{"uid":"a9a6e1b9ebaa070a","status":"passed","time":{"start":1662348908000,"stop":1662348945652,"duration":37652}},{"uid":"834ea0cc4c481610","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000e060>: {\n        Underlying: <*exec.ExitError | 0xc00067e020>{\n            ProcessState: {\n                pid: 7153,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 241357},\n                    Stime: {Sec: 0, Usec: 66837},\n                    Maxrss: 88584,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 7503,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 365,\n                    Nivcsw: 433,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-red\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\noccurred","time":{"start":1661844205000,"stop":1661844205313,"duration":313}},{"uid":"11494b693f885dfb","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000734018>: {\n        Underlying: <*exec.ExitError | 0xc0007ba020>{\n            ProcessState: {\n                pid: 7275,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 163208},\n                    Stime: {Sec: 0, Usec: 58288},\n                    Maxrss: 87408,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4003,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 374,\n                    Nivcsw: 530,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-red\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661754870000,"stop":1661754870232,"duration":232}},{"uid":"6ebd8d4c99dbd8cb","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0008cf578>: {\n        Underlying: <*exec.ExitError | 0xc0006d2d20>{\n            ProcessState: {\n                pid: 7261,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 190603},\n                    Stime: {Sec: 0, Usec: 55063},\n                    Maxrss: 85716,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3785,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 401,\n                    Nivcsw: 493,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-red\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661611962000,"stop":1661611962244,"duration":244}},{"uid":"1407aa4534364f80","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00084a018>: {\n        Underlying: <*exec.ExitError | 0xc0007aa000>{\n            ProcessState: {\n                pid: 7311,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 203597},\n                    Stime: {Sec: 0, Usec: 30731},\n                    Maxrss: 85320,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4145,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 410,\n                    Nivcsw: 477,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-red\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661585077000,"stop":1661585077245,"duration":245}},{"uid":"8db5ee1508d26c07","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000fa88>: {\n        Underlying: <*exec.ExitError | 0xc000716740>{\n            ProcessState: {\n                pid: 6019,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 186764},\n                    Stime: {Sec: 0, Usec: 44661},\n                    Maxrss: 84252,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3558,\n                    Majflt: 2,\n                    Nswap: 0,\n                    Inblock: 344,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 578,\n                    Nivcsw: 558,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-red\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661584447000,"stop":1661584447249,"duration":249}},{"uid":"750abfcde30564f2","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0008103d8>: {\n        Underlying: <*exec.ExitError | 0xc00013ec80>{\n            ProcessState: {\n                pid: 7314,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 189581},\n                    Stime: {Sec: 0, Usec: 30333},\n                    Maxrss: 77976,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3432,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 361,\n                    Nivcsw: 450,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-red\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661580045000,"stop":1661580045217,"duration":217}},{"uid":"51ef1178caae64aa","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000779770>: {\n        Underlying: <*exec.ExitError | 0xc0007389e0>{\n            ProcessState: {\n                pid: 7298,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 192887},\n                    Stime: {Sec: 0, Usec: 84870},\n                    Maxrss: 84924,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3810,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 312,\n                    Nivcsw: 379,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-red\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661578505000,"stop":1661578505267,"duration":267}},{"uid":"9edb35c60a964028","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00044b668>: {\n        Underlying: <*exec.ExitError | 0xc0006b7420>{\n            ProcessState: {\n                pid: 7321,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 238143},\n                    Stime: {Sec: 0, Usec: 49933},\n                    Maxrss: 87164,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3049,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 497,\n                    Nivcsw: 425,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-red\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661572479000,"stop":1661572479256,"duration":256}},{"uid":"ef00b61633f6fd14","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0006ea378>: {\n        Underlying: <*exec.ExitError | 0xc0008c85e0>{\n            ProcessState: {\n                pid: 7131,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 178699},\n                    Stime: {Sec: 0, Usec: 47653},\n                    Maxrss: 78744,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4097,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 342,\n                    Nivcsw: 374,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-red\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660832281000,"stop":1660832281215,"duration":215}},{"uid":"61e696478a276c40","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004961b0>: {\n        Underlying: <*exec.ExitError | 0xc000662060>{\n            ProcessState: {\n                pid: 7054,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 179753},\n                    Stime: {Sec: 0, Usec: 35950},\n                    Maxrss: 82292,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 6378,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 363,\n                    Nivcsw: 330,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-red\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660808797000,"stop":1660808797218,"duration":218}},{"uid":"98e093a2c5568254","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0005cc378>: {\n        Underlying: <*exec.ExitError | 0xc000720f00>{\n            ProcessState: {\n                pid: 7129,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 148879},\n                    Stime: {Sec: 0, Usec: 39178},\n                    Maxrss: 81224,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 6084,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 415,\n                    Nivcsw: 350,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-red\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660299182000,"stop":1660299182183,"duration":183}},{"uid":"a2418f3c9820c060","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0003d30e0>: {\n        Underlying: <*exec.ExitError | 0xc000709b40>{\n            ProcessState: {\n                pid: 7161,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 158288},\n                    Stime: {Sec: 0, Usec: 41456},\n                    Maxrss: 82336,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 5777,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 314,\n                    Nivcsw: 345,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-red\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660295740000,"stop":1660295740197,"duration":197}}]},"Worker Suite:Worker Suite#[It] Intracluster slice tests Slice with netpol should have vl3 router running":{"statistic":{"failed":23,"broken":0,"skipped":6,"passed":27,"unknown":0,"total":56},"items":[{"uid":"d9c5cb52b0593b07","status":"passed","time":{"start":1664094053000,"stop":1664094053345,"duration":345}},{"uid":"8c93ca5b5b445815","status":"passed","time":{"start":1663843963000,"stop":1663843965326,"duration":2326}},{"uid":"766c9790f051c79c","status":"passed","time":{"start":1663829327000,"stop":1663829329633,"duration":2633}},{"uid":"510a2e174320093e","status":"passed","time":{"start":1663343132000,"stop":1663343132864,"duration":864}},{"uid":"bd5833ee0d34f4b4","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 2\nto equal\n    <int>: 1","time":{"start":1663342947000,"stop":1663343016109,"duration":69109}},{"uid":"78b302f878f5d2fe","status":"skipped","statusDetails":"skipped","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"1a783a4f2c44b4af","status":"passed","time":{"start":1663044069000,"stop":1663044072473,"duration":3473}},{"uid":"9208b836274ad7c3","status":"passed","time":{"start":1662348908000,"stop":1662348908290,"duration":290}},{"uid":"26f807534982f600","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000469338>: {\n        Underlying: <*exec.ExitError | 0xc000709d60>{\n            ProcessState: {\n                pid: 7256,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 218947},\n                    Stime: {Sec: 0, Usec: 87578},\n                    Maxrss: 85032,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 5164,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 529,\n                    Nivcsw: 413,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\noccurred","time":{"start":1661844205000,"stop":1661844205591,"duration":591}},{"uid":"441f0d2f407baf9b","status":"skipped","statusDetails":"skipped - Controller & worker should be set up in same. Skipping!!","time":{"start":1661754870000,"stop":1661754870206,"duration":206}},{"uid":"71bf3aba59096b20","status":"skipped","statusDetails":"skipped - Controller & worker should be set up in same. Skipping!!","time":{"start":1661611962000,"stop":1661611962219,"duration":219}},{"uid":"db3e3c7e00fe7359","status":"skipped","statusDetails":"skipped - Controller & worker should be set up in same. Skipping!!","time":{"start":1661585077000,"stop":1661585077222,"duration":222}},{"uid":"4a344de0657d5ec8","status":"skipped","statusDetails":"skipped - Controller & worker should be set up in same. Skipping!!","time":{"start":1661584447000,"stop":1661584447298,"duration":298}},{"uid":"ebc45cbe2f8e73d0","status":"skipped","statusDetails":"skipped - Controller & worker should be set up in same. Skipping!!","time":{"start":1661580045000,"stop":1661580045245,"duration":245}},{"uid":"de8c2c25acefc83b","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0007789d8>: {\n        Underlying: <*exec.ExitError | 0xc000775640>{\n            ProcessState: {\n                pid: 7098,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 243772},\n                    Stime: {Sec: 0, Usec: 65006},\n                    Maxrss: 80920,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 9799,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 404,\n                    Nivcsw: 637,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661578505000,"stop":1661578505612,"duration":612}},{"uid":"2427e8ecbd83c59f","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004c8930>: {\n        Underlying: <*exec.ExitError | 0xc00071d9e0>{\n            ProcessState: {\n                pid: 7111,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 225803},\n                    Stime: {Sec: 0, Usec: 60483},\n                    Maxrss: 87144,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 7791,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 446,\n                    Nivcsw: 428,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661572479000,"stop":1661572479733,"duration":733}},{"uid":"17876f52c58a98a2","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0008bbae8>: {\n        Underlying: <*exec.ExitError | 0xc00050f6c0>{\n            ProcessState: {\n                pid: 7064,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 166399},\n                    Stime: {Sec: 0, Usec: 39618},\n                    Maxrss: 70312,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 6712,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 666,\n                    Nivcsw: 443,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660832281000,"stop":1660832282416,"duration":1416}},{"uid":"4d3469b67813ea59","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000496af8>: {\n        Underlying: <*exec.ExitError | 0xc0006ffee0>{\n            ProcessState: {\n                pid: 7034,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 168832},\n                    Stime: {Sec: 0, Usec: 64317},\n                    Maxrss: 80856,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 6535,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 375,\n                    Nivcsw: 309,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660808797000,"stop":1660808797484,"duration":484}},{"uid":"2478d1e6452ddce2","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0005cc9f0>: {\n        Underlying: <*exec.ExitError | 0xc000721de0>{\n            ProcessState: {\n                pid: 7213,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 145623},\n                    Stime: {Sec: 0, Usec: 47229},\n                    Maxrss: 85716,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 8175,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 266,\n                    Nivcsw: 403,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660299182000,"stop":1660299182380,"duration":380}},{"uid":"4a198e46afaf19c2","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00044b0c8>: {\n        Underlying: <*exec.ExitError | 0xc000806dc0>{\n            ProcessState: {\n                pid: 7190,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 117535},\n                    Stime: {Sec: 0, Usec: 39178},\n                    Maxrss: 82876,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3815,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 434,\n                    Nivcsw: 254,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660295740000,"stop":1660295740316,"duration":316}}]},"bfccb31f41ab2e93c84ed9afa4f7db5b":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"f695352f4be06d8b","status":"passed","time":{"start":1663838486720,"stop":1663838504563,"duration":17843}}]},"64ce5be23276ad8a76ff508b4fd1726c":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"87948cf6428d73ad","status":"passed","time":{"start":1663841056330,"stop":1663841071212,"duration":14882}}]},"e62cb9b3eff5b8ef0f75eafbdf440ecf":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"a1aa042c4ab4f693","status":"passed","time":{"start":1663839828586,"stop":1663839839467,"duration":10881}}]},"5fba37f0937ebd8c6d5d0f0fe3fb17ef":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"fc55182755902fa5","status":"passed","time":{"start":1663840069174,"stop":1663840081113,"duration":11939}}]},"600074a56cef76f11ea75b8fab6f7981":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"7957299af2cf740d","status":"passed","time":{"start":1663840013876,"stop":1663840023397,"duration":9521}}]},"Worker Suite:Worker Suite#[It]  QosProfile tests QosProfile Create validation Create qos profile- validation of BandwidthCeilingKbps > BandwidthGuaranteedKbps":{"statistic":{"failed":0,"broken":0,"skipped":10,"passed":6,"unknown":0,"total":16},"items":[{"uid":"91556b449685fc2b","status":"passed","time":{"start":1664094053000,"stop":1664094053219,"duration":219}},{"uid":"290597bb2fc8db02","status":"passed","time":{"start":1663843963000,"stop":1663843963336,"duration":336}},{"uid":"13b83867c59d3bbb","status":"passed","time":{"start":1663829327000,"stop":1663829327315,"duration":315}},{"uid":"c2fecb6d2a8047b7","status":"passed","time":{"start":1663343132000,"stop":1663343132212,"duration":212}},{"uid":"e187b9274d3780e","status":"passed","time":{"start":1663342947000,"stop":1663342947269,"duration":269}},{"uid":"26cf6a81352b3811","status":"skipped","statusDetails":"skipped","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"45ebfa1fd4dbe336","status":"skipped","statusDetails":"skipped","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"60c502c8247c16e1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662348908000,"stop":1662348908000,"duration":0}},{"uid":"2b06c9e7afb06aa9","status":"passed","time":{"start":1661844205000,"stop":1661844205433,"duration":433}},{"uid":"af3a6233c3ba205b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"57bd0b6779980827","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"a4aff9d305ef80e6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"f4fbed2a64c6fb2d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"2b5d0127fb0e4906","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"6ca8f28e3e9412eb","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"5285e9c69b2911f7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}}]},"4a2c7112fecf2789b5a0c1eb4059c9bc":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"ab5ced7ea44da4d7","status":"passed","time":{"start":1663841201751,"stop":1663841216722,"duration":14971}}]},"Worker Suite:Worker Suite#[It] Intracluster slice tests Iperf connectivity Verify Connectivity between iperf-server and iperf-client":{"statistic":{"failed":1,"broken":0,"skipped":50,"passed":5,"unknown":0,"total":56},"items":[{"uid":"d5758e4da31f508d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1664094053000,"stop":1664094053000,"duration":0}},{"uid":"fad94eeb2f157ea1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663843963000,"stop":1663843963000,"duration":0}},{"uid":"20aa105b6aa6c8de","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663829327000,"stop":1663829327000,"duration":0}},{"uid":"862c41684f8fc4be","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663343132000,"stop":1663343132000,"duration":0}},{"uid":"2a01a5ceba8077bc","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663342947000,"stop":1663342947000,"duration":0}},{"uid":"9ca5b48ec8878874","status":"skipped","statusDetails":"skipped","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"8d7648cd09d7ad1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"6053ef2c01cc8684","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662348908000,"stop":1662348908000,"duration":0}},{"uid":"f67efaaae396144a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"8cf9e67e9badeaec","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"1f30fb83398c4f92","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"9e11f540217918c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"805c5b76c832a715","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"2ef787ee050781d4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"c3273467e80db79b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"75434e9864c23f36","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"d6e56b7b30d1a516","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"e8fa2bd80b7fb2f1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"2562db67c3bdcf3f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"3b5c0947583c57f8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}}]},"257660193b22066f3faf9422b1dd19f1":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"dddb7d3340017410","status":"passed","time":{"start":1663840910647,"stop":1663840925586,"duration":14939}}]},"c83a17a5a8bcff38392bc20be69a6c2f":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"287c70e4906d5526","status":"passed","time":{"start":1663839353088,"stop":1663839364961,"duration":11873}}]},"f6484b4c7ef92aadf1c3a4e37367247a":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"2b3ef881893bd415","status":"passed","time":{"start":1663839430444,"stop":1663839463053,"duration":32609}}]},"Worker Suite:Worker Suite#[It] Hub netpol scenarios: Slice Configs Netpol scenarios Should fail when deploying with invalid namespace in both applicationNamespace and allowedNamespace":{"statistic":{"failed":0,"broken":0,"skipped":23,"passed":33,"unknown":0,"total":56},"items":[{"uid":"ea48a3eeb6c9571","status":"passed","time":{"start":1664094053000,"stop":1664094053592,"duration":592}},{"uid":"92975b9526681d80","status":"passed","time":{"start":1663843963000,"stop":1663843963945,"duration":945}},{"uid":"2300c40240557c5a","status":"passed","time":{"start":1663829327000,"stop":1663829327750,"duration":750}},{"uid":"2eb069baeec55b9d","status":"passed","time":{"start":1663343132000,"stop":1663343132928,"duration":928}},{"uid":"7f14a2d0eb3537bd","status":"passed","time":{"start":1663342947000,"stop":1663342947721,"duration":721}},{"uid":"b0cb9180a00d1221","status":"skipped","statusDetails":"skipped","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"5c20229f846ef78f","status":"passed","time":{"start":1663044069000,"stop":1663044069789,"duration":789}},{"uid":"8d388cdeb7687c0a","status":"passed","time":{"start":1662348908000,"stop":1662348908880,"duration":880}},{"uid":"7e4d991adad19255","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"e7835573633cd2ab","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"87da5c8ca5050dcc","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"9d5c8e407b5a6d62","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"6aaa73e2d4ba7cb6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"c4015dc7b703ce4c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"bd7ece63012204bc","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"c4e68c926e6e44ac","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"259d9609ee150f90","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"400f724cc595a88a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"c00eec3ebea0edae","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"f2285aa25e713989","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}}]},"Hub Suite:Hub Suite#[AfterSuite]":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":287,"unknown":0,"total":287},"items":[{"uid":"1e28191e7e30b864","status":"passed","time":{"start":1664275319000,"stop":1664275322216,"duration":3216}},{"uid":"60be180c5bf1db67","status":"passed","time":{"start":1664270369000,"stop":1664270371124,"duration":2124}},{"uid":"77bb423535481da3","status":"passed","time":{"start":1664245203000,"stop":1664245205977,"duration":2977}},{"uid":"45d116f4a53ba85b","status":"passed","time":{"start":1664160732000,"stop":1664160734256,"duration":2256}},{"uid":"aec7cdb045708945","status":"passed","time":{"start":1664091725000,"stop":1664091727192,"duration":2192}},{"uid":"1569776c8e6a6c44","status":"passed","time":{"start":1663918426000,"stop":1663918427500,"duration":1500}},{"uid":"ec8c3638a484b864","status":"passed","time":{"start":1663841627000,"stop":1663841630231,"duration":3231}},{"uid":"83d4361a55aecf4b","status":"passed","time":{"start":1663827045000,"stop":1663827047221,"duration":2221}},{"uid":"352d7f3ee66bd583","status":"passed","time":{"start":1663767974000,"stop":1663767976362,"duration":2362}},{"uid":"8229e371fd79dc3b","status":"passed","time":{"start":1663669677000,"stop":1663669679341,"duration":2341}},{"uid":"a429e708f3385ee8","status":"passed","time":{"start":1663665323000,"stop":1663665326270,"duration":3270}},{"uid":"ccb2f2d49e3abc9c","status":"passed","time":{"start":1663665170000,"stop":1663665172219,"duration":2219}},{"uid":"105905b326343caf","status":"passed","time":{"start":1663659067000,"stop":1663659067000,"duration":0}},{"uid":"da9f61c7e2896c6f","status":"passed","time":{"start":1663658517000,"stop":1663658519448,"duration":2448}},{"uid":"60276210ddf8669a","status":"passed","time":{"start":1663656809000,"stop":1663656809000,"duration":0}},{"uid":"91de216b7bb1c33","status":"passed","time":{"start":1663583877000,"stop":1663583880375,"duration":3375}},{"uid":"84c9f099139daf8e","status":"passed","time":{"start":1663340950000,"stop":1663340952233,"duration":2233}},{"uid":"f2d3e7aba755e087","status":"passed","time":{"start":1663340650000,"stop":1663340652511,"duration":2511}},{"uid":"522b4a3844a4e5de","status":"passed","time":{"start":1663308473000,"stop":1663308776330,"duration":303330}},{"uid":"1e473028207ec619","status":"passed","time":{"start":1663304356000,"stop":1663304658452,"duration":302452}}]},"Hub Suite:Hub Suite#[It] Cluster CR tests Cluster CR validation Creates cluster secrets":{"statistic":{"failed":11,"broken":0,"skipped":0,"passed":208,"unknown":0,"total":219},"items":[{"uid":"9494fe663e3d91c9","status":"passed","time":{"start":1664275319000,"stop":1664275322628,"duration":3628}},{"uid":"65d72563bb14525","status":"passed","time":{"start":1664245203000,"stop":1664245206805,"duration":3805}},{"uid":"83aeae6fa025b412","status":"passed","time":{"start":1664160732000,"stop":1664160735611,"duration":3611}},{"uid":"1df16b0ae61ffe69","status":"passed","time":{"start":1664091725000,"stop":1664091728594,"duration":3594}},{"uid":"a38b9c9bd7c00a37","status":"passed","time":{"start":1663841627000,"stop":1663841630755,"duration":3755}},{"uid":"c8e8c0d206eed25f","status":"passed","time":{"start":1663827045000,"stop":1663827048627,"duration":3627}},{"uid":"29f9792c7c6e813c","status":"passed","time":{"start":1663767974000,"stop":1663767978329,"duration":4329}},{"uid":"feb176edf258af2d","status":"passed","time":{"start":1663669677000,"stop":1663669680649,"duration":3649}},{"uid":"55cdb8210509a3bd","status":"passed","time":{"start":1663665323000,"stop":1663665326730,"duration":3730}},{"uid":"65fb07d4c96c266e","status":"passed","time":{"start":1663665170000,"stop":1663665178587,"duration":8587}},{"uid":"411a83cc572ac7ad","status":"passed","time":{"start":1663659067000,"stop":1663659070669,"duration":3669}},{"uid":"1e18a57ddf1b0a62","status":"passed","time":{"start":1663656809000,"stop":1663656818600,"duration":9600}},{"uid":"3167b2a747a8f844","status":"passed","time":{"start":1663583877000,"stop":1663583887997,"duration":10997}},{"uid":"f5f45f875ab3e14e","status":"passed","time":{"start":1663340950000,"stop":1663340954356,"duration":4356}},{"uid":"ac2a24ceff9a9c7b","status":"passed","time":{"start":1663340650000,"stop":1663340653580,"duration":3580}},{"uid":"d8ce6f952baa17cf","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004b32d8>: {\n        Underlying: <*exec.ExitError | 0xc000846780>{\n            ProcessState: {\n                pid: 11818,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 214545},\n                    Stime: {Sec: 0, Usec: 71515},\n                    Maxrss: 83648,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 10514,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 597,\n                    Nivcsw: 485,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Cluster%20CR%20tests%20Cluster%20CR%20validation%20Creates%20cluster%20secrets3979663928\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.135.179:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Cluster%20CR%20tests%20Cluster%20CR%20validation%20Creates%20cluster%20secrets3979663928\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.135.179:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Cluster%20CR%20tests%20Cluster%20CR%20validation%20Creates%20cluster%20secrets3979663928\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.135.179:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Cluster%20CR%20tests%20Cluster%20CR%20validation%20Creates%20cluster%20secrets3979663928\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.135.179:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Cluster%20CR%20tests%20Cluster%20CR%20validation%20Creates%20cluster%20secrets3979663928\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.135.179:443: connect: connection refused\noccurred","time":{"start":1663308473000,"stop":1663308473400,"duration":400}},{"uid":"4af7095dfddd3ea6","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000824918>: {\n        Underlying: <*exec.ExitError | 0xc0001574a0>{\n            ProcessState: {\n                pid: 6123,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 188789},\n                    Stime: {Sec: 0, Usec: 56235},\n                    Maxrss: 72872,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3798,\n                    Majflt: 3,\n                    Nswap: 0,\n                    Inblock: 584,\n                    Oublock: 13344,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 371,\n                    Nivcsw: 314,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Cluster%20CR%20tests%20Cluster%20CR%20validation%20Creates%20cluster%20secrets2478816777\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.26.65:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Cluster%20CR%20tests%20Cluster%20CR%20validation%20Creates%20cluster%20secrets2478816777\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.26.65:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Cluster%20CR%20tests%20Cluster%20CR%20validation%20Creates%20cluster%20secrets2478816777\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.26.65:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Cluster%20CR%20tests%20Cluster%20CR%20validation%20Creates%20cluster%20secrets2478816777\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.26.65:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Cluster%20CR%20tests%20Cluster%20CR%20validation%20Creates%20cluster%20secrets2478816777\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.26.65:443: connect: connection refused\noccurred","time":{"start":1663304356000,"stop":1663304357067,"duration":1067}},{"uid":"d6569049a4fd9cf3","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000642bd0>: {\n        Underlying: <*exec.ExitError | 0xc000626000>{\n            ProcessState: {\n                pid: 7615,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 166913},\n                    Stime: {Sec: 0, Usec: 48852},\n                    Maxrss: 84264,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 6683,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 383,\n                    Nivcsw: 230,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Cluster%20CR%20tests%20Cluster%20CR%20validation%20Creates%20cluster%20secrets4174281822\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.179.221:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Cluster%20CR%20tests%20Cluster%20CR%20validation%20Creates%20cluster%20secrets4174281822\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.179.221:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Cluster%20CR%20tests%20Cluster%20CR%20validation%20Creates%20cluster%20secrets4174281822\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.179.221:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Cluster%20CR%20tests%20Cluster%20CR%20validation%20Creates%20cluster%20secrets4174281822\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.179.221:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Cluster%20CR%20tests%20Cluster%20CR%20validation%20Creates%20cluster%20secrets4174281822\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.179.221:443: connect: connection refused\noccurred","time":{"start":1663302519000,"stop":1663302519252,"duration":252}},{"uid":"4f45d86280c65178","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0006382a0>: {\n        Underlying: <*exec.ExitError | 0xc0004fe7e0>{\n            ProcessState: {\n                pid: 7235,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 216655},\n                    Stime: {Sec: 0, Usec: 27574},\n                    Maxrss: 80336,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3059,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13344,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 258,\n                    Nivcsw: 202,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Cluster%20CR%20tests%20Cluster%20CR%20validation%20Creates%20cluster%20secrets2037107775\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.55.244:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Cluster%20CR%20tests%20Cluster%20CR%20validation%20Creates%20cluster%20secrets2037107775\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.55.244:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Cluster%20CR%20tests%20Cluster%20CR%20validation%20Creates%20cluster%20secrets2037107775\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.55.244:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Cluster%20CR%20tests%20Cluster%20CR%20validation%20Creates%20cluster%20secrets2037107775\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.55.244:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Cluster%20CR%20tests%20Cluster%20CR%20validation%20Creates%20cluster%20secrets2037107775\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.55.244:443: connect: connection refused\noccurred","time":{"start":1663264756000,"stop":1663264756314,"duration":314}},{"uid":"c551d77eb111e0e3","status":"passed","time":{"start":1663228190000,"stop":1663228202587,"duration":12587}}]},"Worker Suite:Worker Suite#[It] Intracluster slice tests Slice with netpol should label app ns with kubeslice label":{"statistic":{"failed":0,"broken":0,"skipped":38,"passed":18,"unknown":0,"total":56},"items":[{"uid":"e3e8354239956403","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1664094053000,"stop":1664094053000,"duration":0}},{"uid":"d66ed812d87bee73","status":"passed","time":{"start":1663843963000,"stop":1663843963162,"duration":162}},{"uid":"385953d16bd04db4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663829327000,"stop":1663829327000,"duration":0}},{"uid":"710350788835f569","status":"passed","time":{"start":1663343132000,"stop":1663343132080,"duration":80}},{"uid":"af6e967064e00cc1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663342947000,"stop":1663342947000,"duration":0}},{"uid":"5f8f919c9aa299d7","status":"skipped","statusDetails":"skipped","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"1583e1400f0d5e22","status":"passed","time":{"start":1663044069000,"stop":1663044069089,"duration":89}},{"uid":"a89fa6fee04905d5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662348908000,"stop":1662348908000,"duration":0}},{"uid":"1838bc4ef72e07b0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"d5f10d323dd924ea","status":"skipped","statusDetails":"skipped - Spec skipped because Skip() was called in BeforeAll","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"27d94491c2fccd09","status":"skipped","statusDetails":"skipped - Spec skipped because Skip() was called in BeforeAll","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"bb64b3d542664fd6","status":"skipped","statusDetails":"skipped - Spec skipped because Skip() was called in BeforeAll","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"9d99d083b562a135","status":"skipped","statusDetails":"skipped - Spec skipped because Skip() was called in BeforeAll","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"3f476438a702f6e1","status":"skipped","statusDetails":"skipped - Spec skipped because Skip() was called in BeforeAll","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"7b54da786f23381a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"261bf5b138165450","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"c8952cee7aa22be3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"3fc42fab330d955b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"1345357952dd5706","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"4cb071edab3c4f18","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}}]},"Istio Suite:Istio Suite#[It] Bookinfo tests Bookinfo Scenario 3 without egress and with ingress Should install Productpage on worker cluster-1":{"statistic":{"failed":58,"broken":0,"skipped":0,"passed":14,"unknown":0,"total":72},"items":[{"uid":"fdbed4505aec1c1b","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1664246558000,"stop":1664246770923,"duration":212923}},{"uid":"88ab2a77d024587e","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1664162065000,"stop":1664162245311,"duration":180311}},{"uid":"6758976db1743491","status":"passed","time":{"start":1664093039000,"stop":1664093176225,"duration":137225}},{"uid":"c7ed634c55a550e3","status":"passed","time":{"start":1663842833000,"stop":1663842988856,"duration":155856}},{"uid":"c31dd904d61b77a3","status":"passed","time":{"start":1663828380000,"stop":1663828591353,"duration":211353}},{"uid":"f62e33ceae0a980a","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1663769301000,"stop":1663769481354,"duration":180354}},{"uid":"7030d4545157123b","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1663671043000,"stop":1663671250899,"duration":207899}},{"uid":"8f3ab471083d4cc8","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 3\nto equal\n    <int>: 1","time":{"start":1663666667000,"stop":1663666847487,"duration":180487}},{"uid":"ac0345bff9dd09cf","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 4\nto equal\n    <int>: 1","time":{"start":1663666522000,"stop":1663666702375,"duration":180375}},{"uid":"53f98a4f5b05c15b","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1663585263000,"stop":1663585443688,"duration":180688}},{"uid":"8ecee4b704fb0584","status":"passed","time":{"start":1663342266000,"stop":1663342294220,"duration":28220}},{"uid":"7eae1e2bbd452283","status":"passed","time":{"start":1663341990000,"stop":1663342016105,"duration":26105}},{"uid":"d12336dc175e3669","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1663315204000,"stop":1663315384298,"duration":180298}},{"uid":"722adf6b4a0d6d63","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1663312558000,"stop":1663312738234,"duration":180234}},{"uid":"47da31b2554463ee","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1663055013000,"stop":1663055193333,"duration":180333}},{"uid":"853bb1188e2fc585","status":"passed","time":{"start":1663043831000,"stop":1663043856057,"duration":25057}},{"uid":"206e2eb4365a4288","status":"passed","time":{"start":1663043075000,"stop":1663043106640,"duration":31640}},{"uid":"7e99934f1a8adb1f","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1663040338000,"stop":1663040518660,"duration":180660}},{"uid":"89a3559cbc089770","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1662819574000,"stop":1662819754390,"duration":180390}},{"uid":"530cff9d698681fc","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1662795310000,"stop":1662795490260,"duration":180260}}]},"ca7eed59fd22c60ab16e67b6adf80531":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"76a81ca25bfe6adc","status":"passed","time":{"start":1663842045072,"stop":1663842083342,"duration":38270}}]},"f5d0ad890a355b3201fce3d9e86c7caa":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"61c1a5b54f72602a","status":"passed","time":{"start":1663840656327,"stop":1663840669969,"duration":13642}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Iperf connectivity should onboard deployment in application Namespaces without slice annotations":{"statistic":{"failed":25,"broken":0,"skipped":29,"passed":2,"unknown":0,"total":56},"items":[{"uid":"4458e162dbd3b7ea","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 1\nto equal\n    <int>: 2","time":{"start":1664094053000,"stop":1664094245568,"duration":192568}},{"uid":"93bd5df50fff5137","status":"failed","statusDetails":"Unexpected error:\n    <*errors.StatusError | 0xc000281ea0>: {\n        ErrStatus: {\n            TypeMeta: {Kind: \"\", APIVersion: \"\"},\n            ListMeta: {\n                SelfLink: \"\",\n                ResourceVersion: \"\",\n                Continue: \"\",\n                RemainingItemCount: nil,\n            },\n            Status: \"Failure\",\n            Message: \"namespaces \\\"iperf\\\" already exists\",\n            Reason: \"AlreadyExists\",\n            Details: {Name: \"iperf\", Group: \"\", Kind: \"namespaces\", UID: \"\", Causes: nil, RetryAfterSeconds: 0},\n            Code: 409,\n        },\n    }\n    namespaces \"iperf\" already exists\noccurred","time":{"start":1663843963000,"stop":1663843968340,"duration":5340}},{"uid":"6406b68d2c05c7dc","status":"passed","time":{"start":1663829327000,"stop":1663829342661,"duration":15661}},{"uid":"6c4a72d5279dfd43","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 1\nto equal\n    <int>: 2","time":{"start":1663343132000,"stop":1663343325144,"duration":193144}},{"uid":"135bbd01d2b3c44a","status":"failed","statusDetails":"Unexpected error:\n    <*errors.StatusError | 0xc000526dc0>: {\n        ErrStatus: {\n            TypeMeta: {Kind: \"\", APIVersion: \"\"},\n            ListMeta: {\n                SelfLink: \"\",\n                ResourceVersion: \"\",\n                Continue: \"\",\n                RemainingItemCount: nil,\n            },\n            Status: \"Failure\",\n            Message: \"namespaces \\\"iperf\\\" already exists\",\n            Reason: \"AlreadyExists\",\n            Details: {Name: \"iperf\", Group: \"\", Kind: \"namespaces\", UID: \"\", Causes: nil, RetryAfterSeconds: 0},\n            Code: 409,\n        },\n    }\n    namespaces \"iperf\" already exists\noccurred","time":{"start":1663342947000,"stop":1663342951681,"duration":4681}},{"uid":"7d20d0ca0ca0d4d1","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 1\nto equal\n    <int>: 2","time":{"start":1663044723000,"stop":1663044915461,"duration":192461}},{"uid":"3e9bfa113bca179e","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 1\nto equal\n    <int>: 2","time":{"start":1663044069000,"stop":1663044261779,"duration":192779}},{"uid":"eb87d4a35aed7ac4","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 1\nto equal\n    <int>: 2","time":{"start":1662348908000,"stop":1662349089516,"duration":181516}},{"uid":"483bca29438922cd","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"df2f7aa3c51583a9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"937763fb855eac10","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"679933505d2598e0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"45daf61ad3444d48","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"d4ac226b3e5b63e3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"2e6deea408a38baa","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"d7604b2a3ef79583","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"b1cffe7f741752f3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"dd40c43b37cfcc22","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"3fde2ff935cb2ac3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"c46cb01428e4c7ee","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}}]},"9604a65a8426e66272a7bc40f11466c5":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"28aed943ff4ce86d","status":"passed","time":{"start":1663840151451,"stop":1663840162101,"duration":10650}}]},"Worker Suite:Worker Suite#[It] Namespace reconcile test scenarios: Offboard remove label from app ns":{"statistic":{"failed":0,"broken":0,"skipped":10,"passed":46,"unknown":0,"total":56},"items":[{"uid":"d57915214fb6f558","status":"passed","time":{"start":1664094053000,"stop":1664094053167,"duration":167}},{"uid":"bc70f04648299c6","status":"passed","time":{"start":1663843963000,"stop":1663843963233,"duration":233}},{"uid":"a12844e21f8d5c1f","status":"passed","time":{"start":1663829327000,"stop":1663829327171,"duration":171}},{"uid":"5e0bccbb325f8fd4","status":"passed","time":{"start":1663343132000,"stop":1663343132290,"duration":290}},{"uid":"7ae8dc1aba4504cf","status":"passed","time":{"start":1663342947000,"stop":1663342947360,"duration":360}},{"uid":"bfc4f9a2e2e5723a","status":"skipped","statusDetails":"skipped","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"b2c4c71d143d0beb","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"4643a5166bbd7099","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662348908000,"stop":1662348908000,"duration":0}},{"uid":"150074768115e8b8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"7147981e97376569","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"382d3c4444646e35","status":"passed","time":{"start":1661611962000,"stop":1661611962248,"duration":248}},{"uid":"2c3b977708f7d23a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"51bf8601e2832aff","status":"passed","time":{"start":1661584447000,"stop":1661584447245,"duration":245}},{"uid":"60f922ffac3fa280","status":"passed","time":{"start":1661580045000,"stop":1661580045181,"duration":181}},{"uid":"94bc58ef10cbf8aa","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"7cf8f62b9e4ca910","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"8dd2881a312ad599","status":"passed","time":{"start":1660832281000,"stop":1660832281176,"duration":176}},{"uid":"bd98bd3c5fa6fa56","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"f32a5516494d50f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"13d279c71956fc1b","status":"passed","time":{"start":1660295740000,"stop":1660295740167,"duration":167}}]},"67d89b96557582d248c0deece59d6a5c":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"77baf42838ed69df","status":"passed","time":{"start":1663838705425,"stop":1663838719317,"duration":13892}}]},"4e2d065280cb09b843d6598a0d84f650":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"2618b540c80feab3","status":"passed","time":{"start":1663838675831,"stop":1663838690675,"duration":14844}}]},"Istio Suite:Istio Suite#[AfterSuite]":{"statistic":{"failed":186,"broken":23,"skipped":0,"passed":23,"unknown":0,"total":232},"items":[{"uid":"b9f0228fb49cfd66","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000133dd0>: {\n        Underlying: <*exec.ExitError | 0xc0005ffae0>{\n            ProcessState: {\n                pid: 6131,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 35948},\n                    Stime: {Sec: 0, Usec: 8987},\n                    Maxrss: 41832,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 1856,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 172,\n                    Nivcsw: 78,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error: uninstall: Release not loaded: istiod: release: not found\noccurred","time":{"start":1664282730000,"stop":1664282730181,"duration":181}},{"uid":"b25d6e4f19bccb89","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000e438>: {\n        Underlying: <*exec.ExitError | 0xc0004418e0>{\n            ProcessState: {\n                pid: 6137,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 29802},\n                    Stime: {Sec: 0, Usec: 12772},\n                    Maxrss: 41940,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 1913,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 141,\n                    Nivcsw: 60,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error: uninstall: Release not loaded: istiod: release: not found\noccurred","time":{"start":1664279399000,"stop":1664279399170,"duration":170}},{"uid":"45c42dc403307fe9","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00015d290>: {\n        Underlying: <*exec.ExitError | 0xc00089c4a0>{\n            ProcessState: {\n                pid: 6170,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 42450},\n                    Stime: {Sec: 0, Usec: 7718},\n                    Maxrss: 41708,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2388,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 134,\n                    Nivcsw: 129,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    Error: uninstall: Release not loaded: istiod: release: not found\noccurred","time":{"start":1664277957000,"stop":1664277958104,"duration":1104}},{"uid":"3789874d98c21fd2","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0006164b0>: {\n        Underlying: <*exec.ExitError | 0xc0006952a0>{\n            ProcessState: {\n                pid: 6160,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 47458},\n                    Stime: {Sec: 0, Usec: 15819},\n                    Maxrss: 44816,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 1929,\n                    Majflt: 1,\n                    Nswap: 0,\n                    Inblock: 128,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 253,\n                    Nivcsw: 351,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    Error: uninstall: Release not loaded: istiod: release: not found\noccurred","time":{"start":1664275460000,"stop":1664275461295,"duration":1295}},{"uid":"659162920372ee00","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0006b4168>: {\n        Underlying: <*exec.ExitError | 0xc0006d25e0>{\n            ProcessState: {\n                pid: 6075,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 39180},\n                    Stime: {Sec: 0, Usec: 21370},\n                    Maxrss: 45500,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2421,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 156,\n                    Nivcsw: 210,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    Error: uninstall: Release not loaded: istiod: release: not found\noccurred","time":{"start":1664270759000,"stop":1664270759320,"duration":320}},{"uid":"53199b7dbfaeab32","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000337c68>: {\n        Underlying: <*exec.ExitError | 0xc0004c4e00>{\n            ProcessState: {\n                pid: 6175,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 41063},\n                    Stime: {Sec: 0, Usec: 13687},\n                    Maxrss: 43348,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 1884,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 275,\n                    Nivcsw: 479,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    Error: uninstall: Release not loaded: istiod: release: not found\noccurred","time":{"start":1664261433000,"stop":1664261434385,"duration":1385}},{"uid":"4dbbf8c283ab8849","status":"failed","statusDetails":"Timed out after 210.009s.\nExpected an error to have occurred.  Got:\n    <nil>: nil","time":{"start":1664246558000,"stop":1664246781575,"duration":223575}},{"uid":"fbd2e97407e3d585","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000353380>: {\n        Underlying: <*exec.ExitError | 0xc00048d180>{\n            ProcessState: {\n                pid: 6167,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 44546},\n                    Stime: {Sec: 0, Usec: 8099},\n                    Maxrss: 44500,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 1898,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 156,\n                    Nivcsw: 150,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    Error: uninstall: Release not loaded: istiod: release: not found\noccurred","time":{"start":1664218168000,"stop":1664218169263,"duration":1263}},{"uid":"3a1365b7d09c9572","status":"failed","statusDetails":"Timed out after 210.009s.\nExpected an error to have occurred.  Got:\n    <nil>: nil","time":{"start":1664162065000,"stop":1664162283728,"duration":218728}},{"uid":"52ea20d947fb03fe","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004ea480>: {\n        Underlying: <*exec.ExitError | 0xc0003bb460>{\n            ProcessState: {\n                pid: 6147,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 39109},\n                    Stime: {Sec: 0, Usec: 24443},\n                    Maxrss: 45608,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 1860,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 332,\n                    Nivcsw: 163,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    Error: uninstall: Release not loaded: istiod: release: not found\noccurred","time":{"start":1664106114000,"stop":1664106115641,"duration":1641}},{"uid":"2ea9d7147dd84d53","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000fb90>: {\n        Underlying: <*exec.ExitError | 0xc000b086c0>{\n            ProcessState: {\n                pid: 6152,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 38920},\n                    Stime: {Sec: 0, Usec: 7076},\n                    Maxrss: 45104,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2355,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 345,\n                    Nivcsw: 289,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    Error: uninstall: Release not loaded: istiod: release: not found\noccurred","time":{"start":1664104103000,"stop":1664104104324,"duration":1324}},{"uid":"406d9118fee8c9fa","status":"passed","time":{"start":1664093039000,"stop":1664093069834,"duration":30834}},{"uid":"a8142292a5c20e43","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0003db668>: {\n        Underlying: <*exec.ExitError | 0xc0003e1960>{\n            ProcessState: {\n                pid: 6109,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 56632},\n                    Stime: {Sec: 0, Usec: 10618},\n                    Maxrss: 42876,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2146,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 264,\n                    Nivcsw: 132,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    Error: uninstall: Release not loaded: istiod: release: not found\noccurred","time":{"start":1663933075000,"stop":1663933076743,"duration":1743}},{"uid":"a291a8b00c2baf97","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000a98be8>: {\n        Underlying: <*exec.ExitError | 0xc000ae27e0>{\n            ProcessState: {\n                pid: 6095,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 35172},\n                    Stime: {Sec: 0, Usec: 11724},\n                    Maxrss: 42188,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 1877,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 191,\n                    Nivcsw: 198,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    Error: uninstall: Release not loaded: istiod: release: not found\noccurred","time":{"start":1663918810000,"stop":1663918810207,"duration":207}},{"uid":"847283f00ad2c7f2","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004ea828>: {\n        Underlying: <*exec.ExitError | 0xc0007b9ca0>{\n            ProcessState: {\n                pid: 6138,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 33435},\n                    Stime: {Sec: 0, Usec: 14860},\n                    Maxrss: 43208,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 1876,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 174,\n                    Nivcsw: 77,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    Error: uninstall: Release not loaded: istiod: release: not found\noccurred","time":{"start":1663912526000,"stop":1663912527213,"duration":1213}},{"uid":"50c8642d3db37d30","status":"passed","time":{"start":1663842833000,"stop":1663842872805,"duration":39805}},{"uid":"a8c98c0654fc0c0","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000708630>: {\n        Underlying: <*exec.ExitError | 0xc000075f00>{\n            ProcessState: {\n                pid: 6122,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 41656},\n                    Stime: {Sec: 0, Usec: 11360},\n                    Maxrss: 45540,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 1891,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 237,\n                    Nivcsw: 255,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    Error: uninstall: Release not loaded: istiod: release: not found\noccurred","time":{"start":1663838978000,"stop":1663838979248,"duration":1248}},{"uid":"26b4b9d5848da512","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004c07f8>: {\n        Underlying: <*exec.ExitError | 0xc0004f1620>{\n            ProcessState: {\n                pid: 6130,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 54096},\n                    Stime: {Sec: 0, Usec: 7212},\n                    Maxrss: 40588,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 1883,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 176,\n                    Nivcsw: 124,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    Error: uninstall: Release not loaded: istiod: release: not found\noccurred","time":{"start":1663838505000,"stop":1663838506701,"duration":1701}},{"uid":"27ad0ff5e6657fd","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000eb28>: {\n        Underlying: <*exec.ExitError | 0xc00082c460>{\n            ProcessState: {\n                pid: 6176,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 44611},\n                    Stime: {Sec: 0, Usec: 7435},\n                    Maxrss: 41680,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 1883,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 308,\n                    Nivcsw: 268,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    Error: uninstall: Release not loaded: istiod: release: not found\noccurred","time":{"start":1663836494000,"stop":1663836495303,"duration":1303}},{"uid":"7f6734eb715bfa9","status":"passed","time":{"start":1663828380000,"stop":1663828413168,"duration":33168}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Creation Should fail while creating Project with  project name as blank":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":219,"unknown":0,"total":219},"items":[{"uid":"131419e90814297e","status":"passed","time":{"start":1664275319000,"stop":1664275319274,"duration":274}},{"uid":"ed541e7e193bf258","status":"passed","time":{"start":1664245203000,"stop":1664245203342,"duration":342}},{"uid":"149dc8ec135c2321","status":"passed","time":{"start":1664160732000,"stop":1664160732260,"duration":260}},{"uid":"2eb1ea395cd94466","status":"passed","time":{"start":1664091725000,"stop":1664091725242,"duration":242}},{"uid":"7e2fc7b57b307f8","status":"passed","time":{"start":1663841627000,"stop":1663841627299,"duration":299}},{"uid":"79b784cc3e1c241d","status":"passed","time":{"start":1663827045000,"stop":1663827045253,"duration":253}},{"uid":"5a255f797df762fe","status":"passed","time":{"start":1663767974000,"stop":1663767974265,"duration":265}},{"uid":"336b30c79230f5f9","status":"passed","time":{"start":1663669677000,"stop":1663669677247,"duration":247}},{"uid":"7932327bec67bc1b","status":"passed","time":{"start":1663665323000,"stop":1663665323345,"duration":345}},{"uid":"232d4eac73f91dfe","status":"passed","time":{"start":1663665170000,"stop":1663665170285,"duration":285}},{"uid":"ceb9aca0c363175c","status":"passed","time":{"start":1663659067000,"stop":1663659067282,"duration":282}},{"uid":"6e5c30ce75c1e4fd","status":"passed","time":{"start":1663656809000,"stop":1663656809281,"duration":281}},{"uid":"87cac53e0e269c48","status":"passed","time":{"start":1663583877000,"stop":1663583877299,"duration":299}},{"uid":"580fa1629864343e","status":"passed","time":{"start":1663340950000,"stop":1663340950235,"duration":235}},{"uid":"7f1c27faa32e258c","status":"passed","time":{"start":1663340650000,"stop":1663340650252,"duration":252}},{"uid":"a4d63c95dc267529","status":"passed","time":{"start":1663308473000,"stop":1663308473366,"duration":366}},{"uid":"c524204cd486c084","status":"passed","time":{"start":1663304356000,"stop":1663304356277,"duration":277}},{"uid":"93b6085020311af2","status":"passed","time":{"start":1663302519000,"stop":1663302519243,"duration":243}},{"uid":"5f81a2ab98c92385","status":"passed","time":{"start":1663264756000,"stop":1663264756324,"duration":324}},{"uid":"4247829a0fbaaf0c","status":"passed","time":{"start":1663228190000,"stop":1663228190263,"duration":263}}]},"e159879d7708708041fb74f4ff430d9a":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"1d28aa21ed1b88ab","status":"passed","time":{"start":1663841216722,"stop":1663841229989,"duration":13267}}]},"c94b341ce5364d8301cb9ba0250da8cc":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"4c5c1ccc76e1b581","status":"passed","time":{"start":1663841025975,"stop":1663841038129,"duration":12154}}]},"ca7692694a2409d45091aabfac697d90":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"5d25704abdbc2638","status":"passed","time":{"start":1663841484011,"stop":1663841506334,"duration":22323}}]},"93dcccaf425638cb0a13d59c8000b88a":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"e4b2862b9f1a9c5e","status":"passed","time":{"start":1663840301779,"stop":1663840303193,"duration":1414}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests Iperf App connectivity Verify Iperf traffic":{"statistic":{"failed":0,"broken":0,"skipped":48,"passed":8,"unknown":0,"total":56},"items":[{"uid":"bbfc0fc0bfe2fdc9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1664094053000,"stop":1664094053000,"duration":0}},{"uid":"7c837e0cb4530123","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663843963000,"stop":1663843963000,"duration":0}},{"uid":"9a9c9a97a1b27e72","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663829327000,"stop":1663829327000,"duration":0}},{"uid":"af2f8c04fd2113fc","status":"passed","time":{"start":1663343132000,"stop":1663343193437,"duration":61437}},{"uid":"b9003bac404d98d1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663342947000,"stop":1663342947000,"duration":0}},{"uid":"fd32469b1f452c0d","status":"skipped","statusDetails":"skipped","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"b76c7de624509035","status":"skipped","statusDetails":"skipped","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"d64a508b4c515d9b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662348908000,"stop":1662348908000,"duration":0}},{"uid":"bbea5e6d6e7b4675","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"b7a3b84ce7cf26c3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"a7708c616edd8226","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"bb1b35d191c2acb9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"3f245019dffdec0e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"2a5331ecc78098db","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"5947ec98cf9bcd2a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"bbc8cd25d94a43e3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"36688e4fe4c16aef","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"528127249e998e0a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"a0bc99e2d05b09c8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"ffc67f6897dc058f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}}]},"5fd7ace8d761bb6cfce499e5aa44ee3":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"dc347f94d72f8e4","status":"passed","time":{"start":1663841742751,"stop":1663841776940,"duration":34189}}]},"a639a67070212e6b6bdd9902c445c6f8":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"51a900f9f7172543","status":"passed","time":{"start":1663839400955,"stop":1663839413919,"duration":12964}}]},"ab3f6742397daf36a2b33e1da587e363":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"5f635632ef31a5c5","status":"passed","time":{"start":1663838588708,"stop":1663838597766,"duration":9058}}]},"82833b1884de041f24ce68c05dc9da70":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"93d05ae4a9c4720e","status":"passed","time":{"start":1663838854968,"stop":1663838864339,"duration":9371}}]},"3fead199424f604eb908c63775bbc5fe":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"af379cbb2046e788","status":"passed","time":{"start":1663840393626,"stop":1663840408256,"duration":14630}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Iperf connectivity Verify Connectivity between iperf-server and iperf-client":{"statistic":{"failed":1,"broken":0,"skipped":54,"passed":1,"unknown":0,"total":56},"items":[{"uid":"ec985481b29a2ef","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1664094053000,"stop":1664094053000,"duration":0}},{"uid":"fcecf52cb6791a8a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663843963000,"stop":1663843963000,"duration":0}},{"uid":"475bc41dd0aa2b63","status":"passed","time":{"start":1663829327000,"stop":1663829350085,"duration":23085}},{"uid":"b7037ee3326e204c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663343132000,"stop":1663343132000,"duration":0}},{"uid":"4e4062c7ff8628d1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663342947000,"stop":1663342947000,"duration":0}},{"uid":"4e26c9f599aebc63","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"1a51936f010b6639","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"715f01b458fb8a3b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662348908000,"stop":1662348908000,"duration":0}},{"uid":"58956537c26e4720","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"ad17f0b975fdde94","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"561198aa8b5d8fb","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"27c5715c8e0e140","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"c1f288fd03623b1e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"43c8d8d03e6cf987","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"3f97f4c0d1044bc9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"57dd4203843d73cf","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"5ea5e08694755c4c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"7c90237538704011","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"9c9283b2f17a3ebc","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"ed0ba7f9b7b79cff","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}}]},"5bb519c4561ca268a785a651eae1e5b0":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"82380767051dea9e","status":"passed","time":{"start":1663840956535,"stop":1663840977862,"duration":21327}}]},"c91d9e35d51d0279a8c05d74685cd223":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"2fd10f91f9adb9a9","status":"passed","time":{"start":1663839364963,"stop":1663839375646,"duration":10683}}]},"3fa0feb55035e51d12383770252ab06d":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"a50ebd9017533359","status":"passed","time":{"start":1663839937611,"stop":1663839956550,"duration":18939}}]},"Worker Suite:Worker Suite#[It]  QosProfile tests QosProfile delete validation should remove vl3 router from spoke":{"statistic":{"failed":0,"broken":0,"skipped":11,"passed":5,"unknown":0,"total":16},"items":[{"uid":"736140c07cf58611","status":"passed","time":{"start":1664094053000,"stop":1664094056024,"duration":3024}},{"uid":"68f42c83d3588573","status":"passed","time":{"start":1663843963000,"stop":1663843966059,"duration":3059}},{"uid":"9c91dc6d62460d0f","status":"passed","time":{"start":1663829327000,"stop":1663829330050,"duration":3050}},{"uid":"3b00fad194cece6c","status":"passed","time":{"start":1663343132000,"stop":1663343135025,"duration":3025}},{"uid":"b885bb5a173310fe","status":"passed","time":{"start":1663342947000,"stop":1663342950038,"duration":3038}},{"uid":"d2b0a17f2aed919f","status":"skipped","statusDetails":"skipped","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"6da0d933e7c5eac0","status":"skipped","statusDetails":"skipped","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"e13dc9d47891f78c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662348908000,"stop":1662348908000,"duration":0}},{"uid":"928d06893d2bebf","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"6e9ed6f26ded6729","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"cd34f3e751b7eeca","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"47c3e4db18675120","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"141694a06b33f5ec","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"da46f12d4199ed81","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"a445775450d4e3a3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"e493a900973d166a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}}]},"Hub Suite:Hub Suite#[It] Cluster negative tests Worker Cluster Registration with Wrong token":{"statistic":{"failed":0,"broken":0,"skipped":37,"passed":182,"unknown":0,"total":219},"items":[{"uid":"6ef1b77bddf162d0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1664275319000,"stop":1664275319000,"duration":0}},{"uid":"2d28e8dd6597439f","status":"passed","time":{"start":1664245203000,"stop":1664245513370,"duration":310370}},{"uid":"504a0d134e496cd4","status":"passed","time":{"start":1664160732000,"stop":1664161046992,"duration":314992}},{"uid":"7b0f90e2856700a7","status":"passed","time":{"start":1664091725000,"stop":1664092039065,"duration":314065}},{"uid":"d67a66553c02c7ba","status":"passed","time":{"start":1663841627000,"stop":1663841938909,"duration":311909}},{"uid":"2770f72e1bd0eb21","status":"passed","time":{"start":1663827045000,"stop":1663827353978,"duration":308978}},{"uid":"114a568c1387ada","status":"passed","time":{"start":1663767974000,"stop":1663768288136,"duration":314136}},{"uid":"2801f49869f6d0af","status":"passed","time":{"start":1663669677000,"stop":1663669986728,"duration":309728}},{"uid":"26f814a47c55b200","status":"passed","time":{"start":1663665323000,"stop":1663665639491,"duration":316491}},{"uid":"4b294134508c7c29","status":"passed","time":{"start":1663665170000,"stop":1663665484824,"duration":314824}},{"uid":"472c52e77d57aae6","status":"passed","time":{"start":1663659067000,"stop":1663659376821,"duration":309821}},{"uid":"5217c08db3cff698","status":"passed","time":{"start":1663656809000,"stop":1663657123693,"duration":314693}},{"uid":"b5e3485c872370ec","status":"passed","time":{"start":1663583877000,"stop":1663584188764,"duration":311764}},{"uid":"734b2efa2043fd5e","status":"passed","time":{"start":1663340950000,"stop":1663341259796,"duration":309796}},{"uid":"57fbd6d4978ec92b","status":"passed","time":{"start":1663340650000,"stop":1663340964748,"duration":314748}},{"uid":"a4c8fa413d624197","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663308473000,"stop":1663308473000,"duration":0}},{"uid":"b4adac587eb8f293","status":"passed","time":{"start":1663304356000,"stop":1663304670782,"duration":314782}},{"uid":"1287b0c2145c3267","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663302519000,"stop":1663302519000,"duration":0}},{"uid":"f356f6debba08509","status":"passed","time":{"start":1663264756000,"stop":1663265066810,"duration":310810}},{"uid":"764a1c49be10e336","status":"passed","time":{"start":1663228190000,"stop":1663228501011,"duration":311011}}]},"7ac7c9f8e242377c160cfe88805659a":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"1a89131169505b24","status":"passed","time":{"start":1663840533443,"stop":1663840550676,"duration":17233}}]},"Worker Suite:Worker Suite#[It] Slice Creation scenarios Slice with MEDIUM QOS should have vl3 router running":{"statistic":{"failed":17,"broken":0,"skipped":2,"passed":28,"unknown":0,"total":47},"items":[{"uid":"a38bfb7f2bc702ee","status":"passed","time":{"start":1664094053000,"stop":1664094056547,"duration":3547}},{"uid":"dbd76a8627636973","status":"passed","time":{"start":1663843963000,"stop":1663843968919,"duration":5919}},{"uid":"18a5bd8645d798e6","status":"passed","time":{"start":1663829327000,"stop":1663829331644,"duration":4644}},{"uid":"2dbfcca029e24a33","status":"passed","time":{"start":1663343132000,"stop":1663343135591,"duration":3591}},{"uid":"e11ddc3a9afb2418","status":"passed","time":{"start":1663342947000,"stop":1663342952972,"duration":5972}},{"uid":"ce0749b8ed26af92","status":"skipped","statusDetails":"skipped","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"564ab726673cac7d","status":"skipped","statusDetails":"skipped","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"e8a238e122bc4c6","status":"passed","time":{"start":1662348908000,"stop":1662348912857,"duration":4857}},{"uid":"da87875a46f2e64d","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0002ba510>: {\n        Underlying: <*exec.ExitError | 0xc000490b20>{\n            ProcessState: {\n                pid: 7287,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 246248},\n                    Stime: {Sec: 0, Usec: 49249},\n                    Maxrss: 89296,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4481,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 428,\n                    Nivcsw: 682,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicemediumqos\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\noccurred","time":{"start":1661844205000,"stop":1661844205686,"duration":686}},{"uid":"4accd64d5510a2f1","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000734b70>: {\n        Underlying: <*exec.ExitError | 0xc00017efc0>{\n            ProcessState: {\n                pid: 7222,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 158146},\n                    Stime: {Sec: 0, Usec: 48660},\n                    Maxrss: 82676,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2742,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 436,\n                    Nivcsw: 450,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicemediumqos\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661754870000,"stop":1661754870285,"duration":285}},{"uid":"c44610eb5ac8323d","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0008cfae8>: {\n        Underlying: <*exec.ExitError | 0xc0006d3be0>{\n            ProcessState: {\n                pid: 7301,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 165625},\n                    Stime: {Sec: 0, Usec: 46715},\n                    Maxrss: 72040,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3838,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 263,\n                    Nivcsw: 385,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicemediumqos\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661611962000,"stop":1661611962277,"duration":277}},{"uid":"7a70c0fedbe84237","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0009d9c68>: {\n        Underlying: <*exec.ExitError | 0xc0007ab3c0>{\n            ProcessState: {\n                pid: 7104,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 177356},\n                    Stime: {Sec: 0, Usec: 61689},\n                    Maxrss: 85328,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3772,\n                    Majflt: 1,\n                    Nswap: 0,\n                    Inblock: 264,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 381,\n                    Nivcsw: 392,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicemediumqos\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661585077000,"stop":1661585077316,"duration":316}},{"uid":"b6a1219c68bedef","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000e0d8>: {\n        Underlying: <*exec.ExitError | 0xc0003b1c40>{\n            ProcessState: {\n                pid: 6247,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 188764},\n                    Stime: {Sec: 0, Usec: 48195},\n                    Maxrss: 87308,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4174,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 311,\n                    Nivcsw: 294,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicemediumqos\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661584447000,"stop":1661584447298,"duration":298}},{"uid":"ef7cb4766ac2f668","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0006d2780>: {\n        Underlying: <*exec.ExitError | 0xc000075b20>{\n            ProcessState: {\n                pid: 7294,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 186662},\n                    Stime: {Sec: 0, Usec: 33599},\n                    Maxrss: 75548,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3565,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 417,\n                    Nivcsw: 441,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicemediumqos\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661580045000,"stop":1661580045307,"duration":307}},{"uid":"7359d02804ffa348","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000779650>: {\n        Underlying: <*exec.ExitError | 0xc0007384e0>{\n            ProcessState: {\n                pid: 7289,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 230185},\n                    Stime: {Sec: 0, Usec: 53709},\n                    Maxrss: 89996,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4300,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 354,\n                    Nivcsw: 414,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicemediumqos\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661578505000,"stop":1661578505364,"duration":364}},{"uid":"e6e7c5b073ce0b9c","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0008a2198>: {\n        Underlying: <*exec.ExitError | 0xc00074c640>{\n            ProcessState: {\n                pid: 7234,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 232731},\n                    Stime: {Sec: 0, Usec: 77577},\n                    Maxrss: 90280,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 8943,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 499,\n                    Nivcsw: 394,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicemediumqos\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661572479000,"stop":1661572479379,"duration":379}},{"uid":"2f33dafb51f6da0b","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004c2a50>: {\n        Underlying: <*exec.ExitError | 0xc00076dec0>{\n            ProcessState: {\n                pid: 7146,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 148251},\n                    Stime: {Sec: 0, Usec: 53535},\n                    Maxrss: 89516,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 5155,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 314,\n                    Nivcsw: 360,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicemediumqos\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660832281000,"stop":1660832281276,"duration":276}},{"uid":"e96433d83a0cae1","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004dd020>: {\n        Underlying: <*exec.ExitError | 0xc0007d47e0>{\n            ProcessState: {\n                pid: 7217,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 130784},\n                    Stime: {Sec: 0, Usec: 54493},\n                    Maxrss: 89336,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3769,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 261,\n                    Nivcsw: 265,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicemediumqos\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660808797000,"stop":1660808797260,"duration":260}},{"uid":"d5c79c81ac0886da","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0005cc258>: {\n        Underlying: <*exec.ExitError | 0xc0007209c0>{\n            ProcessState: {\n                pid: 7120,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 133532},\n                    Stime: {Sec: 0, Usec: 34460},\n                    Maxrss: 85304,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 6881,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 377,\n                    Nivcsw: 265,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicemediumqos\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660299182000,"stop":1660299182227,"duration":227}},{"uid":"6454ddafda446f0c","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0000cf878>: {\n        Underlying: <*exec.ExitError | 0xc00078ba20>{\n            ProcessState: {\n                pid: 7175,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 117176},\n                    Stime: {Sec: 0, Usec: 56418},\n                    Maxrss: 86588,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 6539,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 340,\n                    Nivcsw: 285,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicemediumqos\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660295740000,"stop":1660295740257,"duration":257}}]},"991374539383e8ef07445a72b5eef2c5":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"e0bd47d47ad98ced","status":"passed","time":{"start":1663839329405,"stop":1663839342718,"duration":13313}}]},"Worker Suite:Worker Suite#[It] Slice creation scenarios: when slice red is installed workers should have vl3 router pods running":{"statistic":{"failed":23,"broken":0,"skipped":13,"passed":20,"unknown":0,"total":56},"items":[{"uid":"9bdb71645b44db64","status":"passed","time":{"start":1664094053000,"stop":1664094055027,"duration":2027}},{"uid":"62a76d883ad80e44","status":"passed","time":{"start":1663843963000,"stop":1663843967064,"duration":4064}},{"uid":"e2bb1390df8783cf","status":"passed","time":{"start":1663829327000,"stop":1663829331040,"duration":4040}},{"uid":"be0a2ab8048d7903","status":"passed","time":{"start":1663343132000,"stop":1663343136039,"duration":4039}},{"uid":"90b9b4dfb5d8d9d2","status":"passed","time":{"start":1663342947000,"stop":1663342949059,"duration":2059}},{"uid":"aaf8d84553250793","status":"passed","time":{"start":1663044723000,"stop":1663044729032,"duration":6032}},{"uid":"3c1875d347274211","status":"passed","time":{"start":1663044069000,"stop":1663044071021,"duration":2021}},{"uid":"f22e65ae4ae3a09d","status":"passed","time":{"start":1662348908000,"stop":1662348910025,"duration":2025}},{"uid":"476f7dc314b3bce0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"98c3a37734d63a3f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"3b7ccc813960397c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"4c006875a54ffd0f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"8753af89e1455bd9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"37bb0eff2eea1b2f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"bcbe41593efc2599","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"10487af7ba0b9060","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"c1444d947f57f320","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"bba39c270532456","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"a721720e8cf1c1e1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"43f281e297cc3e49","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}}]},"2a86f13ee7be131ee865481ad0e1a248":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"233880013334c6e4","status":"passed","time":{"start":1663843140792,"stop":1663843188275,"duration":47483}}]},"16feb8e78f39bd5b39771692c1491be6":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"63210dadbbd77f37","status":"passed","time":{"start":1663838964417,"stop":1663838976104,"duration":11687}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests Iperf App connectivity Application namespaces should be isolated":{"statistic":{"failed":8,"broken":0,"skipped":48,"passed":0,"unknown":0,"total":56},"items":[{"uid":"abe16dc9ab918677","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1664094053000,"stop":1664094053000,"duration":0}},{"uid":"cc50ebeaa6662a51","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663843963000,"stop":1663843963000,"duration":0}},{"uid":"1f7577b4d45933ab","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663829327000,"stop":1663829327000,"duration":0}},{"uid":"a12e931c77c24ac3","status":"failed","statusDetails":"Expected\n    <bool>: true\nto equal\n    <bool>: false","time":{"start":1663343132000,"stop":1663343181734,"duration":49734}},{"uid":"6ff91accf4bc95e5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663342947000,"stop":1663342947000,"duration":0}},{"uid":"f3cfad3f43784680","status":"skipped","statusDetails":"skipped","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"e60ddc5af962d85b","status":"skipped","statusDetails":"skipped","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"9c5aca12a707b5c7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662348908000,"stop":1662348908000,"duration":0}},{"uid":"ce8bfceeb4ba4f89","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"122e120bcd6fb0a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"7600059cd7c06648","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"92fd8f5d5e1fa968","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"29cb0009afab34e1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"4416d79eee90bcfc","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"5b484616f871c2a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"bdcfed8721ed3c00","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"b44be581377fe210","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"d97c93c657a33794","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"9725a16fe4e8c7af","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"1c93cf49c8f0cdca","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}}]},"Istio Suite:Istio Suite#[It] Bookinfo tests Bookinfo Scenario 3 without egress and with ingress Should install Details, Reviews, Ratings and Serviceexport on worker cluster-2":{"statistic":{"failed":0,"broken":0,"skipped":58,"passed":14,"unknown":0,"total":72},"items":[{"uid":"5a01a54892757630","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1664246558000,"stop":1664246558000,"duration":0}},{"uid":"820f64de7150fde0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1664162065000,"stop":1664162065000,"duration":0}},{"uid":"b71544abb9d8c116","status":"passed","time":{"start":1664093039000,"stop":1664093104409,"duration":65409}},{"uid":"d98c010df6fc459","status":"passed","time":{"start":1663842833000,"stop":1663842919535,"duration":86535}},{"uid":"b6a8980ed6689988","status":"passed","time":{"start":1663828380000,"stop":1663828444356,"duration":64356}},{"uid":"aac40c4370b20113","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663769301000,"stop":1663769301000,"duration":0}},{"uid":"2c107d2ad45cfb96","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663671043000,"stop":1663671043000,"duration":0}},{"uid":"db69fbbfb7020405","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663666667000,"stop":1663666667000,"duration":0}},{"uid":"66847e3992d95e48","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663666522000,"stop":1663666522000,"duration":0}},{"uid":"4f613ee8246a8b62","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663585263000,"stop":1663585263000,"duration":0}},{"uid":"69dd62740802d88","status":"passed","time":{"start":1663342266000,"stop":1663342298061,"duration":32061}},{"uid":"f53b388c65798281","status":"passed","time":{"start":1663341990000,"stop":1663342023901,"duration":33901}},{"uid":"c33cc62588a43755","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663315204000,"stop":1663315204000,"duration":0}},{"uid":"202749964dafff7c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663312558000,"stop":1663312558000,"duration":0}},{"uid":"8f9fc846053b26a3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663055013000,"stop":1663055013000,"duration":0}},{"uid":"492b0e1bda1ecc64","status":"passed","time":{"start":1663043831000,"stop":1663043861188,"duration":30188}},{"uid":"f3852b2f23f2705","status":"passed","time":{"start":1663043075000,"stop":1663043111216,"duration":36216}},{"uid":"1feb437e0ab97441","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663040338000,"stop":1663040338000,"duration":0}},{"uid":"e11b7150a1c61c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662819574000,"stop":1662819574000,"duration":0}},{"uid":"5d08acbe5af5717b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662795310000,"stop":1662795310000,"duration":0}}]},"Hub Suite:Hub Suite#[It] Cluster negative tests Worker Cluster Registration with Wrong ca.cert":{"statistic":{"failed":0,"broken":0,"skipped":37,"passed":182,"unknown":0,"total":219},"items":[{"uid":"85ae849145141c9b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1664275319000,"stop":1664275319000,"duration":0}},{"uid":"af3fe4cd8718f0db","status":"passed","time":{"start":1664245203000,"stop":1664245513529,"duration":310529}},{"uid":"57733d64cb91a3d9","status":"passed","time":{"start":1664160732000,"stop":1664161047496,"duration":315496}},{"uid":"c9ff0bbd1f031c2c","status":"passed","time":{"start":1664091725000,"stop":1664092040014,"duration":315014}},{"uid":"1bea1c0265899d82","status":"passed","time":{"start":1663841627000,"stop":1663841938178,"duration":311178}},{"uid":"565504a9e4c70056","status":"passed","time":{"start":1663827045000,"stop":1663827354613,"duration":309613}},{"uid":"243167fdbb70d9e1","status":"passed","time":{"start":1663767974000,"stop":1663768289568,"duration":315568}},{"uid":"25125c20c0c4a26a","status":"passed","time":{"start":1663669677000,"stop":1663669986689,"duration":309689}},{"uid":"b0c4faa2002f17ed","status":"passed","time":{"start":1663665323000,"stop":1663665633968,"duration":310968}},{"uid":"e62fad024902e557","status":"passed","time":{"start":1663665170000,"stop":1663665479428,"duration":309428}},{"uid":"e2aea91f660350a2","status":"passed","time":{"start":1663659067000,"stop":1663659377425,"duration":310425}},{"uid":"2dcf0690b567ad04","status":"passed","time":{"start":1663656809000,"stop":1663657119390,"duration":310390}},{"uid":"f25bc824ec9d3e84","status":"passed","time":{"start":1663583877000,"stop":1663584187632,"duration":310632}},{"uid":"a3c790627ef6c3d3","status":"passed","time":{"start":1663340950000,"stop":1663341260036,"duration":310036}},{"uid":"a25f62cd1d4db9ea","status":"passed","time":{"start":1663340650000,"stop":1663340965277,"duration":315277}},{"uid":"feee632d4c4fd2aa","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663308473000,"stop":1663308473000,"duration":0}},{"uid":"eb6a058c729df5f7","status":"passed","time":{"start":1663304356000,"stop":1663304671988,"duration":315988}},{"uid":"10b94781993638ee","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663302519000,"stop":1663302519000,"duration":0}},{"uid":"848017e67540d5e7","status":"passed","time":{"start":1663264756000,"stop":1663265072212,"duration":316212}},{"uid":"7da123b83d9049af","status":"passed","time":{"start":1663228190000,"stop":1663228500313,"duration":310313}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting nsm-manager pod Check ping between iperf-server and iperf-client after nsm-manager pod restart":{"statistic":{"failed":0,"broken":0,"skipped":56,"passed":0,"unknown":0,"total":56},"items":[{"uid":"7e914286b54a716","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1664094053000,"stop":1664094053000,"duration":0}},{"uid":"d8d6d4fa90e8fa9d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663843963000,"stop":1663843963000,"duration":0}},{"uid":"39aa9c5da86c5349","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663829327000,"stop":1663829327000,"duration":0}},{"uid":"ed8d74bad206e9ca","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663343132000,"stop":1663343132000,"duration":0}},{"uid":"76882d135ebe875e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663342947000,"stop":1663342947000,"duration":0}},{"uid":"dab16a7695d78d76","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"79e0cec45a9dbf6d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"9356e4aa13eb0ddf","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662348908000,"stop":1662348908000,"duration":0}},{"uid":"2963636467deefac","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"b667a924a5a68b73","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"5f3053d2fea5293b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"f70945cabffa5e34","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"f094bcabe7d18f25","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"8c4226a47f1f2e4d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"d1642c78040ad3a8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"fa7040522711ce5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"48141d0fcaa46345","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"4f7746b37e6c2587","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"1e22290a9c5deab2","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"4040b4d0da3ab12d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Creation Should fail while creating Project with project name as combination of special characters":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":219,"unknown":0,"total":219},"items":[{"uid":"87facf5bd0ef1f7","status":"passed","time":{"start":1664275319000,"stop":1664275319295,"duration":295}},{"uid":"cde54c397843f50","status":"passed","time":{"start":1664245203000,"stop":1664245203307,"duration":307}},{"uid":"139d744eff4915eb","status":"passed","time":{"start":1664160732000,"stop":1664160732286,"duration":286}},{"uid":"710423e08e7b17f6","status":"passed","time":{"start":1664091725000,"stop":1664091725240,"duration":240}},{"uid":"c9198de687eb9a11","status":"passed","time":{"start":1663841627000,"stop":1663841627321,"duration":321}},{"uid":"54473d8211d56ec5","status":"passed","time":{"start":1663827045000,"stop":1663827045254,"duration":254}},{"uid":"99ba214dbc9ac778","status":"passed","time":{"start":1663767974000,"stop":1663767974255,"duration":255}},{"uid":"b9817106a7d15138","status":"passed","time":{"start":1663669677000,"stop":1663669677267,"duration":267}},{"uid":"db1a823793d50d9d","status":"passed","time":{"start":1663665323000,"stop":1663665323291,"duration":291}},{"uid":"6fa960570c4a38aa","status":"passed","time":{"start":1663665170000,"stop":1663665170282,"duration":282}},{"uid":"3858b600d2ce4666","status":"passed","time":{"start":1663659067000,"stop":1663659067300,"duration":300}},{"uid":"c428029d3f1edacf","status":"passed","time":{"start":1663656809000,"stop":1663656809276,"duration":276}},{"uid":"b698e7fd8140311d","status":"passed","time":{"start":1663583877000,"stop":1663583877360,"duration":360}},{"uid":"69ca93bf38c7e5e3","status":"passed","time":{"start":1663340950000,"stop":1663340950253,"duration":253}},{"uid":"6b0deaa39a38e8eb","status":"passed","time":{"start":1663340650000,"stop":1663340650231,"duration":231}},{"uid":"f621f5262db2123d","status":"passed","time":{"start":1663308473000,"stop":1663308473403,"duration":403}},{"uid":"cee916020248a68d","status":"passed","time":{"start":1663304356000,"stop":1663304356283,"duration":283}},{"uid":"ec40e8b32235ac72","status":"passed","time":{"start":1663302519000,"stop":1663302519250,"duration":250}},{"uid":"e1aede8a7c2b6669","status":"passed","time":{"start":1663264756000,"stop":1663264756323,"duration":323}},{"uid":"e6707ec9b90a4c52","status":"passed","time":{"start":1663228190000,"stop":1663228190265,"duration":265}}]},"Istio Suite:Istio Suite#[It] Bookinfo tests Bookinfo Scenario 2 with egress and without ingress Should verify if Curl Traffic works on Productpage test":{"statistic":{"failed":0,"broken":0,"skipped":57,"passed":15,"unknown":0,"total":72},"items":[{"uid":"efc8bcf2f4a7085c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1664246558000,"stop":1664246558000,"duration":0}},{"uid":"7dd22d8d54577cc3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1664162065000,"stop":1664162065000,"duration":0}},{"uid":"31585da082eb2acb","status":"passed","time":{"start":1664093039000,"stop":1664093042455,"duration":3455}},{"uid":"e607456f40dee20e","status":"passed","time":{"start":1663842833000,"stop":1663842836650,"duration":3650}},{"uid":"3e96c1536a7cd4c4","status":"passed","time":{"start":1663828380000,"stop":1663828383499,"duration":3499}},{"uid":"9236583afe063e0e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663769301000,"stop":1663769301000,"duration":0}},{"uid":"67e5de357c32b3bf","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663671043000,"stop":1663671043000,"duration":0}},{"uid":"5a5b061db844b5d6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663666667000,"stop":1663666667000,"duration":0}},{"uid":"4cbb5fc447c0c75","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663666522000,"stop":1663666522000,"duration":0}},{"uid":"6c81c38490150455","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663585263000,"stop":1663585263000,"duration":0}},{"uid":"66d79fd1d0718714","status":"passed","time":{"start":1663342266000,"stop":1663342266681,"duration":681}},{"uid":"335a2d796fea6220","status":"passed","time":{"start":1663341990000,"stop":1663341993561,"duration":3561}},{"uid":"94de1cbd822f0ff9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663315204000,"stop":1663315204000,"duration":0}},{"uid":"6d8a6c2f815fd337","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663312558000,"stop":1663312558000,"duration":0}},{"uid":"fff675704a218961","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663055013000,"stop":1663055013000,"duration":0}},{"uid":"8af5a32132cf69c0","status":"passed","time":{"start":1663043831000,"stop":1663043834532,"duration":3532}},{"uid":"177700ac9a9d453b","status":"passed","time":{"start":1663043075000,"stop":1663043078638,"duration":3638}},{"uid":"61ba29170d529d9f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663040338000,"stop":1663040338000,"duration":0}},{"uid":"91d056611138139f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662819574000,"stop":1662819574000,"duration":0}},{"uid":"9e46f414730e7af1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662795310000,"stop":1662795310000,"duration":0}}]},"3355327d9ab73c8b07a557e8514d8b82":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"b97c2f0a61cffe3f","status":"passed","time":{"start":1663839839474,"stop":1663839850419,"duration":10945}}]},"Worker Suite:Worker Suite#[It] Hub netpol scenarios: Slice Configs Netpol scenarios Should fail while creating namespaces with same name in both allowedNamespace and applicationNamespace":{"statistic":{"failed":0,"broken":0,"skipped":23,"passed":33,"unknown":0,"total":56},"items":[{"uid":"dc4efea5766e0e7","status":"passed","time":{"start":1664094053000,"stop":1664094053369,"duration":369}},{"uid":"55f03e572e0efc8a","status":"passed","time":{"start":1663843963000,"stop":1663843963596,"duration":596}},{"uid":"5a0706973a07141e","status":"passed","time":{"start":1663829327000,"stop":1663829327361,"duration":361}},{"uid":"ea2e7a3628045a82","status":"passed","time":{"start":1663343132000,"stop":1663343132482,"duration":482}},{"uid":"30d8085af996055a","status":"passed","time":{"start":1663342947000,"stop":1663342947409,"duration":409}},{"uid":"be64c54d88cfb966","status":"skipped","statusDetails":"skipped","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"976cc37ea47f59a8","status":"passed","time":{"start":1663044069000,"stop":1663044069370,"duration":370}},{"uid":"d3698fd735697f7b","status":"passed","time":{"start":1662348908000,"stop":1662348908445,"duration":445}},{"uid":"8a4c51547048f39d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"11808bec17b61f1a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"5d1f3f52129e7611","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"49e0f00780778633","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"d4f1dbf5c8b4aec6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"d45ad3b4e794d652","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"8c4986df27fa89fd","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"8aa4c90b54a1b266","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"bac4c1d7400b2452","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"94198843ab048a4b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"f73da2f7d60ef140","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"ca06e6e756849081","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting iperf-server pod Should restart iperf-server pod":{"statistic":{"failed":0,"broken":0,"skipped":56,"passed":0,"unknown":0,"total":56},"items":[{"uid":"9ea9eace2e6a90cc","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1664094053000,"stop":1664094053000,"duration":0}},{"uid":"fab7cf0536e05700","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663843963000,"stop":1663843963000,"duration":0}},{"uid":"1dc86e6332d3dd4a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663829327000,"stop":1663829327000,"duration":0}},{"uid":"408c9bb8936a1c79","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663343132000,"stop":1663343132000,"duration":0}},{"uid":"1ea79c3779a5801b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663342947000,"stop":1663342947000,"duration":0}},{"uid":"8081250bbe48fb2","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"f65b4144331c01ac","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"5bcc8940d841feb7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662348908000,"stop":1662348908000,"duration":0}},{"uid":"8965ffd1cd973fea","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"2f201c39f2637de4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"f4a8568c77581b01","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"aeba418482a7bd02","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"3bbc65071655101f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"ca7d31b55f4212d2","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"d87857ed33ac6263","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"909b275241ff49cb","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"3296b687f015ec6c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"414d751d0312a41f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"b0b94777d2cb702e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"caba256e1cc2bbef","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}}]},"2b423868b0099aa10951dbf36bd46640":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"bfa8c99f205c313d","status":"passed","time":{"start":1663840505288,"stop":1663840516639,"duration":11351}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting worker-operator pod Should restart worker-operator pod":{"statistic":{"failed":0,"broken":0,"skipped":56,"passed":0,"unknown":0,"total":56},"items":[{"uid":"15211c21a54c420b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1664094053000,"stop":1664094053000,"duration":0}},{"uid":"b42b9fd589f78939","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663843963000,"stop":1663843963000,"duration":0}},{"uid":"424004d497d66da3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663829327000,"stop":1663829327000,"duration":0}},{"uid":"ff7eb5df2b8e4193","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663343132000,"stop":1663343132000,"duration":0}},{"uid":"d649f1b4696fd05b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663342947000,"stop":1663342947000,"duration":0}},{"uid":"1e6dc2879d32836f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"61b87cbd94381832","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"8819e7cadc626814","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662348908000,"stop":1662348908000,"duration":0}},{"uid":"c5c051f448e935e0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"9a9b7d209bb06c01","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"dac0496380837d0e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"a408481c08a0d0a7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"ce8318c5093f6d35","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"a5f9abb86d9705b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"1dd4ed06f138f58","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"aadd51aacbaef52e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"408d61094ed1283d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"f01d1a92ba60e192","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"d5729991e9b3cb90","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"e46d1ce3a1d9ef6c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice Creation scenarios Slice with LOW QOS should have vl3 router running":{"statistic":{"failed":17,"broken":0,"skipped":2,"passed":28,"unknown":0,"total":47},"items":[{"uid":"cec593c401dd7bb5","status":"passed","time":{"start":1664094053000,"stop":1664094056251,"duration":3251}},{"uid":"953a8d943b4149cc","status":"passed","time":{"start":1663843963000,"stop":1663843967578,"duration":4578}},{"uid":"14e8e1fd303c8ee2","status":"passed","time":{"start":1663829327000,"stop":1663829330348,"duration":3348}},{"uid":"eac5203f2b35505f","status":"passed","time":{"start":1663343132000,"stop":1663343135404,"duration":3404}},{"uid":"e626bf0d6e5216d8","status":"passed","time":{"start":1663342947000,"stop":1663342950334,"duration":3334}},{"uid":"a4fe29846cbe71a2","status":"skipped","statusDetails":"skipped","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"87f3f337b8546ba3","status":"skipped","statusDetails":"skipped","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"230640294a3a5444","status":"passed","time":{"start":1662348908000,"stop":1662348912686,"duration":4686}},{"uid":"1882dfa2665a9c54","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004695c0>: {\n        Underlying: <*exec.ExitError | 0xc00065c880>{\n            ProcessState: {\n                pid: 7276,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 275895},\n                    Stime: {Sec: 0, Usec: 85202},\n                    Maxrss: 96456,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4595,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 312,\n                    Nivcsw: 644,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicelowqos\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\noccurred","time":{"start":1661844205000,"stop":1661844205437,"duration":437}},{"uid":"d8f312c38f1d96f7","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000734a38>: {\n        Underlying: <*exec.ExitError | 0xc00017eaa0>{\n            ProcessState: {\n                pid: 7213,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 155613},\n                    Stime: {Sec: 0, Usec: 41750},\n                    Maxrss: 81264,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2782,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 291,\n                    Nivcsw: 718,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicelowqos\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661754870000,"stop":1661754870278,"duration":278}},{"uid":"8b9cfbf181d7c35e","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0006f05d0>: {\n        Underlying: <*exec.ExitError | 0xc0006a85c0>{\n            ProcessState: {\n                pid: 7291,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 153745},\n                    Stime: {Sec: 0, Usec: 43927},\n                    Maxrss: 88652,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4153,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 429,\n                    Nivcsw: 325,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicelowqos\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661611962000,"stop":1661611962299,"duration":299}},{"uid":"533af60f240e19d","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0009d9b48>: {\n        Underlying: <*exec.ExitError | 0xc0007aaea0>{\n            ProcessState: {\n                pid: 7094,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 189610},\n                    Stime: {Sec: 0, Usec: 52914},\n                    Maxrss: 85092,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3515,\n                    Majflt: 1,\n                    Nswap: 0,\n                    Inblock: 136,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 437,\n                    Nivcsw: 526,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicelowqos\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661585077000,"stop":1661585077370,"duration":370}},{"uid":"1ee9400b251f9da1","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00013bea8>: {\n        Underlying: <*exec.ExitError | 0xc0003b0d40>{\n            ProcessState: {\n                pid: 6238,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 213236},\n                    Stime: {Sec: 0, Usec: 43437},\n                    Maxrss: 82840,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4045,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 389,\n                    Nivcsw: 498,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicelowqos\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661584447000,"stop":1661584447409,"duration":409}},{"uid":"cb349d98869bf1a0","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0006d2660>: {\n        Underlying: <*exec.ExitError | 0xc000075500>{\n            ProcessState: {\n                pid: 7285,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 177183},\n                    Stime: {Sec: 0, Usec: 30814},\n                    Maxrss: 79996,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3955,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 449,\n                    Nivcsw: 412,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicelowqos\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661580045000,"stop":1661580045265,"duration":265}},{"uid":"c898dacd0afb595e","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000779518>: {\n        Underlying: <*exec.ExitError | 0xc000075fa0>{\n            ProcessState: {\n                pid: 7278,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 255062},\n                    Stime: {Sec: 0, Usec: 50239},\n                    Maxrss: 90280,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3608,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 401,\n                    Nivcsw: 404,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicelowqos\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661578505000,"stop":1661578505388,"duration":388}},{"uid":"b6d1552f84a18ff","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0008a2078>: {\n        Underlying: <*exec.ExitError | 0xc00074c120>{\n            ProcessState: {\n                pid: 7224,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 249331},\n                    Stime: {Sec: 0, Usec: 76408},\n                    Maxrss: 96576,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 8216,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 497,\n                    Nivcsw: 467,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicelowqos\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661572479000,"stop":1661572479442,"duration":442}},{"uid":"e66cc0fe579d7aeb","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0006ea5d0>: {\n        Underlying: <*exec.ExitError | 0xc0008c8c00>{\n            ProcessState: {\n                pid: 7136,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 140565},\n                    Stime: {Sec: 0, Usec: 53745},\n                    Maxrss: 82532,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4199,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 282,\n                    Nivcsw: 342,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicelowqos\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660832281000,"stop":1660832281267,"duration":267}},{"uid":"90c863483faf9005","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004dc828>: {\n        Underlying: <*exec.ExitError | 0xc0007d42c0>{\n            ProcessState: {\n                pid: 7207,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 155805},\n                    Stime: {Sec: 0, Usec: 67375},\n                    Maxrss: 80652,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4200,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 487,\n                    Nivcsw: 354,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicelowqos\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660808797000,"stop":1660808797280,"duration":280}},{"uid":"70e33a27619371c","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0005cc138>: {\n        Underlying: <*exec.ExitError | 0xc000720480>{\n            ProcessState: {\n                pid: 7110,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 130640},\n                    Stime: {Sec: 0, Usec: 39917},\n                    Maxrss: 85196,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 6182,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 293,\n                    Nivcsw: 295,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicelowqos\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660299182000,"stop":1660299182230,"duration":230}},{"uid":"890837a5f9a4b54b","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00044a018>: {\n        Underlying: <*exec.ExitError | 0xc000806160>{\n            ProcessState: {\n                pid: 7166,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 142515},\n                    Stime: {Sec: 0, Usec: 43546},\n                    Maxrss: 79220,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4888,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 376,\n                    Nivcsw: 286,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicelowqos\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660295740000,"stop":1660295740243,"duration":243}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests slice with netpol should install slice on each worker cluster with correct namespaceisolationprofile":{"statistic":{"failed":0,"broken":1,"skipped":39,"passed":16,"unknown":0,"total":56},"items":[{"uid":"8104cf2390a6933e","status":"passed","time":{"start":1664094053000,"stop":1664094053168,"duration":168}},{"uid":"a53b732b889ca252","status":"passed","time":{"start":1663843963000,"stop":1663843963197,"duration":197}},{"uid":"a0299963a8cd549","status":"passed","time":{"start":1663829327000,"stop":1663829327194,"duration":194}},{"uid":"ab654ba7edd8f16c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663343132000,"stop":1663343132000,"duration":0}},{"uid":"ec7e3d08ff168b24","status":"passed","time":{"start":1663342947000,"stop":1663342947208,"duration":208}},{"uid":"472bef7be3e3a819","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"7d402d65c69e84ec","status":"passed","time":{"start":1663044069000,"stop":1663044069224,"duration":224}},{"uid":"c4b23ef83d49f9a2","status":"passed","time":{"start":1662348908000,"stop":1662348908133,"duration":133}},{"uid":"7de98636769354ef","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"66a073eed7a9a266","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"8c0243915cadc01","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"44d38c14bf542d54","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"f72499492e91415f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"3ed3d6cd2fa7c7fc","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"8d1d1d844fb7f35b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"550d9281874ed37f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"8ab30acc0ae11912","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"7f42d02ee97e57ba","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"77bb8a277c2f216","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"5b93153683a52b0f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}}]},"d1a8e0523f363d4242294ec8b40be4f6":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"a2ad94e73e1c5cf3","status":"passed","time":{"start":1663840642843,"stop":1663840656326,"duration":13483}}]},"Worker Suite:Worker Suite#[It] Namespace reconcile test scenarios: Delete deboarded app ns gets deleted":{"statistic":{"failed":0,"broken":0,"skipped":10,"passed":46,"unknown":0,"total":56},"items":[{"uid":"980c1ee67453104f","status":"passed","time":{"start":1664094053000,"stop":1664094063588,"duration":10588}},{"uid":"1c656d5073cc0292","status":"passed","time":{"start":1663843963000,"stop":1663843973923,"duration":10923}},{"uid":"72448ab057d23603","status":"passed","time":{"start":1663829327000,"stop":1663829337613,"duration":10613}},{"uid":"19647d66bd8602c3","status":"passed","time":{"start":1663343132000,"stop":1663343142695,"duration":10695}},{"uid":"da6b9093807fca51","status":"passed","time":{"start":1663342947000,"stop":1663342958081,"duration":11081}},{"uid":"ad05e6e09e197475","status":"skipped","statusDetails":"skipped","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"2a42ad0870c9d4bd","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"74afe4d4bf31bf47","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662348908000,"stop":1662348908000,"duration":0}},{"uid":"28d39ab43cfcba4b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"7b674e701816d2f0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"2e25d4834f475498","status":"passed","time":{"start":1661611962000,"stop":1661611973192,"duration":11192}},{"uid":"4de6f18dd3ae2d2d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"eab9ef4116cc011f","status":"passed","time":{"start":1661584447000,"stop":1661584458008,"duration":11008}},{"uid":"2251b32027c5ac26","status":"passed","time":{"start":1661580045000,"stop":1661580055741,"duration":10741}},{"uid":"213e8652dc3fe0ac","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"aa5c80f34b538831","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"85f85706f8186b36","status":"passed","time":{"start":1660832281000,"stop":1660832291583,"duration":10583}},{"uid":"8baaf6138f0968e6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"8667aa84b2de2312","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"b75ab78ef085c5e5","status":"passed","time":{"start":1660295740000,"stop":1660295750533,"duration":10533}}]},"1fcb813abfc5341f9aaf7fab97bf0e75":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"c1cc21ff63756353","status":"passed","time":{"start":1663841038129,"stop":1663841056328,"duration":18199}}]},"f5f1580fdf3e9167fdd67ab4cf1f04e1":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"1034d2e4576b8286","status":"passed","time":{"start":1663841464284,"stop":1663841484011,"duration":19727}}]},"Istio Suite:Istio Suite#[It] Bookinfo tests Bookinfo Scenario 1 without egress and ingress Should install Productpage on worker cluster-1":{"statistic":{"failed":53,"broken":0,"skipped":0,"passed":19,"unknown":0,"total":72},"items":[{"uid":"d968589ce894d281","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 4\nto equal\n    <int>: 1","time":{"start":1664246558000,"stop":1664246738442,"duration":180442}},{"uid":"ce5b3fa090d4a666","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1664162065000,"stop":1664162245345,"duration":180345}},{"uid":"ae0ea966d3f0e5d5","status":"passed","time":{"start":1664093039000,"stop":1664093228148,"duration":189148}},{"uid":"b093992c8219aebf","status":"passed","time":{"start":1663842833000,"stop":1663843025986,"duration":192986}},{"uid":"c5b769577d34fd47","status":"passed","time":{"start":1663828380000,"stop":1663828424919,"duration":44919}},{"uid":"3d62c6283843426d","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 3\nto equal\n    <int>: 1","time":{"start":1663769301000,"stop":1663769481324,"duration":180324}},{"uid":"af5dc2cfdb7da8bd","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 3\nto equal\n    <int>: 1","time":{"start":1663671043000,"stop":1663671223432,"duration":180432}},{"uid":"8bb6577a1723b852","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1663666667000,"stop":1663666880166,"duration":213166}},{"uid":"8955740b93bc04e4","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1663666522000,"stop":1663666702382,"duration":180382}},{"uid":"f9d9a0a9499a8ad5","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 4\nto equal\n    <int>: 1","time":{"start":1663585263000,"stop":1663585443627,"duration":180627}},{"uid":"a51a29fd98afea01","status":"passed","time":{"start":1663342266000,"stop":1663342288041,"duration":22041}},{"uid":"c92135980a2d7acb","status":"passed","time":{"start":1663341990000,"stop":1663342119091,"duration":129091}},{"uid":"4ad4fbe8f2521d4b","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1663315204000,"stop":1663315384541,"duration":180541}},{"uid":"e06837e1071a9c19","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1663312558000,"stop":1663312738215,"duration":180215}},{"uid":"dc19cf2279a896c6","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1663055013000,"stop":1663055193323,"duration":180323}},{"uid":"399817e2c7c9e047","status":"passed","time":{"start":1663043831000,"stop":1663043849030,"duration":18030}},{"uid":"1cf30db0ce168c0b","status":"passed","time":{"start":1663043075000,"stop":1663043180300,"duration":105300}},{"uid":"fa20c6794d5ac6f1","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1663040338000,"stop":1663040518376,"duration":180376}},{"uid":"16bde7f003ead557","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1662819574000,"stop":1662819754327,"duration":180327}},{"uid":"2a3673c35cc220ef","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1662795310000,"stop":1662795490298,"duration":180298}}]},"4e1149cf9690d80735c60dce15c2d1c3":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"b2fad68f69e9f71b","status":"passed","time":{"start":1663840803231,"stop":1663840816175,"duration":12944}}]},"5a036c4844a2c0497d8e2849c61bb181":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"a68ba6c1e6c9cfeb","status":"passed","time":{"start":1663841805256,"stop":1663841822677,"duration":17421}}]},"Worker Suite:Worker Suite#[It] Slice creation scenarios: when slice red is installed install slice blue workers should get attached to slice blue":{"statistic":{"failed":0,"broken":0,"skipped":37,"passed":19,"unknown":0,"total":56},"items":[{"uid":"9079c5e702aafa1c","status":"passed","time":{"start":1664094053000,"stop":1664094054810,"duration":1810}},{"uid":"4b36cea03ce63ddc","status":"passed","time":{"start":1663843963000,"stop":1663843963749,"duration":749}},{"uid":"8eef8451877e32dc","status":"passed","time":{"start":1663829327000,"stop":1663829328797,"duration":1797}},{"uid":"22ec8d97dd2269e3","status":"passed","time":{"start":1663343132000,"stop":1663343132565,"duration":565}},{"uid":"585438f8d5b30ea4","status":"passed","time":{"start":1663342947000,"stop":1663342948902,"duration":1902}},{"uid":"528e14e2def29c4c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"c237ca3c9658523c","status":"passed","time":{"start":1663044069000,"stop":1663044069993,"duration":993}},{"uid":"e4fffa9b72ef5f2d","status":"passed","time":{"start":1662348908000,"stop":1662348908696,"duration":696}},{"uid":"e5f620beee22a294","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"72d42f07cede60fd","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"29f5547b6a272cb8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"8aa1935873442ce7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"43cbf96270ad252a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"fb47e1e943fc637b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"a344cd57af9818bc","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"9254048968aabaca","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"84f16902ea966f47","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"a1c7b24a9868efce","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"b0b113a3f84633da","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"9292148a6305abe5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}}]},"Empty Suite:Empty Suite#[BeforeSuite]":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":248,"unknown":0,"total":248},"items":[{"uid":"fd71e3b6b18fae14","status":"passed","time":{"start":1664282572000,"stop":1664282572000,"duration":0}},{"uid":"4de000895217c7c6","status":"passed","time":{"start":1664279225000,"stop":1664279225000,"duration":0}},{"uid":"a13e012a4c844932","status":"passed","time":{"start":1664277805000,"stop":1664277805000,"duration":0}},{"uid":"58a226e112c5c63a","status":"passed","time":{"start":1664275170000,"stop":1664275170000,"duration":0}},{"uid":"a736d90dca833720","status":"passed","time":{"start":1664275319000,"stop":1664275319000,"duration":0}},{"uid":"cb2f1dbc299f8210","status":"passed","time":{"start":1664269661000,"stop":1664269661000,"duration":0}},{"uid":"1808a6d85f640aeb","status":"passed","time":{"start":1664261300000,"stop":1664261300000,"duration":0}},{"uid":"2efffb429b069f4e","status":"passed","time":{"start":1664245049000,"stop":1664245049000,"duration":0}},{"uid":"6cc6689c2d1214a9","status":"passed","time":{"start":1664218010000,"stop":1664218010005,"duration":5}},{"uid":"26c276e179450d63","status":"passed","time":{"start":1664160583000,"stop":1664160583000,"duration":0}},{"uid":"61b5aed72cace47c","status":"passed","time":{"start":1664105948000,"stop":1664105948000,"duration":0}},{"uid":"34b59f35f6a05bbe","status":"passed","time":{"start":1664103959000,"stop":1664103959000,"duration":0}},{"uid":"89fcb83d2ec212b0","status":"passed","time":{"start":1664091567000,"stop":1664091567000,"duration":0}},{"uid":"94a06fa1dd783b7c","status":"passed","time":{"start":1663932902000,"stop":1663932902000,"duration":0}},{"uid":"e1391cc96aa47dea","status":"passed","time":{"start":1663917718000,"stop":1663917718000,"duration":0}},{"uid":"8fdfc01d1964bbd0","status":"passed","time":{"start":1663912384000,"stop":1663912384000,"duration":0}},{"uid":"e50a9169261ae41e","status":"passed","time":{"start":1663841452000,"stop":1663841452000,"duration":0}},{"uid":"8f5d3e2916e3cd8f","status":"passed","time":{"start":1663838862000,"stop":1663838862000,"duration":0}},{"uid":"bce9839c724d3c80","status":"passed","time":{"start":1663838333000,"stop":1663838333000,"duration":0}},{"uid":"98c6585577000385","status":"passed","time":{"start":1663836341000,"stop":1663836341000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice validation Slice with MEDIUM QOS should reconcile medium qos slice":{"statistic":{"failed":5,"broken":0,"skipped":4,"passed":0,"unknown":0,"total":9},"items":[{"uid":"531abd47defaec11","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004a9788>: {\n        Underlying: <*exec.ExitError | 0xc00040d860>{\n            ProcessState: {\n                pid: 7260,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 72127},\n                    Stime: {Sec: 0, Usec: 18031},\n                    Maxrss: 43584,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4297,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 230,\n                    Nivcsw: 359,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicemediumqos\\\" not found\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicemediumqos\\\" not found\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicemediumqos\\\" not found\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicemediumqos\\\" not found\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (NotFound): slice.networking.kubeslice.io \"slicemediumqos\" not found\noccurred","time":{"start":1659168739000,"stop":1659168740414,"duration":1414}},{"uid":"46130730b1f11a1a","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00087c120>: {\n        Underlying: <*exec.ExitError | 0xc00069c5e0>{\n            ProcessState: {\n                pid: 7406,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 74896},\n                    Stime: {Sec: 0, Usec: 12482},\n                    Maxrss: 43348,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3354,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 199,\n                    Nivcsw: 297,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicemediumqos\\\" not found\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicemediumqos\\\" not found\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicemediumqos\\\" not found\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicemediumqos\\\" not found\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (NotFound): slice.networking.kubeslice.io \"slicemediumqos\" not found\noccurred","time":{"start":1659164084000,"stop":1659164085098,"duration":1098}},{"uid":"cdf275d41a8481c7","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00049ad20>: {\n        Underlying: <*exec.ExitError | 0xc000843c20>{\n            ProcessState: {\n                pid: 7452,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 44988},\n                    Stime: {Sec: 0, Usec: 22494},\n                    Maxrss: 41588,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2497,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 210,\n                    Nivcsw: 113,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicemediumqos\\\" not found\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicemediumqos\\\" not found\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicemediumqos\\\" not found\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicemediumqos\\\" not found\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (NotFound): slice.networking.kubeslice.io \"slicemediumqos\" not found\noccurred","time":{"start":1659160188000,"stop":1659160188732,"duration":732}},{"uid":"7f1c6ad507808e9d","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000347c38>: {\n        Underlying: <*exec.ExitError | 0xc000778740>{\n            ProcessState: {\n                pid: 8030,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 59177},\n                    Stime: {Sec: 0, Usec: 12680},\n                    Maxrss: 42032,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2537,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 257,\n                    Nivcsw: 140,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicemediumqos\\\" not found\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicemediumqos\\\" not found\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicemediumqos\\\" not found\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicemediumqos\\\" not found\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (NotFound): slice.networking.kubeslice.io \"slicemediumqos\" not found\noccurred","time":{"start":1659119724000,"stop":1659119725126,"duration":1126}},{"uid":"88a740814282788f","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004e9560>: {\n        Underlying: <*exec.ExitError | 0xc0004510c0>{\n            ProcessState: {\n                pid: 7542,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 66414},\n                    Stime: {Sec: 0, Usec: 15626},\n                    Maxrss: 45792,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2977,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 225,\n                    Nivcsw: 318,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicemediumqos\\\" not found\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicemediumqos\\\" not found\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicemediumqos\\\" not found\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicemediumqos\\\" not found\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (NotFound): slice.networking.kubeslice.io \"slicemediumqos\" not found\noccurred","time":{"start":1659116511000,"stop":1659116511874,"duration":874}},{"uid":"f1ae2dd55b9e07f8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659109470000,"stop":1659109470000,"duration":0}},{"uid":"afe8c318d5877363","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659106836000,"stop":1659106836000,"duration":0}},{"uid":"cffa1fbdfce28b40","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659101522000,"stop":1659101522000,"duration":0}},{"uid":"ce0be5d753f333ab","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659082816000,"stop":1659082816000,"duration":0}}]},"Istio Suite:Istio Suite#[It] Bookinfo tests Bookinfo Scenario 1 without egress and ingress Should install Details, Reviews, Ratings and Serviceexport on worker cluster-2":{"statistic":{"failed":1,"broken":0,"skipped":53,"passed":18,"unknown":0,"total":72},"items":[{"uid":"4f171331e4896e40","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1664246558000,"stop":1664246558000,"duration":0}},{"uid":"dd2fe2938281a498","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1664162065000,"stop":1664162065000,"duration":0}},{"uid":"4d614e58d07d45d8","status":"passed","time":{"start":1664093039000,"stop":1664093071851,"duration":32851}},{"uid":"bbad79126c17159d","status":"passed","time":{"start":1663842833000,"stop":1663842866588,"duration":33588}},{"uid":"82118cddeaadcce3","status":"passed","time":{"start":1663828380000,"stop":1663828410927,"duration":30927}},{"uid":"fb1e787abdea95e2","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663769301000,"stop":1663769301000,"duration":0}},{"uid":"38ef8edd25a8f6d4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663671043000,"stop":1663671043000,"duration":0}},{"uid":"db40203477462fb6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663666667000,"stop":1663666667000,"duration":0}},{"uid":"4a8b1d38ad1c364c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663666522000,"stop":1663666522000,"duration":0}},{"uid":"1bdfe9f501e97943","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663585263000,"stop":1663585263000,"duration":0}},{"uid":"8eb299830cda5726","status":"passed","time":{"start":1663342266000,"stop":1663342297043,"duration":31043}},{"uid":"9503110bf95d5c31","status":"passed","time":{"start":1663341990000,"stop":1663342057823,"duration":67823}},{"uid":"f9a5b28508e4f964","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663315204000,"stop":1663315204000,"duration":0}},{"uid":"6fda251d3b91b7e9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663312558000,"stop":1663312558000,"duration":0}},{"uid":"191c569f20746644","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663055013000,"stop":1663055013000,"duration":0}},{"uid":"9af1d393e90fac71","status":"passed","time":{"start":1663043831000,"stop":1663043863761,"duration":32761}},{"uid":"e9c6b14aedde3701","status":"passed","time":{"start":1663043075000,"stop":1663043105087,"duration":30087}},{"uid":"24ef71e6ea371ba6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663040338000,"stop":1663040338000,"duration":0}},{"uid":"3bfb839a24e2894b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662819574000,"stop":1662819574000,"duration":0}},{"uid":"daedffeecc448079","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662795310000,"stop":1662795310000,"duration":0}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Update Should fail when Project is Applied with service account name as blank in Read users":{"statistic":{"failed":26,"broken":0,"skipped":0,"passed":190,"unknown":0,"total":216},"items":[{"uid":"340f4a37cddf60d8","status":"passed","time":{"start":1664275319000,"stop":1664275324937,"duration":5937}},{"uid":"61ebd4db01b831eb","status":"passed","time":{"start":1664245203000,"stop":1664245208120,"duration":5120}},{"uid":"cb8d744fa1937286","status":"passed","time":{"start":1664160732000,"stop":1664160736850,"duration":4850}},{"uid":"7ccdb6da5a25c16","status":"passed","time":{"start":1664091725000,"stop":1664091729906,"duration":4906}},{"uid":"bf7db8f5dc4f1d4f","status":"passed","time":{"start":1663841627000,"stop":1663841632142,"duration":5142}},{"uid":"bc756be70f628199","status":"passed","time":{"start":1663827045000,"stop":1663827049923,"duration":4923}},{"uid":"27769fa6e94ee51a","status":"passed","time":{"start":1663767974000,"stop":1663767978895,"duration":4895}},{"uid":"2818d8cb982af7ee","status":"passed","time":{"start":1663669677000,"stop":1663669681907,"duration":4907}},{"uid":"fb07b796dd009a35","status":"passed","time":{"start":1663665323000,"stop":1663665328114,"duration":5114}},{"uid":"a5d30f3e3411ce3","status":"passed","time":{"start":1663665170000,"stop":1663665174968,"duration":4968}},{"uid":"4c11986069bed3b8","status":"passed","time":{"start":1663659067000,"stop":1663659071984,"duration":4984}},{"uid":"a1206f42dbfcf89d","status":"passed","time":{"start":1663656809000,"stop":1663656813993,"duration":4993}},{"uid":"35b32a74062d3fa5","status":"passed","time":{"start":1663583877000,"stop":1663583882137,"duration":5137}},{"uid":"b0a260e282121526","status":"passed","time":{"start":1663340950000,"stop":1663340954841,"duration":4841}},{"uid":"efa035e78d844e6a","status":"passed","time":{"start":1663340650000,"stop":1663340654915,"duration":4915}},{"uid":"6730acfb7750b6d5","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0006c4018>: {\n        Underlying: <*exec.ExitError | 0xc00077c000>{\n            ProcessState: {\n                pid: 10888,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 224036},\n                    Stime: {Sec: 0, Usec: 102836},\n                    Maxrss: 81568,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 14816,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 343,\n                    Nivcsw: 714,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when applying patch:\",\n                        \"{\\\"status\\\":{}}\",\n                        \"to:\",\n                        \"Resource: \\\"controller.kubeslice.io/v1alpha1, Resource=projects\\\", GroupVersionKind: \\\"controller.kubeslice.io/v1alpha1, Kind=Project\\\"\",\n                        \"Name: \\\"projectupdatetest\\\", Namespace: \\\"kubeslice-controller\\\"\",\n                        \"for: \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20blank%20in%20Read%20users2374150876\\\": error when patching \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20blank%20in%20Read%20users2374150876\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.135.179:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when applying patch:\",\n                    \"{\\\"status\\\":{}}\",\n                    \"to:\",\n                    \"Resource: \\\"controller.kubeslice.io/v1alpha1, Resource=projects\\\", GroupVersionKind: \\\"controller.kubeslice.io/v1alpha1, Kind=Project\\\"\",\n                    \"Name: \\\"projectupdatetest\\\", Namespace: \\\"kubeslice-controller\\\"\",\n                    \"for: \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20blank%20in%20Read%20users2374150876\\\": error when patching \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20blank%20in%20Read%20users2374150876\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.135.179:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when applying patch:\",\n                        \"{\\\"status\\\":{}}\",\n                        \"to:\",\n                        \"Resource: \\\"controller.kubeslice.io/v1alpha1, Resource=projects\\\", GroupVersionKind: \\\"controller.kubeslice.io/v1alpha1, Kind=Project\\\"\",\n                        \"Name: \\\"projectupdatetest\\\", Namespace: \\\"kubeslice-controller\\\"\",\n                        \"for: \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20blank%20in%20Read%20users2374150876\\...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; Error from server (InternalError): error when applying patch:\n    {\"status\":{}}\n    to:\n    Resource: \"controller.kubeslice.io/v1alpha1, Resource=projects\", GroupVersionKind: \"controller.kubeslice.io/v1alpha1, Kind=Project\"\n    Name: \"projectupdatetest\", Namespace: \"kubeslice-controller\"\n    for: \"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20blank%20in%20Read%20users2374150876\": error when patching \"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20blank%20in%20Read%20users2374150876\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.135.179:443: connect: connection refused\noccurred","time":{"start":1663308473000,"stop":1663308533417,"duration":60417}},{"uid":"83f0d52c7b0ed40a","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0003a4060>: {\n        Underlying: <*exec.ExitError | 0xc0007d1280>{\n            ProcessState: {\n                pid: 6237,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 175046},\n                    Stime: {Sec: 0, Usec: 46064},\n                    Maxrss: 72336,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3642,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13344,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 363,\n                    Nivcsw: 238,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20blank%20in%20Read%20users2337549656\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.26.65:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20blank%20in%20Read%20users2337549656\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.26.65:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20blank%20in%20Read%20users2337549656\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.26.65:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20blank%20in%20Read%20users2337549656\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.26.65:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20blank%20in%20Read%20users2337549656\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.26.65:443: connect: connection refused\noccurred","time":{"start":1663304356000,"stop":1663304356267,"duration":267}},{"uid":"1960c5c87015d1ff","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00080af00>: {\n        Underlying: <*exec.ExitError | 0xc000673b80>{\n            ProcessState: {\n                pid: 7569,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 178381},\n                    Stime: {Sec: 0, Usec: 40541},\n                    Maxrss: 84612,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 6751,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 401,\n                    Nivcsw: 353,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20blank%20in%20Read%20users4251130745\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.179.221:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20blank%20in%20Read%20users4251130745\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.179.221:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20blank%20in%20Read%20users4251130745\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.179.221:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20blank%20in%20Read%20users4251130745\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.179.221:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20blank%20in%20Read%20users4251130745\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.179.221:443: connect: connection refused\noccurred","time":{"start":1663302519000,"stop":1663302519257,"duration":257}},{"uid":"9e760b88f7325ef4","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000639cf8>: {\n        Underlying: <*exec.ExitError | 0xc00078abe0>{\n            ProcessState: {\n                pid: 7206,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 201144},\n                    Stime: {Sec: 0, Usec: 47076},\n                    Maxrss: 73080,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2805,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13344,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 253,\n                    Nivcsw: 218,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20blank%20in%20Read%20users517857501\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.55.244:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20blank%20in%20Read%20users517857501\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.55.244:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20blank%20in%20Read%20users517857501\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.55.244:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20blank%20in%20Read%20users517857501\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.55.244:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20blank%20in%20Read%20users517857501\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.55.244:443: connect: connection refused\noccurred","time":{"start":1663264756000,"stop":1663264756318,"duration":318}},{"uid":"cd061e3c6bddf6ab","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00084a018>: {\n        Underlying: <*exec.ExitError | 0xc0004800c0>{\n            ProcessState: {\n                pid: 7249,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 184801},\n                    Stime: {Sec: 0, Usec: 68296},\n                    Maxrss: 81784,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3351,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13344,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 431,\n                    Nivcsw: 292,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20blank%20in%20Read%20users867339459\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.221.241:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20blank%20in%20Read%20users867339459\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.221.241:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20blank%20in%20Read%20users867339459\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.221.241:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20blank%20in%20Read%20users867339459\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.221.241:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20blank%20in%20Read%20users867339459\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.221.241:443: connect: connection refused\noccurred","time":{"start":1663228190000,"stop":1663228190287,"duration":287}}]},"4c1ade6a86c20c412c7402eaf14d5ef1":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"af825aef86903fa3","status":"passed","time":{"start":1663840004631,"stop":1663840013876,"duration":9245}}]},"d4fcc9c4f4ae5455e586fa1924b98649":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"df39b01cc6d13a2e","status":"passed","time":{"start":1663841636684,"stop":1663841661384,"duration":24700}}]},"Worker Suite:Worker Suite#[It] Namespace reconcile test scenarios: Delete deleted app ns entry should get removed from cluster objs":{"statistic":{"failed":0,"broken":0,"skipped":10,"passed":46,"unknown":0,"total":56},"items":[{"uid":"b85f4625758ea46d","status":"passed","time":{"start":1664094053000,"stop":1664094064002,"duration":11002}},{"uid":"20f8ce7ae2687e0c","status":"passed","time":{"start":1663843963000,"stop":1663843974538,"duration":11538}},{"uid":"951e775431161754","status":"passed","time":{"start":1663829327000,"stop":1663829338071,"duration":11071}},{"uid":"d858020a6f61b93e","status":"passed","time":{"start":1663343132000,"stop":1663343143060,"duration":11060}},{"uid":"4daa3ffac035f0c5","status":"passed","time":{"start":1663342947000,"stop":1663342958589,"duration":11589}},{"uid":"d611339f4e27b852","status":"skipped","statusDetails":"skipped","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"64f09a6b65355e39","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"425fcc33a9dd9661","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662348908000,"stop":1662348908000,"duration":0}},{"uid":"530a47d53fe0b896","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"a2b4de87864eaec9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"52e7c3d1f478bf28","status":"passed","time":{"start":1661611962000,"stop":1661611973458,"duration":11458}},{"uid":"9886f800d6c53ba6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"25dde8bfc1427be7","status":"passed","time":{"start":1661584447000,"stop":1661584458199,"duration":11199}},{"uid":"670f58925eb16e8","status":"passed","time":{"start":1661580045000,"stop":1661580056050,"duration":11050}},{"uid":"e1ae8ec305ef0de","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"592371c755efcb68","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"15cb93a996aefe4f","status":"passed","time":{"start":1660832281000,"stop":1660832292230,"duration":11230}},{"uid":"a68c16167562932","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"1784a268ba9df126","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"fd817c32325dd5ba","status":"passed","time":{"start":1660295740000,"stop":1660295750929,"duration":10929}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting nsm-manager pod Should restart nsm-manager pod":{"statistic":{"failed":0,"broken":0,"skipped":56,"passed":0,"unknown":0,"total":56},"items":[{"uid":"8473b07e8ae2aab7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1664094053000,"stop":1664094053000,"duration":0}},{"uid":"77a4dfdcd81197c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663843963000,"stop":1663843963000,"duration":0}},{"uid":"ab741ca6c8559fbe","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663829327000,"stop":1663829327000,"duration":0}},{"uid":"dee858a3da9305ba","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663343132000,"stop":1663343132000,"duration":0}},{"uid":"6e6c4b64eb89e31a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663342947000,"stop":1663342947000,"duration":0}},{"uid":"b4dd0e4fe34ab260","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"ffe5466a5425c6f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"eb372a77d5f5e98e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662348908000,"stop":1662348908000,"duration":0}},{"uid":"fc6c7b3f34d4314e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"3c49417b2256e7c9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"934202b07de4bf9e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"f8d856db33589a8d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"8b416a6975106a7b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"dcec2f776db6af1e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"c5fda82d5684f152","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"ecab6d18993bbe23","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"110f980690607381","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"5981e303f107ed9c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"d74cfd4282b121d6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"fc366e0a0c3434b1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}}]},"Hub Suite:Hub Suite#[It]  QosProfile tests QosProfile delete validation Delete qos profile - negative scenario":{"statistic":{"failed":149,"broken":0,"skipped":0,"passed":0,"unknown":0,"total":149},"items":[{"uid":"1ce3d6e6d5e59033","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000451890>: {\n        Underlying: <*exec.ExitError | 0xc0007a6540>{\n            ProcessState: {\n                pid: 6650,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 154838},\n                    Stime: {Sec: 0, Usec: 50322},\n                    Maxrss: 82024,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 6185,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14104,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 989,\n                    Nivcsw: 461,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario1814993184\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario1814993184\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario1814993184\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario1814993184\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario1814993184\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1661501311000,"stop":1661501315538,"duration":4538}},{"uid":"29726927de4ca0ca","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0006b13b0>: {\n        Underlying: <*exec.ExitError | 0xc0001bc560>{\n            ProcessState: {\n                pid: 6753,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 162528},\n                    Stime: {Sec: 0, Usec: 56885},\n                    Maxrss: 77348,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4147,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14104,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 835,\n                    Nivcsw: 509,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario1673518560\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario1673518560\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario1673518560\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario1673518560\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario1673518560\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1661434220000,"stop":1661434224539,"duration":4539}},{"uid":"82188d0f3759eb19","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0001797a0>: {\n        Underlying: <*exec.ExitError | 0xc0007b7460>{\n            ProcessState: {\n                pid: 6797,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 128944},\n                    Stime: {Sec: 0, Usec: 54703},\n                    Maxrss: 81364,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3771,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14104,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 826,\n                    Nivcsw: 490,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario3977468634\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario3977468634\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario3977468634\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario3977468634\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario3977468634\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1661356056000,"stop":1661356060476,"duration":4476}},{"uid":"edaa492eae13d072","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000e480>: {\n        Underlying: <*exec.ExitError | 0xc0007d4060>{\n            ProcessState: {\n                pid: 6175,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 211788},\n                    Stime: {Sec: 0, Usec: 52947},\n                    Maxrss: 78136,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3790,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14104,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 1046,\n                    Nivcsw: 671,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario2329930294\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario2329930294\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario2329930294\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario2329930294\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario2329930294\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1661352461000,"stop":1661352465706,"duration":4706}},{"uid":"5cdd75934524f846","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0006f1f08>: {\n        Underlying: <*exec.ExitError | 0xc0005fa9a0>{\n            ProcessState: {\n                pid: 6726,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 150132},\n                    Stime: {Sec: 0, Usec: 40945},\n                    Maxrss: 70184,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4196,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14104,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 948,\n                    Nivcsw: 436,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario3536546150\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario3536546150\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario3536546150\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario3536546150\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario3536546150\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1661352207000,"stop":1661352211436,"duration":4436}},{"uid":"9e87d3b562b9459f","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00034e168>: {\n        Underlying: <*exec.ExitError | 0xc0005520e0>{\n            ProcessState: {\n                pid: 6782,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 150025},\n                    Stime: {Sec: 0, Usec: 65865},\n                    Maxrss: 71872,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3842,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14104,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 831,\n                    Nivcsw: 556,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario859807691\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario859807691\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario859807691\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario859807691\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario859807691\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1661352028000,"stop":1661352032518,"duration":4518}},{"uid":"15ea942155e56498","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004f7b30>: {\n        Underlying: <*exec.ExitError | 0xc000718040>{\n            ProcessState: {\n                pid: 6098,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 146324},\n                    Stime: {Sec: 0, Usec: 67534},\n                    Maxrss: 76772,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 8499,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13616,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 836,\n                    Nivcsw: 434,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario4031362300\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario4031362300\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario4031362300\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario4031362300\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario4031362300\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1661351263000,"stop":1661351267481,"duration":4481}},{"uid":"d03048e2b39638a1","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004eb608>: {\n        Underlying: <*exec.ExitError | 0xc00014cd20>{\n            ProcessState: {\n                pid: 6620,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 187175},\n                    Stime: {Sec: 0, Usec: 80825},\n                    Maxrss: 78264,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3574,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14104,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 1285,\n                    Nivcsw: 874,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario4161556296\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario4161556296\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario4161556296\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario4161556296\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario4161556296\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1661188463000,"stop":1661188467706,"duration":4706}},{"uid":"75d3f346c8284831","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0000d7560>: {\n        Underlying: <*exec.ExitError | 0xc00014ba80>{\n            ProcessState: {\n                pid: 6337,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 147238},\n                    Stime: {Sec: 0, Usec: 36809},\n                    Maxrss: 82224,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3544,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14104,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 846,\n                    Nivcsw: 481,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario2610685715\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario2610685715\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario2610685715\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario2610685715\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario2610685715\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1661183038000,"stop":1661183042462,"duration":4462}},{"uid":"316f303230c3ea55","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000fc80>: {\n        Underlying: <*exec.ExitError | 0xc00039d0e0>{\n            ProcessState: {\n                pid: 6135,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 154629},\n                    Stime: {Sec: 0, Usec: 63437},\n                    Maxrss: 77152,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3973,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14104,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 953,\n                    Nivcsw: 594,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario713382670\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario713382670\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario713382670\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario713382670\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario713382670\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1661180055000,"stop":1661180059521,"duration":4521}},{"uid":"67fee4d1f09fe05c","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004e8858>: {\n        Underlying: <*exec.ExitError | 0xc0004a9460>{\n            ProcessState: {\n                pid: 6154,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 164267},\n                    Stime: {Sec: 0, Usec: 73920},\n                    Maxrss: 75720,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4692,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14104,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 920,\n                    Nivcsw: 513,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario2317615998\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario2317615998\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario2317615998\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario2317615998\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20delete%20validation%20Delete%20qos%20profile%20-%20negative%20scenario2317615998\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1661152474000,"stop":1661152478586,"duration":4586}},{"uid":"c25184a295f3251e","status":"failed","statusDetails":"Expected an error to have occurred.  Got:\n    <nil>: nil","time":{"start":1661083598000,"stop":1661083638227,"duration":40227}},{"uid":"4cab2c9ee2ce180b","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00068eee8>: {\n        Underlying: <*exec.ExitError | 0xc00060da60>{\n            ProcessState: {\n                pid: 6152,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 61689},\n                    Stime: {Sec: 0, Usec: 12337},\n                    Maxrss: 42388,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2015,\n                    Majflt: 1,\n                    Nswap: 0,\n                    Inblock: 288,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 204,\n                    Nivcsw: 180,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Cluster \\\"cluster2\\\" is invalid: metadata.name: Invalid value: \\\"cluster2\\\": can't delete cluster which is participating in any slice\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The Cluster \\\"cluster2\\\" is invalid: metadata.name: Invalid value: \\\"cluster2\\\": can't delete cluster which is participating in any slice\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Cluster \\\"cluster2\\\" is invalid: metadata.name: Invalid value: \\\"cluster2\\\": can't delete cluster which is participating in any slice\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The Cluster \\\"cluster2\\\" is invalid: metadata.name: Invalid value: \\\"cluster2\\\": can't delete cluster which is participating in any slice\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The Cluster \"cluster2\" is invalid: metadata.name: Invalid value: \"cluster2\": can't delete cluster which is participating in any slice\noccurred","time":{"start":1661070332000,"stop":1661070704227,"duration":372227}},{"uid":"a0a0ce17e682a18b","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0007c0a80>: {\n        Underlying: <*exec.ExitError | 0xc000691ce0>{\n            ProcessState: {\n                pid: 6341,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 66709},\n                    Stime: {Sec: 0, Usec: 15696},\n                    Maxrss: 42924,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2491,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 224,\n                    Nivcsw: 324,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Cluster \\\"cluster2\\\" is invalid: metadata.name: Invalid value: \\\"cluster2\\\": can't delete cluster which is participating in any slice\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The Cluster \\\"cluster2\\\" is invalid: metadata.name: Invalid value: \\\"cluster2\\\": can't delete cluster which is participating in any slice\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Cluster \\\"cluster2\\\" is invalid: metadata.name: Invalid value: \\\"cluster2\\\": can't delete cluster which is participating in any slice\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The Cluster \\\"cluster2\\\" is invalid: metadata.name: Invalid value: \\\"cluster2\\\": can't delete cluster which is participating in any slice\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The Cluster \"cluster2\" is invalid: metadata.name: Invalid value: \"cluster2\": can't delete cluster which is participating in any slice\noccurred","time":{"start":1661066603000,"stop":1661066974882,"duration":371882}},{"uid":"2122a9659720938f","status":"failed","statusDetails":"Expected an error to have occurred.  Got:\n    <nil>: nil","time":{"start":1661018182000,"stop":1661018211885,"duration":29885}},{"uid":"fdde6f631bd2d936","status":"failed","statusDetails":"Expected an error to have occurred.  Got:\n    <nil>: nil","time":{"start":1661014620000,"stop":1661014651411,"duration":31411}},{"uid":"12730e0e0eb276ea","status":"failed","statusDetails":"Expected an error to have occurred.  Got:\n    <nil>: nil","time":{"start":1661011831000,"stop":1661011872519,"duration":41519}},{"uid":"365bbec59132abc0","status":"failed","statusDetails":"Expected an error to have occurred.  Got:\n    <nil>: nil","time":{"start":1661011780000,"stop":1661011831980,"duration":51980}},{"uid":"d1a914a0b7c5ab8","status":"failed","statusDetails":"Expected an error to have occurred.  Got:\n    <nil>: nil","time":{"start":1661009684000,"stop":1661009716043,"duration":32043}},{"uid":"7f38d0d56c16799c","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00064f218>: {\n        Underlying: <*exec.ExitError | 0xc000670240>{\n            ProcessState: {\n                pid: 6203,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 48153},\n                    Stime: {Sec: 0, Usec: 10318},\n                    Maxrss: 43040,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 1536,\n                    Majflt: 1,\n                    Nswap: 0,\n                    Inblock: 136,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 146,\n                    Nivcsw: 428,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Cluster \\\"cluster2\\\" is invalid: metadata.name: Invalid value: \\\"cluster2\\\": can't delete cluster which is participating in any slice\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The Cluster \\\"cluster2\\\" is invalid: metadata.name: Invalid value: \\\"cluster2\\\": can't delete cluster which is participating in any slice\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The Cluster \\\"cluster2\\\" is invalid: metadata.name: Invalid value: \\\"cluster2\\\": can't delete cluster which is participating in any slice\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The Cluster \\\"cluster2\\\" is invalid: metadata.name: Invalid value: \\\"cluster2\\\": can't delete cluster which is participating in any slice\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The Cluster \"cluster2\" is invalid: metadata.name: Invalid value: \"cluster2\": can't delete cluster which is participating in any slice\noccurred","time":{"start":1661007831000,"stop":1661008198810,"duration":367810}}]},"805089afe7f8d651afa895cca0aa1fa3":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"309499012fec4adf","status":"passed","time":{"start":1663841540356,"stop":1663841562060,"duration":21704}}]},"Worker Suite:Worker Suite#[It] Slice Creation scenarios Slice with LOW QOS should have gateway pod running":{"statistic":{"failed":4,"broken":0,"skipped":19,"passed":24,"unknown":0,"total":47},"items":[{"uid":"651f4eab67de4642","status":"passed","time":{"start":1664094053000,"stop":1664094138141,"duration":85141}},{"uid":"b6707ff2f2692bdb","status":"passed","time":{"start":1663843963000,"stop":1663844128370,"duration":165370}},{"uid":"8d754a220f731e06","status":"passed","time":{"start":1663829327000,"stop":1663829492348,"duration":165348}},{"uid":"66f1e79479e3e7ba","status":"passed","time":{"start":1663343132000,"stop":1663343297321,"duration":165321}},{"uid":"98b4ee58fe1359e0","status":"passed","time":{"start":1663342947000,"stop":1663342992129,"duration":45129}},{"uid":"c930c2a744d5c379","status":"skipped","statusDetails":"skipped","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"8ff45f21c9baad58","status":"skipped","statusDetails":"skipped","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"974b81851b9e9583","status":"passed","time":{"start":1662348908000,"stop":1662348923065,"duration":15065}},{"uid":"fd91aee50d7caf8b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"f3d10113362257f8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"bdda39e964ef6ba0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"87db5fb32f48588d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"24286b78495bd28","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"f36c145611e6087b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"828f9da68e0aace","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"73f3a818e0d16fa6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"7c8643c6ab286ef3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"f91d469747efd535","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"8f2696314b982364","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"2bd1b4515291389b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}}]},"de854ebf908b1a4ed1b267351a1aa2be":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"e2069f23aeccbacb","status":"passed","time":{"start":1663840239413,"stop":1663840251755,"duration":12342}}]},"Istio Suite:Istio Suite#[It] Bookinfo tests Bookinfo Scenario 4 with egress and ingress Should verify if Ping works successfully from Productpage to details endpoint":{"statistic":{"failed":4,"broken":0,"skipped":60,"passed":8,"unknown":0,"total":72},"items":[{"uid":"59981d3570c0e888","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1664246558000,"stop":1664246558000,"duration":0}},{"uid":"a1260fa36ba0ca0d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1664162065000,"stop":1664162065000,"duration":0}},{"uid":"a4cf63dc89d8b300","status":"failed","statusDetails":"Unexpected non-nil/non-zero argument at index 1:\n\t<*shell.ErrWithCmdOutput>: &shell.ErrWithCmdOutput{Underlying:(*exec.ExitError)(0xc000786a60), Output:(*shell.output)(0xc0004d7a70)}","time":{"start":1664093039000,"stop":1664093105966,"duration":66966}},{"uid":"a0d9ec2143ed24ac","status":"passed","time":{"start":1663842833000,"stop":1663842892729,"duration":59729}},{"uid":"c413300e4602a052","status":"failed","statusDetails":"Unexpected non-nil/non-zero argument at index 1:\n\t<*shell.ErrWithCmdOutput>: &shell.ErrWithCmdOutput{Underlying:(*exec.ExitError)(0xc0009b8960), Output:(*shell.output)(0xc000133d10)}","time":{"start":1663828380000,"stop":1663828447791,"duration":67791}},{"uid":"c5d6d996d697ea5c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663769301000,"stop":1663769301000,"duration":0}},{"uid":"66d33c0e957f6bbf","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663671043000,"stop":1663671043000,"duration":0}},{"uid":"afd34e37c24fcaa2","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663666667000,"stop":1663666667000,"duration":0}},{"uid":"3c78cf3b28cd26d0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663666522000,"stop":1663666522000,"duration":0}},{"uid":"66638d1a39b2553","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663585263000,"stop":1663585263000,"duration":0}},{"uid":"186d5ee84f49fb4b","status":"passed","time":{"start":1663342266000,"stop":1663342326310,"duration":60310}},{"uid":"e837bf342346b99c","status":"passed","time":{"start":1663341990000,"stop":1663342078527,"duration":88527}},{"uid":"4c2182777f862229","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663315204000,"stop":1663315204000,"duration":0}},{"uid":"260ed679991544ba","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663312558000,"stop":1663312558000,"duration":0}},{"uid":"7e7b952d062c7f4b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663055013000,"stop":1663055013000,"duration":0}},{"uid":"7f7009252383f8fb","status":"passed","time":{"start":1663043831000,"stop":1663043919299,"duration":88299}},{"uid":"9c759007999e32e9","status":"passed","time":{"start":1663043075000,"stop":1663043135800,"duration":60800}},{"uid":"372980324663ea33","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663040338000,"stop":1663040338000,"duration":0}},{"uid":"2f22f11c93f06ba8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662819574000,"stop":1662819574000,"duration":0}},{"uid":"fc94fe2cbe0d2fd8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662795310000,"stop":1662795310000,"duration":0}}]},"8c50a460060533c08e13329076687182":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"db5ce9cedfeb517f","status":"passed","time":{"start":1663842178251,"stop":1663842198266,"duration":20015}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Creation Should create Project while using valid manifest":{"statistic":{"failed":24,"broken":0,"skipped":0,"passed":195,"unknown":0,"total":219},"items":[{"uid":"7a8fb031122dba7d","status":"passed","time":{"start":1664275319000,"stop":1664275335716,"duration":16716}},{"uid":"3cd73abbeba7d0ca","status":"passed","time":{"start":1664245203000,"stop":1664245211721,"duration":8721}},{"uid":"287fc847210403e8","status":"passed","time":{"start":1664160732000,"stop":1664160739673,"duration":7673}},{"uid":"e6aee595176326ef","status":"passed","time":{"start":1664091725000,"stop":1664091733581,"duration":8581}},{"uid":"1a352d4fd51a4f26","status":"passed","time":{"start":1663841627000,"stop":1663841634919,"duration":7919}},{"uid":"46a91ca404e03634","status":"passed","time":{"start":1663827045000,"stop":1663827053562,"duration":8562}},{"uid":"63af11902f5330e1","status":"passed","time":{"start":1663767974000,"stop":1663767982477,"duration":8477}},{"uid":"2130b446d1902a85","status":"passed","time":{"start":1663669677000,"stop":1663669685551,"duration":8551}},{"uid":"394ab61330522b","status":"passed","time":{"start":1663665323000,"stop":1663665330827,"duration":7827}},{"uid":"7c48aefdc378d2ec","status":"passed","time":{"start":1663665170000,"stop":1663665177764,"duration":7764}},{"uid":"3e812e8ea2681d49","status":"passed","time":{"start":1663659067000,"stop":1663659074766,"duration":7766}},{"uid":"bc3480ae241a392a","status":"passed","time":{"start":1663656809000,"stop":1663656816761,"duration":7761}},{"uid":"d66b8e6185432b77","status":"passed","time":{"start":1663583877000,"stop":1663583885777,"duration":8777}},{"uid":"83f8c2da09585cf6","status":"passed","time":{"start":1663340950000,"stop":1663340957689,"duration":7689}},{"uid":"2dd73be2dd6af43b","status":"passed","time":{"start":1663340650000,"stop":1663340658530,"duration":8530}},{"uid":"aa52734302356c88","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <bool>: false\nto be true","time":{"start":1663308473000,"stop":1663308540779,"duration":67779}},{"uid":"3ce415b4cb8f84a0","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0006ef050>: {\n        Underlying: <*exec.ExitError | 0xc0004dbd00>{\n            ProcessState: {\n                pid: 6132,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 183605},\n                    Stime: {Sec: 0, Usec: 47897},\n                    Maxrss: 84104,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2668,\n                    Majflt: 1,\n                    Nswap: 0,\n                    Inblock: 256,\n                    Oublock: 13344,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 332,\n                    Nivcsw: 273,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20create%20Project%20while%20using%20valid%20manifest1048552398\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.26.65:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20create%20Project%20while%20using%20valid%20manifest1048552398\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.26.65:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20create%20Project%20while%20using%20valid%20manifest1048552398\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.26.65:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20create%20Project%20while%20using%20valid%20manifest1048552398\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.26.65:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20create%20Project%20while%20using%20valid%20manifest1048552398\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.26.65:443: connect: connection refused\noccurred","time":{"start":1663304356000,"stop":1663304356281,"duration":281}},{"uid":"b89ceb4f7924c1d","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <bool>: false\nto be true","time":{"start":1663302519000,"stop":1663302596331,"duration":77331}},{"uid":"da372fb8b5a9d99c","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0001c45d0>: {\n        Underlying: <*exec.ExitError | 0xc0004979e0>{\n            ProcessState: {\n                pid: 7101,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 232951},\n                    Stime: {Sec: 0, Usec: 54355},\n                    Maxrss: 75596,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 5298,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13344,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 516,\n                    Nivcsw: 888,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20create%20Project%20while%20using%20valid%20manifest3457837057\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.55.244:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20create%20Project%20while%20using%20valid%20manifest3457837057\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.55.244:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20create%20Project%20while%20using%20valid%20manifest3457837057\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.55.244:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20create%20Project%20while%20using%20valid%20manifest3457837057\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.55.244:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20create%20Project%20while%20using%20valid%20manifest3457837057\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.55.244:443: connect: connection refused\noccurred","time":{"start":1663264756000,"stop":1663264757308,"duration":1308}},{"uid":"e5fa974ccd27091d","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00084b7a0>: {\n        Underlying: <*exec.ExitError | 0xc000647f20>{\n            ProcessState: {\n                pid: 7148,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 208098},\n                    Stime: {Sec: 0, Usec: 42469},\n                    Maxrss: 86924,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 5225,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13344,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 483,\n                    Nivcsw: 483,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20create%20Project%20while%20using%20valid%20manifest1629692859\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.221.241:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20create%20Project%20while%20using%20valid%20manifest1629692859\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.221.241:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20create%20Project%20while%20using%20valid%20manifest1629692859\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.221.241:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20create%20Project%20while%20using%20valid%20manifest1629692859\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.221.241:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20create%20Project%20while%20using%20valid%20manifest1629692859\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.221.241:443: connect: connection refused\noccurred","time":{"start":1663228190000,"stop":1663228191077,"duration":1077}}]},"Istio Suite:Istio Suite#[It] Bookinfo tests Bookinfo Scenario 1 without egress and ingress Should verify if Curl Traffic works on Productpage test":{"statistic":{"failed":0,"broken":0,"skipped":54,"passed":18,"unknown":0,"total":72},"items":[{"uid":"45407ee7fceeb4a7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1664246558000,"stop":1664246558000,"duration":0}},{"uid":"baad58809bb9e2a6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1664162065000,"stop":1664162065000,"duration":0}},{"uid":"eae147d81ec8ad79","status":"passed","time":{"start":1664093039000,"stop":1664093039282,"duration":282}},{"uid":"26b3c6f5c2bf73be","status":"passed","time":{"start":1663842833000,"stop":1663842834011,"duration":1011}},{"uid":"60e505d5096e157d","status":"passed","time":{"start":1663828380000,"stop":1663828380436,"duration":436}},{"uid":"4eac536743a45588","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663769301000,"stop":1663769301000,"duration":0}},{"uid":"3d3f2fac1855bb6f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663671043000,"stop":1663671043000,"duration":0}},{"uid":"4a2ddc50d71aa7a1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663666667000,"stop":1663666667000,"duration":0}},{"uid":"f757770685f44876","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663666522000,"stop":1663666522000,"duration":0}},{"uid":"6fb26f86e45ef0b9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663585263000,"stop":1663585263000,"duration":0}},{"uid":"c0e38361e9b1ed46","status":"passed","time":{"start":1663342266000,"stop":1663342269354,"duration":3354}},{"uid":"98e90c2969b8b541","status":"passed","time":{"start":1663341990000,"stop":1663341990375,"duration":375}},{"uid":"e6d1a7ea6d00f07d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663315204000,"stop":1663315204000,"duration":0}},{"uid":"c800bc7d0e66d866","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663312558000,"stop":1663312558000,"duration":0}},{"uid":"c2682fe54feaf40e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663055013000,"stop":1663055013000,"duration":0}},{"uid":"d11a6f3a5dfb0e7d","status":"passed","time":{"start":1663043831000,"stop":1663043834278,"duration":3278}},{"uid":"8117bc2d28568dd2","status":"passed","time":{"start":1663043075000,"stop":1663043075551,"duration":551}},{"uid":"f9fa7af15dea66e4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663040338000,"stop":1663040338000,"duration":0}},{"uid":"80ab7dbde32f2cfe","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662819574000,"stop":1662819574000,"duration":0}},{"uid":"d7f3fe3936598d21","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662795310000,"stop":1662795310000,"duration":0}}]},"8a68784a3c96cc9316c6d5d5ebd581c6":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"15a209356c3e1a6a","status":"passed","time":{"start":1663843424914,"stop":1663843465381,"duration":40467}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting iperf-client pod Should restart iperf-client pod":{"statistic":{"failed":0,"broken":0,"skipped":56,"passed":0,"unknown":0,"total":56},"items":[{"uid":"c8e6af7d06444487","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1664094053000,"stop":1664094053000,"duration":0}},{"uid":"62b67f4fd87db7b3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663843963000,"stop":1663843963000,"duration":0}},{"uid":"2919c0eb2f1980c1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663829327000,"stop":1663829327000,"duration":0}},{"uid":"bb13274bc0ca83bb","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663343132000,"stop":1663343132000,"duration":0}},{"uid":"ded5e8c735f3d4fe","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663342947000,"stop":1663342947000,"duration":0}},{"uid":"926c0e10413cadea","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"254ce42c2479c0a3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"bf18af7d9dbb5b81","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662348908000,"stop":1662348908000,"duration":0}},{"uid":"e754f608ee414e0e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"1b773cf23f270ed8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"282a0f31b143fea","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"dfd1e8ddf42f469f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"332c38b62b4bc970","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"1244a264093dd370","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"b82c09aad3b05740","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"a2fe4bd92c780911","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"b673f154df1323e8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"65b51b0637fbc5cb","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"22cbbcb5807ceea7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"3f1bdf2d3340251c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Namespace reconcile test scenarios: Onboard cluster objs should have app ns & attached slice entry":{"statistic":{"failed":9,"broken":0,"skipped":1,"passed":46,"unknown":0,"total":56},"items":[{"uid":"72c9fc368acaa07c","status":"passed","time":{"start":1664094053000,"stop":1664094053238,"duration":238}},{"uid":"19d9db82ef46db8b","status":"passed","time":{"start":1663843963000,"stop":1663843963236,"duration":236}},{"uid":"e82a8394822b4018","status":"passed","time":{"start":1663829327000,"stop":1663829327183,"duration":183}},{"uid":"d953e410595e41ac","status":"passed","time":{"start":1663343132000,"stop":1663343132325,"duration":325}},{"uid":"b29ded9c6fbe220f","status":"passed","time":{"start":1663342947000,"stop":1663342947327,"duration":327}},{"uid":"a10e62794104967c","status":"skipped","statusDetails":"skipped","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"6d1296a18410294d","status":"failed","statusDetails":"Expected\n    <int>: 0\nto equal\n    <int>: 2","time":{"start":1663044069000,"stop":1663044091249,"duration":22249}},{"uid":"ec96360cad334a84","status":"failed","statusDetails":"Expected\n    <int>: 0\nto equal\n    <int>: 2","time":{"start":1662348908000,"stop":1662348929673,"duration":21673}},{"uid":"3ab56ca33c2b180e","status":"failed","statusDetails":"Expected\n    <int>: 0\nto equal\n    <int>: 2","time":{"start":1661844205000,"stop":1661844227038,"duration":22038}},{"uid":"7d68d6ca7c5773f9","status":"failed","statusDetails":"Expected\n    <int>: 0\nto equal\n    <int>: 2","time":{"start":1661754870000,"stop":1661754891762,"duration":21762}},{"uid":"f684b94ff9cbaa6b","status":"passed","time":{"start":1661611962000,"stop":1661611962178,"duration":178}},{"uid":"9a1a7b83d50f4e9d","status":"failed","statusDetails":"Expected\n    <int>: 0\nto equal\n    <int>: 2","time":{"start":1661585077000,"stop":1661585098905,"duration":21905}},{"uid":"efff75aab0b286a6","status":"passed","time":{"start":1661584447000,"stop":1661584447213,"duration":213}},{"uid":"b3f1f2598ec7b96c","status":"passed","time":{"start":1661580045000,"stop":1661580045197,"duration":197}},{"uid":"5a0c2ae2acb1c21a","status":"failed","statusDetails":"Expected\n    <int>: 0\nto equal\n    <int>: 2","time":{"start":1661578505000,"stop":1661578527023,"duration":22023}},{"uid":"88e078d8587aeafa","status":"failed","statusDetails":"Expected\n    <int>: 0\nto equal\n    <int>: 2","time":{"start":1661572479000,"stop":1661572501148,"duration":22148}},{"uid":"669450aa59ab134b","status":"passed","time":{"start":1660832281000,"stop":1660832281144,"duration":144}},{"uid":"9135abeb8e2cca39","status":"failed","statusDetails":"Expected\n    <int>: 0\nto equal\n    <int>: 2","time":{"start":1660808797000,"stop":1660808818719,"duration":21719}},{"uid":"10cfc04b8b336a0e","status":"failed","statusDetails":"Expected\n    <int>: 0\nto equal\n    <int>: 2","time":{"start":1660299182000,"stop":1660299203653,"duration":21653}},{"uid":"3774be67279f862b","status":"passed","time":{"start":1660295740000,"stop":1660295740137,"duration":137}}]},"6153e759334da55a5fc3744f715b0c9":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"61b27872995d60ae","status":"passed","time":{"start":1663840609465,"stop":1663840630394,"duration":20929}}]},"20df56e420230722e50cae881d777a0a":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"23dd71ebd97af3e","status":"passed","time":{"start":1663840421768,"stop":1663840435664,"duration":13896}}]},"5f6c923af7aadfd645300755ad530463":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"822e1e6904e0e21d","status":"passed","time":{"start":1663841340323,"stop":1663841356871,"duration":16548}}]},"97fee0015e46afc1d3e89be8bba28fcf":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"4c8fa4a937e0165d","status":"passed","time":{"start":1663840484431,"stop":1663840505285,"duration":20854}}]},"122d3f7016f730e3ec0ae8b471221186":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"abf5ddd4c604296e","status":"passed","time":{"start":1663841523239,"stop":1663841540342,"duration":17103}}]},"66fff20c555597582276e20dbc836416":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"270e3cc32a3f408c","status":"passed","time":{"start":1663841229990,"stop":1663841245408,"duration":15418}}]},"3a949b4e4fda2890b6c1c97095c8c6f2":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"b1880da14f57f867","status":"passed","time":{"start":1663843547195,"stop":1663843559152,"duration":11957}}]},"Worker Suite:Worker Suite#[It] Intracluster slice tests Iperf connectivity Verify Iperf traffic":{"statistic":{"failed":0,"broken":0,"skipped":51,"passed":5,"unknown":0,"total":56},"items":[{"uid":"6e98505389bca66d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1664094053000,"stop":1664094053000,"duration":0}},{"uid":"89914ca48998b2a7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663843963000,"stop":1663843963000,"duration":0}},{"uid":"89d8c7ab6ea7890e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663829327000,"stop":1663829327000,"duration":0}},{"uid":"749ec1dfa36f4d1c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663343132000,"stop":1663343132000,"duration":0}},{"uid":"64ec08e559753f56","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663342947000,"stop":1663342947000,"duration":0}},{"uid":"6005108bdb98a8c5","status":"skipped","statusDetails":"skipped","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"63a004836a11a663","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"d932ba3959d64d78","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662348908000,"stop":1662348908000,"duration":0}},{"uid":"f6055a860c82c191","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"a060fd9c7ffa3609","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"1e95eeeb30062bcb","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"d8c9c0880e401d6a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"73395a9d55d5d05b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"2a74c8ba061ca6d5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"3cd0ba31c4f0bbb2","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"37c564a63eb77e21","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"bb95be07e74753d5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"f628ee148388006b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"ae99bacad219ebc1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"93c952e4bb2b6a94","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}}]},"305740e80d8332bedb72a4da3e77f591":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"509f040481a5ae65","status":"passed","time":{"start":1663839254875,"stop":1663839266288,"duration":11413}}]},"6ececfe5b95751fd3e359ed70c302d0a":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"ae407f7cf8c8ff67","status":"passed","time":{"start":1663840721473,"stop":1663840736539,"duration":15066}}]},"Worker Suite:Worker Suite#[It] Slice creation scenarios: when slice red is installed install slice blue workers should have gateway pods from both slices":{"statistic":{"failed":2,"broken":0,"skipped":38,"passed":16,"unknown":0,"total":56},"items":[{"uid":"fe81eeaa831ce0a1","status":"passed","time":{"start":1664094053000,"stop":1664094218637,"duration":165637}},{"uid":"2b98dd9dc62184a1","status":"passed","time":{"start":1663843963000,"stop":1663844049172,"duration":86172}},{"uid":"9d85435bb67d4a8a","status":"failed","statusDetails":"Timed out after 180.002s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1663829327000,"stop":1663829507845,"duration":180845}},{"uid":"8a3c9433f0415a3","status":"passed","time":{"start":1663343132000,"stop":1663343144733,"duration":12733}},{"uid":"c900cfc71aa6e957","status":"passed","time":{"start":1663342947000,"stop":1663342959729,"duration":12729}},{"uid":"9af88758bd342c0c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"bd23e6b94f50d704","status":"failed","statusDetails":"Timed out after 120.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1663044069000,"stop":1663044190399,"duration":121399}},{"uid":"5cbd5667d4281063","status":"passed","time":{"start":1662348908000,"stop":1662348921347,"duration":13347}},{"uid":"79ad51329d267ce6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"7568107bd2de62c1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"4c133f8cba87c353","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"2416ef114877b86a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"4e7918b223e340f6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"5e72a78f2df8b574","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"dbdbe7563a8dfe39","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"7ab63ae684c9fb2d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"66e2a217e11ca447","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"7ac2f98b7a731107","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"e93f7d98170f432","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"50896baa337f0314","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}}]},"197fb14677cc5282ce4907169e29c9dd":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"8323b4a3db5bce11","status":"passed","time":{"start":1663842235500,"stop":1663842252603,"duration":17103}}]},"7d157f970faa52b41404cc52bad64124":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"d89e29a8dca6c706","status":"passed","time":{"start":1663843024098,"stop":1663843088036,"duration":63938}}]},"bb9082e2ce3b69b05cb644e37cef8ddd":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"ea3240d2fa1d4d67","status":"passed","time":{"start":1663839985608,"stop":1663839995575,"duration":9967}}]},"7381a2e86e269134027f648f31fdc0de":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"73facc098f87dcfa","status":"passed","time":{"start":1663839007353,"stop":1663839014294,"duration":6941}}]},"81aa4c27f4b0d321ba9ae36f9648d58f":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"8afa34bd596fa040","status":"passed","time":{"start":1663839463069,"stop":1663839473507,"duration":10438}}]},"898804b1480b70204d6fdcdbe10828e6":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"16c47953651f0cd6","status":"passed","time":{"start":1663842507847,"stop":1663842548630,"duration":40783}}]},"cf78aaf473bf13dc3055db08ca6fbe3d":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"c0b2a1f11bf660ba","status":"passed","time":{"start":1663840925587,"stop":1663840941160,"duration":15573}}]},"5773e18b796579891a8cdab667cdd627":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"392e405761c69ffe","status":"passed","time":{"start":1663842007209,"stop":1663842045068,"duration":37859}}]},"a4f19457356c6221aeedae7f033ac252":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"c76d4cbb32cb79b3","status":"passed","time":{"start":1663840550686,"stop":1663840563122,"duration":12436}}]},"90218cb56955191fa8f15007815ff982":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"db73d43acc86c419","status":"passed","time":{"start":1663841245409,"stop":1663841261806,"duration":16397}}]},"650173bcfe3ae780dd357cf423271f08":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"b624299420cc3938","status":"passed","time":{"start":1663841180896,"stop":1663841201750,"duration":20854}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting vl3 pod Should restart vl3 pod":{"statistic":{"failed":0,"broken":0,"skipped":55,"passed":1,"unknown":0,"total":56},"items":[{"uid":"3414b82d96f18e79","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1664094053000,"stop":1664094053000,"duration":0}},{"uid":"cba609685c890ce","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663843963000,"stop":1663843963000,"duration":0}},{"uid":"282c27319c322f04","status":"passed","time":{"start":1663829327000,"stop":1663829337637,"duration":10637}},{"uid":"f2ad94ef5753577b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663343132000,"stop":1663343132000,"duration":0}},{"uid":"f04101a32f3dfe87","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663342947000,"stop":1663342947000,"duration":0}},{"uid":"4274a85f0b96831b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"7856d6774cb0d44e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"604296451d27452f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662348908000,"stop":1662348908000,"duration":0}},{"uid":"5684070c6dedb89a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"b553f673ed18de4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"6cbfefa9d958b61b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"ad4b2ec22202de52","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"4799abbfb9b59932","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"58aa1e3209bed2c9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"9ac11b20acfb89df","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"d77de24e490d6e58","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"bb7f048e68880f49","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"12f4fd71e8305024","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"6e9da996a03df39f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"4704c9aba44cacc0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}}]},"8b472224898d232a99656d9adf534e5e":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"a84652938924170c","status":"passed","time":{"start":1663838690677,"stop":1663838705423,"duration":14746}}]},"Worker Suite:Worker Suite#[It] Slice Config Negative Scenario tests Slice Config Negative Scenario Creation Should fail while creating sliceconfig with invalid values on slice name":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":55,"unknown":0,"total":56},"items":[{"uid":"639ba09f27daa017","status":"passed","time":{"start":1664094053000,"stop":1664094054121,"duration":1121}},{"uid":"bf74155f8c6b0ae7","status":"passed","time":{"start":1663843963000,"stop":1663843963874,"duration":874}},{"uid":"3927e1fe2cdb0252","status":"passed","time":{"start":1663829327000,"stop":1663829328203,"duration":1203}},{"uid":"108d303283cad504","status":"passed","time":{"start":1663343132000,"stop":1663343133063,"duration":1063}},{"uid":"4dea96041ffd5962","status":"passed","time":{"start":1663342947000,"stop":1663342947544,"duration":544}},{"uid":"291742a93f910f17","status":"passed","time":{"start":1663044723000,"stop":1663044723729,"duration":729}},{"uid":"c6cb34457197f93","status":"skipped","statusDetails":"skipped","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"1e95f653b65c2fd2","status":"passed","time":{"start":1662348908000,"stop":1662348908504,"duration":504}},{"uid":"6166e3bb4e56b615","status":"passed","time":{"start":1661844205000,"stop":1661844205594,"duration":594}},{"uid":"cacdd1f222a09788","status":"passed","time":{"start":1661754870000,"stop":1661754870384,"duration":384}},{"uid":"e1fe5a0d443981db","status":"passed","time":{"start":1661611962000,"stop":1661611962554,"duration":554}},{"uid":"fa56fdff5b3fb08b","status":"passed","time":{"start":1661585077000,"stop":1661585077506,"duration":506}},{"uid":"ead6cb3c94a54b53","status":"passed","time":{"start":1661584447000,"stop":1661584447438,"duration":438}},{"uid":"a15349353408f82b","status":"passed","time":{"start":1661580045000,"stop":1661580045431,"duration":431}},{"uid":"7e7760fdabf87cc0","status":"passed","time":{"start":1661578505000,"stop":1661578505519,"duration":519}},{"uid":"8176e736452b6199","status":"passed","time":{"start":1661572479000,"stop":1661572479544,"duration":544}},{"uid":"2cac2431f32beae0","status":"passed","time":{"start":1660832281000,"stop":1660832281440,"duration":440}},{"uid":"6003aa082d9a15e2","status":"passed","time":{"start":1660808797000,"stop":1660808797416,"duration":416}},{"uid":"a9c35014c0d5286a","status":"passed","time":{"start":1660299182000,"stop":1660299182377,"duration":377}},{"uid":"52382223a86a0c0","status":"passed","time":{"start":1660295740000,"stop":1660295740342,"duration":342}}]},"53ba1ba95f74149d90d49fe25580216b":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"aa9dc4036c2da8e5","status":"passed","time":{"start":1663842979450,"stop":1663843024096,"duration":44646}}]},"f8aa1b57ede955f10fb70a638badc5ad":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"4729ba8e7bb82b7","status":"passed","time":{"start":1663842219618,"stop":1663842235498,"duration":15880}}]},"93c8a50fbbec5aa4f0b666c5995a5132":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"fa19ca7d5959c1a9","status":"passed","time":{"start":1663840435665,"stop":1663840445862,"duration":10197}}]},"9dbff3843cff2ce330f2ad5faf5963eb":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"c6c16762d1c1a90c","status":"passed","time":{"start":1663841506381,"stop":1663841523238,"duration":16857}}]},"Worker Suite:Worker Suite#[It] Slice validation Slice with LOW QOS should reconcile low qos slice":{"statistic":{"failed":2,"broken":0,"skipped":3,"passed":4,"unknown":0,"total":9},"items":[{"uid":"163950f3c0e6d910","status":"passed","time":{"start":1659168739000,"stop":1659168739395,"duration":395}},{"uid":"de409261aa8cd036","status":"passed","time":{"start":1659164084000,"stop":1659164084315,"duration":315}},{"uid":"e4c1a18074ae344e","status":"passed","time":{"start":1659160188000,"stop":1659160188292,"duration":292}},{"uid":"34602a2dfa4121a2","status":"passed","time":{"start":1659119724000,"stop":1659119724310,"duration":310}},{"uid":"1e46ed45be6838ba","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000130ac8>: {\n        Underlying: <*exec.ExitError | 0xc00079e000>{\n            ProcessState: {\n                pid: 7527,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 50928},\n                    Stime: {Sec: 0, Usec: 18188},\n                    Maxrss: 45792,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2561,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 184,\n                    Nivcsw: 163,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicelowqos\\\" not found\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicelowqos\\\" not found\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicelowqos\\\" not found\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicelowqos\\\" not found\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (NotFound): slice.networking.kubeslice.io \"slicelowqos\" not found\noccurred","time":{"start":1659116511000,"stop":1659116511891,"duration":891}},{"uid":"4d3617c2bce05dfb","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00088d680>: {\n        Underlying: <*exec.ExitError | 0xc0007e2400>{\n            ProcessState: {\n                pid: 7608,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 61643},\n                    Stime: {Sec: 0, Usec: 14225},\n                    Maxrss: 41880,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2500,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 187,\n                    Nivcsw: 280,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicelowqos\\\" not found\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicelowqos\\\" not found\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicelowqos\\\" not found\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slicelowqos\\\" not found\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (NotFound): slice.networking.kubeslice.io \"slicelowqos\" not found\noccurred","time":{"start":1659109470000,"stop":1659109470829,"duration":829}},{"uid":"70e2f9f641c12986","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659106836000,"stop":1659106836000,"duration":0}},{"uid":"5d48e97edd82140c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659101522000,"stop":1659101522000,"duration":0}},{"uid":"758a38f347a57677","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659082816000,"stop":1659082816000,"duration":0}}]},"Worker Suite:Worker Suite#[It]  QosProfile tests QosProfile Create validation Creates qosprofile":{"statistic":{"failed":8,"broken":0,"skipped":2,"passed":6,"unknown":0,"total":16},"items":[{"uid":"54244265b3132595","status":"passed","time":{"start":1664094053000,"stop":1664094053431,"duration":431}},{"uid":"caa68325c5eff463","status":"passed","time":{"start":1663843963000,"stop":1663843963611,"duration":611}},{"uid":"750792930c539706","status":"passed","time":{"start":1663829327000,"stop":1663829327866,"duration":866}},{"uid":"b65e508c59a80088","status":"passed","time":{"start":1663343132000,"stop":1663343133322,"duration":1322}},{"uid":"9e867709bdad413","status":"passed","time":{"start":1663342947000,"stop":1663342947496,"duration":496}},{"uid":"92537cc13c9aa34a","status":"skipped","statusDetails":"skipped","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"42f8a951596158bc","status":"skipped","statusDetails":"skipped","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"6d23ffbe2b9b82a1","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004a8fc0>: {\n        Underlying: <*exec.ExitError | 0xc0006c02e0>{\n            ProcessState: {\n                pid: 7624,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 220498},\n                    Stime: {Sec: 0, Usec: 64852},\n                    Maxrss: 84356,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4524,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 17168,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 882,\n                    Nivcsw: 622,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile14237864\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile14237864\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile14237864\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile14237864\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile14237864\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1662348908000,"stop":1662348909308,"duration":1308}},{"uid":"10b2ffe525c8069d","status":"passed","time":{"start":1661844205000,"stop":1661844206973,"duration":1973}},{"uid":"56377b85ade8e235","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000734c90>: {\n        Underlying: <*exec.ExitError | 0xc00017f4e0>{\n            ProcessState: {\n                pid: 7231,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 194309},\n                    Stime: {Sec: 0, Usec: 28939},\n                    Maxrss: 84624,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4130,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 448,\n                    Nivcsw: 513,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile409231599\\\": Internal error occurred: failed calling webhook \\\"msliceqosconfig.kb.io\\\": failed to call webhook: the server could not find the requested resource\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile409231599\\\": Internal error occurred: failed calling webhook \\\"msliceqosconfig.kb.io\\\": failed to call webhook: the server could not find the requested resource\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile409231599\\\": Internal error occurred: failed calling webhook \\\"msliceqosconfig.kb.io\\\": failed to call webhook: the server could not find the requested resource\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile409231599\\\": Internal error occurred: failed calling webhook \\\"msliceqosconfig.kb.io\\\": failed to call webhook: the server could not find the requested resource\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile409231599\": Internal error occurred: failed calling webhook \"msliceqosconfig.kb.io\": failed to call webhook: the server could not find the requested resource\noccurred","time":{"start":1661754870000,"stop":1661754870227,"duration":227}},{"uid":"cdfccedb32805a6d","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0008ce8d0>: {\n        Underlying: <*exec.ExitError | 0xc000075160>{\n            ProcessState: {\n                pid: 7126,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 186229},\n                    Stime: {Sec: 0, Usec: 50789},\n                    Maxrss: 83332,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3346,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 311,\n                    Nivcsw: 468,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile478452098\\\": Internal error occurred: failed calling webhook \\\"msliceqosconfig.kb.io\\\": failed to call webhook: the server could not find the requested resource\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile478452098\\\": Internal error occurred: failed calling webhook \\\"msliceqosconfig.kb.io\\\": failed to call webhook: the server could not find the requested resource\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile478452098\\\": Internal error occurred: failed calling webhook \\\"msliceqosconfig.kb.io\\\": failed to call webhook: the server could not find the requested resource\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile478452098\\\": Internal error occurred: failed calling webhook \\\"msliceqosconfig.kb.io\\\": failed to call webhook: the server could not find the requested resource\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile478452098\": Internal error occurred: failed calling webhook \"msliceqosconfig.kb.io\": failed to call webhook: the server could not find the requested resource\noccurred","time":{"start":1661611962000,"stop":1661611962282,"duration":282}},{"uid":"39dff07005bfd391","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00090d2c0>: {\n        Underlying: <*exec.ExitError | 0xc00096b7e0>{\n            ProcessState: {\n                pid: 7253,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 203666},\n                    Stime: {Sec: 0, Usec: 16972},\n                    Maxrss: 73652,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3787,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 243,\n                    Nivcsw: 541,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1956553570\\\": Internal error occurred: failed calling webhook \\\"msliceqosconfig.kb.io\\\": failed to call webhook: the server could not find the requested resource\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1956553570\\\": Internal error occurred: failed calling webhook \\\"msliceqosconfig.kb.io\\\": failed to call webhook: the server could not find the requested resource\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1956553570\\\": Internal error occurred: failed calling webhook \\\"msliceqosconfig.kb.io\\\": failed to call webhook: the server could not find the requested resource\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1956553570\\\": Internal error occurred: failed calling webhook \\\"msliceqosconfig.kb.io\\\": failed to call webhook: the server could not find the requested resource\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1956553570\": Internal error occurred: failed calling webhook \"msliceqosconfig.kb.io\": failed to call webhook: the server could not find the requested resource\noccurred","time":{"start":1661585077000,"stop":1661585077211,"duration":211}},{"uid":"7bc68a0e88891c85","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000e2b8>: {\n        Underlying: <*exec.ExitError | 0xc0004982a0>{\n            ProcessState: {\n                pid: 6256,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 209933},\n                    Stime: {Sec: 0, Usec: 28814},\n                    Maxrss: 76176,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3487,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 450,\n                    Nivcsw: 699,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1035375976\\\": Internal error occurred: failed calling webhook \\\"msliceqosconfig.kb.io\\\": failed to call webhook: the server could not find the requested resource\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1035375976\\\": Internal error occurred: failed calling webhook \\\"msliceqosconfig.kb.io\\\": failed to call webhook: the server could not find the requested resource\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1035375976\\\": Internal error occurred: failed calling webhook \\\"msliceqosconfig.kb.io\\\": failed to call webhook: the server could not find the requested resource\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1035375976\\\": Internal error occurred: failed calling webhook \\\"msliceqosconfig.kb.io\\\": failed to call webhook: the server could not find the requested resource\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1035375976\": Internal error occurred: failed calling webhook \"msliceqosconfig.kb.io\": failed to call webhook: the server could not find the requested resource\noccurred","time":{"start":1661584447000,"stop":1661584447271,"duration":271}},{"uid":"430de593b8f536f7","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0008947f8>: {\n        Underlying: <*exec.ExitError | 0xc00066f4c0>{\n            ProcessState: {\n                pid: 7218,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 179043},\n                    Stime: {Sec: 0, Usec: 40691},\n                    Maxrss: 84332,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4528,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 305,\n                    Nivcsw: 496,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile2172885303\\\": Internal error occurred: failed calling webhook \\\"msliceqosconfig.kb.io\\\": failed to call webhook: the server could not find the requested resource\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile2172885303\\\": Internal error occurred: failed calling webhook \\\"msliceqosconfig.kb.io\\\": failed to call webhook: the server could not find the requested resource\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile2172885303\\\": Internal error occurred: failed calling webhook \\\"msliceqosconfig.kb.io\\\": failed to call webhook: the server could not find the requested resource\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile2172885303\\\": Internal error occurred: failed calling webhook \\\"msliceqosconfig.kb.io\\\": failed to call webhook: the server could not find the requested resource\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile2172885303\": Internal error occurred: failed calling webhook \"msliceqosconfig.kb.io\": failed to call webhook: the server could not find the requested resource\noccurred","time":{"start":1661580045000,"stop":1661580045231,"duration":231}},{"uid":"d7e26e5ec18d1799","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000866e10>: {\n        Underlying: <*exec.ExitError | 0xc0001705a0>{\n            ProcessState: {\n                pid: 7215,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 283655},\n                    Stime: {Sec: 0, Usec: 147820},\n                    Maxrss: 88092,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 7354,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 608,\n                    Nivcsw: 769,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile644429197\\\": Internal error occurred: failed calling webhook \\\"msliceqosconfig.kb.io\\\": failed to call webhook: the server could not find the requested resource\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile644429197\\\": Internal error occurred: failed calling webhook \\\"msliceqosconfig.kb.io\\\": failed to call webhook: the server could not find the requested resource\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile644429197\\\": Internal error occurred: failed calling webhook \\\"msliceqosconfig.kb.io\\\": failed to call webhook: the server could not find the requested resource\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile644429197\\\": Internal error occurred: failed calling webhook \\\"msliceqosconfig.kb.io\\\": failed to call webhook: the server could not find the requested resource\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile644429197\": Internal error occurred: failed calling webhook \"msliceqosconfig.kb.io\": failed to call webhook: the server could not find the requested resource\noccurred","time":{"start":1661578505000,"stop":1661578505425,"duration":425}},{"uid":"b2526d8eff8749cd","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00083b320>: {\n        Underlying: <*exec.ExitError | 0xc000680760>{\n            ProcessState: {\n                pid: 7326,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 246465},\n                    Stime: {Sec: 0, Usec: 53400},\n                    Maxrss: 88600,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3743,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 421,\n                    Nivcsw: 411,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile491100803\\\": Internal error occurred: failed calling webhook \\\"msliceqosconfig.kb.io\\\": failed to call webhook: the server could not find the requested resource\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile491100803\\\": Internal error occurred: failed calling webhook \\\"msliceqosconfig.kb.io\\\": failed to call webhook: the server could not find the requested resource\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile491100803\\\": Internal error occurred: failed calling webhook \\\"msliceqosconfig.kb.io\\\": failed to call webhook: the server could not find the requested resource\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile491100803\\\": Internal error occurred: failed calling webhook \\\"msliceqosconfig.kb.io\\\": failed to call webhook: the server could not find the requested resource\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile491100803\": Internal error occurred: failed calling webhook \"msliceqosconfig.kb.io\": failed to call webhook: the server could not find the requested resource\noccurred","time":{"start":1661572479000,"stop":1661572479300,"duration":300}}]},"Worker Suite:Worker Suite#[It] Intracluster slice tests Slice with netpol should reconcile netpol in app NS":{"statistic":{"failed":1,"broken":0,"skipped":38,"passed":17,"unknown":0,"total":56},"items":[{"uid":"d3bf6fb61641b4d9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1664094053000,"stop":1664094053000,"duration":0}},{"uid":"9cee7c21cb37a2","status":"passed","time":{"start":1663843963000,"stop":1663843963176,"duration":176}},{"uid":"87c9ec054ba0c563","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663829327000,"stop":1663829327000,"duration":0}},{"uid":"dd724abd18529dce","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0007dc570>: {\n        Underlying: <*exec.ExitError | 0xc0007f8340>{\n            ProcessState: {\n                pid: 7725,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 49147},\n                    Stime: {Sec: 0, Usec: 21063},\n                    Maxrss: 43272,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2186,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 164,\n                    Nivcsw: 216,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (NotFound): networkpolicies.networking.k8s.io \\\"slice-netpol-iperf\\\" not found\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (NotFound): networkpolicies.networking.k8s.io \\\"slice-netpol-iperf\\\" not found\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (NotFound): networkpolicies.networking.k8s.io \\\"slice-netpol-iperf\\\" not found\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (NotFound): networkpolicies.networking.k8s.io \\\"slice-netpol-iperf\\\" not found\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (NotFound): networkpolicies.networking.k8s.io \"slice-netpol-iperf\" not found\noccurred","time":{"start":1663343132000,"stop":1663343140850,"duration":8850}},{"uid":"d36d5a393e22d14","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663342947000,"stop":1663342947000,"duration":0}},{"uid":"22a11f59cafafc60","status":"skipped","statusDetails":"skipped","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"b631f5383a1bd584","status":"passed","time":{"start":1663044069000,"stop":1663044069201,"duration":201}},{"uid":"5caab0f7781479db","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662348908000,"stop":1662348908000,"duration":0}},{"uid":"9621a14826a8aeec","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"d06ab1fbe660b229","status":"skipped","statusDetails":"skipped - Spec skipped because Skip() was called in BeforeAll","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"6ee4a737597466b4","status":"skipped","statusDetails":"skipped - Spec skipped because Skip() was called in BeforeAll","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"1c3e680e1d9aa172","status":"skipped","statusDetails":"skipped - Spec skipped because Skip() was called in BeforeAll","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"e1d87546b2d5eca0","status":"skipped","statusDetails":"skipped - Spec skipped because Skip() was called in BeforeAll","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"6766d870f32f34ee","status":"skipped","statusDetails":"skipped - Spec skipped because Skip() was called in BeforeAll","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"aa67149fc0ee5d8a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"e907dbece23f7b0f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"20c128e3003afe08","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"31b11f2d72313b84","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"73e92772cb87ca71","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"21edcf4724cf6993","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}}]},"54f1c147c6d815a0e2864057f7e51bbd":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"29d58a1996f0f560","status":"passed","time":{"start":1663841987750,"stop":1663842007207,"duration":19457}}]},"1d39f0599dbac2bf88483e4fd8b2ea77":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"7c1bbd0d4f0fbce4","status":"passed","time":{"start":1663838840895,"stop":1663838854960,"duration":14065}}]},"e041be46660dcb38d680d940e156b4ca":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"1a6f6d25ffb94849","status":"passed","time":{"start":1663838719317,"stop":1663838731539,"duration":12222}}]},"43022538eeff463deb5f1d4f54ef2e8f":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"b578e7822247858f","status":"passed","time":{"start":1663840326780,"stop":1663840337103,"duration":10323}}]},"Empty Suite:Empty Suite#[It] Hub Creation tests Hub Installtion Test Senarios Should fail if Hub is installed again in the same cluster":{"statistic":{"failed":0,"broken":0,"skipped":40,"passed":208,"unknown":0,"total":248},"items":[{"uid":"78bbe226d42b8f2a","status":"passed","time":{"start":1664282572000,"stop":1664282575281,"duration":3281}},{"uid":"582b1105c91bdf30","status":"passed","time":{"start":1664279225000,"stop":1664279228049,"duration":3049}},{"uid":"fa03ad6b58a024a6","status":"passed","time":{"start":1664277805000,"stop":1664277807544,"duration":2544}},{"uid":"1ea58ad122a240e2","status":"passed","time":{"start":1664275170000,"stop":1664275172922,"duration":2922}},{"uid":"d1e6fa576658d3e8","status":"passed","time":{"start":1664275319000,"stop":1664275321763,"duration":2763}},{"uid":"c3db2a1ccaf8d2d9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1664269661000,"stop":1664269661000,"duration":0}},{"uid":"375bdd21f1a1cdf3","status":"passed","time":{"start":1664261300000,"stop":1664261302780,"duration":2780}},{"uid":"4cb6fa0718f3677d","status":"passed","time":{"start":1664245049000,"stop":1664245052454,"duration":3454}},{"uid":"2c218a15c6afcc75","status":"passed","time":{"start":1664218010000,"stop":1664218012899,"duration":2899}},{"uid":"7151394f189c446e","status":"passed","time":{"start":1664160583000,"stop":1664160585672,"duration":2672}},{"uid":"70dc48a540830db6","status":"passed","time":{"start":1664105948000,"stop":1664105952043,"duration":4043}},{"uid":"f2efac46d4ae95a8","status":"passed","time":{"start":1664103959000,"stop":1664103961977,"duration":2977}},{"uid":"f347558b8dd42875","status":"passed","time":{"start":1664091567000,"stop":1664091569199,"duration":2199}},{"uid":"ca1f098aaf723114","status":"passed","time":{"start":1663932902000,"stop":1663932905681,"duration":3681}},{"uid":"8c4484d83e0b79e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663917718000,"stop":1663917718000,"duration":0}},{"uid":"c273278eeed4596e","status":"passed","time":{"start":1663912384000,"stop":1663912386323,"duration":2323}},{"uid":"ebc91dd0179dc9fe","status":"passed","time":{"start":1663841452000,"stop":1663841455942,"duration":3942}},{"uid":"fc1f72552561c362","status":"passed","time":{"start":1663838862000,"stop":1663838864842,"duration":2842}},{"uid":"7c69bb6328ac9d98","status":"passed","time":{"start":1663838333000,"stop":1663838336599,"duration":3599}},{"uid":"69551beef627211f","status":"passed","time":{"start":1663836341000,"stop":1663836343499,"duration":2499}}]},"9826cd1a4f9313d9b245128c7fdbdc7e":{"statistic":{"failed":1,"broken":0,"skipped":0,"passed":0,"unknown":0,"total":1},"items":[{"uid":"c534b26bff0ba391","status":"failed","statusDetails":"Timed out retrying after 30000ms: Expected to find element: `div.echarts-for-react`, but never found it.","time":{"start":1663839709936,"stop":1663839828498,"duration":118562}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests slice with netpol Should contain application namespaces":{"statistic":{"failed":0,"broken":0,"skipped":40,"passed":16,"unknown":0,"total":56},"items":[{"uid":"46ba7ec37bd1c23","status":"passed","time":{"start":1664094053000,"stop":1664094053135,"duration":135}},{"uid":"d38c0036fae2f9c6","status":"passed","time":{"start":1663843963000,"stop":1663843963194,"duration":194}},{"uid":"7294ca5130dfec5a","status":"passed","time":{"start":1663829327000,"stop":1663829327158,"duration":158}},{"uid":"84f081671bc7c25a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663343132000,"stop":1663343132000,"duration":0}},{"uid":"74e3eb9ec7f1f240","status":"passed","time":{"start":1663342947000,"stop":1663342947226,"duration":226}},{"uid":"d8941230a94489b9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"cc1c801e3fb48ae3","status":"passed","time":{"start":1663044069000,"stop":1663044069220,"duration":220}},{"uid":"b7df453b70aa26d0","status":"passed","time":{"start":1662348908000,"stop":1662348908141,"duration":141}},{"uid":"a6bed29858a6cb51","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"4489cef890de3b4e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"629eafd2753eaff2","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"b39c10703061eaa8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"5212c02f4d2a78e6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"25c8ad5c60537495","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"5cb30bf242c79f42","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"7a60a5763445536f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"cfbfc9d00f6391ef","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"6aa2e3bb35b78e32","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"f196b21f6a4e329a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"35d37fa1d1af956a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice Config Negative Scenario tests Slice Config Negative Scenario Update Should fail while updating sliceconfig with invalid values on slice subnet":{"statistic":{"failed":0,"broken":0,"skipped":14,"passed":42,"unknown":0,"total":56},"items":[{"uid":"51453f71214f821b","status":"passed","time":{"start":1664094053000,"stop":1664094053412,"duration":412}},{"uid":"793ff365a9593a3c","status":"passed","time":{"start":1663843963000,"stop":1663843963650,"duration":650}},{"uid":"a831c9ac6d4391b8","status":"passed","time":{"start":1663829327000,"stop":1663829327634,"duration":634}},{"uid":"7c64cd1c58d39172","status":"passed","time":{"start":1663343132000,"stop":1663343132676,"duration":676}},{"uid":"cd81713b4fbd33f","status":"passed","time":{"start":1663342947000,"stop":1663342947409,"duration":409}},{"uid":"1d592543be26787","status":"passed","time":{"start":1663044723000,"stop":1663044723499,"duration":499}},{"uid":"92370b2931d0cfab","status":"skipped","statusDetails":"skipped","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"e616e4871269f82a","status":"passed","time":{"start":1662348908000,"stop":1662348908577,"duration":577}},{"uid":"ef90a7afdcb180f8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"656699a1e1620303","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"deece24d36c06d5d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"328bd93562cd25d3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"b441e736a1f88468","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"9bf7322acb7d0e15","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"a8d90f527ae0fa1a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"e18b415d49c73d90","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"476e5d637b4b572a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"153893885e5371cd","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"e363da716206e0a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"59208380cb3fb644","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}}]},"39b1f45e73d5a9ed6d4e86546d789280":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"55815b9c9ad8cd2f","status":"passed","time":{"start":1663839612469,"stop":1663839623727,"duration":11258}}]},"975c96f820e9c01526c4eb3f5bb4b0aa":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"6871c2af76c738cf","status":"passed","time":{"start":1663839536076,"stop":1663839549874,"duration":13798}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests Iperf App connectivity Should install iperf-sleep on client cluster":{"statistic":{"failed":11,"broken":1,"skipped":36,"passed":8,"unknown":0,"total":56},"items":[{"uid":"511ddd5c9c0cf9a6","status":"failed","statusDetails":"Timed out after 120.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1664094053000,"stop":1664094176500,"duration":123500}},{"uid":"e869ac0b2f837e03","status":"failed","statusDetails":"Timed out after 120.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1663843963000,"stop":1663844088089,"duration":125089}},{"uid":"88b6bf6cc65bc1fb","status":"failed","statusDetails":"Timed out after 120.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1663829327000,"stop":1663829450487,"duration":123487}},{"uid":"ce4f4549a7e9a0b4","status":"passed","time":{"start":1663343132000,"stop":1663343143436,"duration":11436}},{"uid":"6edc27507e9053c5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663342947000,"stop":1663342947000,"duration":0}},{"uid":"99ffa04755aa24b9","status":"skipped","statusDetails":"skipped","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"dec3ce8d43219690","status":"broken","statusDetails":"interrupted","time":{"start":1663044069000,"stop":1663044094706,"duration":25706}},{"uid":"167b7af55a90ab12","status":"failed","statusDetails":"Timed out after 120.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1662348908000,"stop":1662349031619,"duration":123619}},{"uid":"3d81a38e3a5cfa76","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"f88578f67e630085","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"6d0932e2d42795ad","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"a41225ca20fbeb4d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"113db7e783a158bb","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"dad562b9b76a6df4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"3c5204700b0caf9a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"130587c55b300e70","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"9137f697eeb6eda9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"e3fa65532ee144ca","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"70d33360ebb66a96","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"fc7652fbc8491091","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}}]},"Empty Suite:Empty Suite#[It] Hub Creation tests Hub Installtion Test Senarios Should successfully pass if Cert is installed first and then Hub":{"statistic":{"failed":39,"broken":0,"skipped":1,"passed":208,"unknown":0,"total":248},"items":[{"uid":"21c82d2a83668c63","status":"passed","time":{"start":1664282572000,"stop":1664282662964,"duration":90964}},{"uid":"a3db3a79fb57c5ab","status":"passed","time":{"start":1664279225000,"stop":1664279336881,"duration":111881}},{"uid":"96d23dc0a173b539","status":"passed","time":{"start":1664277805000,"stop":1664277846622,"duration":41622}},{"uid":"36782768603a0d51","status":"passed","time":{"start":1664275170000,"stop":1664275257029,"duration":87029}},{"uid":"8d56f92858dd8ce1","status":"passed","time":{"start":1664275319000,"stop":1664275412167,"duration":93167}},{"uid":"5192f8da9f1f8c82","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004481b0>: {\n        Underlying: <*exec.ExitError | 0xc000460000>{\n            ProcessState: {\n                pid: 5951,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 3, Usec: 172457},\n                    Stime: {Sec: 0, Usec: 453208},\n                    Maxrss: 78772,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3821,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12848,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 41370,\n                    Nivcsw: 9121,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-0.3.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 27 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 27 resources with timeout of 5m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-0.3.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 27 resource(s)\n    wait.go:48: [debug] beginning wait for 27 resources with timeout of 5m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-controller\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-manager-metrics-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-controller-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-editor-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"workersliceconfigs.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"projects.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workerslicegateways.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"clusters.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"sliceconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"serviceexportconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"sliceqosconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workerserviceimports.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-serving-cert\" Certificate\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-selfsigned-issuer\" Issuer\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-validating-webhook-configuration\" ValidatingWebhookConfiguration\n    uninstall.go:144: [debug] purge requested for kubeslice-controller\n    Error: INSTALLATION FAILED: release kubeslice-controller failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-controller failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1664269661000,"stop":1664269985836,"duration":324836}},{"uid":"e4f2ba84d40dcba8","status":"passed","time":{"start":1664261300000,"stop":1664261380890,"duration":80890}},{"uid":"87efc79f1ffae520","status":"passed","time":{"start":1664245049000,"stop":1664245097995,"duration":48995}},{"uid":"6a66763bb33050c7","status":"passed","time":{"start":1664218010000,"stop":1664218047411,"duration":37411}},{"uid":"da02092dd7054c76","status":"passed","time":{"start":1664160583000,"stop":1664160623398,"duration":40398}},{"uid":"11b289109dfd01fc","status":"passed","time":{"start":1664105948000,"stop":1664105995372,"duration":47372}},{"uid":"45d9d24f44cfef14","status":"passed","time":{"start":1664103959000,"stop":1664103990201,"duration":31201}},{"uid":"15d6aea34af5ac03","status":"passed","time":{"start":1664091567000,"stop":1664091613763,"duration":46763}},{"uid":"6c0af679e08fecef","status":"passed","time":{"start":1663932902000,"stop":1663933001311,"duration":99311}},{"uid":"e2d99ffcc4a0fa62","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0002d0108>: {\n        Underlying: <*exec.ExitError | 0xc00031e020>{\n            ProcessState: {\n                pid: 6000,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 2, Usec: 446284},\n                    Stime: {Sec: 0, Usec: 326171},\n                    Maxrss: 81068,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3508,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12848,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 35306,\n                    Nivcsw: 7430,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-0.3.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 27 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 27 resources with timeout of 5m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-0.3.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 27 resource(s)\n    wait.go:48: [debug] beginning wait for 27 resources with timeout of 5m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-controller\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-manager-metrics-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-controller-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-editor-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"workersliceconfigs.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workerslicegateways.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"clusters.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"projects.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"serviceexportconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"sliceconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"sliceqosconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workerserviceimports.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-serving-cert\" Certificate\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-selfsigned-issuer\" Issuer\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-validating-webhook-configuration\" ValidatingWebhookConfiguration\n    uninstall.go:144: [debug] purge requested for kubeslice-controller\n    Error: INSTALLATION FAILED: release kubeslice-controller failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-controller failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1663917718000,"stop":1663918095925,"duration":377925}},{"uid":"a17281bb717ffe28","status":"passed","time":{"start":1663912384000,"stop":1663912413834,"duration":29834}},{"uid":"608ed199e1292d9f","status":"passed","time":{"start":1663841452000,"stop":1663841502274,"duration":50274}},{"uid":"dcb2618ff126e81c","status":"passed","time":{"start":1663838862000,"stop":1663838954196,"duration":92196}},{"uid":"800319c059bb7594","status":"passed","time":{"start":1663838333000,"stop":1663838431774,"duration":98774}},{"uid":"1dbedca75ebbba3e","status":"passed","time":{"start":1663836341000,"stop":1663836371182,"duration":30182}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests slice with netpol should contain the allowed namespaces":{"statistic":{"failed":0,"broken":0,"skipped":40,"passed":16,"unknown":0,"total":56},"items":[{"uid":"ea24593ccf941030","status":"passed","time":{"start":1664094053000,"stop":1664094053132,"duration":132}},{"uid":"7416bd30767efc2a","status":"passed","time":{"start":1663843963000,"stop":1663843963206,"duration":206}},{"uid":"c7271c6951623647","status":"passed","time":{"start":1663829327000,"stop":1663829327164,"duration":164}},{"uid":"d780fefdef54f77a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663343132000,"stop":1663343132000,"duration":0}},{"uid":"6d064fb23b0c1d8f","status":"passed","time":{"start":1663342947000,"stop":1663342947248,"duration":248}},{"uid":"4013226d88ce372e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"6e983497ff74d0d5","status":"passed","time":{"start":1663044069000,"stop":1663044069179,"duration":179}},{"uid":"ea6283f4634f9c55","status":"passed","time":{"start":1662348908000,"stop":1662348908145,"duration":145}},{"uid":"5e45c519c81dbbf9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"f84011b8bdca50d6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"b22d1b1b92508627","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"366c74132df5fa89","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"ce010680e723273f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"e555d589f63e76c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"e97b261df10aa18b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"63e519488d53d2","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"e500a602ac4a484a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"d7977b35335146da","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"fd77bbe4f1f4eb1b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"c92421f5075bac7c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}}]},"6e46b045cdb849d3cae0d272a1fa3a0b":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"81aee38fc52d4f08","status":"passed","time":{"start":1663839092611,"stop":1663839244947,"duration":152336}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Creation Should create Multiple Projects in controller using valid manifest":{"statistic":{"failed":24,"broken":0,"skipped":0,"passed":195,"unknown":0,"total":219},"items":[{"uid":"64162b1f7b2b8d72","status":"passed","time":{"start":1664275319000,"stop":1664275326817,"duration":7817}},{"uid":"5564dff407eacb01","status":"passed","time":{"start":1664245203000,"stop":1664245210926,"duration":7926}},{"uid":"5150a89ed4fd6ce8","status":"passed","time":{"start":1664160732000,"stop":1664160739731,"duration":7731}},{"uid":"e06be4454b28a7f2","status":"passed","time":{"start":1664091725000,"stop":1664091732767,"duration":7767}},{"uid":"263932ab7380ece1","status":"passed","time":{"start":1663841627000,"stop":1663841634916,"duration":7916}},{"uid":"c103849433178781","status":"passed","time":{"start":1663827045000,"stop":1663827052765,"duration":7765}},{"uid":"9a66b7bbe3c4c89b","status":"passed","time":{"start":1663767974000,"stop":1663767981764,"duration":7764}},{"uid":"a9c440d0ba413007","status":"passed","time":{"start":1663669677000,"stop":1663669684806,"duration":7806}},{"uid":"68251b8a57b42b6e","status":"passed","time":{"start":1663665323000,"stop":1663665330933,"duration":7933}},{"uid":"e38051eeb2ddd4f","status":"passed","time":{"start":1663665170000,"stop":1663665177867,"duration":7867}},{"uid":"1d58a636371af2e0","status":"passed","time":{"start":1663659067000,"stop":1663659074873,"duration":7873}},{"uid":"fd24a1f3769cce6e","status":"passed","time":{"start":1663656809000,"stop":1663656816843,"duration":7843}},{"uid":"1977cb6103d3e7ca","status":"passed","time":{"start":1663583877000,"stop":1663583885049,"duration":8049}},{"uid":"6c857fef32feb86d","status":"passed","time":{"start":1663340950000,"stop":1663340957730,"duration":7730}},{"uid":"6c543317e4cdfe1c","status":"passed","time":{"start":1663340650000,"stop":1663340657768,"duration":7768}},{"uid":"45e0c9c320bd7587","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <bool>: false\nto be true","time":{"start":1663308473000,"stop":1663308534396,"duration":61396}},{"uid":"fc7dd022ce0e83b2","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0006ef248>: {\n        Underlying: <*exec.ExitError | 0xc00064a740>{\n            ProcessState: {\n                pid: 6150,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 185512},\n                    Stime: {Sec: 0, Usec: 32979},\n                    Maxrss: 73600,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4073,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13344,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 426,\n                    Nivcsw: 359,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20create%20Multiple%20Projects%20in%20controller%20using%20valid%20manifest579231020\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.26.65:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20create%20Multiple%20Projects%20in%20controller%20using%20valid%20manifest579231020\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.26.65:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20create%20Multiple%20Projects%20in%20controller%20using%20valid%20manifest579231020\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.26.65:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20create%20Multiple%20Projects%20in%20controller%20using%20valid%20manifest579231020\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.26.65:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20create%20Multiple%20Projects%20in%20controller%20using%20valid%20manifest579231020\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.26.65:443: connect: connection refused\noccurred","time":{"start":1663304356000,"stop":1663304356288,"duration":288}},{"uid":"43e0bf59224ec428","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <bool>: false\nto be true","time":{"start":1663302519000,"stop":1663302586413,"duration":67413}},{"uid":"7b18916f4239f29c","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0006d1998>: {\n        Underlying: <*exec.ExitError | 0xc0004a7040>{\n            ProcessState: {\n                pid: 7120,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 173090},\n                    Stime: {Sec: 0, Usec: 50660},\n                    Maxrss: 88504,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4015,\n                    Majflt: 1,\n                    Nswap: 0,\n                    Inblock: 160,\n                    Oublock: 13344,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 359,\n                    Nivcsw: 214,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20create%20Multiple%20Projects%20in%20controller%20using%20valid%20manifest742559906\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.55.244:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20create%20Multiple%20Projects%20in%20controller%20using%20valid%20manifest742559906\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.55.244:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20create%20Multiple%20Projects%20in%20controller%20using%20valid%20manifest742559906\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.55.244:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20create%20Multiple%20Projects%20in%20controller%20using%20valid%20manifest742559906\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.55.244:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20create%20Multiple%20Projects%20in%20controller%20using%20valid%20manifest742559906\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.55.244:443: connect: connection refused\noccurred","time":{"start":1663264756000,"stop":1663264756293,"duration":293}},{"uid":"80f4c4758295f60e","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00084b9e0>: {\n        Underlying: <*exec.ExitError | 0xc0006649a0>{\n            ProcessState: {\n                pid: 7167,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 175414},\n                    Stime: {Sec: 0, Usec: 39866},\n                    Maxrss: 86112,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4420,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13344,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 317,\n                    Nivcsw: 274,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20create%20Multiple%20Projects%20in%20controller%20using%20valid%20manifest190927270\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.221.241:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20create%20Multiple%20Projects%20in%20controller%20using%20valid%20manifest190927270\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.221.241:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20create%20Multiple%20Projects%20in%20controller%20using%20valid%20manifest190927270\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.221.241:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20create%20Multiple%20Projects%20in%20controller%20using%20valid%20manifest190927270\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.221.241:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20create%20Multiple%20Projects%20in%20controller%20using%20valid%20manifest190927270\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.221.241:443: connect: connection refused\noccurred","time":{"start":1663228190000,"stop":1663228190285,"duration":285}}]},"Worker Suite:Worker Suite#[It] Slice Config Negative Scenario tests Slice Config Negative Scenario Creation Should fail while creating sliceconfig with invalid values on slice subnet":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":55,"unknown":0,"total":56},"items":[{"uid":"da2b9339e9baa310","status":"passed","time":{"start":1664094053000,"stop":1664094053329,"duration":329}},{"uid":"768944d45160e282","status":"passed","time":{"start":1663843963000,"stop":1663843963457,"duration":457}},{"uid":"e0cfeb7ac75c9f70","status":"passed","time":{"start":1663829327000,"stop":1663829327581,"duration":581}},{"uid":"5a48aa9abe1937a5","status":"passed","time":{"start":1663343132000,"stop":1663343132435,"duration":435}},{"uid":"957cf9ac67d25907","status":"passed","time":{"start":1663342947000,"stop":1663342947242,"duration":242}},{"uid":"5348512b5f543da5","status":"passed","time":{"start":1663044723000,"stop":1663044723368,"duration":368}},{"uid":"ad7a0432ba32cb10","status":"skipped","statusDetails":"skipped","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"4790c1e5f756fb76","status":"passed","time":{"start":1662348908000,"stop":1662348908246,"duration":246}},{"uid":"a8b5ce8ba7ccecf9","status":"passed","time":{"start":1661844205000,"stop":1661844205297,"duration":297}},{"uid":"54c70353f01fc3be","status":"passed","time":{"start":1661754870000,"stop":1661754870225,"duration":225}},{"uid":"b66ea16fc1017a14","status":"passed","time":{"start":1661611962000,"stop":1661611962274,"duration":274}},{"uid":"b098770a9900a552","status":"passed","time":{"start":1661585077000,"stop":1661585077230,"duration":230}},{"uid":"7c6b9313bdc0d52a","status":"passed","time":{"start":1661584447000,"stop":1661584447273,"duration":273}},{"uid":"382e9fe0fd744a70","status":"passed","time":{"start":1661580045000,"stop":1661580045236,"duration":236}},{"uid":"3d65d89c4db16d92","status":"passed","time":{"start":1661578505000,"stop":1661578505294,"duration":294}},{"uid":"92f8873c8a832dd9","status":"passed","time":{"start":1661572479000,"stop":1661572479338,"duration":338}},{"uid":"6fd7ca3f06a0a0f0","status":"passed","time":{"start":1660832281000,"stop":1660832281217,"duration":217}},{"uid":"4dc867951f6583de","status":"passed","time":{"start":1660808797000,"stop":1660808797206,"duration":206}},{"uid":"7911afcd026fa615","status":"passed","time":{"start":1660299182000,"stop":1660299182192,"duration":192}},{"uid":"f52baeb43e0e1058","status":"passed","time":{"start":1660295740000,"stop":1660295740187,"duration":187}}]},"8b9c335cfd130140e2521b162b19d12c":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"4f2547ec0e85d2e8","status":"passed","time":{"start":1663838976104,"stop":1663838987025,"duration":10921}}]},"5d86fe482dee8dca8366b92e0ebaefb4":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"926884eeeac11dac","status":"passed","time":{"start":1663840251755,"stop":1663840264887,"duration":13132}}]},"Worker Suite:Worker Suite#[It] Slice validation Slice with MEDIUM QOS should have gateway pod running":{"statistic":{"failed":1,"broken":0,"skipped":3,"passed":5,"unknown":0,"total":9},"items":[{"uid":"b3b6e4a8fa0c91c1","status":"passed","time":{"start":1659168739000,"stop":1659168739027,"duration":27}},{"uid":"8230e5aa7484ed6b","status":"passed","time":{"start":1659164084000,"stop":1659164084038,"duration":38}},{"uid":"a6c5b4296cbc546a","status":"passed","time":{"start":1659160188000,"stop":1659160188098,"duration":98}},{"uid":"b68db7ec5e46e791","status":"passed","time":{"start":1659119724000,"stop":1659119724043,"duration":43}},{"uid":"c82a0259af00b654","status":"passed","time":{"start":1659116511000,"stop":1659116511028,"duration":28}},{"uid":"a7cc472df3f4a042","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659109470000,"stop":1659109470000,"duration":0}},{"uid":"1b5e1219b3c47024","status":"failed","statusDetails":"Timed out after 180.004s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1659106836000,"stop":1659107016302,"duration":180302}},{"uid":"da28f429be91085e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659101522000,"stop":1659101522000,"duration":0}},{"uid":"5fb9844f703d68ba","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1659082816000,"stop":1659082816000,"duration":0}}]},"4c898315f99996565fd069d98e1c2af8":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"9a1d5f7e91fcfca8","status":"passed","time":{"start":1663838864340,"stop":1663838874748,"duration":10408}}]},"Worker Suite:Worker Suite#[It] Attach/Deattach Scenario Attach cluster from slice  when Deattach cluster from slice  should have vl3 router running on attached cluster":{"statistic":{"failed":1,"broken":0,"skipped":35,"passed":11,"unknown":0,"total":47},"items":[{"uid":"eae096742bde818a","status":"passed","time":{"start":1664094053000,"stop":1664094053005,"duration":5}},{"uid":"f5a1ff7a5a9a7e4b","status":"passed","time":{"start":1663843963000,"stop":1663843963011,"duration":11}},{"uid":"a267c2419ce43c1c","status":"passed","time":{"start":1663829327000,"stop":1663829327007,"duration":7}},{"uid":"abdaa755ed3d9547","status":"passed","time":{"start":1663343132000,"stop":1663343132012,"duration":12}},{"uid":"f4a971ec72b62ef","status":"passed","time":{"start":1663342947000,"stop":1663342947020,"duration":20}},{"uid":"94d207b6f487e105","status":"passed","time":{"start":1663044723000,"stop":1663044723009,"duration":9}},{"uid":"cbb8b6a95caaa145","status":"passed","time":{"start":1663044069000,"stop":1663044069009,"duration":9}},{"uid":"58ba3e9f0c7f348e","status":"passed","time":{"start":1662348908000,"stop":1662348908006,"duration":6}},{"uid":"3f5beb70fa50158d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"abbb52c640cd9fda","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"d1a2a2321264dcfb","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"e0e5dc964c87821b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"3344ea9a05620af8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"d530c90978bc3edb","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"2a3fd51afab1e235","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"cdae3da582e504d4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"9c595a5c928b322e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"2ec179450bf101f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"d15540ac1143177f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"5a0c683f56f7bd2b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}}]},"95a9a38ba2c86ee640ea084278b925c7":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"23d43b6af19cd6db","status":"passed","time":{"start":1663840092158,"stop":1663840101896,"duration":9738}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Deletion Should Delete an existing project successfully":{"statistic":{"failed":24,"broken":0,"skipped":0,"passed":192,"unknown":0,"total":216},"items":[{"uid":"78a8081048f6bb26","status":"passed","time":{"start":1664275319000,"stop":1664275323838,"duration":4838}},{"uid":"2eb30389887513c0","status":"passed","time":{"start":1664245203000,"stop":1664245207864,"duration":4864}},{"uid":"8a351906318ad650","status":"passed","time":{"start":1664160732000,"stop":1664160736797,"duration":4797}},{"uid":"2dac442dea5ff870","status":"passed","time":{"start":1664091725000,"stop":1664091729732,"duration":4732}},{"uid":"d5b55608d2d6169f","status":"passed","time":{"start":1663841627000,"stop":1663841632000,"duration":5000}},{"uid":"2cb08461fffaebb1","status":"passed","time":{"start":1663827045000,"stop":1663827049756,"duration":4756}},{"uid":"caf4dc7e899aa006","status":"passed","time":{"start":1663767974000,"stop":1663767978775,"duration":4775}},{"uid":"65b8172954f2bcc5","status":"passed","time":{"start":1663669677000,"stop":1663669681800,"duration":4800}},{"uid":"cc8f478607edb798","status":"passed","time":{"start":1663665323000,"stop":1663665327966,"duration":4966}},{"uid":"e26464b17660035","status":"passed","time":{"start":1663665170000,"stop":1663665174830,"duration":4830}},{"uid":"77a6f95010dd9b3","status":"passed","time":{"start":1663659067000,"stop":1663659071819,"duration":4819}},{"uid":"7e71ad3914e27d8c","status":"passed","time":{"start":1663656809000,"stop":1663656813968,"duration":4968}},{"uid":"1e370d8101bffd4b","status":"passed","time":{"start":1663583877000,"stop":1663583881965,"duration":4965}},{"uid":"321e3148f6f65dc2","status":"passed","time":{"start":1663340950000,"stop":1663340954747,"duration":4747}},{"uid":"72bef6bdff51f44b","status":"passed","time":{"start":1663340650000,"stop":1663340654738,"duration":4738}},{"uid":"a86cc67c31471c74","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <bool>: false\nto be true","time":{"start":1663308473000,"stop":1663308594383,"duration":121383}},{"uid":"9badc39afad9d6f1","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0006eed38>: {\n        Underlying: <*exec.ExitError | 0xc0003f7900>{\n            ProcessState: {\n                pid: 6274,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 196028},\n                    Stime: {Sec: 0, Usec: 50049},\n                    Maxrss: 83860,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2974,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13344,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 489,\n                    Nivcsw: 272,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Deletion%20Should%20Delete%20an%20existing%20project%20successfully4134615137\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.26.65:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Deletion%20Should%20Delete%20an%20existing%20project%20successfully4134615137\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.26.65:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Deletion%20Should%20Delete%20an%20existing%20project%20successfully4134615137\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.26.65:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Deletion%20Should%20Delete%20an%20existing%20project%20successfully4134615137\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.26.65:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Deletion%20Should%20Delete%20an%20existing%20project%20successfully4134615137\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.26.65:443: connect: connection refused\noccurred","time":{"start":1663304356000,"stop":1663304356307,"duration":307}},{"uid":"e871e217e95fa6bf","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00080a828>: {\n        Underlying: <*exec.ExitError | 0xc00079fa40>{\n            ProcessState: {\n                pid: 7606,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 169462},\n                    Stime: {Sec: 0, Usec: 38514},\n                    Maxrss: 84708,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 5541,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 361,\n                    Nivcsw: 303,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Deletion%20Should%20Delete%20an%20existing%20project%20successfully4189280629\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.179.221:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Deletion%20Should%20Delete%20an%20existing%20project%20successfully4189280629\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.179.221:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Deletion%20Should%20Delete%20an%20existing%20project%20successfully4189280629\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.179.221:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Deletion%20Should%20Delete%20an%20existing%20project%20successfully4189280629\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.179.221:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Deletion%20Should%20Delete%20an%20existing%20project%20successfully4189280629\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.179.221:443: connect: connection refused\noccurred","time":{"start":1663302519000,"stop":1663302519253,"duration":253}},{"uid":"fc1a959447f5b0c6","status":"failed","statusDetails":"Timed out after 60.105s.\nExpected\n    <bool>: false\nto be true","time":{"start":1663264756000,"stop":1663264877526,"duration":121526}},{"uid":"923c32e44b1c4975","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <bool>: false\nto be true","time":{"start":1663228190000,"stop":1663228256654,"duration":66654}}]},"53d9c5a3ca55152d311f194319b26e8c":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"c8deadbbfab7c06e","status":"passed","time":{"start":1663840516639,"stop":1663840533443,"duration":16804}}]},"Istio Suite:Istio Suite#[It] Bookinfo tests Bookinfo Scenario 4 with egress and ingress Should install Productpage on worker cluster-1":{"statistic":{"failed":60,"broken":0,"skipped":0,"passed":12,"unknown":0,"total":72},"items":[{"uid":"33ed41b7892f932c","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 3\nto equal\n    <int>: 1","time":{"start":1664246558000,"stop":1664246738447,"duration":180447}},{"uid":"4bb08f1dbe8eb76f","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1664162065000,"stop":1664162245325,"duration":180325}},{"uid":"15d15f93ad8d3c95","status":"passed","time":{"start":1664093039000,"stop":1664093063103,"duration":24103}},{"uid":"ef41aa15cf400f54","status":"passed","time":{"start":1663842833000,"stop":1663842866019,"duration":33019}},{"uid":"b8eec52f10a38bd5","status":"passed","time":{"start":1663828380000,"stop":1663828409812,"duration":29812}},{"uid":"390ba1a74f573104","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1663769301000,"stop":1663769513193,"duration":212193}},{"uid":"8d8f9773685130a6","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 4\nto equal\n    <int>: 1","time":{"start":1663671043000,"stop":1663671223400,"duration":180400}},{"uid":"65c16c3275158fe0","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 4\nto equal\n    <int>: 1","time":{"start":1663666667000,"stop":1663666847609,"duration":180609}},{"uid":"e040255a90a1817c","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 3\nto equal\n    <int>: 1","time":{"start":1663666522000,"stop":1663666702436,"duration":180436}},{"uid":"71fa3c08d4976730","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 3\nto equal\n    <int>: 1","time":{"start":1663585263000,"stop":1663585443589,"duration":180589}},{"uid":"1631b1c063533cb4","status":"passed","time":{"start":1663342266000,"stop":1663342393946,"duration":127946}},{"uid":"93a531ac729b5eab","status":"passed","time":{"start":1663341990000,"stop":1663342094960,"duration":104960}},{"uid":"fad5f07ad6bcf6cf","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1663315204000,"stop":1663315384311,"duration":180311}},{"uid":"2e5aeee84074c01a","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1663312558000,"stop":1663312738292,"duration":180292}},{"uid":"3e6fde33abb901b6","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1663055013000,"stop":1663055193282,"duration":180282}},{"uid":"5c39febf89ac64c","status":"passed","time":{"start":1663043831000,"stop":1663043957524,"duration":126524}},{"uid":"db8ecdb426be7f6d","status":"passed","time":{"start":1663043075000,"stop":1663043205323,"duration":130323}},{"uid":"f40e7c4a5b0f8658","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1663040338000,"stop":1663040518316,"duration":180316}},{"uid":"bcb98b340a742b00","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1662819574000,"stop":1662819754373,"duration":180373}},{"uid":"ed427ba7d35a197d","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1662795310000,"stop":1662795490282,"duration":180282}}]},"a54a6a78b97f016646cef9f097dfe413":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":0,"unknown":1,"total":1},"items":[{"uid":"747793b4b050203b","status":"unknown","time":{"start":1663841201751,"stop":0,"duration":-1663841201751}}]},"Worker Suite:Worker Suite#[AfterSuite]":{"statistic":{"failed":10,"broken":14,"skipped":0,"passed":308,"unknown":0,"total":332},"items":[{"uid":"826acd7a8ffe22f1","status":"passed","time":{"start":1664283136000,"stop":1664283439109,"duration":303109}},{"uid":"2e1b1d946ce78362","status":"passed","time":{"start":1664279812000,"stop":1664280115053,"duration":303053}},{"uid":"96c9718cd7b46406","status":"passed","time":{"start":1664278354000,"stop":1664278656838,"duration":302838}},{"uid":"99ac8f64643529dc","status":"passed","time":{"start":1664275871000,"stop":1664276173950,"duration":302950}},{"uid":"640417039a5007ed","status":"passed","time":{"start":1664271148000,"stop":1664271150399,"duration":2399}},{"uid":"7516a82ad1cfa580","status":"passed","time":{"start":1664261838000,"stop":1664262141290,"duration":303290}},{"uid":"2db461f137191f9d","status":"passed","time":{"start":1664247752000,"stop":1664248117149,"duration":365149}},{"uid":"4b03c7f550ed7917","status":"passed","time":{"start":1664218591000,"stop":1664218894138,"duration":303138}},{"uid":"1868b5d810f21dd8","status":"passed","time":{"start":1664196101000,"stop":1664196101026,"duration":26}},{"uid":"6aa0c009392e66aa","status":"passed","time":{"start":1664192951000,"stop":1664192951043,"duration":43}},{"uid":"38217e0e543b2b15","status":"passed","time":{"start":1664191201000,"stop":1664191201014,"duration":14}},{"uid":"499c8f362677b49a","status":"passed","time":{"start":1664163199000,"stop":1664163562018,"duration":363018}},{"uid":"2092f90677e1a600","status":"passed","time":{"start":1664106542000,"stop":1664106846731,"duration":304731}},{"uid":"1740365c4a746a7a","status":"passed","time":{"start":1664104522000,"stop":1664104825193,"duration":303193}},{"uid":"3e9b7512c9919b90","status":"passed","time":{"start":1664094053000,"stop":1664094083414,"duration":30414}},{"uid":"cd1d08f55ed2d38b","status":"passed","time":{"start":1663933497000,"stop":1663933801857,"duration":304857}},{"uid":"dbcd4ee138d6e61f","status":"passed","time":{"start":1663919193000,"stop":1663919194745,"duration":1745}},{"uid":"8fce37714a20ea52","status":"passed","time":{"start":1663912943000,"stop":1663913245985,"duration":302985}},{"uid":"d0af672885b43e64","status":"failed","statusDetails":"Timed out after 210.010s.\nExpected an error to have occurred.  Got:\n    <nil>: nil","time":{"start":1663843963000,"stop":1663844180330,"duration":217330}},{"uid":"c4ef310ac22e9376","status":"passed","time":{"start":1663839375000,"stop":1663839677627,"duration":302627}}]},"Worker Suite:Worker Suite#[It]  QosProfile tests QosProfile delete validation Delete qos profile - negative scenario":{"statistic":{"failed":1,"broken":0,"skipped":10,"passed":5,"unknown":0,"total":16},"items":[{"uid":"7e1b4542c7dc45cb","status":"passed","time":{"start":1664094053000,"stop":1664094055881,"duration":2881}},{"uid":"cc79d236a2f47d1e","status":"passed","time":{"start":1663843963000,"stop":1663843966315,"duration":3315}},{"uid":"899882f3fece0cfd","status":"passed","time":{"start":1663829327000,"stop":1663829330661,"duration":3661}},{"uid":"ee9b88e3426bc575","status":"passed","time":{"start":1663343132000,"stop":1663343139944,"duration":7944}},{"uid":"956e42254164bd4f","status":"passed","time":{"start":1663342947000,"stop":1663342949181,"duration":2181}},{"uid":"cb8632558c4c893d","status":"skipped","statusDetails":"skipped","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"d48befa97fbdca3a","status":"skipped","statusDetails":"skipped","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"48bf2bc96422128d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662348908000,"stop":1662348908000,"duration":0}},{"uid":"db97c82e8f95dfb","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00048d9c8>: {\n        Underlying: <*exec.ExitError | 0xc0004fcc20>{\n            ProcessState: {\n                pid: 7094,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 252308},\n                    Stime: {Sec: 0, Usec: 68811},\n                    Maxrss: 78900,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 9040,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 269,\n                    Nivcsw: 452,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"red1\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster2\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"red1\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster2\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"red1\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster2\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"red1\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster2\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"red1\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster2\noccurred","time":{"start":1661844205000,"stop":1661844205638,"duration":638}},{"uid":"df003cdeeb161066","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"cd6f8e26812d8bc6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"da70a7111c14b61e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"978e5c57c8787a60","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"3064f88deb557681","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"9c8518af53d899fc","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"b32a3fb99080c12b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Attach/Deattach Scenario Attach cluster from slice  when Deattach cluster from slice  should have gateway pod deleted from deattached cluster":{"statistic":{"failed":0,"broken":0,"skipped":36,"passed":11,"unknown":0,"total":47},"items":[{"uid":"2b80c5bdb263544c","status":"passed","time":{"start":1664094053000,"stop":1664094053006,"duration":6}},{"uid":"fe15e21b4bc46d66","status":"passed","time":{"start":1663843963000,"stop":1663843963016,"duration":16}},{"uid":"6e0025bf4dc66746","status":"passed","time":{"start":1663829327000,"stop":1663829327010,"duration":10}},{"uid":"68ced59a00a82f6b","status":"passed","time":{"start":1663343132000,"stop":1663343137025,"duration":5025}},{"uid":"ab3ced28dc78aa81","status":"passed","time":{"start":1663342947000,"stop":1663342947005,"duration":5}},{"uid":"e4e2e2f3794cd6a5","status":"passed","time":{"start":1663044723000,"stop":1663044723004,"duration":4}},{"uid":"21cd38de358a46f8","status":"passed","time":{"start":1663044069000,"stop":1663044069005,"duration":5}},{"uid":"25c858c204e2d474","status":"passed","time":{"start":1662348908000,"stop":1662348908009,"duration":9}},{"uid":"d749ff8fd2d1acf","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"740a7bcbc09d226c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"77a71bc114d36679","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"9165579ba6a09d3c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"4f93493dc6e4397d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"c077161d658296f6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"71b8cb9d3cb2c32e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"8380642870534124","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"a9753c985561045f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"542b877b460cdf3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"878085e3f2803815","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"2c6d1f100e96027a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}}]},"ec7cbae6cab4494b1c62900b376e8995":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"c714da1a07f70688","status":"passed","time":{"start":1663841822678,"stop":1663841854635,"duration":31957}}]},"Empty Suite:Empty Suite#[It] Hub Creation tests Hub Installtion Test Senarios Should fail if Hub is installed before installing Cert":{"statistic":{"failed":1,"broken":0,"skipped":0,"passed":247,"unknown":0,"total":248},"items":[{"uid":"5cd8eebccb515ee5","status":"passed","time":{"start":1664282572000,"stop":1664282573828,"duration":1828}},{"uid":"2c426850e5fe5839","status":"passed","time":{"start":1664279225000,"stop":1664279226475,"duration":1475}},{"uid":"b1e299296697fb81","status":"passed","time":{"start":1664277805000,"stop":1664277806994,"duration":1994}},{"uid":"cfdc0d3b0bab50ea","status":"passed","time":{"start":1664275170000,"stop":1664275171546,"duration":1546}},{"uid":"5efbc5a4b491b8b","status":"passed","time":{"start":1664275319000,"stop":1664275320521,"duration":1521}},{"uid":"d15578c9b162ac74","status":"passed","time":{"start":1664269661000,"stop":1664269662889,"duration":1889}},{"uid":"e3099e2bb2ded5ad","status":"passed","time":{"start":1664261300000,"stop":1664261301482,"duration":1482}},{"uid":"389b3937836891d6","status":"passed","time":{"start":1664245049000,"stop":1664245051737,"duration":2737}},{"uid":"75ab70ebd11fd71e","status":"passed","time":{"start":1664218010000,"stop":1664218011433,"duration":1433}},{"uid":"d1e1e7baba520d3","status":"passed","time":{"start":1664160583000,"stop":1664160584612,"duration":1612}},{"uid":"34849a8576278a35","status":"passed","time":{"start":1664105948000,"stop":1664105951616,"duration":3616}},{"uid":"1b203c103176bfe6","status":"passed","time":{"start":1664103959000,"stop":1664103960633,"duration":1633}},{"uid":"7e68ef78e77107b","status":"passed","time":{"start":1664091567000,"stop":1664091568230,"duration":1230}},{"uid":"7ee59eac4c7f9c20","status":"passed","time":{"start":1663932902000,"stop":1663932903987,"duration":1987}},{"uid":"7f903e74cd28baee","status":"passed","time":{"start":1663917718000,"stop":1663917719386,"duration":1386}},{"uid":"9d52584c3d9286a","status":"passed","time":{"start":1663912384000,"stop":1663912385650,"duration":1650}},{"uid":"5296ace84dd8d92","status":"passed","time":{"start":1663841452000,"stop":1663841455471,"duration":3471}},{"uid":"42cde22c39106455","status":"passed","time":{"start":1663838862000,"stop":1663838862418,"duration":418}},{"uid":"5c1d44aa64d398c9","status":"passed","time":{"start":1663838333000,"stop":1663838335257,"duration":2257}},{"uid":"7a97a8a697463605","status":"passed","time":{"start":1663836341000,"stop":1663836342600,"duration":1600}}]},"Worker Suite:Worker Suite#[It] Slice creation scenarios: when slice red is installed workers should get attached to slice":{"statistic":{"failed":13,"broken":0,"skipped":0,"passed":43,"unknown":0,"total":56},"items":[{"uid":"a714405ec37d018d","status":"passed","time":{"start":1664094053000,"stop":1664094055619,"duration":2619}},{"uid":"55324267f733d099","status":"passed","time":{"start":1663843963000,"stop":1663843963778,"duration":778}},{"uid":"636f14d371ae15f0","status":"passed","time":{"start":1663829327000,"stop":1663829327711,"duration":711}},{"uid":"db14df0ed1745806","status":"passed","time":{"start":1663343132000,"stop":1663343132534,"duration":534}},{"uid":"b8753d9284956e1a","status":"passed","time":{"start":1663342947000,"stop":1663342950272,"duration":3272}},{"uid":"6ab072db556fb822","status":"passed","time":{"start":1663044723000,"stop":1663044726422,"duration":3422}},{"uid":"b14b52ce72c28d98","status":"passed","time":{"start":1663044069000,"stop":1663044072293,"duration":3293}},{"uid":"8e10f5cea3bfd45e","status":"passed","time":{"start":1662348908000,"stop":1662348908486,"duration":486}},{"uid":"5494a2ac08496b79","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004688d0>: {\n        Underlying: <*exec.ExitError | 0xc000708d00>{\n            ProcessState: {\n                pid: 7158,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 256737},\n                    Stime: {Sec: 0, Usec: 69278},\n                    Maxrss: 86000,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 7572,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 430,\n                    Nivcsw: 521,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-red\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\noccurred","time":{"start":1661844205000,"stop":1661844205310,"duration":310}},{"uid":"4c6570c7b91e0cd1","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0007342a0>: {\n        Underlying: <*exec.ExitError | 0xc0007a5980>{\n            ProcessState: {\n                pid: 7112,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 162433},\n                    Stime: {Sec: 0, Usec: 59066},\n                    Maxrss: 76416,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 5207,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 265,\n                    Nivcsw: 518,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-red\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661754870000,"stop":1661754870225,"duration":225}},{"uid":"7b97d22a8f51cd74","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0006f1938>: {\n        Underlying: <*exec.ExitError | 0xc000861440>{\n            ProcessState: {\n                pid: 7101,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 194063},\n                    Stime: {Sec: 0, Usec: 41290},\n                    Maxrss: 87416,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3749,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 306,\n                    Nivcsw: 713,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-red\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661611962000,"stop":1661611962265,"duration":265}},{"uid":"6e452045e6877728","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00090c570>: {\n        Underlying: <*exec.ExitError | 0xc000a81200>{\n            ProcessState: {\n                pid: 7138,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 187266},\n                    Stime: {Sec: 0, Usec: 39013},\n                    Maxrss: 79272,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4614,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 356,\n                    Nivcsw: 618,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-red\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661585077000,"stop":1661585077242,"duration":242}},{"uid":"68dc6aba4cc1d9ac","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000f098>: {\n        Underlying: <*exec.ExitError | 0xc000499cc0>{\n            ProcessState: {\n                pid: 6007,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 199368},\n                    Stime: {Sec: 0, Usec: 60186},\n                    Maxrss: 79848,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3618,\n                    Majflt: 7,\n                    Nswap: 0,\n                    Inblock: 7440,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 484,\n                    Nivcsw: 668,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-red\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661584447000,"stop":1661584447339,"duration":339}},{"uid":"3ace927798f18ce6","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000810018>: {\n        Underlying: <*exec.ExitError | 0xc00013e040>{\n            ProcessState: {\n                pid: 7308,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 180411},\n                    Stime: {Sec: 0, Usec: 37585},\n                    Maxrss: 87560,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3704,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 392,\n                    Nivcsw: 311,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-red\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661580045000,"stop":1661580045202,"duration":202}},{"uid":"30a12c1d63228170","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000778bb8>: {\n        Underlying: <*exec.ExitError | 0xc0000740e0>{\n            ProcessState: {\n                pid: 7235,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 261763},\n                    Stime: {Sec: 0, Usec: 52352},\n                    Maxrss: 90508,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4766,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 362,\n                    Nivcsw: 415,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-red\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661578505000,"stop":1661578505293,"duration":293}},{"uid":"c44ef1cb1751245a","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004c9128>: {\n        Underlying: <*exec.ExitError | 0xc0008735e0>{\n            ProcessState: {\n                pid: 7243,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 221905},\n                    Stime: {Sec: 0, Usec: 60519},\n                    Maxrss: 83556,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 5479,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 271,\n                    Nivcsw: 365,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-red\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661572479000,"stop":1661572479271,"duration":271}},{"uid":"5d5acab2734fbe49","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0006ea018>: {\n        Underlying: <*exec.ExitError | 0xc0008c8040>{\n            ProcessState: {\n                pid: 7156,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 167218},\n                    Stime: {Sec: 0, Usec: 38004},\n                    Maxrss: 78816,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4139,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 362,\n                    Nivcsw: 299,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-red\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660832281000,"stop":1660832281219,"duration":219}},{"uid":"720c89f065c115e0","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0007f0fc0>: {\n        Underlying: <*exec.ExitError | 0xc00081faa0>{\n            ProcessState: {\n                pid: 7019,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 203396},\n                    Stime: {Sec: 0, Usec: 122805},\n                    Maxrss: 74888,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 7668,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 597,\n                    Nivcsw: 744,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-red\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660808797000,"stop":1660808798270,"duration":1270}},{"uid":"c6bd852ade0c5aac","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0005ccf18>: {\n        Underlying: <*exec.ExitError | 0xc000715fc0>{\n            ProcessState: {\n                pid: 7062,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 133368},\n                    Stime: {Sec: 0, Usec: 68705},\n                    Maxrss: 83528,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 10888,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 604,\n                    Nivcsw: 531,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-red\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660299182000,"stop":1660299183078,"duration":1078}},{"uid":"2a5c21ab42b102e","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004ca930>: {\n        Underlying: <*exec.ExitError | 0xc000158d40>{\n            ProcessState: {\n                pid: 7235,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 137512},\n                    Stime: {Sec: 0, Usec: 42017},\n                    Maxrss: 83500,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3187,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 342,\n                    Nivcsw: 329,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-red\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660295740000,"stop":1660295740165,"duration":165}}]},"73006f46b2606ecde6a7dcd996b7f117":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"fb9ce98e9848210","status":"passed","time":{"start":1663838454554,"stop":1663838486720,"duration":32166}}]},"Istio Suite:Istio Suite#[It] Bookinfo tests Bookinfo Scenario 2 with egress and without ingress Should install Productpage on worker cluster-1":{"statistic":{"failed":56,"broken":0,"skipped":0,"passed":16,"unknown":0,"total":72},"items":[{"uid":"b0e3b25d4c85f9a9","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1664246558000,"stop":1664246738350,"duration":180350}},{"uid":"260415b96c478c00","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1664162065000,"stop":1664162245462,"duration":180462}},{"uid":"8d8594df518fab83","status":"passed","time":{"start":1664093039000,"stop":1664093063858,"duration":24858}},{"uid":"bd7b9444ce5214c2","status":"passed","time":{"start":1663842833000,"stop":1663842863971,"duration":30971}},{"uid":"45b39c9c00f3b25","status":"passed","time":{"start":1663828380000,"stop":1663828409230,"duration":29230}},{"uid":"3167bbf6f7c42d03","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 4\nto equal\n    <int>: 1","time":{"start":1663769301000,"stop":1663769481352,"duration":180352}},{"uid":"3e53f02dee09f7d5","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1663671043000,"stop":1663671223337,"duration":180337}},{"uid":"84ec101d9d6509e","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1663666667000,"stop":1663666847435,"duration":180435}},{"uid":"26a7c72a17f0b31f","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1663666522000,"stop":1663666728984,"duration":206984}},{"uid":"e84a43abb729afdd","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1663585263000,"stop":1663585479286,"duration":216286}},{"uid":"fe5748da1986de86","status":"passed","time":{"start":1663342266000,"stop":1663342295110,"duration":29110}},{"uid":"e15769ff52486824","status":"passed","time":{"start":1663341990000,"stop":1663342008136,"duration":18136}},{"uid":"a8ecd24d7cf63ba5","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1663315204000,"stop":1663315384222,"duration":180222}},{"uid":"d31f6a270b5a9438","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1663312558000,"stop":1663312738236,"duration":180236}},{"uid":"f1b7ec4eb4ae61f","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1663055013000,"stop":1663055193361,"duration":180361}},{"uid":"c9732cd8d505af60","status":"passed","time":{"start":1663043831000,"stop":1663043849243,"duration":18243}},{"uid":"d202ed3280e81cfe","status":"passed","time":{"start":1663043075000,"stop":1663043096750,"duration":21750}},{"uid":"34a171fa49a4a49e","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1663040338000,"stop":1663040518275,"duration":180275}},{"uid":"c5b41e9ed2d31c0","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1662819574000,"stop":1662819754348,"duration":180348}},{"uid":"273a2c309c9f2a93","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1662795310000,"stop":1662795490290,"duration":180290}}]},"7006d1754916419dd0fd38545ee9a4ce":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"2d029f2d3186ebb0","status":"passed","time":{"start":1663840792447,"stop":1663840803231,"duration":10784}}]},"Worker Suite:Worker Suite#[It] Namespace reconcile test scenarios: Offboard slice get detached from app ns in cluster objects":{"statistic":{"failed":0,"broken":0,"skipped":10,"passed":46,"unknown":0,"total":56},"items":[{"uid":"5225a46e5564dfd","status":"passed","time":{"start":1664094053000,"stop":1664094053156,"duration":156}},{"uid":"7e17bca15984bc45","status":"passed","time":{"start":1663843963000,"stop":1663843963214,"duration":214}},{"uid":"12b846da85060979","status":"passed","time":{"start":1663829327000,"stop":1663829327197,"duration":197}},{"uid":"c47eb3520cf6226","status":"passed","time":{"start":1663343132000,"stop":1663343132398,"duration":398}},{"uid":"211dc267abe200db","status":"passed","time":{"start":1663342947000,"stop":1663342947359,"duration":359}},{"uid":"3fb910d03713b35b","status":"skipped","statusDetails":"skipped","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"c85c458b274e19dd","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"8b71575d223770ff","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662348908000,"stop":1662348908000,"duration":0}},{"uid":"5adeb5d66e2d2e28","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"6edcbe0ac7cfd62","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"54c9304b314d8c8d","status":"passed","time":{"start":1661611962000,"stop":1661611962209,"duration":209}},{"uid":"ad034bff7992a92","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"d5ca0bf123615d96","status":"passed","time":{"start":1661584447000,"stop":1661584447200,"duration":200}},{"uid":"6029bc50c9249a37","status":"passed","time":{"start":1661580045000,"stop":1661580045169,"duration":169}},{"uid":"177a28ef64ca43e0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"1a6dfe1162d59e78","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"498f92bdf86a924a","status":"passed","time":{"start":1660832281000,"stop":1660832281163,"duration":163}},{"uid":"45c329870f108767","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"71b954fdb737555e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"8df457d3ee0f61ca","status":"passed","time":{"start":1660295740000,"stop":1660295740138,"duration":138}}]},"b726b070d6a53c2529a6ca0351ac3b7f":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"fa019e9b7c9ac3b","status":"passed","time":{"start":1663843088092,"stop":1663843140726,"duration":52634}}]},"Worker Suite:Worker Suite#[It] Slice Creation scenarios Slice with LOW QOS should reconcile low qos slice":{"statistic":{"failed":0,"broken":0,"skipped":23,"passed":24,"unknown":0,"total":47},"items":[{"uid":"926822d2757a061b","status":"passed","time":{"start":1664094053000,"stop":1664094053614,"duration":614}},{"uid":"2fade9c5835bd410","status":"passed","time":{"start":1663843963000,"stop":1663843963554,"duration":554}},{"uid":"1ed2bad65d830899","status":"passed","time":{"start":1663829327000,"stop":1663829327523,"duration":523}},{"uid":"5917f8ccf50eb22f","status":"passed","time":{"start":1663343132000,"stop":1663343132351,"duration":351}},{"uid":"2d2ed4634fa3a041","status":"passed","time":{"start":1663342947000,"stop":1663342947442,"duration":442}},{"uid":"ead5e9e0bbc8b694","status":"skipped","statusDetails":"skipped","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"efabe741bf98e94c","status":"skipped","statusDetails":"skipped","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"e5efa1dbbd076cb4","status":"passed","time":{"start":1662348908000,"stop":1662348908522,"duration":522}},{"uid":"88be68a93f4d0843","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"d6f706b1db9b99ec","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"34c4708bfcb8fc6d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"9ac9b621fbf21537","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"f751f069e22dc8ab","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"47076450b378bc75","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"c31c19823058277b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"586bc686bf0e470e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"475c41c3d55257db","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"ec479d989181e3c9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"9b20e25c2a5d3171","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"82688c46bcfce6c4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}}]},"1a3893f1d3253d5136c00864585128a5":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"acd6333d10017ed","status":"passed","time":{"start":1663840162110,"stop":1663840174411,"duration":12301}}]},"1d6cbb90919ae0989303e9cc16625db3":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"879436e33e9ad824","status":"passed","time":{"start":1663839058008,"stop":1663839063299,"duration":5291}}]},"Worker Suite:Worker Suite#[It] NodeRestartSlice Testing node restart should re-establish connection on node restart":{"statistic":{"failed":0,"broken":0,"skipped":47,"passed":0,"unknown":0,"total":47},"items":[{"uid":"4401f68ad695232d","status":"skipped","statusDetails":"skipped - Spec skipped because Skip() was called in BeforeAll","time":{"start":1664094053000,"stop":1664094053000,"duration":0}},{"uid":"ef3787568dced24b","status":"skipped","statusDetails":"skipped - Spec skipped because Skip() was called in BeforeAll","time":{"start":1663843963000,"stop":1663843963000,"duration":0}},{"uid":"365f806241e2be76","status":"skipped","statusDetails":"skipped - Spec skipped because Skip() was called in BeforeAll","time":{"start":1663829327000,"stop":1663829327000,"duration":0}},{"uid":"92b6a623f8af6926","status":"skipped","statusDetails":"skipped - Spec skipped because Skip() was called in BeforeAll","time":{"start":1663343132000,"stop":1663343132000,"duration":0}},{"uid":"3b0b7ee6c541c37c","status":"skipped","statusDetails":"skipped - Spec skipped because Skip() was called in BeforeAll","time":{"start":1663342947000,"stop":1663342947000,"duration":0}},{"uid":"83f6ef6187461ffd","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"be78965b6330b509","status":"skipped","statusDetails":"skipped","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"36c68a5d6124db39","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662348908000,"stop":1662348908000,"duration":0}},{"uid":"c90116881625762b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"447c7dc578a0c7ef","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"d47099c1db7be19c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"570592b1b64ec0c7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"555643decb27059f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"32ed6beff6cd7928","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"81f6c3f9cb7d56ec","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"4c35e8e068f20ee3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"32f442b946f84f8e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"91e1ff4203d606b3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"d9a11fb6de248829","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"8bb7628b8ba05538","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}}]},"Worker Suite:Worker Suite#[It]  QosProfile tests QosProfile delete validation should remove gateway pod from spoke":{"statistic":{"failed":0,"broken":0,"skipped":11,"passed":5,"unknown":0,"total":16},"items":[{"uid":"88008cf683c8aabf","status":"passed","time":{"start":1664094053000,"stop":1664094053004,"duration":4}},{"uid":"82975affe84ded5","status":"passed","time":{"start":1663843963000,"stop":1663843963015,"duration":15}},{"uid":"d470ef75cb78a1a1","status":"passed","time":{"start":1663829327000,"stop":1663829327005,"duration":5}},{"uid":"7d9e4a65c911bfee","status":"passed","time":{"start":1663343132000,"stop":1663343132005,"duration":5}},{"uid":"f8036cd931ac1114","status":"passed","time":{"start":1663342947000,"stop":1663342947004,"duration":4}},{"uid":"af971280f72b67e6","status":"skipped","statusDetails":"skipped","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"7569e3ac3a6568aa","status":"skipped","statusDetails":"skipped","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"b0eaf9ecf121abca","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662348908000,"stop":1662348908000,"duration":0}},{"uid":"c831c5a47c4e3737","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"326758e93af1d623","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"93199e39cb8a6797","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"6400130a59c27ae2","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"7844fd06609df6ef","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"423ed20200b1b330","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"eda2449c3c0fb2cc","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"9bed7068ed65bd63","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}}]},"9916ea3a62faf4149b798d26a6d9cf73":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"f9f4b8333a050e1d","status":"passed","time":{"start":1663839014295,"stop":1663839021061,"duration":6766}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests slice with netpol should have gateway pod running":{"statistic":{"failed":2,"broken":0,"skipped":37,"passed":17,"unknown":0,"total":56},"items":[{"uid":"b42ac2701f59c611","status":"passed","time":{"start":1664094053000,"stop":1664094135290,"duration":82290}},{"uid":"f7d19bc28cfe5ae9","status":"passed","time":{"start":1663843963000,"stop":1663844127828,"duration":164828}},{"uid":"1c23321372860d48","status":"passed","time":{"start":1663829327000,"stop":1663829369156,"duration":42156}},{"uid":"b47c13b30bc1f35b","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1663343132000,"stop":1663343326799,"duration":194799}},{"uid":"463461611c3d8189","status":"passed","time":{"start":1663342947000,"stop":1663342993212,"duration":46212}},{"uid":"443d270eb229fc56","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"27c10b35c59a79a","status":"passed","time":{"start":1663044069000,"stop":1663044151397,"duration":82397}},{"uid":"95f59a7e8966d0c7","status":"passed","time":{"start":1662348908000,"stop":1662348948220,"duration":40220}},{"uid":"5dfe6beceb8c61ce","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"f9fa7a32cb7376af","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"52ec833057031e68","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"4e7c85dbbd80e4af","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"2b3e6ef120b11628","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"df08f591eca8e0c8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"2fad6ce21754ee9a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"fdcf15c9c974c6ef","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"bc70b806e651abf1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"9ccf4b19a13242d5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"7f25c723508252b3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"9214189e190f1652","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice Config Negative Scenario tests Slice Config Negative Scenario Update Should fail while updating sliceconfig with with invalid DSCP class in qosprofiledetails":{"statistic":{"failed":1,"broken":0,"skipped":14,"passed":41,"unknown":0,"total":56},"items":[{"uid":"b33716c414dd5d43","status":"passed","time":{"start":1664094053000,"stop":1664094053693,"duration":693}},{"uid":"f2784da302f7cfd2","status":"passed","time":{"start":1663843963000,"stop":1663843964074,"duration":1074}},{"uid":"c0ffd21c6731cf6a","status":"passed","time":{"start":1663829327000,"stop":1663829327749,"duration":749}},{"uid":"64f86878689e7a98","status":"passed","time":{"start":1663343132000,"stop":1663343132925,"duration":925}},{"uid":"3d87e7d2363de7fe","status":"passed","time":{"start":1663342947000,"stop":1663342947901,"duration":901}},{"uid":"81fd3250a3cb6c68","status":"passed","time":{"start":1663044723000,"stop":1663044723855,"duration":855}},{"uid":"c2682a632e16eee0","status":"skipped","statusDetails":"skipped","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"cea43f74a5cf26c3","status":"passed","time":{"start":1662348908000,"stop":1662348908952,"duration":952}},{"uid":"790535c55014fbc8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"5ade1f5dbbfacfe5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"79c08f371a4303e5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"f2fde110ccef0b56","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"4dac4cc997800235","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"4a42292c63cb111c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"681c0ae15cf19c83","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"ef797daaeaa2f543","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"5ca36efcf2220f98","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"1f05c560c987e04f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"5a83bd996154abc7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"b6cbd68a04bc9afc","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}}]},"deb9656fb63b131331cff53e3b09061d":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"4a44359271a45dd","status":"passed","time":{"start":1663840356888,"stop":1663840366786,"duration":9898}}]},"Empty Suite:Empty Suite#[It] Hub Deletion tests Hub Uninstalltion Test Scenarios Should pass if project is uninstalled first and then hub":{"statistic":{"failed":41,"broken":0,"skipped":0,"passed":207,"unknown":0,"total":248},"items":[{"uid":"b956fedcb75c500b","status":"passed","time":{"start":1664282572000,"stop":1664282634203,"duration":62203}},{"uid":"5c20335157bd13c7","status":"passed","time":{"start":1664279225000,"stop":1664279282224,"duration":57224}},{"uid":"16bd32634c83ba12","status":"passed","time":{"start":1664277805000,"stop":1664277909737,"duration":104737}},{"uid":"ce4766374f5cd18e","status":"passed","time":{"start":1664275170000,"stop":1664275226747,"duration":56747}},{"uid":"dc9313ca388a2a61","status":"passed","time":{"start":1664275319000,"stop":1664275362372,"duration":43372}},{"uid":"1cba91c6e808baab","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004481e0>: {\n        Underlying: <*exec.ExitError | 0xc000326020>{\n            ProcessState: {\n                pid: 5983,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 3, Usec: 104022},\n                    Stime: {Sec: 0, Usec: 548777},\n                    Maxrss: 79892,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 6104,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12848,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 41831,\n                    Nivcsw: 9438,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-0.3.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 27 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 27 resources with timeout of 5m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-0.3.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 27 resource(s)\n    wait.go:48: [debug] beginning wait for 27 resources with timeout of 5m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-controller\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-manager-metrics-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-controller-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-editor-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"workersliceconfigs.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workerslicegateways.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"clusters.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"projects.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"serviceexportconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"sliceconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"sliceqosconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workerserviceimports.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-serving-cert\" Certificate\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-selfsigned-issuer\" Issuer\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-validating-webhook-configuration\" ValidatingWebhookConfiguration\n    uninstall.go:144: [debug] purge requested for kubeslice-controller\n    Error: INSTALLATION FAILED: release kubeslice-controller failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-controller failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1664269661000,"stop":1664270042151,"duration":381151}},{"uid":"a75838ed0afbd0f4","status":"passed","time":{"start":1664261300000,"stop":1664261347858,"duration":47858}},{"uid":"94622eb0c372988c","status":"passed","time":{"start":1664245049000,"stop":1664245146614,"duration":97614}},{"uid":"cda269b4a7857fb9","status":"passed","time":{"start":1664218010000,"stop":1664218125760,"duration":115760}},{"uid":"72ca4ee34e4dca76","status":"passed","time":{"start":1664160583000,"stop":1664160686446,"duration":103446}},{"uid":"d9c87d242a1c5dc4","status":"passed","time":{"start":1664105948000,"stop":1664106058034,"duration":110034}},{"uid":"60d8f055f227036a","status":"passed","time":{"start":1664103959000,"stop":1664104066631,"duration":107631}},{"uid":"fa32a5c9c884e43b","status":"passed","time":{"start":1664091567000,"stop":1664091673976,"duration":106976}},{"uid":"4bce1a1c8d3503d8","status":"passed","time":{"start":1663932902000,"stop":1663932969479,"duration":67479}},{"uid":"7bc351ac352130ff","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0002d0108>: {\n        Underlying: <*exec.ExitError | 0xc00031e020>{\n            ProcessState: {\n                pid: 5964,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 2, Usec: 499662},\n                    Stime: {Sec: 0, Usec: 282423},\n                    Maxrss: 81068,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4304,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12848,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 34898,\n                    Nivcsw: 7604,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-0.3.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 27 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 27 resources with timeout of 5m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-0.3.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 27 resource(s)\n    wait.go:48: [debug] beginning wait for 27 resources with timeout of 5m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-controller\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-manager-metrics-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-controller-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-editor-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"workersliceconfigs.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"projects.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workerslicegateways.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"clusters.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"sliceconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"serviceexportconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"sliceqosconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workerserviceimports.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-serving-cert\" Certificate\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-selfsigned-issuer\" Issuer\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-validating-webhook-configuration\" ValidatingWebhookConfiguration\n    uninstall.go:144: [debug] purge requested for kubeslice-controller\n    Error: INSTALLATION FAILED: release kubeslice-controller failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-controller failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1663917718000,"stop":1663918046423,"duration":328423}},{"uid":"81e7f5500e60361c","status":"passed","time":{"start":1663912384000,"stop":1663912491550,"duration":107550}},{"uid":"d4a73f7a7f041743","status":"passed","time":{"start":1663841452000,"stop":1663841568908,"duration":116908}},{"uid":"2d7b7d7bca3a7156","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000e960>: {\n        Underlying: <*exec.ExitError | 0xc0004d6500>{\n            ProcessState: {\n                pid: 5963,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 42287},\n                    Stime: {Sec: 0, Usec: 38763},\n                    Maxrss: 43712,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2540,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 24,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 475,\n                    Nivcsw: 187,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"Error: INSTALLATION FAILED: failed to fetch https://github.com/kubeslice/kubeslice/releases/download/kubeslice-controller-0.3.0/kubeslice-controller-0.3.0.tgz : 503 Service Unavailable\",\n                        \"helm.go:84: [debug] failed to fetch https://github.com/kubeslice/kubeslice/releases/download/kubeslice-controller-0.3.0/kubeslice-controller-0.3.0.tgz : 503 Service Unavailable\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\",\n                        \"helm.sh/helm/v3/pkg/downloader.(*ChartDownloader).DownloadTo\",\n                        \"\\thelm.sh/helm/v3/pkg/downloader/chart_downloader.go:100\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:753\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:190\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:125\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:255\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1581\",\n                        \"INSTALLATION FAILED\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:127\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:856\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:974\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.4.0/command.go:902\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/hel...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    Error: INSTALLATION FAILED: failed to fetch https://github.com/kubeslice/kubeslice/releases/download/kubeslice-controller-0.3.0/kubeslice-controller-0.3.0.tgz : 503 Service Unavailable\n    helm.go:84: [debug] failed to fetch https://github.com/kubeslice/kubeslice/releases/download/kubeslice-controller-0.3.0/kubeslice-controller-0.3.0.tgz : 503 Service Unavailable\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\n    helm.sh/helm/v3/pkg/downloader.(*ChartDownloader).DownloadTo\n    \thelm.sh/helm/v3/pkg/downloader/chart_downloader.go:100\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:753\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:190\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:125\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1663838862000,"stop":1663838882713,"duration":20713}},{"uid":"ee6b38e223c81dd5","status":"passed","time":{"start":1663838333000,"stop":1663838399323,"duration":66323}},{"uid":"99341b3c803fca06","status":"passed","time":{"start":1663836341000,"stop":1663836458839,"duration":117839}}]},"650731062dba83e51a5de99f2389e5a8":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"2ce4b377689c20c7","status":"passed","time":{"start":1663841895488,"stop":1663841914224,"duration":18736}}]},"9ee400eb4983ac38fd5712d13aff46c3":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"2130339cd2787711","status":"passed","time":{"start":1663842083351,"stop":1663842104767,"duration":21416}}]},"625bec7f878cb6ad3debd0b68a6d211b":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"df10f1686aceaf67","status":"passed","time":{"start":1663843559153,"stop":1663843634613,"duration":75460}}]},"Worker Suite:Worker Suite#[It] Hub netpol scenarios: Slice Configs Netpol scenarios Should fail when deploying with invalid clusters in both applicationNamespace and allowedNamespace":{"statistic":{"failed":0,"broken":0,"skipped":23,"passed":33,"unknown":0,"total":56},"items":[{"uid":"5ad98f650ef17f13","status":"passed","time":{"start":1664094053000,"stop":1664094053909,"duration":909}},{"uid":"cc3557b76310a9ef","status":"passed","time":{"start":1663843963000,"stop":1663843964518,"duration":1518}},{"uid":"7750fc5edcf29c39","status":"passed","time":{"start":1663829327000,"stop":1663829328166,"duration":1166}},{"uid":"81593f3eda089f6d","status":"passed","time":{"start":1663343132000,"stop":1663343133257,"duration":1257}},{"uid":"de922587be02b606","status":"passed","time":{"start":1663342947000,"stop":1663342948305,"duration":1305}},{"uid":"a111cdc01a6ce7c4","status":"skipped","statusDetails":"skipped","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"42ae9634874ad18c","status":"passed","time":{"start":1663044069000,"stop":1663044070106,"duration":1106}},{"uid":"1864f49b9386a6b5","status":"passed","time":{"start":1662348908000,"stop":1662348909367,"duration":1367}},{"uid":"be5aa2fa212e2489","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"9fdd5da9a9c0ea1c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"234c6e0f704150c2","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"17efd4b0f80b2a0c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"d98cb2fe6bceb4f9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"79a7c7306f2bb01b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"87124cd76bd0cffd","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"2b23f86de93c6f1d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"22bfe8e7b9952476","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"24dfc86c653369c0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"1398910f30c53639","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"fe009663d3aa37aa","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}}]},"44d2b1c57b0afb2458291c326fe19948":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"53330a0e9bb0b101","status":"passed","time":{"start":1663838659558,"stop":1663838675831,"duration":16273}}]},"9e961de234f0ed66e7bca3b623c194c7":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"c3bc5d03a9228380","status":"passed","time":{"start":1663839850450,"stop":1663839864543,"duration":14093}}]},"4b9be6fbd9a339a2a8dc6aa68d164669":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"97aa8ca99ade71f1","status":"passed","time":{"start":1663841356871,"stop":1663841374167,"duration":17296}}]},"dd4c8a15295f8cfa320b3c3aea27a649":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"c217196591b0461d","status":"passed","time":{"start":1663839244948,"stop":1663839254875,"duration":9927}}]},"Istio Suite:Istio Suite#[It] Bookinfo tests Bookinfo Scenario 2 with egress and without ingress Should install Details, Reviews, Ratings and Serviceexport on worker cluster-2":{"statistic":{"failed":1,"broken":0,"skipped":56,"passed":15,"unknown":0,"total":72},"items":[{"uid":"376f12ed2ed7c2cf","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1664246558000,"stop":1664246558000,"duration":0}},{"uid":"eaed2a193a956fd7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1664162065000,"stop":1664162065000,"duration":0}},{"uid":"98b2a79a527f12ce","status":"passed","time":{"start":1664093039000,"stop":1664093071808,"duration":32808}},{"uid":"934f6352cf7b842b","status":"passed","time":{"start":1663842833000,"stop":1663842868638,"duration":35638}},{"uid":"31eaaea59b5a7e09","status":"passed","time":{"start":1663828380000,"stop":1663828412098,"duration":32098}},{"uid":"1547bf7da5a43409","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663769301000,"stop":1663769301000,"duration":0}},{"uid":"c4e485eae37acfdf","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663671043000,"stop":1663671043000,"duration":0}},{"uid":"861ea4cb54f71036","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663666667000,"stop":1663666667000,"duration":0}},{"uid":"aae963f2c8b3db69","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663666522000,"stop":1663666522000,"duration":0}},{"uid":"d28cc0db8ed29741","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663585263000,"stop":1663585263000,"duration":0}},{"uid":"e6aefc021873914c","status":"passed","time":{"start":1663342266000,"stop":1663342297126,"duration":31126}},{"uid":"90ebe6e47d44dfba","status":"passed","time":{"start":1663341990000,"stop":1663342020997,"duration":30997}},{"uid":"9ba4e7dd682a7b12","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663315204000,"stop":1663315204000,"duration":0}},{"uid":"bdd99dbe913afd2a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663312558000,"stop":1663312558000,"duration":0}},{"uid":"6a8c2d61f74b620d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663055013000,"stop":1663055013000,"duration":0}},{"uid":"660fdd582045707","status":"passed","time":{"start":1663043831000,"stop":1663043864232,"duration":33232}},{"uid":"ba8c0cd119a7b689","status":"passed","time":{"start":1663043075000,"stop":1663043112388,"duration":37388}},{"uid":"ddcf56711d66851","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663040338000,"stop":1663040338000,"duration":0}},{"uid":"9b25d032a03709f0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662819574000,"stop":1662819574000,"duration":0}},{"uid":"24fb560452b749c3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662795310000,"stop":1662795310000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Hub netpol scenarios: Slice Configs Netpol scenarios Should remove successfully while Deleting Slice after removing the applicationNamespace in namespaceIsolationProfile part in slice configs":{"statistic":{"failed":0,"broken":0,"skipped":23,"passed":33,"unknown":0,"total":56},"items":[{"uid":"5ace556535b3eb3e","status":"passed","time":{"start":1664094053000,"stop":1664094063806,"duration":10806}},{"uid":"cba3df1d210aebaa","status":"passed","time":{"start":1663843963000,"stop":1663843974242,"duration":11242}},{"uid":"e5e2c856be8f16fc","status":"passed","time":{"start":1663829327000,"stop":1663829337754,"duration":10754}},{"uid":"a53952040613c038","status":"passed","time":{"start":1663343132000,"stop":1663343142899,"duration":10899}},{"uid":"198bf763212c35d4","status":"passed","time":{"start":1663342947000,"stop":1663342957756,"duration":10756}},{"uid":"610e7f2adb2159c9","status":"skipped","statusDetails":"skipped","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"53daed9f505670bb","status":"passed","time":{"start":1663044069000,"stop":1663044079977,"duration":10977}},{"uid":"6598c174ab8b47a","status":"passed","time":{"start":1662348908000,"stop":1662348918996,"duration":10996}},{"uid":"eae7e4d94802ab20","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"f18f29d2858c0d41","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"36cd36ebe3eaeb80","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"22264f7672151cae","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"c43b6d5a7ef52bf9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"b403a03f520457f2","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"b5c9d21a15a38ae5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"a5b83e406fd93c25","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"d3f90d50467d513a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"53c3b1560abc31f5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"2ecb17c64020439a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"c52a50f99fbbb084","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting nsm-kernel-forwarder pod Check ping between iperf-server and iperf-client after nsm-kernel-forwarder pod restart":{"statistic":{"failed":0,"broken":0,"skipped":56,"passed":0,"unknown":0,"total":56},"items":[{"uid":"bac1ef6b432babea","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1664094053000,"stop":1664094053000,"duration":0}},{"uid":"f16d5916ccdb4c89","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663843963000,"stop":1663843963000,"duration":0}},{"uid":"61209c5422f39dc4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663829327000,"stop":1663829327000,"duration":0}},{"uid":"b2c7a798b32a4c01","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663343132000,"stop":1663343132000,"duration":0}},{"uid":"e74f8e6709a90372","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663342947000,"stop":1663342947000,"duration":0}},{"uid":"d530f0e879d2ba5b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"eb3b657637037cfc","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"98e76a7f4775e9da","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662348908000,"stop":1662348908000,"duration":0}},{"uid":"6b5feebc218feb53","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"2e36a1a5764448a1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"b40cf7f38609c545","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"c51f1c3761566a68","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"d28889b49e75aa8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"ba31d19c76a2ef14","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"d4b3f0603ab2bba","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"c76c597d4218a1b4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"ba5d82b19eddff91","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"aebc1bd80316ba7e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"85487c84e8ca86f8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"ad8edf6be7023e17","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}}]},"ad316331ce8e265b64950d3a87549922":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"9c9a9c0a90bd6cda","status":"passed","time":{"start":1663840736541,"stop":1663840752434,"duration":15893}}]},"8a767dbc7b00157a30b337d35eba05aa":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"613737a90df5e01","status":"passed","time":{"start":1663840126053,"stop":1663840140145,"duration":14092}}]},"544da4ec127f66bbb048d1759c30455b":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"aec22b27e796170e","status":"passed","time":{"start":1663839549876,"stop":1663839559304,"duration":9428}}]},"Worker Suite:Worker Suite#[It] Slice Creation scenarios Slice with MEDIUM QOS should have gateway pod running":{"statistic":{"failed":4,"broken":0,"skipped":19,"passed":24,"unknown":0,"total":47},"items":[{"uid":"cf0a9bf6822a4732","status":"passed","time":{"start":1664094053000,"stop":1664094078048,"duration":25048}},{"uid":"f0af5d7b672c6952","status":"passed","time":{"start":1663843963000,"stop":1663843988084,"duration":25084}},{"uid":"6e18be0207e95e19","status":"passed","time":{"start":1663829327000,"stop":1663829352060,"duration":25060}},{"uid":"106baff56c21e7fd","status":"passed","time":{"start":1663343132000,"stop":1663343157066,"duration":25066}},{"uid":"b605007bdd9cdb7d","status":"passed","time":{"start":1663342947000,"stop":1663342967043,"duration":20043}},{"uid":"f86d579c3225a54e","status":"skipped","statusDetails":"skipped","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"3191f4d19c55fca8","status":"skipped","statusDetails":"skipped","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"aca1eb167cb45338","status":"passed","time":{"start":1662348908000,"stop":1662348933088,"duration":25088}},{"uid":"fe8b84be6cb53771","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"9004f78de0591fe0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"a163181453316c56","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"1573d265a1f921eb","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"71ce76ef25872769","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"c8c2c1f3cbd28cca","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"a47f31af68b9fb57","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"703e488c44c4dbb8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"4b48970583d0baf9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"6b1267010d9a1c54","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"672d3c8786f95f79","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"be8ca8ee7c8b0fe7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}}]},"Worker Suite:Worker Suite#[It] NodeRestartSlice Testing node restart should have vl3 router running":{"statistic":{"failed":32,"broken":0,"skipped":6,"passed":9,"unknown":0,"total":47},"items":[{"uid":"413e1cb00c06faf","status":"skipped","statusDetails":"skipped - Scenario needs atleast 2 kubeslice gateway nodes in each worker cluster. Skipping!!","time":{"start":1664094053000,"stop":1664094053211,"duration":211}},{"uid":"6df67d5e6b1e3a52","status":"skipped","statusDetails":"skipped - Scenario needs atleast 2 kubeslice gateway nodes in each worker cluster. Skipping!!","time":{"start":1663843963000,"stop":1663843964569,"duration":1569}},{"uid":"29f035a092fc8796","status":"skipped","statusDetails":"skipped - Scenario needs atleast 2 kubeslice gateway nodes in each worker cluster. Skipping!!","time":{"start":1663829327000,"stop":1663829327264,"duration":264}},{"uid":"5c108641f23ba1d4","status":"skipped","statusDetails":"skipped - Scenario needs atleast 2 kubeslice gateway nodes in each worker cluster. Skipping!!","time":{"start":1663343132000,"stop":1663343134895,"duration":2895}},{"uid":"5a48867896378b50","status":"skipped","statusDetails":"skipped - Scenario needs atleast 2 kubeslice gateway nodes in each worker cluster. Skipping!!","time":{"start":1663342947000,"stop":1663342947808,"duration":808}},{"uid":"45c1adb0e5444df8","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1663044723000,"stop":1663044783699,"duration":60699}},{"uid":"4efb66f9ff203e8b","status":"skipped","statusDetails":"skipped","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"afff9409c86ce699","status":"passed","time":{"start":1662348908000,"stop":1662348911266,"duration":3266}},{"uid":"22d48265324d9cdb","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1661844205000,"stop":1661844265429,"duration":60429}},{"uid":"d938fcfc2fe7cb6a","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1661754870000,"stop":1661754930344,"duration":60344}},{"uid":"28602454d682aa45","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1661611962000,"stop":1661612022414,"duration":60414}},{"uid":"a78bd7494a2a7eb1","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1661585077000,"stop":1661585137346,"duration":60346}},{"uid":"b9e5cd9ce35047fa","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1661584447000,"stop":1661584507341,"duration":60341}},{"uid":"d1f0304a69722a7","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1661580045000,"stop":1661580105393,"duration":60393}},{"uid":"1d91c9a48cdd7bad","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1661578505000,"stop":1661578565417,"duration":60417}},{"uid":"2fda9ef594df6649","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1661572479000,"stop":1661572539412,"duration":60412}},{"uid":"74753db8d9b87460","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1660832281000,"stop":1660832341295,"duration":60295}},{"uid":"e6f7ca5e15c401fc","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1660808797000,"stop":1660808857312,"duration":60312}},{"uid":"9707232065015d73","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1660299182000,"stop":1660299242232,"duration":60232}},{"uid":"ef52de3f0fa8e531","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1660295740000,"stop":1660295800296,"duration":60296}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests slice with netpol Should label all application namespaces with kubeslice namespace":{"statistic":{"failed":0,"broken":0,"skipped":40,"passed":16,"unknown":0,"total":56},"items":[{"uid":"684d85d4a2921867","status":"passed","time":{"start":1664094053000,"stop":1664094053152,"duration":152}},{"uid":"24b41de4d2b5b2a1","status":"passed","time":{"start":1663843963000,"stop":1663843963201,"duration":201}},{"uid":"cd8537a1f22b16cb","status":"passed","time":{"start":1663829327000,"stop":1663829327201,"duration":201}},{"uid":"b534da9c2d0d0f76","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663343132000,"stop":1663343132000,"duration":0}},{"uid":"6dd10c6e08afa283","status":"passed","time":{"start":1663342947000,"stop":1663342947223,"duration":223}},{"uid":"b20f4da2175b45","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"67f04061dedad363","status":"passed","time":{"start":1663044069000,"stop":1663044069228,"duration":228}},{"uid":"80ead9258e0dde9c","status":"passed","time":{"start":1662348908000,"stop":1662348908239,"duration":239}},{"uid":"74170158a138b13","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"75692a6a333f26b8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"3f8af537cdf37827","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"75206208c57f5586","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"50588fadb9371bb5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"c1f3f80895673175","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"855229b51cf6146c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"6ead3fd861447c65","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"4b96b562f450a3e3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"c5a6f9b1e5589237","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"ea0f571e1703d74c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"6add127231c55114","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}}]},"e570111dbed8a2aabd8685bfc61934a":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"d11dda52d3308fde","status":"passed","time":{"start":1663842268586,"stop":1663842280494,"duration":11908}}]},"Worker Suite:Worker Suite#[It] Slice Creation scenarios Slice with MEDIUM QOS should reconcile medium qos slice":{"statistic":{"failed":0,"broken":0,"skipped":23,"passed":24,"unknown":0,"total":47},"items":[{"uid":"6d617fdb26c9b626","status":"passed","time":{"start":1664094053000,"stop":1664094053426,"duration":426}},{"uid":"1e47ca51da8ec211","status":"passed","time":{"start":1663843963000,"stop":1663843963582,"duration":582}},{"uid":"d05cd3ed0873c745","status":"passed","time":{"start":1663829327000,"stop":1663829327326,"duration":326}},{"uid":"7f2a03fb90f583d0","status":"passed","time":{"start":1663343132000,"stop":1663343132373,"duration":373}},{"uid":"89dac349e5b9eea","status":"passed","time":{"start":1663342947000,"stop":1663342947408,"duration":408}},{"uid":"5b77c38782bf4ef4","status":"skipped","statusDetails":"skipped","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"e10c1c309cfa7e97","status":"skipped","statusDetails":"skipped","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"b28471853df0626e","status":"passed","time":{"start":1662348908000,"stop":1662348908495,"duration":495}},{"uid":"a342fdeb47ae71f8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"af3ff75705cd6c29","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"21fe891e3bf3854d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"2710389000432fc3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"d710ebe527520350","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"61d574d7c95449aa","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"51b730c4de75916c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"3667e4005b74aa6a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"a88cd6ff713322df","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"2f1612e3bfd1d243","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"3746b2ba88c57bd5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"91900d866d8d3c2b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}}]},"ceed5ea4dd43ffed23096af9f3c16995":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"6ae5a6029a476b8","status":"passed","time":{"start":1663841879892,"stop":1663841895487,"duration":15595}}]},"f7c97bbecc955de3a533674b6a9404df":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"3983fa70f0f99af1","status":"passed","time":{"start":1663839678392,"stop":1663839695352,"duration":16960}}]},"Worker Suite:Worker Suite#[It] NodeRestartSlice Testing node restart iperf connectivity across multi cluster":{"statistic":{"failed":9,"broken":0,"skipped":38,"passed":0,"unknown":0,"total":47},"items":[{"uid":"cf90ffbd76ce2ecb","status":"skipped","statusDetails":"skipped - Spec skipped because Skip() was called in BeforeAll","time":{"start":1664094053000,"stop":1664094053000,"duration":0}},{"uid":"2ae7f70a24568e18","status":"skipped","statusDetails":"skipped - Spec skipped because Skip() was called in BeforeAll","time":{"start":1663843963000,"stop":1663843963000,"duration":0}},{"uid":"2159c968205e02c","status":"skipped","statusDetails":"skipped - Spec skipped because Skip() was called in BeforeAll","time":{"start":1663829327000,"stop":1663829327000,"duration":0}},{"uid":"5955b86438e46abf","status":"skipped","statusDetails":"skipped - Spec skipped because Skip() was called in BeforeAll","time":{"start":1663343132000,"stop":1663343132000,"duration":0}},{"uid":"f3c4aa8752f2ff2c","status":"skipped","statusDetails":"skipped - Spec skipped because Skip() was called in BeforeAll","time":{"start":1663342947000,"stop":1663342947000,"duration":0}},{"uid":"953bc8875b91d1f1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"11641fdd40682965","status":"skipped","statusDetails":"skipped","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"63136416bc75516e","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <bool>: false\nto be true","time":{"start":1662348908000,"stop":1662348970783,"duration":62783}},{"uid":"b252ab46f7824628","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"31e65391a34de01f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"bab1369012fd84d0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"a3fb2d1abdb0721","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"c25f04ef7ff023c7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"d81dfd73aeb8e4af","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"4ea9883bc65fc442","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"73e56cd0dddbd4fd","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"f4eea7f5e6a5acb9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"27799419333af5b1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"322ffeb50fc37847","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"c123415bfa37b331","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Iperf connectivity Verify Iperf traffic":{"statistic":{"failed":0,"broken":0,"skipped":55,"passed":1,"unknown":0,"total":56},"items":[{"uid":"1ce6588f743485fa","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1664094053000,"stop":1664094053000,"duration":0}},{"uid":"c6954e558fb518af","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663843963000,"stop":1663843963000,"duration":0}},{"uid":"2909b409b48ca5e3","status":"passed","time":{"start":1663829327000,"stop":1663829388385,"duration":61385}},{"uid":"d9dfc0ebfcb685aa","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663343132000,"stop":1663343132000,"duration":0}},{"uid":"b2878905094b979d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663342947000,"stop":1663342947000,"duration":0}},{"uid":"a3164befa5e7c6cb","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"855ee1ab5bba115","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"50e55987c01817b4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662348908000,"stop":1662348908000,"duration":0}},{"uid":"cba352a377063e9c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"272f925f49e31de9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"3f2a3245676109d8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"76437315e1a4425c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"8dc20bcb30943a1d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"53597780f33090d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"1a9eed50c2c2c191","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"b03292f05649a379","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"a97e4cd4fe3823e2","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"72e95de472e49e4e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"a1b002457298af83","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"bd9d6c5987d793f4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}}]},"4ed2cf64870968707ddff3cc9efdb82a":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"ca0dcc3ed8879da0","status":"passed","time":{"start":1663840303213,"stop":1663840304382,"duration":1169}}]},"Worker Suite:Worker Suite#[It] Slice validation Slice with LOW QOS should have vl3 router running":{"statistic":{"failed":2,"broken":0,"skipped":0,"passed":7,"unknown":0,"total":9},"items":[{"uid":"46aca11723b30ab4","status":"passed","time":{"start":1659168739000,"stop":1659168753412,"duration":14412}},{"uid":"23c755beac37ab66","status":"passed","time":{"start":1659164084000,"stop":1659164087272,"duration":3272}},{"uid":"88103c88c0b75f1c","status":"passed","time":{"start":1659160188000,"stop":1659160191197,"duration":3197}},{"uid":"3d2e40ff3ef5d0e5","status":"passed","time":{"start":1659119724000,"stop":1659119725567,"duration":1567}},{"uid":"502e7078fbd0aa84","status":"passed","time":{"start":1659116511000,"stop":1659116511230,"duration":230}},{"uid":"b2daf468d54fc42b","status":"passed","time":{"start":1659109470000,"stop":1659109470577,"duration":577}},{"uid":"e5cba288b1a3ee92","status":"passed","time":{"start":1659106836000,"stop":1659106864747,"duration":28747}},{"uid":"9676d1b60703e180","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1659101522000,"stop":1659101582708,"duration":60708}},{"uid":"3834bb8efced8f8","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 4\nto equal\n    <int>: 1","time":{"start":1659082816000,"stop":1659082876703,"duration":60703}}]},"Hub Suite:Hub Suite#[It]  QosProfile tests QosProfile Create validation Create qos profile- validation of BandwidthCeilingKbps > BandwidthGuaranteedKbps":{"statistic":{"failed":0,"broken":0,"skipped":128,"passed":21,"unknown":0,"total":149},"items":[{"uid":"d767fc83717b0b59","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661501311000,"stop":1661501311000,"duration":0}},{"uid":"77a9d1c91ffe9d4b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661434220000,"stop":1661434220000,"duration":0}},{"uid":"19faeb6c534a07c9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661356056000,"stop":1661356056000,"duration":0}},{"uid":"cf9313be5d59208c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661352461000,"stop":1661352461000,"duration":0}},{"uid":"e6d5a1f3f8c4e663","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661352207000,"stop":1661352207000,"duration":0}},{"uid":"93b327d8c9e08018","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661352028000,"stop":1661352028000,"duration":0}},{"uid":"b2dc580598fa6992","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661351263000,"stop":1661351263000,"duration":0}},{"uid":"ab3b35e695d44f82","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661188463000,"stop":1661188463000,"duration":0}},{"uid":"1de041c4d3f9b9d7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661183038000,"stop":1661183038000,"duration":0}},{"uid":"8f243e486b7e0b1f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661180055000,"stop":1661180055000,"duration":0}},{"uid":"a420b35e1405998f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661152474000,"stop":1661152474000,"duration":0}},{"uid":"6f987cd3f468c846","status":"passed","time":{"start":1661083598000,"stop":1661083604343,"duration":6343}},{"uid":"6c71ca8f0013171e","status":"passed","time":{"start":1661070332000,"stop":1661070338286,"duration":6286}},{"uid":"93a98cae41798b3e","status":"passed","time":{"start":1661066603000,"stop":1661066609334,"duration":6334}},{"uid":"b1d40cbaa6bf14b0","status":"passed","time":{"start":1661018182000,"stop":1661018188253,"duration":6253}},{"uid":"24c3e8b0f4b7b990","status":"passed","time":{"start":1661014620000,"stop":1661014626329,"duration":6329}},{"uid":"40b0168102d2ebfc","status":"passed","time":{"start":1661011831000,"stop":1661011837288,"duration":6288}},{"uid":"357086ff3676e0d3","status":"passed","time":{"start":1661011780000,"stop":1661011786332,"duration":6332}},{"uid":"f3093b0dbc1ea7a4","status":"passed","time":{"start":1661009684000,"stop":1661009690285,"duration":6285}},{"uid":"ab262e3a5aef9b65","status":"passed","time":{"start":1661007831000,"stop":1661007837266,"duration":6266}}]},"b4b0359689f67aaec9b575438d9f9e70":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"c4d7229929e5452a","status":"passed","time":{"start":1663840993533,"stop":1663841009358,"duration":15825}}]},"1e382c3391f2a26d9da25c8c6937ae5d":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"2cc4bea62652d5d9","status":"passed","time":{"start":1663838826725,"stop":1663838840894,"duration":14169}}]},"Worker Suite:Worker Suite#[It] Attach/Deattach Scenario Attach cluster from slice  when Deattach cluster from slice  should have vl3 router deleted from deattach cluster":{"statistic":{"failed":34,"broken":1,"skipped":0,"passed":12,"unknown":0,"total":47},"items":[{"uid":"80fa70c8f83b814e","status":"passed","time":{"start":1664094053000,"stop":1664094102104,"duration":49104}},{"uid":"e456ed6fea7aabb4","status":"passed","time":{"start":1663843963000,"stop":1663843998662,"duration":35662}},{"uid":"eacaddf606c8ec77","status":"passed","time":{"start":1663829327000,"stop":1663829378089,"duration":51089}},{"uid":"d96500a22cbe9ab3","status":"passed","time":{"start":1663343132000,"stop":1663343303649,"duration":171649}},{"uid":"d571319e023c946c","status":"passed","time":{"start":1663342947000,"stop":1663343040742,"duration":93742}},{"uid":"c3c130517f1c9959","status":"passed","time":{"start":1663044723000,"stop":1663044814435,"duration":91435}},{"uid":"502531b8d44b3640","status":"passed","time":{"start":1663044069000,"stop":1663044162747,"duration":93747}},{"uid":"b14da86b24ade3de","status":"passed","time":{"start":1662348908000,"stop":1662348940407,"duration":32407}},{"uid":"9748c32287d0c2c1","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000f4b8>: {\n        Underlying: <*exec.ExitError | 0xc00067f2e0>{\n            ProcessState: {\n                pid: 7143,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 250197},\n                    Stime: {Sec: 0, Usec: 81961},\n                    Maxrss: 88952,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 8108,\n                    Majflt: 2,\n                    Nswap: 0,\n                    Inblock: 264,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 575,\n                    Nivcsw: 730,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicetwocluster\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\noccurred","time":{"start":1661844205000,"stop":1661844205383,"duration":383}},{"uid":"f454cd4dc00f3085","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004be480>: {\n        Underlying: <*exec.ExitError | 0xc000658120>{\n            ProcessState: {\n                pid: 7280,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 163878},\n                    Stime: {Sec: 0, Usec: 51961},\n                    Maxrss: 74664,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4218,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 408,\n                    Nivcsw: 514,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicetwocluster\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661754870000,"stop":1661754870202,"duration":202}},{"uid":"164ecf7249dbd691","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0008cf7b8>: {\n        Underlying: <*exec.ExitError | 0xc0006d3360>{\n            ProcessState: {\n                pid: 7271,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 188173},\n                    Stime: {Sec: 0, Usec: 34562},\n                    Maxrss: 73440,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3764,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 331,\n                    Nivcsw: 517,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicetwocluster\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661611962000,"stop":1661611962228,"duration":228}},{"uid":"772d7e0693e2cb4","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00090c270>: {\n        Underlying: <*exec.ExitError | 0xc000a80ac0>{\n            ProcessState: {\n                pid: 7133,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 166559},\n                    Stime: {Sec: 0, Usec: 66623},\n                    Maxrss: 76488,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3259,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 376,\n                    Nivcsw: 507,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicetwocluster\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661585077000,"stop":1661585077214,"duration":214}},{"uid":"115dded2744bbe29","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00013a9f0>: {\n        Underlying: <*exec.ExitError | 0xc0006aa2a0>{\n            ProcessState: {\n                pid: 6193,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 221008},\n                    Stime: {Sec: 0, Usec: 29189},\n                    Maxrss: 69624,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3562,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 404,\n                    Nivcsw: 566,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicetwocluster\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661584447000,"stop":1661584447267,"duration":267}},{"uid":"d64bb4fb2f5188a6","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0006d2378>: {\n        Underlying: <*exec.ExitError | 0xc00013e620>{\n            ProcessState: {\n                pid: 7065,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 175824},\n                    Stime: {Sec: 0, Usec: 69511},\n                    Maxrss: 84816,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 10031,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 461,\n                    Nivcsw: 577,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicetwocluster\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661580045000,"stop":1661580045275,"duration":275}},{"uid":"13d09ae5114db91d","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00016e348>: {\n        Underlying: <*exec.ExitError | 0xc0000dae20>{\n            ProcessState: {\n                pid: 7068,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 296073},\n                    Stime: {Sec: 0, Usec: 143682},\n                    Maxrss: 79340,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 11281,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 644,\n                    Nivcsw: 643,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicetwocluster\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661578505000,"stop":1661578506579,"duration":1579}},{"uid":"6d94ff61d86e5d84","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00083aff0>: {\n        Underlying: <*exec.ExitError | 0xc000680040>{\n            ProcessState: {\n                pid: 7316,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 267549},\n                    Stime: {Sec: 0, Usec: 39932},\n                    Maxrss: 89432,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3507,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 317,\n                    Nivcsw: 479,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicetwocluster\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661572479000,"stop":1661572479317,"duration":317}},{"uid":"ce8bf07087d48db","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004c3668>: {\n        Underlying: <*exec.ExitError | 0xc00086e180>{\n            ProcessState: {\n                pid: 7106,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 150399},\n                    Stime: {Sec: 0, Usec: 57505},\n                    Maxrss: 83564,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 5957,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 366,\n                    Nivcsw: 376,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicetwocluster\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660832281000,"stop":1660832281200,"duration":200}},{"uid":"dd97ab75259c6dfc","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000f938>: {\n        Underlying: <*exec.ExitError | 0xc0006ff640>{\n            ProcessState: {\n                pid: 7024,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 191010},\n                    Stime: {Sec: 0, Usec: 62370},\n                    Maxrss: 83420,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 7217,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 485,\n                    Nivcsw: 457,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicetwocluster\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660808797000,"stop":1660808797245,"duration":245}},{"uid":"3b25b12501699b75","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000442ed0>: {\n        Underlying: <*exec.ExitError | 0xc0006ef680>{\n            ProcessState: {\n                pid: 7247,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 132812},\n                    Stime: {Sec: 0, Usec: 30989},\n                    Maxrss: 93756,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3766,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 372,\n                    Nivcsw: 261,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicetwocluster\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660299182000,"stop":1660299182166,"duration":166}},{"uid":"7b396665c2d82388","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004cadb0>: {\n        Underlying: <*exec.ExitError | 0xc0007937c0>{\n            ProcessState: {\n                pid: 7105,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 196941},\n                    Stime: {Sec: 0, Usec: 74317},\n                    Maxrss: 77184,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4905,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 421,\n                    Nivcsw: 556,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicetwocluster\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660295740000,"stop":1660295741128,"duration":1128}}]},"Hub Suite:Hub Suite#[BeforeSuite]":{"statistic":{"failed":68,"broken":0,"skipped":0,"passed":219,"unknown":0,"total":287},"items":[{"uid":"e614c4e9d82f87db","status":"passed","time":{"start":1664275319000,"stop":1664275417089,"duration":98089}},{"uid":"d03c21b9e5e413dc","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0003480d8>: {\n        Underlying: <*exec.ExitError | 0xc00041a000>{\n            ProcessState: {\n                pid: 6016,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 3, Usec: 146942},\n                    Stime: {Sec: 0, Usec: 544407},\n                    Maxrss: 79456,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 9165,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12848,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 41701,\n                    Nivcsw: 8960,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-0.3.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 27 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 27 resources with timeout of 5m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-0.3.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 27 resource(s)\n    wait.go:48: [debug] beginning wait for 27 resources with timeout of 5m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-controller\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-manager-metrics-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-controller-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-editor-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"workersliceconfigs.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workerslicegateways.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"clusters.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"projects.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"serviceexportconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"sliceconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"sliceqosconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workerserviceimports.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-serving-cert\" Certificate\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-selfsigned-issuer\" Issuer\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-validating-webhook-configuration\" ValidatingWebhookConfiguration\n    uninstall.go:144: [debug] purge requested for kubeslice-controller\n    Error: INSTALLATION FAILED: release kubeslice-controller failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-controller failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1664270369000,"stop":1664270757200,"duration":388200}},{"uid":"10653849a33bbbea","status":"passed","time":{"start":1664245203000,"stop":1664245285890,"duration":82890}},{"uid":"fa30306d88e602bf","status":"passed","time":{"start":1664160732000,"stop":1664160824918,"duration":92918}},{"uid":"10227c1416862830","status":"passed","time":{"start":1664091725000,"stop":1664091821728,"duration":96728}},{"uid":"f47fe1454417c2fa","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000e120>: {\n        Underlying: <*exec.ExitError | 0xc000496000>{\n            ProcessState: {\n                pid: 6039,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 2, Usec: 419110},\n                    Stime: {Sec: 0, Usec: 351494},\n                    Maxrss: 79396,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 8518,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12848,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 34504,\n                    Nivcsw: 6759,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-0.3.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 27 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 27 resources with timeout of 5m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-0.3.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 27 resource(s)\n    wait.go:48: [debug] beginning wait for 27 resources with timeout of 5m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-controller\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-manager-metrics-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-controller-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-editor-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"clusters.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workerslicegateways.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"serviceexportconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workersliceconfigs.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"sliceconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"sliceqosconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workerserviceimports.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"projects.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-serving-cert\" Certificate\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-selfsigned-issuer\" Issuer\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-validating-webhook-configuration\" ValidatingWebhookConfiguration\n    uninstall.go:144: [debug] purge requested for kubeslice-controller\n    Error: INSTALLATION FAILED: release kubeslice-controller failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-controller failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1663918426000,"stop":1663918809186,"duration":383186}},{"uid":"6dbd1b70619d7993","status":"passed","time":{"start":1663841627000,"stop":1663841710372,"duration":83372}},{"uid":"27327f72adf19f70","status":"passed","time":{"start":1663827045000,"stop":1663827141458,"duration":96458}},{"uid":"aad1bf75b1d97bba","status":"passed","time":{"start":1663767974000,"stop":1663768055032,"duration":81032}},{"uid":"d040f9f954d1631b","status":"passed","time":{"start":1663669677000,"stop":1663669780288,"duration":103288}},{"uid":"53cfb9bbd974627","status":"passed","time":{"start":1663665323000,"stop":1663665411221,"duration":88221}},{"uid":"56b114984a1122ab","status":"passed","time":{"start":1663665170000,"stop":1663665258215,"duration":88215}},{"uid":"4c1d04d8b4ff57ef","status":"passed","time":{"start":1663659067000,"stop":1663659148969,"duration":81969}},{"uid":"98ae5b0a36525265","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000e048>: {\n        Underlying: <*exec.ExitError | 0xc00048c000>{\n            ProcessState: {\n                pid: 6041,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 3, Usec: 482718},\n                    Stime: {Sec: 0, Usec: 632868},\n                    Maxrss: 77756,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 9851,\n                    Majflt: 1,\n                    Nswap: 0,\n                    Inblock: 256,\n                    Oublock: 12848,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 44798,\n                    Nivcsw: 10226,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-0.3.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 27 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 27 resources with timeout of 5m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                     ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-0.3.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 27 resource(s)\n    wait.go:48: [debug] beginning wait for 27 resources with timeout of 5m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-controller\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-manager-metrics-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-controller-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-editor-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"workersliceconfigs.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"projects.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workerslicegateways.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"serviceexportconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"sliceconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"sliceqosconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workerserviceimports.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"clusters.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-serving-cert\" Certificate\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-selfsigned-issuer\" Issuer\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-validating-webhook-configuration\" ValidatingWebhookConfiguration\n    uninstall.go:144: [debug] purge requested for kubeslice-controller\n    Error: INSTALLATION FAILED: release kubeslice-controller failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-controller failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1663658517000,"stop":1663658907079,"duration":390079}},{"uid":"508387ff3143e966","status":"passed","time":{"start":1663656809000,"stop":1663656907028,"duration":98028}},{"uid":"5fd09ed80afbff51","status":"passed","time":{"start":1663583877000,"stop":1663583975912,"duration":98912}},{"uid":"f6892663f82aaeb8","status":"passed","time":{"start":1663340950000,"stop":1663341047384,"duration":97384}},{"uid":"d8afdc8877bb40a9","status":"passed","time":{"start":1663340650000,"stop":1663340736796,"duration":86796}},{"uid":"c33d2470f5366ebb","status":"passed","time":{"start":1663308473000,"stop":1663308582394,"duration":109394}},{"uid":"5e2cdf29837fe54c","status":"passed","time":{"start":1663304356000,"stop":1663304463392,"duration":107392}}]},"64fce553c7fcb6a32c7279cf4df0a21d":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"471a1a02a41a6f05","status":"passed","time":{"start":1663839559306,"stop":1663839594282,"duration":34976}}]},"4b3bcd902e455fa9b10abf3602e3f18d":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"184c0b6d1af539c8","status":"passed","time":{"start":1663839342721,"stop":1663839353085,"duration":10364}}]},"d7acd27ba064605786c0a95771c05a2":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"6b1fcb9a3672bb18","status":"passed","time":{"start":1663841319647,"stop":1663841340322,"duration":20675}}]},"Worker Suite:Worker Suite#[It] Slice deletion tests: Slice is installed & validated Slice object is deleted from hub should remove gateway pod from spoke":{"statistic":{"failed":0,"broken":0,"skipped":42,"passed":14,"unknown":0,"total":56},"items":[{"uid":"f433f49b59269d00","status":"passed","time":{"start":1664094053000,"stop":1664094085110,"duration":32110}},{"uid":"ca1ac894542b8609","status":"passed","time":{"start":1663843963000,"stop":1663843995157,"duration":32157}},{"uid":"f34c0644506763","status":"passed","time":{"start":1663829327000,"stop":1663829359109,"duration":32109}},{"uid":"925fd7de0f781ac7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663343132000,"stop":1663343132000,"duration":0}},{"uid":"a8e9888ec35dfa5f","status":"passed","time":{"start":1663342947000,"stop":1663342979122,"duration":32122}},{"uid":"92dfa97c7c306f6c","status":"skipped","statusDetails":"skipped","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"ad6b4ecc8c4ee565","status":"passed","time":{"start":1663044069000,"stop":1663044069011,"duration":11}},{"uid":"78bbfe3551b2b3f0","status":"passed","time":{"start":1662348908000,"stop":1662348910037,"duration":2037}},{"uid":"bd061ee4c55ded7a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"ef2fd20991551d83","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"b31b5567fa765444","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"cc6c4907abc995f3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"62325b4cb464bbd7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"819e6bd0e15c744e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"e2607ff6685b72d2","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"b150432b4c14fba2","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"7c3e21a2d4fd850d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"5c3f709f3f5fe60e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"929fcfa78387f8af","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"2a9ef5d74ac33d05","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}}]},"7bf298af4e576962c2d7e95b17ea2ca9":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"e22a7ac11a577703","status":"passed","time":{"start":1663839966683,"stop":1663839985582,"duration":18899}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Creation Should fail while creating service accounts as combination of special characters":{"statistic":{"failed":6,"broken":0,"skipped":0,"passed":213,"unknown":0,"total":219},"items":[{"uid":"250ba658eff626dd","status":"passed","time":{"start":1664275319000,"stop":1664275319356,"duration":356}},{"uid":"93b93f5b0d634688","status":"passed","time":{"start":1664245203000,"stop":1664245203368,"duration":368}},{"uid":"4ba44ea2fc06cc49","status":"passed","time":{"start":1664160732000,"stop":1664160732283,"duration":283}},{"uid":"6e3d0aaadf1467b5","status":"passed","time":{"start":1664091725000,"stop":1664091725245,"duration":245}},{"uid":"4df14a0c581ce6bc","status":"passed","time":{"start":1663841627000,"stop":1663841627365,"duration":365}},{"uid":"9a57bcb455867d09","status":"passed","time":{"start":1663827045000,"stop":1663827045263,"duration":263}},{"uid":"90df1ff27960a83","status":"passed","time":{"start":1663767974000,"stop":1663767974277,"duration":277}},{"uid":"201e2f67f3ee0f63","status":"passed","time":{"start":1663669677000,"stop":1663669677290,"duration":290}},{"uid":"cc6ec119c38df154","status":"passed","time":{"start":1663665323000,"stop":1663665323332,"duration":332}},{"uid":"b778d2a37c0b9a1c","status":"passed","time":{"start":1663665170000,"stop":1663665170287,"duration":287}},{"uid":"c2fe3b9ebce2e78c","status":"passed","time":{"start":1663659067000,"stop":1663659067295,"duration":295}},{"uid":"21b9bb87114fa6ef","status":"passed","time":{"start":1663656809000,"stop":1663656809381,"duration":381}},{"uid":"ef2a6eaae38b1e9b","status":"passed","time":{"start":1663583877000,"stop":1663583877343,"duration":343}},{"uid":"ab3825628fef24b5","status":"passed","time":{"start":1663340950000,"stop":1663340950233,"duration":233}},{"uid":"7a662aff0a814c8a","status":"passed","time":{"start":1663340650000,"stop":1663340650249,"duration":249}},{"uid":"8565c07f893de2f3","status":"passed","time":{"start":1663308473000,"stop":1663308473346,"duration":346}},{"uid":"717538163931b322","status":"passed","time":{"start":1663304356000,"stop":1663304356295,"duration":295}},{"uid":"4b8d3eda63aaa60a","status":"passed","time":{"start":1663302519000,"stop":1663302519265,"duration":265}},{"uid":"91f236dfd1e27afc","status":"passed","time":{"start":1663264756000,"stop":1663264756299,"duration":299}},{"uid":"5edac07fe55817aa","status":"passed","time":{"start":1663228190000,"stop":1663228190278,"duration":278}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting nsm-kernel-forwarder pod Should restart nsm-kernel-forwarder pod":{"statistic":{"failed":0,"broken":0,"skipped":56,"passed":0,"unknown":0,"total":56},"items":[{"uid":"c222e58440744fdc","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1664094053000,"stop":1664094053000,"duration":0}},{"uid":"60d8592da591ddc7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663843963000,"stop":1663843963000,"duration":0}},{"uid":"76621040880b4c18","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663829327000,"stop":1663829327000,"duration":0}},{"uid":"57fd810f5df0529","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663343132000,"stop":1663343132000,"duration":0}},{"uid":"7ede4bc949d65284","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663342947000,"stop":1663342947000,"duration":0}},{"uid":"bab1604755dc4f8f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"f54c44ba325a682e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"c7df81c0d0a6faec","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662348908000,"stop":1662348908000,"duration":0}},{"uid":"3b26182dcdc41865","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"2f03587f0d886acb","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"140f31bc64e92cd9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"d3746a0a44be434","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"d73c6c777539b8c6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"bf81a7bb2f799acc","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"1e60db2b48bcf957","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"728bbf6484958452","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"e0e3cba859ce36b4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"84e51ba34eca9079","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"3fe899c8a904648b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"24c1d41b52cd0959","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}}]},"28b578ad440ac83f1ca56da8947fc1d9":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"ad0b926569403b6","status":"passed","time":{"start":1663842548658,"stop":1663842592159,"duration":43501}}]},"c3807cc2d91f8a834f6909bca653c80c":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"2b72809a24f4a2d9","status":"passed","time":{"start":1663841286426,"stop":1663841302991,"duration":16565}}]},"Worker Suite:Worker Suite#[It]  QosProfile tests QosProfile delete validation Delete qosProfile positive scenario":{"statistic":{"failed":0,"broken":0,"skipped":11,"passed":5,"unknown":0,"total":16},"items":[{"uid":"102f2d2d14538a","status":"passed","time":{"start":1664094053000,"stop":1664094053393,"duration":393}},{"uid":"44066a4187621ba6","status":"passed","time":{"start":1663843963000,"stop":1663843963648,"duration":648}},{"uid":"bba72d7419f5323d","status":"passed","time":{"start":1663829327000,"stop":1663829327537,"duration":537}},{"uid":"3bcbc088a4e9608","status":"passed","time":{"start":1663343132000,"stop":1663343132471,"duration":471}},{"uid":"cafa8a0c7b0c8664","status":"passed","time":{"start":1663342947000,"stop":1663342947495,"duration":495}},{"uid":"6371d1296d488ade","status":"skipped","statusDetails":"skipped","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"d9dbc3aba3f08d90","status":"skipped","statusDetails":"skipped","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"899da889ae2fa6e9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662348908000,"stop":1662348908000,"duration":0}},{"uid":"60cdbe906792bd5a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"2cf368eb6ed830ed","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"4f48b9a048a5c8bf","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"a29f4c5c7f29e8b0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"7894400e1f9e6da4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"4a93daf1db12328","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"3af3d49caf80d8c4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"1e1304e464873599","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}}]},"cf966ae587c4908f38f17a0fb4989601":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"420525f884d11e68","status":"passed","time":{"start":1663840775924,"stop":1663840792446,"duration":16522}}]},"20f7ea4d83b24bfda5098880c7258792":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"f07677746068193b","status":"passed","time":{"start":1663841275479,"stop":1663841286426,"duration":10947}}]},"5c39d3b34500e1d01faa2c1fb72b7151":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"a1df983ab1b4345b","status":"passed","time":{"start":1663843690720,"stop":1663843711228,"duration":20508}}]},"e23b1899ec0b47255cb92cf1675934ec":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"67310dffde64e113","status":"passed","time":{"start":1663842927040,"stop":1663842979448,"duration":52408}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting mesh-dns pod Check ping between iperf-server and iperf-client after mesh-dns pod restart":{"statistic":{"failed":0,"broken":0,"skipped":56,"passed":0,"unknown":0,"total":56},"items":[{"uid":"2f9ff55f939ac168","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1664094053000,"stop":1664094053000,"duration":0}},{"uid":"a2334248df7315d1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663843963000,"stop":1663843963000,"duration":0}},{"uid":"1483c13298348b37","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663829327000,"stop":1663829327000,"duration":0}},{"uid":"9f0b11ca95bad6aa","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663343132000,"stop":1663343132000,"duration":0}},{"uid":"6ae15c6fb6a41762","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663342947000,"stop":1663342947000,"duration":0}},{"uid":"3446ea721c8b7d89","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"538a11e006559e1e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"9c515adb02e4bf3c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662348908000,"stop":1662348908000,"duration":0}},{"uid":"ed886fe832ae3bbf","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"b48ed8135872c5e3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"2afcbab9cd5408ef","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"2086ea31fb96902c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"c52ca08667842ed","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"53f44c4fe4695f56","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"f73a0bbff4f59a76","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"909f72f81f0800a4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"4a0805df51341ecb","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"4d75542c9f4f09a1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"4662c5300922074d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"5a44f16fcfd50051","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}}]},"47a1493869d67f473c0f847097bcea84":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"236d5b9e6d2c1063","status":"passed","time":{"start":1663839864543,"stop":1663839875779,"duration":11236}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests slice with netpol should have vl3 router running":{"statistic":{"failed":37,"broken":0,"skipped":0,"passed":19,"unknown":0,"total":56},"items":[{"uid":"e72b4a1cdd28941","status":"passed","time":{"start":1664094053000,"stop":1664094057503,"duration":4503}},{"uid":"b931f46844c12b2c","status":"passed","time":{"start":1663843963000,"stop":1663843970805,"duration":7805}},{"uid":"5fda19c73c68ead2","status":"passed","time":{"start":1663829327000,"stop":1663829331573,"duration":4573}},{"uid":"32b10c516f937e15","status":"passed","time":{"start":1663343132000,"stop":1663343136326,"duration":4326}},{"uid":"6b4d055c54f1b025","status":"passed","time":{"start":1663342947000,"stop":1663342947449,"duration":449}},{"uid":"b1f2f4209b765506","status":"failed","statusDetails":"Timed out after 280.000s.\nExpected\n    <int>: 2\nto equal\n    <int>: 1","time":{"start":1663044723000,"stop":1663045018428,"duration":295428}},{"uid":"9732ee24e4b5b9ea","status":"passed","time":{"start":1663044069000,"stop":1663044075870,"duration":6870}},{"uid":"f6e3fb8cff53a060","status":"passed","time":{"start":1662348908000,"stop":1662348914888,"duration":6888}},{"uid":"3690bf568643aa65","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0003f9470>: {\n        Underlying: <*exec.ExitError | 0xc000403f40>{\n            ProcessState: {\n                pid: 7297,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 232557},\n                    Stime: {Sec: 0, Usec: 70597},\n                    Maxrss: 92404,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4094,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 420,\n                    Nivcsw: 629,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\noccurred","time":{"start":1661844205000,"stop":1661844205633,"duration":633}},{"uid":"b4acc7dde5577690","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000734060>: {\n        Underlying: <*exec.ExitError | 0xc0007a4f40>{\n            ProcessState: {\n                pid: 7092,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 176987},\n                    Stime: {Sec: 0, Usec: 58995},\n                    Maxrss: 74624,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3261,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 381,\n                    Nivcsw: 568,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661754870000,"stop":1661754870513,"duration":513}},{"uid":"a67c86b4f48e988b","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0008ce6c0>: {\n        Underlying: <*exec.ExitError | 0xc000074440>{\n            ProcessState: {\n                pid: 7106,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 190827},\n                    Stime: {Sec: 0, Usec: 40601},\n                    Maxrss: 86464,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4559,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 450,\n                    Nivcsw: 915,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661611962000,"stop":1661611962704,"duration":704}},{"uid":"b782f4bad8d76d6","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00090c030>: {\n        Underlying: <*exec.ExitError | 0xc000a800a0>{\n            ProcessState: {\n                pid: 7112,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 197039},\n                    Stime: {Sec: 0, Usec: 49259},\n                    Maxrss: 85024,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3700,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 417,\n                    Nivcsw: 715,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661585077000,"stop":1661585077590,"duration":590}},{"uid":"4ab91ddcf5e1d3eb","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000363470>: {\n        Underlying: <*exec.ExitError | 0xc000825200>{\n            ProcessState: {\n                pid: 6172,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 209965},\n                    Stime: {Sec: 0, Usec: 44415},\n                    Maxrss: 75824,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3204,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 397,\n                    Nivcsw: 688,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661584447000,"stop":1661584447586,"duration":586}},{"uid":"954f5abfeae757ce","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000810570>: {\n        Underlying: <*exec.ExitError | 0xc000626ae0>{\n            ProcessState: {\n                pid: 7228,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 199943},\n                    Stime: {Sec: 0, Usec: 39988},\n                    Maxrss: 82476,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4558,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 320,\n                    Nivcsw: 809,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661580045000,"stop":1661580045511,"duration":511}},{"uid":"6c551f040bddfcdc","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0007782a0>: {\n        Underlying: <*exec.ExitError | 0xc000774420>{\n            ProcessState: {\n                pid: 7074,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 239443},\n                    Stime: {Sec: 0, Usec: 97400},\n                    Maxrss: 86832,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 11012,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 512,\n                    Nivcsw: 638,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661578505000,"stop":1661578505714,"duration":714}},{"uid":"8e71fa55c6b99f29","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004c9fb0>: {\n        Underlying: <*exec.ExitError | 0xc000886880>{\n            ProcessState: {\n                pid: 7248,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 267937},\n                    Stime: {Sec: 0, Usec: 46597},\n                    Maxrss: 90744,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 6607,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 563,\n                    Nivcsw: 469,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661572479000,"stop":1661572479726,"duration":726}},{"uid":"16ab229423139acd","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0006ead20>: {\n        Underlying: <*exec.ExitError | 0xc0008cb180>{\n            ProcessState: {\n                pid: 7085,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 181211},\n                    Stime: {Sec: 0, Usec: 53539},\n                    Maxrss: 72000,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 6414,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 444,\n                    Nivcsw: 515,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660832281000,"stop":1660832281553,"duration":553}},{"uid":"3271f69ef5bca0e5","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0001266f0>: {\n        Underlying: <*exec.ExitError | 0xc000663480>{\n            ProcessState: {\n                pid: 7142,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 166188},\n                    Stime: {Sec: 0, Usec: 47482},\n                    Maxrss: 86340,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4052,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 391,\n                    Nivcsw: 257,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660808797000,"stop":1660808797552,"duration":552}},{"uid":"974d621c049fb879","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000350948>: {\n        Underlying: <*exec.ExitError | 0xc000828e60>{\n            ProcessState: {\n                pid: 7252,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 133880},\n                    Stime: {Sec: 0, Usec: 44626},\n                    Maxrss: 82964,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2723,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 590,\n                    Nivcsw: 512,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660299182000,"stop":1660299182459,"duration":459}},{"uid":"c662752d8114479a","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004ca618>: {\n        Underlying: <*exec.ExitError | 0xc00065fc60>{\n            ProcessState: {\n                pid: 7215,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 141036},\n                    Stime: {Sec: 0, Usec: 43094},\n                    Maxrss: 74312,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3629,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 312,\n                    Nivcsw: 567,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660295740000,"stop":1660295740433,"duration":433}}]},"e4ba3657d630071023422788f207ca51":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"f608c0ba3fb540ee","status":"passed","time":{"start":1663841686019,"stop":1663841708162,"duration":22143}}]},"a26e83fe08a107d8c1e6530af2dfb10e":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"75f690a07c944395","status":"passed","time":{"start":1663840702898,"stop":1663840721472,"duration":18574}}]},"18eae222bc4828fc4db05ba70db94ba0":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"c92b5391916755a6","status":"passed","time":{"start":1663840941160,"stop":1663840956535,"duration":15375}}]},"Worker Suite:Worker Suite#[It] Slice validation Slice with MEDIUM QOS should have vl3 router running":{"statistic":{"failed":3,"broken":0,"skipped":0,"passed":6,"unknown":0,"total":9},"items":[{"uid":"24dbffb43308dcfe","status":"passed","time":{"start":1659168739000,"stop":1659168739689,"duration":689}},{"uid":"32d8b5c697e6c847","status":"passed","time":{"start":1659164084000,"stop":1659164084631,"duration":631}},{"uid":"49c7996737d1d261","status":"passed","time":{"start":1659160188000,"stop":1659160188690,"duration":690}},{"uid":"dad5cb3f70e541bb","status":"passed","time":{"start":1659119724000,"stop":1659119724538,"duration":538}},{"uid":"3696a0b392e3f2eb","status":"passed","time":{"start":1659116511000,"stop":1659116511419,"duration":419}},{"uid":"df9e0e068ede27d4","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <int>: 3\nto equal\n    <int>: 1","time":{"start":1659109470000,"stop":1659109530750,"duration":60750}},{"uid":"b166459c6ae18a80","status":"passed","time":{"start":1659106836000,"stop":1659106836774,"duration":774}},{"uid":"55bf1b7112ff65f6","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1659101522000,"stop":1659101582765,"duration":60765}},{"uid":"6b9bc19dfb598b06","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <int>: 4\nto equal\n    <int>: 1","time":{"start":1659082816000,"stop":1659082876914,"duration":60914}}]},"Hub Suite:Hub Suite#[It] Cluster negative tests Worker Cluster Registration with Wrong clustername":{"statistic":{"failed":5,"broken":0,"skipped":37,"passed":177,"unknown":0,"total":219},"items":[{"uid":"7124dc2f79d3d6c0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1664275319000,"stop":1664275319000,"duration":0}},{"uid":"c1843a8e50225326","status":"passed","time":{"start":1664245203000,"stop":1664245391982,"duration":188982}},{"uid":"dade5f68b6720e3","status":"passed","time":{"start":1664160732000,"stop":1664160902057,"duration":170057}},{"uid":"d20ae2de79fc6059","status":"passed","time":{"start":1664091725000,"stop":1664091863968,"duration":138968}},{"uid":"a96a5869ba4d7cce","status":"passed","time":{"start":1663841627000,"stop":1663841672658,"duration":45658}},{"uid":"e59bec7eef4b1a35","status":"passed","time":{"start":1663827045000,"stop":1663827229262,"duration":184262}},{"uid":"cfe38fd31d4d6ca6","status":"passed","time":{"start":1663767974000,"stop":1663768143874,"duration":169874}},{"uid":"26cd443e03a60e1c","status":"passed","time":{"start":1663669677000,"stop":1663669870727,"duration":193727}},{"uid":"1d9cc1f8ca7111d7","status":"passed","time":{"start":1663665323000,"stop":1663665506831,"duration":183831}},{"uid":"9b859a8b889448e","status":"passed","time":{"start":1663665170000,"stop":1663665358467,"duration":188467}},{"uid":"8837064429d7a463","status":"passed","time":{"start":1663659067000,"stop":1663659246491,"duration":179491}},{"uid":"4d9f94f24e2e75cf","status":"passed","time":{"start":1663656809000,"stop":1663656972634,"duration":163634}},{"uid":"bfc61098c95f10b2","status":"passed","time":{"start":1663583877000,"stop":1663584069397,"duration":192397}},{"uid":"c0bacee27f1a9dad","status":"passed","time":{"start":1663340950000,"stop":1663341104585,"duration":154585}},{"uid":"41f56cf83b94121c","status":"passed","time":{"start":1663340650000,"stop":1663340837778,"duration":187778}},{"uid":"d8e20688c64875ed","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663308473000,"stop":1663308473000,"duration":0}},{"uid":"65999c42132d65bc","status":"failed","statusDetails":"Timed out after 210.002s.\nExpected an error to have occurred.  Got:\n    <nil>: nil","time":{"start":1663304356000,"stop":1663304717575,"duration":361575}},{"uid":"d1d67827f61112f6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663302519000,"stop":1663302519000,"duration":0}},{"uid":"5f3d462a953f6c69","status":"failed","statusDetails":"Timed out after 210.002s.\nExpected an error to have occurred.  Got:\n    <nil>: nil","time":{"start":1663264756000,"stop":1663265149474,"duration":393474}},{"uid":"8a68c5f0e02893cc","status":"failed","statusDetails":"Timed out after 210.000s.\nExpected an error to have occurred.  Got:\n    <nil>: nil","time":{"start":1663228190000,"stop":1663228581932,"duration":391932}}]},"2f40971170d14d2aeaac75d2bd1cb2c5":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"7c9a19cb56025682","status":"passed","time":{"start":1663838608740,"stop":1663838642354,"duration":33614}}]},"15a408cc7d2a7be3d22a5ca9c739caa0":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"c9cffe3dcf7a56ca","status":"passed","time":{"start":1663838874749,"stop":1663838911243,"duration":36494}}]},"6089bedb5ca5d27c97e225d14408bc76":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"33e974354044e974","status":"passed","time":{"start":1663839663336,"stop":1663839678386,"duration":15050}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting mesh-dns pod Should restart mesh-dns pod":{"statistic":{"failed":0,"broken":0,"skipped":56,"passed":0,"unknown":0,"total":56},"items":[{"uid":"54de4cb317994ddf","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1664094053000,"stop":1664094053000,"duration":0}},{"uid":"920784112589d7c9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663843963000,"stop":1663843963000,"duration":0}},{"uid":"725578476d439117","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663829327000,"stop":1663829327000,"duration":0}},{"uid":"dded5a4f248b099b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663343132000,"stop":1663343132000,"duration":0}},{"uid":"96a0abf9c31d4d4e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663342947000,"stop":1663342947000,"duration":0}},{"uid":"6a898adc23c054e9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"65feedfdd8cd68ee","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"8c55c67b068f5469","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662348908000,"stop":1662348908000,"duration":0}},{"uid":"fba51997b321a4ae","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"2853d4fbc0b639d2","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"ac3dfc0cbd4ba59c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"7e1cf7084ca6af67","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"a6dc1922abea6d8f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"afa904ee254b7dc6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"72acdc5f7e2b1d28","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"bfa3c728cc766a73","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"eed80751e61e42b1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"492cded192505f05","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"cd3931edeb60e724","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"e69aed4f153b962","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}}]},"94b385348efb221c10fa855ebbb33026":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"48cec68360606b7f","status":"passed","time":{"start":1663840174412,"stop":1663840187184,"duration":12772}}]},"5475567dfb4c6cdd5b94d01a741b3d5b":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"6b116b7fc0a69356","status":"passed","time":{"start":1663840752435,"stop":1663840764481,"duration":12046}}]},"Empty Suite:Empty Suite#[It] Hub Creation tests Hub Installtion Test Senarios Should fail if Cert is installed again in the same cluster":{"statistic":{"failed":0,"broken":0,"skipped":40,"passed":208,"unknown":0,"total":248},"items":[{"uid":"9cbd3ba06089b700","status":"passed","time":{"start":1664282572000,"stop":1664282572267,"duration":267}},{"uid":"eea20bd4d353b97","status":"passed","time":{"start":1664279225000,"stop":1664279225267,"duration":267}},{"uid":"59fc2457d004f8a5","status":"passed","time":{"start":1664277805000,"stop":1664277805318,"duration":318}},{"uid":"125d7845426dc7b7","status":"passed","time":{"start":1664275170000,"stop":1664275170321,"duration":321}},{"uid":"c2b129c3c373498f","status":"passed","time":{"start":1664275319000,"stop":1664275319328,"duration":328}},{"uid":"218338d00f54da5c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1664269661000,"stop":1664269661000,"duration":0}},{"uid":"e417666e4991c913","status":"passed","time":{"start":1664261300000,"stop":1664261300295,"duration":295}},{"uid":"64240ea72b475188","status":"passed","time":{"start":1664245049000,"stop":1664245049325,"duration":325}},{"uid":"9e28af6d7ee09125","status":"passed","time":{"start":1664218010000,"stop":1664218010362,"duration":362}},{"uid":"c8d83350d846ac6f","status":"passed","time":{"start":1664160583000,"stop":1664160583342,"duration":342}},{"uid":"8b00dece91a09085","status":"passed","time":{"start":1664105948000,"stop":1664105948351,"duration":351}},{"uid":"8cd1e23e48e8f2a0","status":"passed","time":{"start":1664103959000,"stop":1664103959252,"duration":252}},{"uid":"a5de3fd9417ad1f7","status":"passed","time":{"start":1664091567000,"stop":1664091567317,"duration":317}},{"uid":"7681f9cdff50a0f2","status":"passed","time":{"start":1663932902000,"stop":1663932902326,"duration":326}},{"uid":"bc6e2beb1a14f14e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663917718000,"stop":1663917718000,"duration":0}},{"uid":"8ee5747944ed894f","status":"passed","time":{"start":1663912384000,"stop":1663912384243,"duration":243}},{"uid":"769437f23b764de5","status":"passed","time":{"start":1663841452000,"stop":1663841452301,"duration":301}},{"uid":"fcbaca3a578186b0","status":"passed","time":{"start":1663838862000,"stop":1663838862282,"duration":282}},{"uid":"bdd91dba1e69f294","status":"passed","time":{"start":1663838333000,"stop":1663838333318,"duration":318}},{"uid":"d33336ae1b06f9f","status":"passed","time":{"start":1663836341000,"stop":1663836341293,"duration":293}}]},"Hub Suite:Hub Suite#[It]  QosProfile tests QosProfile Create validation Creates qosprofile":{"statistic":{"failed":128,"broken":0,"skipped":0,"passed":21,"unknown":0,"total":149},"items":[{"uid":"4a4860c4cf29447a","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0000b1338>: {\n        Underlying: <*exec.ExitError | 0xc0006bd360>{\n            ProcessState: {\n                pid: 6635,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 135543},\n                    Stime: {Sec: 0, Usec: 58735},\n                    Maxrss: 77980,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4428,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14104,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 868,\n                    Nivcsw: 487,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile3742660238\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile3742660238\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile3742660238\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile3742660238\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile3742660238\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1661501311000,"stop":1661501319252,"duration":8252}},{"uid":"fa39efcc9fc328fc","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0006b1278>: {\n        Underlying: <*exec.ExitError | 0xc0006d1340>{\n            ProcessState: {\n                pid: 6738,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 114422},\n                    Stime: {Sec: 0, Usec: 96818},\n                    Maxrss: 83208,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3984,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14104,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 1048,\n                    Nivcsw: 538,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile3848206832\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile3848206832\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile3848206832\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile3848206832\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile3848206832\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1661434220000,"stop":1661434227539,"duration":7539}},{"uid":"158fbc830ddcac8b","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0001784f8>: {\n        Underlying: <*exec.ExitError | 0xc00070a040>{\n            ProcessState: {\n                pid: 6781,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 137136},\n                    Stime: {Sec: 0, Usec: 56468},\n                    Maxrss: 67968,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3964,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14104,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 733,\n                    Nivcsw: 469,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1089868911\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1089868911\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1089868911\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1089868911\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1089868911\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1661356056000,"stop":1661356062517,"duration":6517}},{"uid":"401b2d0452faf79e","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0006ce7b0>: {\n        Underlying: <*exec.ExitError | 0xc000691960>{\n            ProcessState: {\n                pid: 6161,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 159178},\n                    Stime: {Sec: 0, Usec: 83778},\n                    Maxrss: 81324,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3612,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14104,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 820,\n                    Nivcsw: 531,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile2022566549\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile2022566549\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile2022566549\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile2022566549\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile2022566549\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1661352461000,"stop":1661352469576,"duration":8576}},{"uid":"4f86bc50546ea33e","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000e090>: {\n        Underlying: <*exec.ExitError | 0xc0009800c0>{\n            ProcessState: {\n                pid: 6712,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 127511},\n                    Stime: {Sec: 0, Usec: 42503},\n                    Maxrss: 77436,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3998,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14104,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 800,\n                    Nivcsw: 464,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile2106556113\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile2106556113\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile2106556113\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile2106556113\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile2106556113\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1661352207000,"stop":1661352214464,"duration":7464}},{"uid":"fe9452fb0b1680b0","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00081b7d0>: {\n        Underlying: <*exec.ExitError | 0xc000429f40>{\n            ProcessState: {\n                pid: 6768,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 152247},\n                    Stime: {Sec: 0, Usec: 65836},\n                    Maxrss: 70040,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3780,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14104,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 873,\n                    Nivcsw: 521,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile2022265779\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile2022265779\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile2022265779\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile2022265779\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile2022265779\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1661352028000,"stop":1661352035512,"duration":7512}},{"uid":"33c9e68f40c4f5de","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004cb968>: {\n        Underlying: <*exec.ExitError | 0xc0008c65e0>{\n            ProcessState: {\n                pid: 6082,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 137313},\n                    Stime: {Sec: 0, Usec: 74898},\n                    Maxrss: 78320,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 8874,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13616,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 844,\n                    Nivcsw: 1089,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile3930494543\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile3930494543\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile3930494543\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile3930494543\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile3930494543\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1661351263000,"stop":1661351277469,"duration":14469}},{"uid":"1b0b7a7ff76eff97","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004761c8>: {\n        Underlying: <*exec.ExitError | 0xc000483960>{\n            ProcessState: {\n                pid: 6605,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 219241},\n                    Stime: {Sec: 0, Usec: 99279},\n                    Maxrss: 76252,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3740,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14104,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 1184,\n                    Nivcsw: 725,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1359327084\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1359327084\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1359327084\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1359327084\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1359327084\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1661188463000,"stop":1661188471770,"duration":8770}},{"uid":"59acfcd17290b207","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0000d6558>: {\n        Underlying: <*exec.ExitError | 0xc0005c7180>{\n            ProcessState: {\n                pid: 6323,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 136153},\n                    Stime: {Sec: 0, Usec: 38293},\n                    Maxrss: 82884,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3930,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14104,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 880,\n                    Nivcsw: 390,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile3251245932\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile3251245932\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile3251245932\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile3251245932\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile3251245932\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1661183038000,"stop":1661183045440,"duration":7440}},{"uid":"e98ffdfcf23e7113","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000666b58>: {\n        Underlying: <*exec.ExitError | 0xc000621240>{\n            ProcessState: {\n                pid: 6121,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 135655},\n                    Stime: {Sec: 0, Usec: 71595},\n                    Maxrss: 76692,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4993,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14104,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 878,\n                    Nivcsw: 438,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1560004569\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1560004569\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1560004569\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1560004569\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1560004569\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1661180055000,"stop":1661180063215,"duration":8215}},{"uid":"e63325d4e7aa28a5","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0001eb458>: {\n        Underlying: <*exec.ExitError | 0xc000638f00>{\n            ProcessState: {\n                pid: 6139,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 170535},\n                    Stime: {Sec: 0, Usec: 68939},\n                    Maxrss: 75804,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4258,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14104,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 884,\n                    Nivcsw: 770,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1593006879\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1593006879\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1593006879\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                        \"ensure CRDs are installed first\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"error: resource mapping not found for name: \\\"profile4\\\" namespace: \\\"\\\" from \\\"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1593006879\\\": no matches for kind \\\"SliceQoSConfig\\\" in version \\\"controller.kubeslice.io/v1alpha1\\\"\",\n                    \"ensure CRDs are installed first\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; error: resource mapping not found for name: \"profile4\" namespace: \"\" from \"/tmp/%20QosProfile%20tests%20QosProfile%20Create%20validation%20Creates%20qosprofile1593006879\": no matches for kind \"SliceQoSConfig\" in version \"controller.kubeslice.io/v1alpha1\"\n    ensure CRDs are installed first\noccurred","time":{"start":1661152474000,"stop":1661152482518,"duration":8518}},{"uid":"a1d1dc9a090ff3cf","status":"passed","time":{"start":1661083598000,"stop":1661083599486,"duration":1486}},{"uid":"8198e552b3245c55","status":"passed","time":{"start":1661070332000,"stop":1661070340454,"duration":8454}},{"uid":"962d4da1aabd8f74","status":"passed","time":{"start":1661066603000,"stop":1661066605377,"duration":2377}},{"uid":"c80144c41a63a81c","status":"passed","time":{"start":1661018182000,"stop":1661018184081,"duration":2081}},{"uid":"4b784b4655361be1","status":"passed","time":{"start":1661014620000,"stop":1661014621489,"duration":1489}},{"uid":"62cecc1dfd6f786a","status":"passed","time":{"start":1661011831000,"stop":1661011840540,"duration":9540}},{"uid":"a837313995e0c06c","status":"passed","time":{"start":1661011780000,"stop":1661011788505,"duration":8505}},{"uid":"a67145573841a6ea","status":"passed","time":{"start":1661009684000,"stop":1661009685434,"duration":1434}},{"uid":"60dfc81fc105b6d6","status":"passed","time":{"start":1661007831000,"stop":1661007833089,"duration":2089}}]},"660ddec4a74d10555b7413cea0f8a38c":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"db39ba84491522b9","status":"passed","time":{"start":1663840313515,"stop":1663840326780,"duration":13265}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting iperf-client pod Check ping between iperf-server and iperf-client after iperf-client pod restart":{"statistic":{"failed":0,"broken":0,"skipped":56,"passed":0,"unknown":0,"total":56},"items":[{"uid":"efbd6e71d7d57583","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1664094053000,"stop":1664094053000,"duration":0}},{"uid":"2578f8d89275c0a2","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663843963000,"stop":1663843963000,"duration":0}},{"uid":"792249b37261fdfc","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663829327000,"stop":1663829327000,"duration":0}},{"uid":"d36696b4876a4464","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663343132000,"stop":1663343132000,"duration":0}},{"uid":"326509fcab04f14f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663342947000,"stop":1663342947000,"duration":0}},{"uid":"2c308ea47f229aa0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"8d07a22cde1337a7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"1fca5d6461662ddf","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662348908000,"stop":1662348908000,"duration":0}},{"uid":"5c241307aa6296db","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"fde747318530b907","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"4c998bde6a802b2a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"884b18b00f66167a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"74cc8da19e9cec66","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"70dc208251a2ec37","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"7222796b4d2aa11d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"af14cbdb9b9a69b6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"3067af1dd0e0e0ed","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"30285a81205946e7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"614d5b7d2c1a71ac","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"17355c47a78ffa8a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests slice with netpol Should install networkpolicies in all application namespaces":{"statistic":{"failed":0,"broken":0,"skipped":40,"passed":16,"unknown":0,"total":56},"items":[{"uid":"bb3667fb391f79b4","status":"passed","time":{"start":1664094053000,"stop":1664094053299,"duration":299}},{"uid":"2f824c8813da3b6d","status":"passed","time":{"start":1663843963000,"stop":1663843963405,"duration":405}},{"uid":"4270fdd7f1193e80","status":"passed","time":{"start":1663829327000,"stop":1663829327345,"duration":345}},{"uid":"ef4998f26329f362","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663343132000,"stop":1663343132000,"duration":0}},{"uid":"63994dc8688eb1f3","status":"passed","time":{"start":1663342947000,"stop":1663342947467,"duration":467}},{"uid":"b709948847085595","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"70a3f134a94a722b","status":"passed","time":{"start":1663044069000,"stop":1663044069440,"duration":440}},{"uid":"b113cfea0aee742c","status":"passed","time":{"start":1662348908000,"stop":1662348908296,"duration":296}},{"uid":"b69f2e6b55cc9b45","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"b95084324ffcd2ac","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"16f4d74866a3e47b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"87a0370108a63528","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"33e6afd657a582b1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"725873a99af47eae","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"ba40c43acbeb7564","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"65b594b9485302dc","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"c4634056cf7f64e2","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"b754077ad098a7c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"f8b23fccc62594e5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"5f51308b0d424df0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}}]},"Istio Suite:Istio Suite#[It] Bookinfo tests Bookinfo Scenario 3 without egress and with ingress Should verify if Curl Traffic works on Productpage test":{"statistic":{"failed":0,"broken":0,"skipped":58,"passed":14,"unknown":0,"total":72},"items":[{"uid":"9fec9ffd816078c0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1664246558000,"stop":1664246558000,"duration":0}},{"uid":"85dc23d3c970346c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1664162065000,"stop":1664162065000,"duration":0}},{"uid":"d7a9f59ab84ec03f","status":"passed","time":{"start":1664093039000,"stop":1664093039372,"duration":372}},{"uid":"9adb3f85e0e5a1bc","status":"passed","time":{"start":1663842833000,"stop":1663842833560,"duration":560}},{"uid":"592c0db96644addb","status":"passed","time":{"start":1663828380000,"stop":1663828380542,"duration":542}},{"uid":"56d14dcd758495d5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663769301000,"stop":1663769301000,"duration":0}},{"uid":"adeb3680658739a0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663671043000,"stop":1663671043000,"duration":0}},{"uid":"7309cbff0f429cf4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663666667000,"stop":1663666667000,"duration":0}},{"uid":"aa97ad4790780d17","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663666522000,"stop":1663666522000,"duration":0}},{"uid":"95c5cd19cd614352","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663585263000,"stop":1663585263000,"duration":0}},{"uid":"bac96471d10d7fd9","status":"passed","time":{"start":1663342266000,"stop":1663342275396,"duration":9396}},{"uid":"36e90f3dafadc7a4","status":"passed","time":{"start":1663341990000,"stop":1663341999306,"duration":9306}},{"uid":"2ddde8f1779bafc","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663315204000,"stop":1663315204000,"duration":0}},{"uid":"219c32ac1a16c149","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663312558000,"stop":1663312558000,"duration":0}},{"uid":"21a9f4e566d19073","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663055013000,"stop":1663055013000,"duration":0}},{"uid":"92c904a7ff85e66","status":"passed","time":{"start":1663043831000,"stop":1663043840302,"duration":9302}},{"uid":"715650d99643413","status":"passed","time":{"start":1663043075000,"stop":1663043084370,"duration":9370}},{"uid":"53d11a8d8367c24f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663040338000,"stop":1663040338000,"duration":0}},{"uid":"da90a3707433f08d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662819574000,"stop":1662819574000,"duration":0}},{"uid":"b4b8c0ebd849c7d3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662795310000,"stop":1662795310000,"duration":0}}]},"70440774ee53e7183c955d5d9648ba4c":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"1f38aaec5e7c27c4","status":"passed","time":{"start":1663840290069,"stop":1663840301745,"duration":11676}}]},"90fe7cd6afc8b30a488e82d3e25fa204":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"3ea107e277d51e56","status":"passed","time":{"start":1663840816175,"stop":1663840829904,"duration":13729}}]},"f6c4be354979926ca20293b27032c3f3":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"76c5c6227ad49f3e","status":"passed","time":{"start":1663842635150,"stop":1663842679163,"duration":44013}}]},"Worker Suite:Worker Suite#[It] Hub netpol scenarios: Slice Configs Netpol scenarios Should fail when deploying clusters in namespaceisolationprofile with * and a cluster name in both applicationNamespace and allowedNamespace":{"statistic":{"failed":0,"broken":0,"skipped":23,"passed":33,"unknown":0,"total":56},"items":[{"uid":"11a3a8607f73eae7","status":"passed","time":{"start":1664094053000,"stop":1664094053507,"duration":507}},{"uid":"23aaddc624fbf716","status":"passed","time":{"start":1663843963000,"stop":1663843963557,"duration":557}},{"uid":"499017d8d38e253a","status":"passed","time":{"start":1663829327000,"stop":1663829327450,"duration":450}},{"uid":"4543859350f912ba","status":"passed","time":{"start":1663343132000,"stop":1663343132569,"duration":569}},{"uid":"f189f339a9e1b75e","status":"passed","time":{"start":1663342947000,"stop":1663342947459,"duration":459}},{"uid":"ac1b283582f4407b","status":"skipped","statusDetails":"skipped","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"d26bf7646f1c10b6","status":"passed","time":{"start":1663044069000,"stop":1663044069484,"duration":484}},{"uid":"6049a55a659da158","status":"passed","time":{"start":1662348908000,"stop":1662348908527,"duration":527}},{"uid":"d2f7ced8524d507f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"811c3fe238560661","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"d472be9bbb2847af","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"d48b5d3625e00c77","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"a99d197257089012","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"c76209c7b49bdcf2","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"17451cce686a7b8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"628b1b82efaf6691","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"e59f6b7c3d0297d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"122dffff7cf89791","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"20992d36dc97c400","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"75f8e1c62cb14df9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Update Should update successfully when Project applied with valid service account name in Write users":{"statistic":{"failed":26,"broken":0,"skipped":0,"passed":190,"unknown":0,"total":216},"items":[{"uid":"85842f9a97927d66","status":"passed","time":{"start":1664275319000,"stop":1664275323292,"duration":4292}},{"uid":"dfaf1dc6a31a8f16","status":"passed","time":{"start":1664245203000,"stop":1664245207565,"duration":4565}},{"uid":"100ed9e048d750de","status":"passed","time":{"start":1664160732000,"stop":1664160736193,"duration":4193}},{"uid":"80f4005a910d2f71","status":"passed","time":{"start":1664091725000,"stop":1664091729180,"duration":4180}},{"uid":"4d7611a32bb0bab3","status":"passed","time":{"start":1663841627000,"stop":1663841631565,"duration":4565}},{"uid":"d5d33c2501f9a2b9","status":"passed","time":{"start":1663827045000,"stop":1663827050220,"duration":5220}},{"uid":"eaaf924c27aff2bd","status":"passed","time":{"start":1663767974000,"stop":1663767978230,"duration":4230}},{"uid":"c4103cce2c10d2ee","status":"passed","time":{"start":1663669677000,"stop":1663669682218,"duration":5218}},{"uid":"42cbc7f2ed5cde8a","status":"passed","time":{"start":1663665323000,"stop":1663665327644,"duration":4644}},{"uid":"7985c28f87751c2a","status":"passed","time":{"start":1663665170000,"stop":1663665174316,"duration":4316}},{"uid":"ccdc4627c34bb51","status":"passed","time":{"start":1663659067000,"stop":1663659071388,"duration":4388}},{"uid":"88bd4acc71c28a3a","status":"passed","time":{"start":1663656809000,"stop":1663656813258,"duration":4258}},{"uid":"c23d5e8819e3dadb","status":"passed","time":{"start":1663583877000,"stop":1663583881569,"duration":4569}},{"uid":"cc318e4752de729c","status":"passed","time":{"start":1663340950000,"stop":1663340955088,"duration":5088}},{"uid":"27d3f0017d293305","status":"passed","time":{"start":1663340650000,"stop":1663340655179,"duration":5179}},{"uid":"b24040dcd8d7173a","status":"failed","statusDetails":"Timed out after 60.214s.\nExpected\n    <bool>: false\nto be true","time":{"start":1663308473000,"stop":1663308540771,"duration":67771}},{"uid":"904893bcfd97a1a3","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000824600>: {\n        Underlying: <*exec.ExitError | 0xc0003f7bc0>{\n            ProcessState: {\n                pid: 6197,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 177955},\n                    Stime: {Sec: 0, Usec: 43500},\n                    Maxrss: 79104,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3998,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13344,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 300,\n                    Nivcsw: 222,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20applied%20with%20valid%20service%20account%20name%20in%20Write%20users3299442415\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.26.65:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20applied%20with%20valid%20service%20account%20name%20in%20Write%20users3299442415\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.26.65:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20applied%20with%20valid%20service%20account%20name%20in%20Write%20users3299442415\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.26.65:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20applied%20with%20valid%20service%20account%20name%20in%20Write%20users3299442415\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.26.65:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20applied%20with%20valid%20service%20account%20name%20in%20Write%20users3299442415\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.26.65:443: connect: connection refused\noccurred","time":{"start":1663304356000,"stop":1663304356275,"duration":275}},{"uid":"fb508a477ae97a93","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00080a018>: {\n        Underlying: <*exec.ExitError | 0xc0003e42e0>{\n            ProcessState: {\n                pid: 7533,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 143533},\n                    Stime: {Sec: 0, Usec: 49211},\n                    Maxrss: 82372,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 5470,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 367,\n                    Nivcsw: 375,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20applied%20with%20valid%20service%20account%20name%20in%20Write%20users892000607\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.179.221:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20applied%20with%20valid%20service%20account%20name%20in%20Write%20users892000607\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.179.221:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20applied%20with%20valid%20service%20account%20name%20in%20Write%20users892000607\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.179.221:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20applied%20with%20valid%20service%20account%20name%20in%20Write%20users892000607\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.179.221:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20applied%20with%20valid%20service%20account%20name%20in%20Write%20users892000607\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.179.221:443: connect: connection refused\noccurred","time":{"start":1663302519000,"stop":1663302519262,"duration":262}},{"uid":"67df667f07b97a7b","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000639338>: {\n        Underlying: <*exec.ExitError | 0xc00045c8e0>{\n            ProcessState: {\n                pid: 7168,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 190350},\n                    Stime: {Sec: 0, Usec: 60750},\n                    Maxrss: 84080,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2795,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13344,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 395,\n                    Nivcsw: 415,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20applied%20with%20valid%20service%20account%20name%20in%20Write%20users1680728379\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.55.244:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20applied%20with%20valid%20service%20account%20name%20in%20Write%20users1680728379\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.55.244:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20applied%20with%20valid%20service%20account%20name%20in%20Write%20users1680728379\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.55.244:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20applied%20with%20valid%20service%20account%20name%20in%20Write%20users1680728379\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.55.244:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20applied%20with%20valid%20service%20account%20name%20in%20Write%20users1680728379\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.55.244:443: connect: connection refused\noccurred","time":{"start":1663264756000,"stop":1663264756301,"duration":301}},{"uid":"d0a17a27e3ce65f1","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00084a870>: {\n        Underlying: <*exec.ExitError | 0xc0000d8680>{\n            ProcessState: {\n                pid: 7211,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 219723},\n                    Stime: {Sec: 0, Usec: 36620},\n                    Maxrss: 85484,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4255,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13344,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 304,\n                    Nivcsw: 244,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20applied%20with%20valid%20service%20account%20name%20in%20Write%20users1647958460\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.221.241:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20applied%20with%20valid%20service%20account%20name%20in%20Write%20users1647958460\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.221.241:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20applied%20with%20valid%20service%20account%20name%20in%20Write%20users1647958460\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.221.241:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20applied%20with%20valid%20service%20account%20name%20in%20Write%20users1647958460\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.221.241:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20applied%20with%20valid%20service%20account%20name%20in%20Write%20users1647958460\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.221.241:443: connect: connection refused\noccurred","time":{"start":1663228190000,"stop":1663228190329,"duration":329}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Update Should fail when Project is Applied with service account name as combination of special characters in Read users":{"statistic":{"failed":26,"broken":0,"skipped":0,"passed":190,"unknown":0,"total":216},"items":[{"uid":"b1651b60e5894bbc","status":"passed","time":{"start":1664275319000,"stop":1664275323948,"duration":4948}},{"uid":"272da62114f1708c","status":"passed","time":{"start":1664245203000,"stop":1664245207980,"duration":4980}},{"uid":"e330d67567c3b363","status":"passed","time":{"start":1664160732000,"stop":1664160737897,"duration":5897}},{"uid":"ea2c8a4642799ee4","status":"passed","time":{"start":1664091725000,"stop":1664091730873,"duration":5873}},{"uid":"b65a52d1fa2d8c59","status":"passed","time":{"start":1663841627000,"stop":1663841632113,"duration":5113}},{"uid":"9ec0123f68c5a803","status":"passed","time":{"start":1663827045000,"stop":1663827050916,"duration":5916}},{"uid":"9e4eeb597a6b051c","status":"passed","time":{"start":1663767974000,"stop":1663767979879,"duration":5879}},{"uid":"8df4ac38872bc9cd","status":"passed","time":{"start":1663669677000,"stop":1663669682921,"duration":5921}},{"uid":"6d14359a2c5757f3","status":"passed","time":{"start":1663665323000,"stop":1663665328178,"duration":5178}},{"uid":"8c4e1011febc8cf0","status":"passed","time":{"start":1663665170000,"stop":1663665175061,"duration":5061}},{"uid":"98e518499c073dbb","status":"passed","time":{"start":1663659067000,"stop":1663659072023,"duration":5023}},{"uid":"7dc23ea6f09bad93","status":"passed","time":{"start":1663656809000,"stop":1663656813973,"duration":4973}},{"uid":"ff9602d6d7fbfc72","status":"passed","time":{"start":1663583877000,"stop":1663583883187,"duration":6187}},{"uid":"e71f1ca976efc788","status":"passed","time":{"start":1663340950000,"stop":1663340954877,"duration":4877}},{"uid":"32d52925c8e2d31c","status":"passed","time":{"start":1663340650000,"stop":1663340655902,"duration":5902}},{"uid":"83eb6c78e4e5d6dc","status":"failed","statusDetails":"Timed out after 60.167s.\nExpected\n    <bool>: false\nto be true","time":{"start":1663308473000,"stop":1663308594547,"duration":121547}},{"uid":"4501479d5f01c37","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0008254a0>: {\n        Underlying: <*exec.ExitError | 0xc000774580>{\n            ProcessState: {\n                pid: 6228,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 177878},\n                    Stime: {Sec: 0, Usec: 60554},\n                    Maxrss: 85888,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3817,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13344,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 579,\n                    Nivcsw: 454,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Read%20users2886710707\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.26.65:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Read%20users2886710707\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.26.65:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Read%20users2886710707\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.26.65:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Read%20users2886710707\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.26.65:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Read%20users2886710707\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.26.65:443: connect: connection refused\noccurred","time":{"start":1663304356000,"stop":1663304356333,"duration":333}},{"uid":"a91e109c696a2c43","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00047b368>: {\n        Underlying: <*exec.ExitError | 0xc000751a40>{\n            ProcessState: {\n                pid: 7560,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 173155},\n                    Stime: {Sec: 0, Usec: 55094},\n                    Maxrss: 83588,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 7167,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 383,\n                    Nivcsw: 290,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Read%20users2583596707\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.179.221:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Read%20users2583596707\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.179.221:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Read%20users2583596707\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.179.221:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Read%20users2583596707\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.179.221:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Read%20users2583596707\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.179.221:443: connect: connection refused\noccurred","time":{"start":1663302519000,"stop":1663302519256,"duration":256}},{"uid":"b1ab38673cb963dd","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0006d18a8>: {\n        Underlying: <*exec.ExitError | 0xc0006e8080>{\n            ProcessState: {\n                pid: 7197,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 213173},\n                    Stime: {Sec: 0, Usec: 42634},\n                    Maxrss: 83628,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3797,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13344,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 413,\n                    Nivcsw: 297,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Read%20users1632907331\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.55.244:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Read%20users1632907331\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.55.244:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Read%20users1632907331\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.55.244:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Read%20users1632907331\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.55.244:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Read%20users1632907331\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.55.244:443: connect: connection refused\noccurred","time":{"start":1663264756000,"stop":1663264756306,"duration":306}},{"uid":"15f5aecbab263331","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00084b140>: {\n        Underlying: <*exec.ExitError | 0xc00065c560>{\n            ProcessState: {\n                pid: 7239,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 200139},\n                    Stime: {Sec: 0, Usec: 48033},\n                    Maxrss: 85012,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4127,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13344,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 310,\n                    Nivcsw: 360,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Read%20users1771196906\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.221.241:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Read%20users1771196906\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.221.241:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Read%20users1771196906\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.221.241:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Read%20users1771196906\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.221.241:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Read%20users1771196906\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.221.241:443: connect: connection refused\noccurred","time":{"start":1663228190000,"stop":1663228190305,"duration":305}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Iperf connectivity Should install slice":{"statistic":{"failed":29,"broken":0,"skipped":0,"passed":20,"unknown":0,"total":49},"items":[{"uid":"3c921c498b7b45cc","status":"passed","time":{"start":1662348908000,"stop":1662348908260,"duration":260}},{"uid":"c40635d0d2aaa996","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004688e8>: {\n        Underlying: <*exec.ExitError | 0xc000708100>{\n            ProcessState: {\n                pid: 7138,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 249795},\n                    Stime: {Sec: 0, Usec: 62448},\n                    Maxrss: 90740,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 7322,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 413,\n                    Nivcsw: 397,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\noccurred","time":{"start":1661844205000,"stop":1661844205286,"duration":286}},{"uid":"237a215a03366311","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0008f0018>: {\n        Underlying: <*exec.ExitError | 0xc00095e960>{\n            ProcessState: {\n                pid: 7088,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 169526},\n                    Stime: {Sec: 0, Usec: 65498},\n                    Maxrss: 84788,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4448,\n                    Majflt: 1,\n                    Nswap: 0,\n                    Inblock: 48,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 358,\n                    Nivcsw: 561,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661754870000,"stop":1661754870289,"duration":289}},{"uid":"a658e32c1cd40c18","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004425e8>: {\n        Underlying: <*exec.ExitError | 0xc0007b1d80>{\n            ProcessState: {\n                pid: 7266,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 227669},\n                    Stime: {Sec: 0, Usec: 62805},\n                    Maxrss: 81584,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3523,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 782,\n                    Nivcsw: 436,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661611962000,"stop":1661611962291,"duration":291}},{"uid":"8b53401a459dff6a","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00084bd40>: {\n        Underlying: <*exec.ExitError | 0xc0006fb6e0>{\n            ProcessState: {\n                pid: 7258,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 176391},\n                    Stime: {Sec: 0, Usec: 53684},\n                    Maxrss: 84804,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3717,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 320,\n                    Nivcsw: 410,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661585077000,"stop":1661585077246,"duration":246}},{"uid":"b4592669ce917cbf","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00048e210>: {\n        Underlying: <*exec.ExitError | 0xc000717d00>{\n            ProcessState: {\n                pid: 6167,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 156261},\n                    Stime: {Sec: 0, Usec: 58598},\n                    Maxrss: 91564,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4112,\n                    Majflt: 1,\n                    Nswap: 0,\n                    Inblock: 128,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 321,\n                    Nivcsw: 488,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661584447000,"stop":1661584447254,"duration":254}},{"uid":"5e2f5c89299ff842","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0001917a0>: {\n        Underlying: <*exec.ExitError | 0xc00083f0a0>{\n            ProcessState: {\n                pid: 7223,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 176362},\n                    Stime: {Sec: 0, Usec: 34412},\n                    Maxrss: 85092,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4031,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 392,\n                    Nivcsw: 977,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661580045000,"stop":1661580045233,"duration":233}},{"uid":"a7736798c199909b","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0007780f0>: {\n        Underlying: <*exec.ExitError | 0xc000774500>{\n            ProcessState: {\n                pid: 7093,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 244570},\n                    Stime: {Sec: 0, Usec: 65995},\n                    Maxrss: 87740,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 12288,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 450,\n                    Nivcsw: 494,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661578505000,"stop":1661578505298,"duration":298}},{"uid":"e0c1fe4260693465","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000895350>: {\n        Underlying: <*exec.ExitError | 0xc0007f9760>{\n            ProcessState: {\n                pid: 7106,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 274360},\n                    Stime: {Sec: 0, Usec: 77803},\n                    Maxrss: 88844,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 9306,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 868,\n                    Nivcsw: 791,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661572479000,"stop":1661572480334,"duration":1334}},{"uid":"d336dad48b1ea343","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004c2ae0>: {\n        Underlying: <*exec.ExitError | 0xc00086e5c0>{\n            ProcessState: {\n                pid: 7116,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 190582},\n                    Stime: {Sec: 0, Usec: 34651},\n                    Maxrss: 84548,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4907,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 362,\n                    Nivcsw: 279,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660832281000,"stop":1660832281207,"duration":207}},{"uid":"df1a04414ab30732","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000127110>: {\n        Underlying: <*exec.ExitError | 0xc0006ba4e0>{\n            ProcessState: {\n                pid: 7162,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 158077},\n                    Stime: {Sec: 0, Usec: 56745},\n                    Maxrss: 92508,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4012,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 333,\n                    Nivcsw: 533,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660808797000,"stop":1660808797247,"duration":247}},{"uid":"ea42b83a1b9092cf","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00066c3a8>: {\n        Underlying: <*exec.ExitError | 0xc0006471c0>{\n            ProcessState: {\n                pid: 7067,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 128316},\n                    Stime: {Sec: 0, Usec: 44247},\n                    Maxrss: 80264,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 9521,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 412,\n                    Nivcsw: 300,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660299182000,"stop":1660299182197,"duration":197}},{"uid":"eacf557b92ba96b8","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0000ce5b8>: {\n        Underlying: <*exec.ExitError | 0xc0008b8b00>{\n            ProcessState: {\n                pid: 7210,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 132413},\n                    Stime: {Sec: 0, Usec: 46985},\n                    Maxrss: 80556,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3993,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 377,\n                    Nivcsw: 203,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660295740000,"stop":1660295740186,"duration":186}},{"uid":"9dcf6e391f45ea53","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000649ad0>: {\n        Underlying: <*exec.ExitError | 0xc000829900>{\n            ProcessState: {\n                pid: 7169,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 169532},\n                    Stime: {Sec: 0, Usec: 59139},\n                    Maxrss: 82468,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 5967,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 299,\n                    Nivcsw: 316,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660293657000,"stop":1660293657257,"duration":257}},{"uid":"82afcbf8277b6889","status":"failed","statusDetails":"Unexpected error:\n    <*errors.StatusError | 0xc0000f6a00>: {\n        ErrStatus: {\n            TypeMeta: {Kind: \"\", APIVersion: \"\"},\n            ListMeta: {\n                SelfLink: \"\",\n                ResourceVersion: \"\",\n                Continue: \"\",\n                RemainingItemCount: nil,\n            },\n            Status: \"Failure\",\n            Message: \"namespaces \\\"iperf\\\" already exists\",\n            Reason: \"AlreadyExists\",\n            Details: {Name: \"iperf\", Group: \"\", Kind: \"namespaces\", UID: \"\", Causes: nil, RetryAfterSeconds: 0},\n            Code: 409,\n        },\n    }\n    namespaces \"iperf\" already exists\noccurred","time":{"start":1660113628000,"stop":1660113628360,"duration":360}},{"uid":"8898fd59cdc037cd","status":"failed","statusDetails":"Unexpected error:\n    <*errors.StatusError | 0xc00098bf40>: {\n        ErrStatus: {\n            TypeMeta: {Kind: \"\", APIVersion: \"\"},\n            ListMeta: {\n                SelfLink: \"\",\n                ResourceVersion: \"\",\n                Continue: \"\",\n                RemainingItemCount: nil,\n            },\n            Status: \"Failure\",\n            Message: \"namespaces \\\"iperf\\\" already exists\",\n            Reason: \"AlreadyExists\",\n            Details: {Name: \"iperf\", Group: \"\", Kind: \"namespaces\", UID: \"\", Causes: nil, RetryAfterSeconds: 0},\n            Code: 409,\n        },\n    }\n    namespaces \"iperf\" already exists\noccurred","time":{"start":1660104898000,"stop":1660104898528,"duration":528}},{"uid":"f7f482a66199998b","status":"passed","time":{"start":1660067259000,"stop":1660067259240,"duration":240}},{"uid":"16cb0f9377c13402","status":"passed","time":{"start":1660047551000,"stop":1660047551904,"duration":904}},{"uid":"81397e1af8087c56","status":"passed","time":{"start":1659982549000,"stop":1659982550984,"duration":1984}},{"uid":"55f4eddab014d14","status":"passed","time":{"start":1659970182000,"stop":1659970182838,"duration":838}}]},"23f47302066d7d8b350601f9cc542b61":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"a4f52899dedb6e4e","status":"passed","time":{"start":1663839413934,"stop":1663839430443,"duration":16509}}]},"Istio Suite:Istio Suite#[It] Bookinfo tests Bookinfo Scenario 1 without egress and ingress Should verify if Ping works successfully from Productpage to details endpoint":{"statistic":{"failed":6,"broken":0,"skipped":54,"passed":12,"unknown":0,"total":72},"items":[{"uid":"f0d0ab41b438bb00","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1664246558000,"stop":1664246558000,"duration":0}},{"uid":"7870e6bfd1478ff3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1664162065000,"stop":1664162065000,"duration":0}},{"uid":"e193fc644d59542","status":"passed","time":{"start":1664093039000,"stop":1664093097810,"duration":58810}},{"uid":"ad855eaea75b92f1","status":"passed","time":{"start":1663842833000,"stop":1663842892308,"duration":59308}},{"uid":"ce1f66f84a9b7ff9","status":"passed","time":{"start":1663828380000,"stop":1663828439382,"duration":59382}},{"uid":"f49895ba9c87af23","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663769301000,"stop":1663769301000,"duration":0}},{"uid":"fa47fe68644b932b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663671043000,"stop":1663671043000,"duration":0}},{"uid":"d65d8ef3d861df16","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663666667000,"stop":1663666667000,"duration":0}},{"uid":"281bf55d22529bf","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663666522000,"stop":1663666522000,"duration":0}},{"uid":"cc4a5ac0f8990b9b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663585263000,"stop":1663585263000,"duration":0}},{"uid":"20fee5b4b413b928","status":"failed","statusDetails":"Unexpected non-nil/non-zero argument at index 1:\n\t<*shell.ErrWithCmdOutput>: &shell.ErrWithCmdOutput{Underlying:(*exec.ExitError)(0xc0004cccc0), Output:(*shell.output)(0xc00009d158)}","time":{"start":1663342266000,"stop":1663342332680,"duration":66680}},{"uid":"c7ed5e95bdb8d04e","status":"passed","time":{"start":1663341990000,"stop":1663342049896,"duration":59896}},{"uid":"478e30820f304bfd","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663315204000,"stop":1663315204000,"duration":0}},{"uid":"9fe50bd243eac0dc","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663312558000,"stop":1663312558000,"duration":0}},{"uid":"ccfffb96648ef3e1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663055013000,"stop":1663055013000,"duration":0}},{"uid":"88b0f28cf8bc871d","status":"failed","statusDetails":"Unexpected non-nil/non-zero argument at index 1:\n\t<*shell.ErrWithCmdOutput>: &shell.ErrWithCmdOutput{Underlying:(*exec.ExitError)(0xc0005f2c00), Output:(*shell.output)(0xc000490e28)}","time":{"start":1663043831000,"stop":1663043897886,"duration":66886}},{"uid":"9aa7f6dbd9bed433","status":"passed","time":{"start":1663043075000,"stop":1663043135208,"duration":60208}},{"uid":"eb94510176218015","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663040338000,"stop":1663040338000,"duration":0}},{"uid":"4a487778518b128f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662819574000,"stop":1662819574000,"duration":0}},{"uid":"86411691f3276eff","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662795310000,"stop":1662795310000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Intracluster slice tests Slice with netpol should reconcile netpol config":{"statistic":{"failed":9,"broken":0,"skipped":29,"passed":18,"unknown":0,"total":56},"items":[{"uid":"1ecf4448d2ee345f","status":"failed","statusDetails":"Expected\n    <[]string | len:0, cap:0>: nil\nto contain elements\n    <[]string | len:1, cap:1>: [\"iperf\"]\nthe missing elements were\n    <[]string | len:1, cap:1>: [\"iperf\"]","time":{"start":1664094053000,"stop":1664094100315,"duration":47315}},{"uid":"c7e2fa12bcb986d9","status":"passed","time":{"start":1663843963000,"stop":1663843965357,"duration":2357}},{"uid":"ee96cf6f95e41c11","status":"failed","statusDetails":"\n\tError Trace:\tnamespace.go:15\n\t            \t\t\t\tt_intra_slice_test.go:98\n\t            \t\t\t\tsuite.go:596\n\t            \t\t\t\tasm_amd64.s:1581\n\tError:      \tReceived unexpected error:\n\t            \tnamespaces \"iperf\" already exists\n\tTest:       \tIntracluster slice tests Slice with netpol should reconcile netpol config\n","time":{"start":1663829327000,"stop":1663829371952,"duration":44952}},{"uid":"491aef7d27682b69","status":"passed","time":{"start":1663343132000,"stop":1663343135694,"duration":3694}},{"uid":"f6951a2b94261bc2","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663342947000,"stop":1663342947000,"duration":0}},{"uid":"82fc235d39fd0e76","status":"skipped","statusDetails":"skipped","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"3c83827c5535a59f","status":"passed","time":{"start":1663044069000,"stop":1663044071285,"duration":2285}},{"uid":"4dc51b57091f9dd2","status":"failed","statusDetails":"\n\tError Trace:\tnamespace.go:15\n\t            \t\t\t\tt_intra_slice_test.go:98\n\t            \t\t\t\tsuite.go:596\n\t            \t\t\t\tasm_amd64.s:1581\n\tError:      \tReceived unexpected error:\n\t            \tnamespaces \"iperf\" already exists\n\tTest:       \tIntracluster slice tests Slice with netpol should reconcile netpol config\n","time":{"start":1662348908000,"stop":1662348916781,"duration":8781}},{"uid":"5213fb1fa9cf7106","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"d72c7c11f9b556d7","status":"skipped","statusDetails":"skipped - Spec skipped because Skip() was called in BeforeAll","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"f7e8db10b0c7f2d8","status":"skipped","statusDetails":"skipped - Spec skipped because Skip() was called in BeforeAll","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"541d546f465a1a1","status":"skipped","statusDetails":"skipped - Spec skipped because Skip() was called in BeforeAll","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"c5887bae2dfdfa38","status":"skipped","statusDetails":"skipped - Spec skipped because Skip() was called in BeforeAll","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"4a439b1c31f91b41","status":"skipped","statusDetails":"skipped - Spec skipped because Skip() was called in BeforeAll","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"e281ccec06d5fb39","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"c30d86a7c721e642","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"fa9258c646703d4d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"d9f832b7af9640d8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"a0d675ae50a80275","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"9d5f92a1e32ba1e8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Namespace reconcile test scenarios: Onboard label app ns on workers":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":55,"unknown":0,"total":56},"items":[{"uid":"9e920a99adf435f4","status":"passed","time":{"start":1664094053000,"stop":1664094054046,"duration":1046}},{"uid":"3a149c08aa8e2828","status":"passed","time":{"start":1663843963000,"stop":1663843964132,"duration":1132}},{"uid":"967ac38e9b56ca29","status":"passed","time":{"start":1663829327000,"stop":1663829327862,"duration":862}},{"uid":"8eb6ac1566e37dfe","status":"passed","time":{"start":1663343132000,"stop":1663343133620,"duration":1620}},{"uid":"686f5d3d26ccac85","status":"passed","time":{"start":1663342947000,"stop":1663342948395,"duration":1395}},{"uid":"b325d5bdacd9cee1","status":"skipped","statusDetails":"skipped","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"e4c9e2a1d416d824","status":"passed","time":{"start":1663044069000,"stop":1663044069817,"duration":817}},{"uid":"fc8cc8b1a96679e9","status":"passed","time":{"start":1662348908000,"stop":1662348908682,"duration":682}},{"uid":"4c3089a464a192bb","status":"passed","time":{"start":1661844205000,"stop":1661844205804,"duration":804}},{"uid":"4b061e94c6403789","status":"passed","time":{"start":1661754870000,"stop":1661754870711,"duration":711}},{"uid":"73f51aeb264087f3","status":"passed","time":{"start":1661611962000,"stop":1661611963174,"duration":1174}},{"uid":"a818dbb9868c7995","status":"passed","time":{"start":1661585077000,"stop":1661585077801,"duration":801}},{"uid":"42aadae216c3d87a","status":"passed","time":{"start":1661584447000,"stop":1661584448119,"duration":1119}},{"uid":"22733e4b76b19d07","status":"passed","time":{"start":1661580045000,"stop":1661580045797,"duration":797}},{"uid":"56d15e35e9598ca1","status":"passed","time":{"start":1661578505000,"stop":1661578505782,"duration":782}},{"uid":"45b4bfb31a3c6c81","status":"passed","time":{"start":1661572479000,"stop":1661572479826,"duration":826}},{"uid":"84cd19c53efd3ab4","status":"passed","time":{"start":1660832281000,"stop":1660832281719,"duration":719}},{"uid":"ed236ef7ddc8b5b","status":"passed","time":{"start":1660808797000,"stop":1660808797641,"duration":641}},{"uid":"d727d2d177b96a1c","status":"passed","time":{"start":1660299182000,"stop":1660299182576,"duration":576}},{"uid":"639a60d7406e1ed1","status":"passed","time":{"start":1660295740000,"stop":1660295740624,"duration":624}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests Iperf App connectivity should have vl3 router running":{"statistic":{"failed":33,"broken":1,"skipped":0,"passed":22,"unknown":0,"total":56},"items":[{"uid":"21a16ea6df20cf4b","status":"passed","time":{"start":1664094053000,"stop":1664094053703,"duration":703}},{"uid":"9a32340b44e732d4","status":"passed","time":{"start":1663843963000,"stop":1663843963895,"duration":895}},{"uid":"d252f345c3e7adc0","status":"passed","time":{"start":1663829327000,"stop":1663829327898,"duration":898}},{"uid":"b2d77fcc048d7492","status":"passed","time":{"start":1663343132000,"stop":1663343132690,"duration":690}},{"uid":"e537c0cc5ed23692","status":"passed","time":{"start":1663342947000,"stop":1663342949925,"duration":2925}},{"uid":"c0f160d2bca1e23b","status":"broken","statusDetails":"interrupted","time":{"start":1663044723000,"stop":1663044914375,"duration":191375}},{"uid":"9c4f4a2f0f6b8890","status":"passed","time":{"start":1663044069000,"stop":1663044070242,"duration":1242}},{"uid":"360026ac951ee1d4","status":"passed","time":{"start":1662348908000,"stop":1662348908863,"duration":863}},{"uid":"5e7fda9ca9653053","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0002bb428>: {\n        Underlying: <*exec.ExitError | 0xc0004b3940>{\n            ProcessState: {\n                pid: 7306,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 234338},\n                    Stime: {Sec: 0, Usec: 80806},\n                    Maxrss: 82452,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4300,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 452,\n                    Nivcsw: 554,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\noccurred","time":{"start":1661844205000,"stop":1661844205595,"duration":595}},{"uid":"b900c3b8da2cb89d","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000734180>: {\n        Underlying: <*exec.ExitError | 0xc0007a5460>{\n            ProcessState: {\n                pid: 7102,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 169157},\n                    Stime: {Sec: 0, Usec: 63903},\n                    Maxrss: 83076,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 5297,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 376,\n                    Nivcsw: 406,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661754870000,"stop":1661754870441,"duration":441}},{"uid":"88a7541829bc854a","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0008ce798>: {\n        Underlying: <*exec.ExitError | 0xc000074c40>{\n            ProcessState: {\n                pid: 7116,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 243558},\n                    Stime: {Sec: 0, Usec: 44652},\n                    Maxrss: 82708,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3450,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 355,\n                    Nivcsw: 609,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661611962000,"stop":1661611962631,"duration":631}},{"uid":"3ddaadb86181e209","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00090c150>: {\n        Underlying: <*exec.ExitError | 0xc000a805c0>{\n            ProcessState: {\n                pid: 7122,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 178465},\n                    Stime: {Sec: 0, Usec: 51556},\n                    Maxrss: 81444,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3395,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 276,\n                    Nivcsw: 417,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661585077000,"stop":1661585077527,"duration":527}},{"uid":"47434529d1bee15a","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00013a768>: {\n        Underlying: <*exec.ExitError | 0xc000825da0>{\n            ProcessState: {\n                pid: 6182,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 228202},\n                    Stime: {Sec: 0, Usec: 29928},\n                    Maxrss: 86092,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3412,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 290,\n                    Nivcsw: 494,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661584447000,"stop":1661584447508,"duration":508}},{"uid":"89f059c467136a8a","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0008106c0>: {\n        Underlying: <*exec.ExitError | 0xc000627000>{\n            ProcessState: {\n                pid: 7238,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 185452},\n                    Stime: {Sec: 0, Usec: 33718},\n                    Maxrss: 78140,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3623,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 356,\n                    Nivcsw: 373,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661580045000,"stop":1661580045452,"duration":452}},{"uid":"35759955519f403b","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000778018>: {\n        Underlying: <*exec.ExitError | 0xc000774040>{\n            ProcessState: {\n                pid: 7084,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 249551},\n                    Stime: {Sec: 0, Usec: 72450},\n                    Maxrss: 87784,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 11790,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 288,\n                    Nivcsw: 446,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661578505000,"stop":1661578505621,"duration":621}},{"uid":"908146a04ed760af","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0008a2db0>: {\n        Underlying: <*exec.ExitError | 0xc0004d8e40>{\n            ProcessState: {\n                pid: 7258,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 262784},\n                    Stime: {Sec: 0, Usec: 63705},\n                    Maxrss: 91384,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 5494,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 295,\n                    Nivcsw: 551,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661572479000,"stop":1661572479618,"duration":618}},{"uid":"b0eb87b8d5c5491d","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0006eae40>: {\n        Underlying: <*exec.ExitError | 0xc0008cb6a0>{\n            ProcessState: {\n                pid: 7096,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 133537},\n                    Stime: {Sec: 0, Usec: 86406},\n                    Maxrss: 78624,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 6779,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 376,\n                    Nivcsw: 348,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660832281000,"stop":1660832281433,"duration":433}},{"uid":"4f3f14917e294823","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000989518>: {\n        Underlying: <*exec.ExitError | 0xc000710900>{\n            ProcessState: {\n                pid: 7152,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 159208},\n                    Stime: {Sec: 0, Usec: 55537},\n                    Maxrss: 83908,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4107,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 377,\n                    Nivcsw: 344,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660808797000,"stop":1660808797413,"duration":413}},{"uid":"c6dbabf0998b8d95","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000350ba0>: {\n        Underlying: <*exec.ExitError | 0xc000829380>{\n            ProcessState: {\n                pid: 7262,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 137557},\n                    Stime: {Sec: 0, Usec: 33459},\n                    Maxrss: 86792,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4348,\n                    Majflt: 1,\n                    Nswap: 0,\n                    Inblock: 160,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 505,\n                    Nivcsw: 381,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660299182000,"stop":1660299182351,"duration":351}},{"uid":"ad89085f103f7bb8","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004ca780>: {\n        Underlying: <*exec.ExitError | 0xc000158400>{\n            ProcessState: {\n                pid: 7225,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 140590},\n                    Stime: {Sec: 0, Usec: 37215},\n                    Maxrss: 86032,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3706,\n                    Majflt: 1,\n                    Nswap: 0,\n                    Inblock: 104,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 367,\n                    Nivcsw: 290,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660295740000,"stop":1660295740325,"duration":325}}]},"a440d3dac070bc95f7bad31a77e29be6":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"2dca7c17a9b9df88","status":"passed","time":{"start":1663841071213,"stop":1663841088268,"duration":17055}}]},"431ba22d6d80b5a2c9d7ea753c213206":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"34d17806a1aac153","status":"passed","time":{"start":1663841009384,"stop":1663841025975,"duration":16591}}]},"Istio Suite:Istio Suite#[It] Bookinfo tests Bookinfo Scenario 4 with egress and ingress Should verify if Curl Traffic works on Productpage test":{"statistic":{"failed":0,"broken":0,"skipped":60,"passed":12,"unknown":0,"total":72},"items":[{"uid":"66d7afa894bdc8c0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1664246558000,"stop":1664246558000,"duration":0}},{"uid":"b657c8e48d319a56","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1664162065000,"stop":1664162065000,"duration":0}},{"uid":"4128adfa9ce87cb9","status":"passed","time":{"start":1664093039000,"stop":1664093048322,"duration":9322}},{"uid":"c44089725b397777","status":"passed","time":{"start":1663842833000,"stop":1663842833988,"duration":988}},{"uid":"9969b3eaa9e71054","status":"passed","time":{"start":1663828380000,"stop":1663828389384,"duration":9384}},{"uid":"afaf3040e8aaeff7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663769301000,"stop":1663769301000,"duration":0}},{"uid":"a884be80d0e5b2e4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663671043000,"stop":1663671043000,"duration":0}},{"uid":"3f00f94d8078d75e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663666667000,"stop":1663666667000,"duration":0}},{"uid":"ef73504f14a2e247","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663666522000,"stop":1663666522000,"duration":0}},{"uid":"6adaf692fd84367f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663585263000,"stop":1663585263000,"duration":0}},{"uid":"f39a606215f1fff5","status":"passed","time":{"start":1663342266000,"stop":1663342266420,"duration":420}},{"uid":"4e775f3469606211","status":"passed","time":{"start":1663341990000,"stop":1663341990458,"duration":458}},{"uid":"b6f09630147ed455","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663315204000,"stop":1663315204000,"duration":0}},{"uid":"18158b17c97fd049","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663312558000,"stop":1663312558000,"duration":0}},{"uid":"25f1a81f57ab5865","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663055013000,"stop":1663055013000,"duration":0}},{"uid":"3ca7d9b2afa6acb4","status":"passed","time":{"start":1663043831000,"stop":1663043831357,"duration":357}},{"uid":"28d47663ba7beb54","status":"passed","time":{"start":1663043075000,"stop":1663043075428,"duration":428}},{"uid":"e5e6101f37f9f19b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663040338000,"stop":1663040338000,"duration":0}},{"uid":"6a1064b99c2898cf","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662819574000,"stop":1662819574000,"duration":0}},{"uid":"5743bc50e65dd928","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1662795310000,"stop":1662795310000,"duration":0}}]},"Istio Suite:Istio Suite#[BeforeSuite]":{"statistic":{"failed":160,"broken":0,"skipped":0,"passed":72,"unknown":0,"total":232},"items":[{"uid":"84ae0c4d512835b8","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000518018>: {\n        Underlying: <*exec.ExitError | 0xc0003a4000>{\n            ProcessState: {\n                pid: 6112,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 2, Usec: 162667},\n                    Stime: {Sec: 0, Usec: 190704},\n                    Maxrss: 85012,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 12518,\n                    Majflt: 4,\n                    Nswap: 0,\n                    Inblock: 664,\n                    Oublock: 14808,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 10846,\n                    Nivcsw: 3340,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.5.2.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:165: [debug] Clearing discovery cache\",\n                        \"wait.go:48: [debug] beginning wait for 5 resources with timeout of 1m0s\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:310: [debug] Starting delete for \\\"kubeslice-preinstall\\\" ServiceAccount\",\n                        \"client.go:339: [debug] serviceaccounts \\\"kubeslice-preinstall\\\" not found\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:310: [debug] Starting delete for \\\"kubeslice-preinstall-role\\\" ClusterRole\",\n                        \"client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \\\"kubeslice-preinstall-role\\\" not found\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:310: [debug] Starting delete for \\\"kubeslice-preinstall-rolebinding\\\" ClusterRoleBinding\",\n                        \"client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \\\"kubeslice-preinstall-rolebinding\\\" not found\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:310: [debug] Starting delete for \\\"kubeslice-worker-preinstall-configmap\\\" ConfigMap\",\n                        \"client.go:339: [debug] configmaps \\\"kubeslice-worker-preinstall-configmap\\\" not found\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:310: [debug] Starting delete for \\\"kubeslice-worker-preinstall-job\\\" Job\",\n                        \"client.go:339: [debug] jobs.batch \\\"kubeslice-worker-preinstall-job\\\" not found\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:540: [debug] Watching for changes to Job kubeslice-worker-preinstall-job with timeout of 5m0s\",\n                        \"client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: ADDED\",\n                        \"client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 0, jobs failed: 0, jobs succeeded: 0\",\n                        \"client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\",\n                        \"client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\",\n                        \"client.go:568:...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.5.2.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:165: [debug] Clearing discovery cache\n    wait.go:48: [debug] beginning wait for 5 resources with timeout of 1m0s\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:310: [debug] Starting delete for \"kubeslice-preinstall\" ServiceAccount\n    client.go:339: [debug] serviceaccounts \"kubeslice-preinstall\" not found\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:310: [debug] Starting delete for \"kubeslice-preinstall-role\" ClusterRole\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-preinstall-role\" not found\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:310: [debug] Starting delete for \"kubeslice-preinstall-rolebinding\" ClusterRoleBinding\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-preinstall-rolebinding\" not found\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:310: [debug] Starting delete for \"kubeslice-worker-preinstall-configmap\" ConfigMap\n    client.go:339: [debug] configmaps \"kubeslice-worker-preinstall-configmap\" not found\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:310: [debug] Starting delete for \"kubeslice-worker-preinstall-job\" Job\n    client.go:339: [debug] jobs.batch \"kubeslice-worker-preinstall-job\" not found\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:540: [debug] Watching for changes to Job kubeslice-worker-preinstall-job with timeout of 5m0s\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: ADDED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 0, jobs failed: 0, jobs succeeded: 0\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 1, jobs succeeded: 0\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 2, jobs succeeded: 0\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 3, jobs succeeded: 0\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 4, jobs succeeded: 0\n    client.go:310: [debug] Starting delete for \"kubeslice-worker-preinstall-job\" Job\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:310: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"jaeger\" Service\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-svc\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:339: [debug] services \"jaeger\" not found\n    client.go:339: [debug] services \"kubeslice-dns\" not found\n    client.go:339: [debug] services \"nsm-admission-webhook-svc\" not found\n    client.go:339: [debug] services \"kubeslice-webhook-service\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:310: [debug] Starting delete for \"jaeger\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook\" Deployment\n    client.go:310: [debug] Starting delete for \"prefix-service\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:339: [debug] deployments.apps \"nsm-admission-webhook\" not found\n    client.go:339: [debug] deployments.apps \"kubeslice-dns\" not found\n    client.go:339: [debug] deployments.apps \"prefix-service\" not found\n    client.go:339: [debug] deployments.apps \"jaeger\" not found\n    client.go:339: [debug] deployments.apps \"kubeslice-operator\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsm-kernel-forwarder\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:339: [debug] daemonsets.apps \"nsmgr\" not found\n    client.go:339: [debug] daemonsets.apps \"nsm-kernel-forwarder\" not found\n    client.go:339: [debug] daemonsets.apps \"kubeslice-netop\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:339: [debug] rolebindings.rbac.authorization.k8s.io \"kubeslice-leader-election-rolebinding\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:339: [debug] roles.rbac.authorization.k8s.io \"kubeslice-leader-election-role\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-kubernetes-dashboard\" not found\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"nsm-role-binding\" not found\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-dns-rolebinding\" not found\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-manager-rolebinding\" not found\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-proxy-rolebinding\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRole\n    client.go:310: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-manager-role\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-dns-role\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-metrics-reader\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-proxy-role\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"nsm-role\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-kubernetes-dashboard\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"aggregate-network-services-view\" not found\n    client.go:310: [debug] Starting delete for \"networkserviceendpoints.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservicemanagers.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservices.networkservicemesh.io\" CustomResourceDefinition\n    client.go:339: [debug] customresourcedefinitions.apiextensions.k8s.io \"networkservices.networkservicemesh.io\" not found\n    client.go:339: [debug] customresourcedefinitions.apiextensions.k8s.io \"networkserviceendpoints.networkservicemesh.io\" not found\n    client.go:339: [debug] customresourcedefinitions.apiextensions.k8s.io \"networkservicemanagers.networkservicemesh.io\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:339: [debug] configmaps \"kubeslice-manager-config\" not found\n    client.go:339: [debug] configmaps \"nsm-config\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-image-pull-secret\" Secret\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-certs\" Secret\n    client.go:339: [debug] secrets \"nsm-admission-webhook-certs\" not found\n    client.go:339: [debug] secrets \"kubeslice-admission-webhook-certs\" not found\n    client.go:339: [debug] secrets \"kubeslice-image-pull-secret\" not found\n    client.go:339: [debug] secrets \"kubeslice-hub\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:339: [debug] serviceaccounts \"nse-acc\" not found\n    client.go:339: [debug] serviceaccounts \"forward-plane-acc\" not found\n    client.go:339: [debug] serviceaccounts \"kubeslice-controller-manager\" not found\n    client.go:339: [debug] serviceaccounts \"kubeslice-kubernetes-dashboard\" not found\n    client.go:339: [debug] serviceaccounts \"kubeslice-netop\" not found\n    client.go:339: [debug] serviceaccounts \"vpn-gateway-server\" not found\n    client.go:339: [debug] serviceaccounts \"slice-router\" not found\n    client.go:339: [debug] serviceaccounts \"vpn-gateway-client\" not found\n    client.go:339: [debug] serviceaccounts \"nsc-acc\" not found\n    client.go:339: [debug] serviceaccounts \"kubeslice-dns\" not found\n    client.go:339: [debug] serviceaccounts \"nsmgr-acc\" not found\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-cfg\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:339: [debug] mutatingwebhookconfigurations.admissionregistration.k8s.io \"kubeslice-mutating-webhook-configuration\" not found\n    client.go:339: [debug] mutatingwebhookconfigurations.admissionregistration.k8s.io \"nsm-admission-webhook-cfg\" not found\n    uninstall.go:144: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: failed pre-install: timed out waiting for the condition\n    helm.go:84: [debug] failed pre-install: timed out waiting for the condition\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:361\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1664282730000,"stop":1664283135724,"duration":405724}},{"uid":"ebe4b3c87ece4cfa","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000132048>: {\n        Underlying: <*exec.ExitError | 0xc0004d2000>{\n            ProcessState: {\n                pid: 6120,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 2, Usec: 380644},\n                    Stime: {Sec: 0, Usec: 157329},\n                    Maxrss: 83652,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 12555,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14808,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 9171,\n                    Nivcsw: 2996,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.5.2.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:165: [debug] Clearing discovery cache\",\n                        \"wait.go:48: [debug] beginning wait for 5 resources with timeout of 1m0s\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:310: [debug] Starting delete for \\\"kubeslice-preinstall\\\" ServiceAccount\",\n                        \"client.go:339: [debug] serviceaccounts \\\"kubeslice-preinstall\\\" not found\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:310: [debug] Starting delete for \\\"kubeslice-preinstall-role\\\" ClusterRole\",\n                        \"client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \\\"kubeslice-preinstall-role\\\" not found\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:310: [debug] Starting delete for \\\"kubeslice-preinstall-rolebinding\\\" ClusterRoleBinding\",\n                        \"client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \\\"kubeslice-preinstall-rolebinding\\\" not found\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:310: [debug] Starting delete for \\\"kubeslice-worker-preinstall-configmap\\\" ConfigMap\",\n                        \"client.go:339: [debug] configmaps \\\"kubeslice-worker-preinstall-configmap\\\" not found\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:310: [debug] Starting delete for \\\"kubeslice-worker-preinstall-job\\\" Job\",\n                        \"client.go:339: [debug] jobs.batch \\\"kubeslice-worker-preinstall-job\\\" not found\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:540: [debug] Watching for changes to Job kubeslice-worker-preinstall-job with timeout of 5m0s\",\n                        \"client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: ADDED\",\n                        \"client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 0, jobs failed: 0, jobs succeeded: 0\",\n                        \"client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\",\n                        \"client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\",\n                        \"client.go:568: [d...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.5.2.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:165: [debug] Clearing discovery cache\n    wait.go:48: [debug] beginning wait for 5 resources with timeout of 1m0s\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:310: [debug] Starting delete for \"kubeslice-preinstall\" ServiceAccount\n    client.go:339: [debug] serviceaccounts \"kubeslice-preinstall\" not found\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:310: [debug] Starting delete for \"kubeslice-preinstall-role\" ClusterRole\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-preinstall-role\" not found\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:310: [debug] Starting delete for \"kubeslice-preinstall-rolebinding\" ClusterRoleBinding\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-preinstall-rolebinding\" not found\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:310: [debug] Starting delete for \"kubeslice-worker-preinstall-configmap\" ConfigMap\n    client.go:339: [debug] configmaps \"kubeslice-worker-preinstall-configmap\" not found\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:310: [debug] Starting delete for \"kubeslice-worker-preinstall-job\" Job\n    client.go:339: [debug] jobs.batch \"kubeslice-worker-preinstall-job\" not found\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:540: [debug] Watching for changes to Job kubeslice-worker-preinstall-job with timeout of 5m0s\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: ADDED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 0, jobs failed: 0, jobs succeeded: 0\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 1, jobs succeeded: 0\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 2, jobs succeeded: 0\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 3, jobs succeeded: 0\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 4, jobs succeeded: 0\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 5, jobs succeeded: 0\n    client.go:310: [debug] Starting delete for \"kubeslice-worker-preinstall-job\" Job\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:310: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-svc\" Service\n    client.go:310: [debug] Starting delete for \"jaeger\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:339: [debug] services \"nsm-admission-webhook-svc\" not found\n    client.go:339: [debug] services \"kubeslice-webhook-service\" not found\n    client.go:339: [debug] services \"kubeslice-dns\" not found\n    client.go:339: [debug] services \"jaeger\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:310: [debug] Starting delete for \"jaeger\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook\" Deployment\n    client.go:310: [debug] Starting delete for \"prefix-service\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:339: [debug] deployments.apps \"kubeslice-operator\" not found\n    client.go:339: [debug] deployments.apps \"kubeslice-dns\" not found\n    client.go:339: [debug] deployments.apps \"jaeger\" not found\n    client.go:339: [debug] deployments.apps \"nsm-admission-webhook\" not found\n    client.go:339: [debug] deployments.apps \"prefix-service\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsm-kernel-forwarder\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:339: [debug] daemonsets.apps \"nsmgr\" not found\n    client.go:339: [debug] daemonsets.apps \"kubeslice-netop\" not found\n    client.go:339: [debug] daemonsets.apps \"nsm-kernel-forwarder\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:339: [debug] rolebindings.rbac.authorization.k8s.io \"kubeslice-leader-election-rolebinding\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:339: [debug] roles.rbac.authorization.k8s.io \"kubeslice-leader-election-role\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRoleBinding\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-manager-rolebinding\" not found\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-kubernetes-dashboard\" not found\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"nsm-role-binding\" not found\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-dns-rolebinding\" not found\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-proxy-rolebinding\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRole\n    client.go:310: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-manager-role\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-metrics-reader\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"aggregate-network-services-view\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"nsm-role\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-dns-role\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-kubernetes-dashboard\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-proxy-role\" not found\n    client.go:310: [debug] Starting delete for \"networkservices.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkserviceendpoints.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservicemanagers.networkservicemesh.io\" CustomResourceDefinition\n    client.go:339: [debug] customresourcedefinitions.apiextensions.k8s.io \"networkservices.networkservicemesh.io\" not found\n    client.go:339: [debug] customresourcedefinitions.apiextensions.k8s.io \"networkservicemanagers.networkservicemesh.io\" not found\n    client.go:339: [debug] customresourcedefinitions.apiextensions.k8s.io \"networkserviceendpoints.networkservicemesh.io\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:339: [debug] configmaps \"kubeslice-manager-config\" not found\n    client.go:339: [debug] configmaps \"nsm-config\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-image-pull-secret\" Secret\n    client.go:339: [debug] secrets \"kubeslice-hub\" not found\n    client.go:339: [debug] secrets \"nsm-admission-webhook-certs\" not found\n    client.go:339: [debug] secrets \"kubeslice-admission-webhook-certs\" not found\n    client.go:339: [debug] secrets \"kubeslice-image-pull-secret\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:339: [debug] serviceaccounts \"nsc-acc\" not found\n    client.go:339: [debug] serviceaccounts \"nsmgr-acc\" not found\n    client.go:339: [debug] serviceaccounts \"nse-acc\" not found\n    client.go:339: [debug] serviceaccounts \"vpn-gateway-client\" not found\n    client.go:339: [debug] serviceaccounts \"kubeslice-netop\" not found\n    client.go:339: [debug] serviceaccounts \"vpn-gateway-server\" not found\n    client.go:339: [debug] serviceaccounts \"slice-router\" not found\n    client.go:339: [debug] serviceaccounts \"kubeslice-dns\" not found\n    client.go:339: [debug] serviceaccounts \"forward-plane-acc\" not found\n    client.go:339: [debug] serviceaccounts \"kubeslice-controller-manager\" not found\n    client.go:339: [debug] serviceaccounts \"kubeslice-kubernetes-dashboard\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-cfg\" MutatingWebhookConfiguration\n    client.go:339: [debug] mutatingwebhookconfigurations.admissionregistration.k8s.io \"nsm-admission-webhook-cfg\" not found\n    client.go:339: [debug] mutatingwebhookconfigurations.admissionregistration.k8s.io \"kubeslice-mutating-webhook-configuration\" not found\n    uninstall.go:144: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: failed pre-install: timed out waiting for the condition\n    helm.go:84: [debug] failed pre-install: timed out waiting for the condition\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:361\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1664279399000,"stop":1664279811031,"duration":412031}},{"uid":"aca9d5bf83448509","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000e060>: {\n        Underlying: <*exec.ExitError | 0xc000136000>{\n            ProcessState: {\n                pid: 6125,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 3, Usec: 447292},\n                    Stime: {Sec: 0, Usec: 464158},\n                    Maxrss: 83736,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 16009,\n                    Majflt: 2,\n                    Nswap: 0,\n                    Inblock: 392,\n                    Oublock: 14352,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 48075,\n                    Nivcsw: 8147,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.3.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:165: [debug] Clearing discovery cache\",\n                        \"wait.go:48: [debug] beginning wait for 4 resources with timeout of 1m0s\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 44 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 44 resources with timeout of 5m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.3.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:165: [debug] Clearing discovery cache\n    wait.go:48: [debug] beginning wait for 4 resources with timeout of 1m0s\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 44 resource(s)\n    wait.go:48: [debug] beginning wait for 44 resources with timeout of 5m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:310: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"jaeger\" Service\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-svc\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:310: [debug] Starting delete for \"jaeger\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook\" Deployment\n    client.go:310: [debug] Starting delete for \"prefix-service\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsm-kernel-forwarder\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"networkservices.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkserviceendpoints.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservicemanagers.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-cfg\" MutatingWebhookConfiguration\n    uninstall.go:144: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1664277957000,"stop":1664278353603,"duration":396603}},{"uid":"f978c2b823451c2f","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000616018>: {\n        Underlying: <*exec.ExitError | 0xc000770000>{\n            ProcessState: {\n                pid: 6116,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 3, Usec: 962358},\n                    Stime: {Sec: 0, Usec: 691305},\n                    Maxrss: 83816,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 11772,\n                    Majflt: 16,\n                    Nswap: 0,\n                    Inblock: 2080,\n                    Oublock: 14352,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 53565,\n                    Nivcsw: 11023,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.3.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:165: [debug] Clearing discovery cache\",\n                        \"wait.go:48: [debug] beginning wait for 4 resources with timeout of 1m0s\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 44 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 44 resources with timeout of 5m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-o...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.3.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:165: [debug] Clearing discovery cache\n    wait.go:48: [debug] beginning wait for 4 resources with timeout of 1m0s\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 44 resource(s)\n    wait.go:48: [debug] beginning wait for 44 resources with timeout of 5m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:310: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"jaeger\" Service\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-svc\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:310: [debug] Starting delete for \"jaeger\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook\" Deployment\n    client.go:310: [debug] Starting delete for \"prefix-service\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsm-kernel-forwarder\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"networkservices.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkserviceendpoints.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservicemanagers.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-cfg\" MutatingWebhookConfiguration\n    uninstall.go:144: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1664275460000,"stop":1664275869708,"duration":409708}},{"uid":"421242b59778158e","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004be1b0>: {\n        Underlying: <*exec.ExitError | 0xc00014c020>{\n            ProcessState: {\n                pid: 6055,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 3, Usec: 66299},\n                    Stime: {Sec: 0, Usec: 542367},\n                    Maxrss: 85004,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 6983,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12848,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 41811,\n                    Nivcsw: 9445,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-0.3.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 27 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 27 resources with timeout of 5m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-0.3.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 27 resource(s)\n    wait.go:48: [debug] beginning wait for 27 resources with timeout of 5m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-controller\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-manager-metrics-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-controller-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-editor-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"workersliceconfigs.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workerslicegateways.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"clusters.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"projects.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"serviceexportconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"sliceconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"sliceqosconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workerserviceimports.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-serving-cert\" Certificate\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-selfsigned-issuer\" Issuer\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-validating-webhook-configuration\" ValidatingWebhookConfiguration\n    uninstall.go:144: [debug] purge requested for kubeslice-controller\n    Error: INSTALLATION FAILED: release kubeslice-controller failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-controller failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1664270759000,"stop":1664271147896,"duration":388896}},{"uid":"a8048678b22df572","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004de060>: {\n        Underlying: <*exec.ExitError | 0xc00040e000>{\n            ProcessState: {\n                pid: 6133,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 3, Usec: 407450},\n                    Stime: {Sec: 0, Usec: 695052},\n                    Maxrss: 82004,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 12444,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13888,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 50936,\n                    Nivcsw: 9474,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.3.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:165: [debug] Clearing discovery cache\",\n                        \"wait.go:48: [debug] beginning wait for 4 resources with timeout of 1m0s\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 44 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 44 resources with timeout of 5m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.3.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:165: [debug] Clearing discovery cache\n    wait.go:48: [debug] beginning wait for 4 resources with timeout of 1m0s\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 44 resource(s)\n    wait.go:48: [debug] beginning wait for 44 resources with timeout of 5m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:310: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"jaeger\" Service\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-svc\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:310: [debug] Starting delete for \"jaeger\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook\" Deployment\n    client.go:310: [debug] Starting delete for \"prefix-service\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsm-kernel-forwarder\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"networkservices.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkserviceendpoints.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservicemanagers.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-cfg\" MutatingWebhookConfiguration\n    uninstall.go:144: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1664261433000,"stop":1664261836559,"duration":403559}},{"uid":"18812e9c434b5252","status":"passed","time":{"start":1664246558000,"stop":1664246773527,"duration":215527}},{"uid":"73fcebf6848babb5","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000528030>: {\n        Underlying: <*exec.ExitError | 0xc00002e000>{\n            ProcessState: {\n                pid: 6124,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 3, Usec: 750549},\n                    Stime: {Sec: 0, Usec: 559927},\n                    Maxrss: 83684,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 10135,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14352,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 51406,\n                    Nivcsw: 9424,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.3.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:165: [debug] Clearing discovery cache\",\n                        \"wait.go:48: [debug] beginning wait for 4 resources with timeout of 1m0s\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 44 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 44 resources with timeout of 5m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubesl...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.3.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:165: [debug] Clearing discovery cache\n    wait.go:48: [debug] beginning wait for 4 resources with timeout of 1m0s\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 44 resource(s)\n    wait.go:48: [debug] beginning wait for 44 resources with timeout of 5m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:310: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"jaeger\" Service\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-svc\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:310: [debug] Starting delete for \"jaeger\" Deployment\n    client.go:310: [debug] Starting delete for \"prefix-service\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsm-kernel-forwarder\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"networkserviceendpoints.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservices.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservicemanagers.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-cfg\" MutatingWebhookConfiguration\n    uninstall.go:144: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1664218168000,"stop":1664218589426,"duration":421426}},{"uid":"8dd87a37dad455b3","status":"passed","time":{"start":1664162065000,"stop":1664162257886,"duration":192886}},{"uid":"6442b24b153c13db","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004ea018>: {\n        Underlying: <*exec.ExitError | 0xc00071c000>{\n            ProcessState: {\n                pid: 6105,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 5, Usec: 511827},\n                    Stime: {Sec: 0, Usec: 864099},\n                    Maxrss: 85068,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 12415,\n                    Majflt: 5,\n                    Nswap: 0,\n                    Inblock: 1024,\n                    Oublock: 14352,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 56138,\n                    Nivcsw: 11760,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.3.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:165: [debug] Clearing discovery cache\",\n                        \"wait.go:48: [debug] beginning wait for 4 resources with timeout of 1m0s\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 44 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 44 resources with timeout of 5m0s\",\n                        \"ready.go:291: [debug] DaemonSet is not ready: kubeslice-system/kubeslice-netop. 0 out of 1 expected pods have been scheduled\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not rea...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.3.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:165: [debug] Clearing discovery cache\n    wait.go:48: [debug] beginning wait for 4 resources with timeout of 1m0s\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 44 resource(s)\n    wait.go:48: [debug] beginning wait for 44 resources with timeout of 5m0s\n    ready.go:291: [debug] DaemonSet is not ready: kubeslice-system/kubeslice-netop. 0 out of 1 expected pods have been scheduled\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:310: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"jaeger\" Service\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-svc\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:310: [debug] Starting delete for \"jaeger\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook\" Deployment\n    client.go:310: [debug] Starting delete for \"prefix-service\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsm-kernel-forwarder\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"networkservices.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkserviceendpoints.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservicemanagers.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-cfg\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    uninstall.go:144: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1664106114000,"stop":1664106540660,"duration":426660}},{"uid":"9f14ed48225e01ec","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00068c018>: {\n        Underlying: <*exec.ExitError | 0xc0006dc000>{\n            ProcessState: {\n                pid: 6109,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 4, Usec: 112879},\n                    Stime: {Sec: 0, Usec: 515669},\n                    Maxrss: 82968,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 8549,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14352,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 49479,\n                    Nivcsw: 9602,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.3.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:165: [debug] Clearing discovery cache\",\n                        \"wait.go:48: [debug] beginning wait for 4 resources with timeout of 1m0s\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 44 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 44 resources with timeout of 5m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubesli...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.3.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:165: [debug] Clearing discovery cache\n    wait.go:48: [debug] beginning wait for 4 resources with timeout of 1m0s\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 44 resource(s)\n    wait.go:48: [debug] beginning wait for 44 resources with timeout of 5m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:310: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-svc\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:310: [debug] Starting delete for \"jaeger\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:310: [debug] Starting delete for \"jaeger\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook\" Deployment\n    client.go:310: [debug] Starting delete for \"prefix-service\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsm-kernel-forwarder\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:310: [debug] Starting delete for \"networkservices.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkserviceendpoints.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservicemanagers.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-cfg\" MutatingWebhookConfiguration\n    uninstall.go:144: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1664104103000,"stop":1664104520648,"duration":417648}},{"uid":"fc3713835f9958b1","status":"passed","time":{"start":1664093039000,"stop":1664093219036,"duration":180036}},{"uid":"845fc4333d817e41","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004181b0>: {\n        Underlying: <*exec.ExitError | 0xc0003e0000>{\n            ProcessState: {\n                pid: 6066,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 4, Usec: 821467},\n                    Stime: {Sec: 0, Usec: 923259},\n                    Maxrss: 83756,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 14534,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14352,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 55671,\n                    Nivcsw: 11582,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.3.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:165: [debug] Clearing discovery cache\",\n                        \"wait.go:48: [debug] beginning wait for 4 resources with timeout of 1m0s\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 44 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 44 resources with timeout of 5m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admissio...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.3.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:165: [debug] Clearing discovery cache\n    wait.go:48: [debug] beginning wait for 4 resources with timeout of 1m0s\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 44 resource(s)\n    wait.go:48: [debug] beginning wait for 44 resources with timeout of 5m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:310: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-svc\" Service\n    client.go:310: [debug] Starting delete for \"jaeger\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:310: [debug] Starting delete for \"jaeger\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook\" Deployment\n    client.go:310: [debug] Starting delete for \"prefix-service\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsm-kernel-forwarder\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"networkservices.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkserviceendpoints.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservicemanagers.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-cfg\" MutatingWebhookConfiguration\n    uninstall.go:144: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1663933075000,"stop":1663933494585,"duration":419585}},{"uid":"1a52c21a860c719","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000e078>: {\n        Underlying: <*exec.ExitError | 0xc000486020>{\n            ProcessState: {\n                pid: 6077,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 2, Usec: 431551},\n                    Stime: {Sec: 0, Usec: 343636},\n                    Maxrss: 80852,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 9718,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12848,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 35966,\n                    Nivcsw: 8186,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-0.3.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 27 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 27 resources with timeout of 5m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\",\n                        ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-controller-0.3.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 27 resource(s)\n    wait.go:48: [debug] beginning wait for 27 resources with timeout of 5m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-controller/kubeslice-controller-manager. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-controller\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-manager-metrics-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-controller-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-editor-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"workersliceconfigs.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workerslicegateways.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"clusters.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"projects.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"serviceexportconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"sliceconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"sliceqosconfigs.controller.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"workerserviceimports.worker.kubeslice.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-ovpn-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-serving-cert\" Certificate\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-selfsigned-issuer\" Issuer\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-validating-webhook-configuration\" ValidatingWebhookConfiguration\n    uninstall.go:144: [debug] purge requested for kubeslice-controller\n    Error: INSTALLATION FAILED: release kubeslice-controller failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-controller failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1663918810000,"stop":1663919192101,"duration":382101}},{"uid":"f1014b015fe25c2a","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00043a060>: {\n        Underlying: <*exec.ExitError | 0xc0005c2000>{\n            ProcessState: {\n                pid: 6095,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 3, Usec: 297912},\n                    Stime: {Sec: 0, Usec: 401700},\n                    Maxrss: 81556,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 10842,\n                    Majflt: 5,\n                    Nswap: 0,\n                    Inblock: 784,\n                    Oublock: 13888,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 48452,\n                    Nivcsw: 9633,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.3.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:165: [debug] Clearing discovery cache\",\n                        \"wait.go:48: [debug] beginning wait for 4 resources with timeout of 1m0s\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 44 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 44 resources with timeout of 5m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.3.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:165: [debug] Clearing discovery cache\n    wait.go:48: [debug] beginning wait for 4 resources with timeout of 1m0s\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 44 resource(s)\n    wait.go:48: [debug] beginning wait for 44 resources with timeout of 5m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:310: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-svc\" Service\n    client.go:310: [debug] Starting delete for \"jaeger\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:310: [debug] Starting delete for \"jaeger\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook\" Deployment\n    client.go:310: [debug] Starting delete for \"prefix-service\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsm-kernel-forwarder\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"networkservices.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkserviceendpoints.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservicemanagers.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:310: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-cfg\" MutatingWebhookConfiguration\n    uninstall.go:144: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1663912526000,"stop":1663912942256,"duration":416256}},{"uid":"f1f72c64d82605de","status":"passed","time":{"start":1663842833000,"stop":1663843063957,"duration":230957}},{"uid":"2e1337b980d5293a","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000708018>: {\n        Underlying: <*exec.ExitError | 0xc0006fe000>{\n            ProcessState: {\n                pid: 6077,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 3, Usec: 888013},\n                    Stime: {Sec: 0, Usec: 446845},\n                    Maxrss: 83812,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 12289,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14352,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 51405,\n                    Nivcsw: 9370,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.3.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:165: [debug] Clearing discovery cache\",\n                        \"wait.go:48: [debug] beginning wait for 4 resources with timeout of 1m0s\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 44 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 44 resources with timeout of 5m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is no...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.3.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:165: [debug] Clearing discovery cache\n    wait.go:48: [debug] beginning wait for 4 resources with timeout of 1m0s\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 44 resource(s)\n    wait.go:48: [debug] beginning wait for 44 resources with timeout of 5m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:310: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"jaeger\" Service\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-svc\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:310: [debug] Starting delete for \"jaeger\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook\" Deployment\n    client.go:310: [debug] Starting delete for \"prefix-service\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsm-kernel-forwarder\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:310: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"networkservices.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkserviceendpoints.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservicemanagers.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-cfg\" MutatingWebhookConfiguration\n    uninstall.go:144: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1663838978000,"stop":1663839373762,"duration":395762}},{"uid":"2c7bf6e23468e899","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000e060>: {\n        Underlying: <*exec.ExitError | 0xc000698000>{\n            ProcessState: {\n                pid: 6087,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 4, Usec: 582052},\n                    Stime: {Sec: 0, Usec: 782396},\n                    Maxrss: 83920,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 10095,\n                    Majflt: 2,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14352,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 52826,\n                    Nivcsw: 11398,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.3.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:165: [debug] Clearing discovery cache\",\n                        \"wait.go:48: [debug] beginning wait for 4 resources with timeout of 1m0s\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 44 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 44 resources with timeout of 5m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.3.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:165: [debug] Clearing discovery cache\n    wait.go:48: [debug] beginning wait for 4 resources with timeout of 1m0s\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 44 resource(s)\n    wait.go:48: [debug] beginning wait for 44 resources with timeout of 5m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:310: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-svc\" Service\n    client.go:310: [debug] Starting delete for \"jaeger\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook\" Deployment\n    client.go:310: [debug] Starting delete for \"jaeger\" Deployment\n    client.go:310: [debug] Starting delete for \"prefix-service\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsm-kernel-forwarder\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"networkserviceendpoints.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservices.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservicemanagers.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-cfg\" MutatingWebhookConfiguration\n    uninstall.go:144: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1663838505000,"stop":1663838904931,"duration":399931}},{"uid":"4c89cff395eec49b","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00032c030>: {\n        Underlying: <*exec.ExitError | 0xc0006a0000>{\n            ProcessState: {\n                pid: 6133,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 3, Usec: 308179},\n                    Stime: {Sec: 0, Usec: 552814},\n                    Maxrss: 83360,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 11395,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13888,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 48803,\n                    Nivcsw: 8837,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.3.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:165: [debug] Clearing discovery cache\",\n                        \"wait.go:48: [debug] beginning wait for 4 resources with timeout of 1m0s\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 44 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 44 resources with timeout of 5m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.3.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:165: [debug] Clearing discovery cache\n    wait.go:48: [debug] beginning wait for 4 resources with timeout of 1m0s\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 44 resource(s)\n    wait.go:48: [debug] beginning wait for 44 resources with timeout of 5m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:310: [debug] Starting delete for \"jaeger\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-svc\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:310: [debug] Starting delete for \"jaeger\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook\" Deployment\n    client.go:310: [debug] Starting delete for \"prefix-service\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsm-kernel-forwarder\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"networkservices.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkserviceendpoints.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservicemanagers.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:310: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-cfg\" MutatingWebhookConfiguration\n    uninstall.go:144: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1663836494000,"stop":1663836885756,"duration":391756}},{"uid":"a4b499f5c4190dc5","status":"passed","time":{"start":1663828380000,"stop":1663828551471,"duration":171471}}]},"7cdab5bfb04a9ce940a590b14c9ad3e1":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"7371f911749c168c","status":"passed","time":{"start":1663839602434,"stop":1663839612469,"duration":10035}}]},"Worker Suite:Worker Suite#[It] Slice Config Negative Scenario tests Slice Config Negative Scenario Update Should fail while updating sliceconfig with bandwithguaranteedkbps more than bandwidthceilingkbps in qosprofiledetailsnvalid ":{"statistic":{"failed":0,"broken":0,"skipped":14,"passed":42,"unknown":0,"total":56},"items":[{"uid":"16bade6d12d0e4bd","status":"passed","time":{"start":1664094053000,"stop":1664094053562,"duration":562}},{"uid":"be8e06484cb43b9","status":"passed","time":{"start":1663843963000,"stop":1663843963654,"duration":654}},{"uid":"40d9eed77cac5b65","status":"passed","time":{"start":1663829327000,"stop":1663829327713,"duration":713}},{"uid":"4b8e4ca0aa7b519d","status":"passed","time":{"start":1663343132000,"stop":1663343132582,"duration":582}},{"uid":"74b39d627257c6d0","status":"passed","time":{"start":1663342947000,"stop":1663342947538,"duration":538}},{"uid":"5738291c87b108ad","status":"passed","time":{"start":1663044723000,"stop":1663044723470,"duration":470}},{"uid":"6d5ed4840beda05e","status":"skipped","statusDetails":"skipped","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"3833ac77775def48","status":"passed","time":{"start":1662348908000,"stop":1662348908643,"duration":643}},{"uid":"ab09f0054c2e7f64","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"378d2afbddc41a40","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"36520b0e20999d4c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"3f7f23d8c95e1dab","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"23881546fc7d7041","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"e1d0ec1bdb70c6de","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"2e8b3f8a6b1a08d9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"510058c78723a37c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"37ecdcf4f925aa67","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"12168e5c90449c47","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"508565ea32d74e03","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"b4e9dc4b51d6d55e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}}]},"b45b50eedaecd101650056f4b1f4acdf":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"fb50891a7e2158e5","status":"passed","time":{"start":1663840337103,"stop":1663840347196,"duration":10093}}]},"555d99582e26ea7a2495c47b60c032b7":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"97930069b623ab24","status":"passed","time":{"start":1663840764507,"stop":1663840775923,"duration":11416}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Update Should update successfully when Project Applied while removing a service account name in Write users":{"statistic":{"failed":26,"broken":0,"skipped":0,"passed":190,"unknown":0,"total":216},"items":[{"uid":"f23130f65d3426e8","status":"passed","time":{"start":1664275319000,"stop":1664275323814,"duration":4814}},{"uid":"9507d031cbdae7c3","status":"passed","time":{"start":1664245203000,"stop":1664245209038,"duration":6038}},{"uid":"cab959d80d5c8d86","status":"passed","time":{"start":1664160732000,"stop":1664160736899,"duration":4899}},{"uid":"6f37c0b54694cdf6","status":"passed","time":{"start":1664091725000,"stop":1664091729770,"duration":4770}},{"uid":"8c032e5e9ed7ce5","status":"passed","time":{"start":1663841627000,"stop":1663841632472,"duration":5472}},{"uid":"3527c4e35d3a46e","status":"passed","time":{"start":1663827045000,"stop":1663827049769,"duration":4769}},{"uid":"59f131e7a055382e","status":"passed","time":{"start":1663767974000,"stop":1663767978723,"duration":4723}},{"uid":"c943322fa870168d","status":"passed","time":{"start":1663669677000,"stop":1663669681820,"duration":4820}},{"uid":"757d2d1efb2c167f","status":"passed","time":{"start":1663665323000,"stop":1663665328294,"duration":5294}},{"uid":"e52fc54bf0f8501d","status":"passed","time":{"start":1663665170000,"stop":1663665174786,"duration":4786}},{"uid":"201b2e787daf0bd0","status":"passed","time":{"start":1663659067000,"stop":1663659071955,"duration":4955}},{"uid":"bd593ad95f2ef416","status":"passed","time":{"start":1663656809000,"stop":1663656813881,"duration":4881}},{"uid":"d7bc0663147239e5","status":"passed","time":{"start":1663583877000,"stop":1663583882249,"duration":5249}},{"uid":"b4373e423deaa315","status":"passed","time":{"start":1663340950000,"stop":1663340955780,"duration":5780}},{"uid":"1c0b79c0fb5da4dc","status":"passed","time":{"start":1663340650000,"stop":1663340654778,"duration":4778}},{"uid":"89da1c1091ae08a7","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <bool>: false\nto be true","time":{"start":1663308473000,"stop":1663308539742,"duration":66742}},{"uid":"9201243da349fe74","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0006efe48>: {\n        Underlying: <*exec.ExitError | 0xc000695a80>{\n            ProcessState: {\n                pid: 6217,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 182771},\n                    Stime: {Sec: 0, Usec: 55625},\n                    Maxrss: 87600,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3693,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13344,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 351,\n                    Nivcsw: 368,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Write%20users3094949081\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.26.65:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Write%20users3094949081\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.26.65:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Write%20users3094949081\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.26.65:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Write%20users3094949081\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.26.65:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Write%20users3094949081\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.26.65:443: connect: connection refused\noccurred","time":{"start":1663304356000,"stop":1663304356290,"duration":290}},{"uid":"23ead4df81377cc5","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00047ad38>: {\n        Underlying: <*exec.ExitError | 0xc000751500>{\n            ProcessState: {\n                pid: 7551,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 179928},\n                    Stime: {Sec: 0, Usec: 27380},\n                    Maxrss: 76824,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 5139,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 238,\n                    Nivcsw: 236,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Write%20users492055572\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.179.221:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Write%20users492055572\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.179.221:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Write%20users492055572\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.179.221:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Write%20users492055572\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.179.221:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Write%20users492055572\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.179.221:443: connect: connection refused\noccurred","time":{"start":1663302519000,"stop":1663302519256,"duration":256}},{"uid":"12ee02c8d1578e2c","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000639590>: {\n        Underlying: <*exec.ExitError | 0xc00045d4e0>{\n            ProcessState: {\n                pid: 7187,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 208513},\n                    Stime: {Sec: 0, Usec: 40098},\n                    Maxrss: 86788,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2769,\n                    Majflt: 2,\n                    Nswap: 0,\n                    Inblock: 512,\n                    Oublock: 13344,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 412,\n                    Nivcsw: 377,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Write%20users3311215591\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.55.244:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Write%20users3311215591\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.55.244:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Write%20users3311215591\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.55.244:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Write%20users3311215591\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.55.244:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Write%20users3311215591\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.55.244:443: connect: connection refused\noccurred","time":{"start":1663264756000,"stop":1663264756309,"duration":309}},{"uid":"3c9d48b6e9d18e7f","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004e0f90>: {\n        Underlying: <*exec.ExitError | 0xc0006bde60>{\n            ProcessState: {\n                pid: 7229,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 181896},\n                    Stime: {Sec: 0, Usec: 38701},\n                    Maxrss: 88664,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3528,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13344,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 472,\n                    Nivcsw: 303,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Write%20users1064650058\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.221.241:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Write%20users1064650058\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.221.241:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Write%20users1064650058\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.221.241:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Write%20users1064650058\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.221.241:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Write%20users1064650058\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.221.241:443: connect: connection refused\noccurred","time":{"start":1663228190000,"stop":1663228190287,"duration":287}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Update Should Update successfullyy when Project is Applied with valid service account name in Read users":{"statistic":{"failed":26,"broken":0,"skipped":0,"passed":190,"unknown":0,"total":216},"items":[{"uid":"4c278baa5d4006c0","status":"passed","time":{"start":1664275319000,"stop":1664275327380,"duration":8380}},{"uid":"917c98f675ce0b11","status":"passed","time":{"start":1664245203000,"stop":1664245212462,"duration":9462}},{"uid":"435045127cce898a","status":"passed","time":{"start":1664160732000,"stop":1664160740913,"duration":8913}},{"uid":"f333dc4e6838ea0b","status":"passed","time":{"start":1664091725000,"stop":1664091733195,"duration":8195}},{"uid":"6b2cc518f8c719bc","status":"passed","time":{"start":1663841627000,"stop":1663841636625,"duration":9625}},{"uid":"b17222e098a05838","status":"passed","time":{"start":1663827045000,"stop":1663827053225,"duration":8225}},{"uid":"674473d62d38b3d0","status":"passed","time":{"start":1663767974000,"stop":1663767982297,"duration":8297}},{"uid":"c8ca6db28ee14662","status":"passed","time":{"start":1663669677000,"stop":1663669685230,"duration":8230}},{"uid":"5d7b297f2ce4e189","status":"passed","time":{"start":1663665323000,"stop":1663665332694,"duration":9694}},{"uid":"9b3d1083225bcb4a","status":"passed","time":{"start":1663665170000,"stop":1663665179062,"duration":9062}},{"uid":"fe277a3295bf8541","status":"passed","time":{"start":1663659067000,"stop":1663659075469,"duration":8469}},{"uid":"357fb83dc3346d79","status":"passed","time":{"start":1663656809000,"stop":1663656817312,"duration":8312}},{"uid":"e5cab689c1fd437a","status":"passed","time":{"start":1663583877000,"stop":1663583885611,"duration":8611}},{"uid":"89b1c89a1e74823c","status":"passed","time":{"start":1663340950000,"stop":1663340958780,"duration":8780}},{"uid":"3e03c4c6236bc51e","status":"passed","time":{"start":1663340650000,"stop":1663340658162,"duration":8162}},{"uid":"5427c14e83bfa5a9","status":"failed","statusDetails":"Timed out after 60.069s.\nExpected\n    <bool>: false\nto be true","time":{"start":1663308473000,"stop":1663308545932,"duration":72932}},{"uid":"ca54405468b18601","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000824318>: {\n        Underlying: <*exec.ExitError | 0xc0003f70e0>{\n            ProcessState: {\n                pid: 6187,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 199673},\n                    Stime: {Sec: 0, Usec: 35941},\n                    Maxrss: 82124,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3604,\n                    Majflt: 1,\n                    Nswap: 0,\n                    Inblock: 256,\n                    Oublock: 13344,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 268,\n                    Nivcsw: 554,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20Update%20successfullyy%20when%20Project%20is%20Applied%20with%20valid%20service%20account%20name%20in%20Read%20users1147865679\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.26.65:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20Update%20successfullyy%20when%20Project%20is%20Applied%20with%20valid%20service%20account%20name%20in%20Read%20users1147865679\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.26.65:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20Update%20successfullyy%20when%20Project%20is%20Applied%20with%20valid%20service%20account%20name%20in%20Read%20users1147865679\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.26.65:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20Update%20successfullyy%20when%20Project%20is%20Applied%20with%20valid%20service%20account%20name%20in%20Read%20users1147865679\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.26.65:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Update%20Should%20Update%20successfullyy%20when%20Project%20is%20Applied%20with%20valid%20service%20account%20name%20in%20Read%20users1147865679\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.26.65:443: connect: connection refused\noccurred","time":{"start":1663304356000,"stop":1663304356304,"duration":304}},{"uid":"20bf7ae30c893b09","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000fea8>: {\n        Underlying: <*exec.ExitError | 0xc000736da0>{\n            ProcessState: {\n                pid: 7525,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 173339},\n                    Stime: {Sec: 0, Usec: 32249},\n                    Maxrss: 82784,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 7530,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 295,\n                    Nivcsw: 322,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20Update%20successfullyy%20when%20Project%20is%20Applied%20with%20valid%20service%20account%20name%20in%20Read%20users168651302\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.179.221:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20Update%20successfullyy%20when%20Project%20is%20Applied%20with%20valid%20service%20account%20name%20in%20Read%20users168651302\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.179.221:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20Update%20successfullyy%20when%20Project%20is%20Applied%20with%20valid%20service%20account%20name%20in%20Read%20users168651302\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.179.221:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20Update%20successfullyy%20when%20Project%20is%20Applied%20with%20valid%20service%20account%20name%20in%20Read%20users168651302\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.179.221:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Update%20Should%20Update%20successfullyy%20when%20Project%20is%20Applied%20with%20valid%20service%20account%20name%20in%20Read%20users168651302\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.179.221:443: connect: connection refused\noccurred","time":{"start":1663302519000,"stop":1663302519265,"duration":265}},{"uid":"60cb3411ec369f6f","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000638d68>: {\n        Underlying: <*exec.ExitError | 0xc00014d2e0>{\n            ProcessState: {\n                pid: 7158,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 208212},\n                    Stime: {Sec: 0, Usec: 48991},\n                    Maxrss: 92488,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4201,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13344,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 338,\n                    Nivcsw: 295,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20Update%20successfullyy%20when%20Project%20is%20Applied%20with%20valid%20service%20account%20name%20in%20Read%20users1442882761\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.55.244:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20Update%20successfullyy%20when%20Project%20is%20Applied%20with%20valid%20service%20account%20name%20in%20Read%20users1442882761\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.55.244:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20Update%20successfullyy%20when%20Project%20is%20Applied%20with%20valid%20service%20account%20name%20in%20Read%20users1442882761\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.55.244:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20Update%20successfullyy%20when%20Project%20is%20Applied%20with%20valid%20service%20account%20name%20in%20Read%20users1442882761\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.55.244:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Update%20Should%20Update%20successfullyy%20when%20Project%20is%20Applied%20with%20valid%20service%20account%20name%20in%20Read%20users1442882761\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.55.244:443: connect: connection refused\noccurred","time":{"start":1663264756000,"stop":1663264756330,"duration":330}},{"uid":"8ea0d59a9ac606d9","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00084a750>: {\n        Underlying: <*exec.ExitError | 0xc000653e60>{\n            ProcessState: {\n                pid: 7202,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 193094},\n                    Stime: {Sec: 0, Usec: 65734},\n                    Maxrss: 74956,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3902,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13344,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 372,\n                    Nivcsw: 389,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20Update%20successfullyy%20when%20Project%20is%20Applied%20with%20valid%20service%20account%20name%20in%20Read%20users1950204206\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.221.241:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20Update%20successfullyy%20when%20Project%20is%20Applied%20with%20valid%20service%20account%20name%20in%20Read%20users1950204206\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.221.241:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20Update%20successfullyy%20when%20Project%20is%20Applied%20with%20valid%20service%20account%20name%20in%20Read%20users1950204206\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.221.241:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20Update%20successfullyy%20when%20Project%20is%20Applied%20with%20valid%20service%20account%20name%20in%20Read%20users1950204206\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.221.241:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Update%20Should%20Update%20successfullyy%20when%20Project%20is%20Applied%20with%20valid%20service%20account%20name%20in%20Read%20users1950204206\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.221.241:443: connect: connection refused\noccurred","time":{"start":1663228190000,"stop":1663228190312,"duration":312}}]},"Worker Suite:Worker Suite#[It] Attach/Deattach Scenario Attach cluster from slice  when Deattach cluster from slice  should have gateway pod running on attached cluster":{"statistic":{"failed":0,"broken":0,"skipped":36,"passed":11,"unknown":0,"total":47},"items":[{"uid":"46447fb224ff7dcf","status":"passed","time":{"start":1664094053000,"stop":1664094053201,"duration":201}},{"uid":"4010b2a116295bdb","status":"passed","time":{"start":1663843963000,"stop":1663843963251,"duration":251}},{"uid":"89ae893f111f2c88","status":"passed","time":{"start":1663829327000,"stop":1663829327195,"duration":195}},{"uid":"d172385471bf9025","status":"passed","time":{"start":1663343132000,"stop":1663343132197,"duration":197}},{"uid":"8891f058d86a4812","status":"passed","time":{"start":1663342947000,"stop":1663342947244,"duration":244}},{"uid":"971d4a596823479d","status":"passed","time":{"start":1663044723000,"stop":1663044723189,"duration":189}},{"uid":"ee7e88f722bd4632","status":"passed","time":{"start":1663044069000,"stop":1663044069234,"duration":234}},{"uid":"7bc0f4d96c4fbae9","status":"passed","time":{"start":1662348908000,"stop":1662348908195,"duration":195}},{"uid":"6b9243d8d3936367","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"ec1dfa5271eb13ba","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661754870000,"stop":1661754870000,"duration":0}},{"uid":"ca05c4cbf936e973","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"bd63a0b728626443","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"b6ef3d008bfc49da","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"dfe31f6d5d983f65","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"6883dcb4a110964c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"5d96117f97af06ad","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1661572479000,"stop":1661572479000,"duration":0}},{"uid":"4982514423ed9821","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660832281000,"stop":1660832281000,"duration":0}},{"uid":"f88d39f93ca0fa8f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660808797000,"stop":1660808797000,"duration":0}},{"uid":"d3425fc9e6ffe8d5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660299182000,"stop":1660299182000,"duration":0}},{"uid":"1e84f89bc917f3cf","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Intracluster slice tests Iperf connectivity should onboard deployment in application Namespaces without slice annotations":{"statistic":{"failed":49,"broken":0,"skipped":1,"passed":6,"unknown":0,"total":56},"items":[{"uid":"279f1b940904d939","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 1\nto equal\n    <int>: 2","time":{"start":1664094053000,"stop":1664094243162,"duration":190162}},{"uid":"12fa9df4f66e7480","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 1\nto equal\n    <int>: 2","time":{"start":1663843963000,"stop":1663844153967,"duration":190967}},{"uid":"40036dcb83601ae0","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 1\nto equal\n    <int>: 2","time":{"start":1663829327000,"stop":1663829517372,"duration":190372}},{"uid":"c6e08e66f0cd79d8","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 1\nto equal\n    <int>: 2","time":{"start":1663343132000,"stop":1663343322038,"duration":190038}},{"uid":"c2bf1fc4fb2f6dd6","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 1\nto equal\n    <int>: 2","time":{"start":1663342947000,"stop":1663343137587,"duration":190587}},{"uid":"c3a215ddf902787d","status":"skipped","statusDetails":"skipped","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"53a61ff2f72fe091","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 1\nto equal\n    <int>: 2","time":{"start":1663044069000,"stop":1663044259563,"duration":190563}},{"uid":"bb1d280d25722e47","status":"failed","statusDetails":"Unexpected error:\n    <*errors.StatusError | 0xc000929a40>: {\n        ErrStatus: {\n            TypeMeta: {Kind: \"\", APIVersion: \"\"},\n            ListMeta: {\n                SelfLink: \"\",\n                ResourceVersion: \"\",\n                Continue: \"\",\n                RemainingItemCount: nil,\n            },\n            Status: \"Failure\",\n            Message: \"namespaces \\\"appns1\\\" already exists\",\n            Reason: \"AlreadyExists\",\n            Details: {Name: \"appns1\", Group: \"\", Kind: \"namespaces\", UID: \"\", Causes: nil, RetryAfterSeconds: 0},\n            Code: 409,\n        },\n    }\n    namespaces \"appns1\" already exists\noccurred","time":{"start":1662348908000,"stop":1662348912393,"duration":4393}},{"uid":"62187bf11a4b9605","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000469488>: {\n        Underlying: <*exec.ExitError | 0xc00065c2e0>{\n            ProcessState: {\n                pid: 7266,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 254055},\n                    Stime: {Sec: 0, Usec: 30794},\n                    Maxrss: 87476,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3104,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 560,\n                    Nivcsw: 450,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\noccurred","time":{"start":1661844205000,"stop":1661844205540,"duration":540}},{"uid":"fc2742c06943e0b5","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004bf818>: {\n        Underlying: <*exec.ExitError | 0xc0006589e0>{\n            ProcessState: {\n                pid: 7288,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 177560},\n                    Stime: {Sec: 0, Usec: 36991},\n                    Maxrss: 81984,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3949,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 425,\n                    Nivcsw: 375,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661754870000,"stop":1661754870436,"duration":436}},{"uid":"feb8f9d1f9d8f6b7","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0006f0228>: {\n        Underlying: <*exec.ExitError | 0xc0006c5760>{\n            ProcessState: {\n                pid: 7281,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 204410},\n                    Stime: {Sec: 0, Usec: 42585},\n                    Maxrss: 74204,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4302,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 396,\n                    Nivcsw: 373,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661611962000,"stop":1661611962490,"duration":490}},{"uid":"b34b29b681f555a4","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00090d1a0>: {\n        Underlying: <*exec.ExitError | 0xc00096b2c0>{\n            ProcessState: {\n                pid: 7242,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 199888},\n                    Stime: {Sec: 0, Usec: 32634},\n                    Maxrss: 82008,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 5590,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 373,\n                    Nivcsw: 424,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661585077000,"stop":1661585077457,"duration":457}},{"uid":"5cc79a559af9c8dd","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000362030>: {\n        Underlying: <*exec.ExitError | 0xc000824040>{\n            ProcessState: {\n                pid: 6030,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 183740},\n                    Stime: {Sec: 0, Usec: 43937},\n                    Maxrss: 75996,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3781,\n                    Majflt: 1,\n                    Nswap: 0,\n                    Inblock: 240,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 235,\n                    Nivcsw: 438,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661584447000,"stop":1661584447536,"duration":536}},{"uid":"4f8b63b63beaafc9","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0008946c0>: {\n        Underlying: <*exec.ExitError | 0xc00066efa0>{\n            ProcessState: {\n                pid: 7208,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 186074},\n                    Stime: {Sec: 0, Usec: 47508},\n                    Maxrss: 83684,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3822,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 12880,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 381,\n                    Nivcsw: 458,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661580045000,"stop":1661580045439,"duration":439}},{"uid":"55390aebaa3e14f3","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000866570>: {\n        Underlying: <*exec.ExitError | 0xc000858e40>{\n            ProcessState: {\n                pid: 7108,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 236135},\n                    Stime: {Sec: 0, Usec: 66283},\n                    Maxrss: 88660,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 8113,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 280,\n                    Nivcsw: 380,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661578505000,"stop":1661578505593,"duration":593}},{"uid":"7b751b00662ac37d","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004c8ab0>: {\n        Underlying: <*exec.ExitError | 0xc00071df00>{\n            ProcessState: {\n                pid: 7121,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 260887},\n                    Stime: {Sec: 0, Usec: 66195},\n                    Maxrss: 94132,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 9700,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 595,\n                    Nivcsw: 636,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1661572479000,"stop":1661572479862,"duration":862}},{"uid":"1f6dc4f746746f60","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0008bbc08>: {\n        Underlying: <*exec.ExitError | 0xc00050fbe0>{\n            ProcessState: {\n                pid: 7074,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 179506},\n                    Stime: {Sec: 0, Usec: 58614},\n                    Maxrss: 80152,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 7595,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 405,\n                    Nivcsw: 411,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660832281000,"stop":1660832281462,"duration":462}},{"uid":"35807fefa231a726","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000496d20>: {\n        Underlying: <*exec.ExitError | 0xc000930400>{\n            ProcessState: {\n                pid: 7044,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 184573},\n                    Stime: {Sec: 0, Usec: 53833},\n                    Maxrss: 82224,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 7897,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 289,\n                    Nivcsw: 449,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660808797000,"stop":1660808797491,"duration":491}},{"uid":"107e5f38bf8d71f5","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00066d4b8>: {\n        Underlying: <*exec.ExitError | 0xc000917600>{\n            ProcessState: {\n                pid: 7222,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 136603},\n                    Stime: {Sec: 0, Usec: 54641},\n                    Maxrss: 93836,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 10623,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 327,\n                    Nivcsw: 527,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660299182000,"stop":1660299182381,"duration":381}},{"uid":"3ea76e28d6be1481","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0000ce3c0>: {\n        Underlying: <*exec.ExitError | 0xc0008b8540>{\n            ProcessState: {\n                pid: 7200,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 155972},\n                    Stime: {Sec: 0, Usec: 14179},\n                    Maxrss: 79248,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3736,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 16256,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 279,\n                    Nivcsw: 219,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1660295740000,"stop":1660295740341,"duration":341}}]},"36f30658c6f20b25d1e43ac7df747487":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"7884eb6dcc60e63a","status":"passed","time":{"start":1663839375646,"stop":1663839387855,"duration":12209}}]},"Worker Suite:Worker Suite#[It] Slice Config Negative Scenario tests Slice Config Negative Scenario Creation Should fail while creating sliceconfig with bandwithguaranteedkbps more than bandwidthceilingkbps in qosprofiledetailsnvalid ":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":55,"unknown":0,"total":56},"items":[{"uid":"a978daedbe465c9c","status":"passed","time":{"start":1664094053000,"stop":1664094053001,"duration":1}},{"uid":"f54109973a2977cc","status":"passed","time":{"start":1663843963000,"stop":1663843963000,"duration":0}},{"uid":"5695f79f2eb73ff7","status":"passed","time":{"start":1663829327000,"stop":1663829327004,"duration":4}},{"uid":"b83cf49b71b5bfe0","status":"passed","time":{"start":1663343132000,"stop":1663343132469,"duration":469}},{"uid":"a151c0010efe002e","status":"passed","time":{"start":1663342947000,"stop":1663342947000,"duration":0}},{"uid":"449b954471ba5e61","status":"passed","time":{"start":1663044723000,"stop":1663044723000,"duration":0}},{"uid":"c67b87348eda6554","status":"skipped","statusDetails":"skipped","time":{"start":1663044069000,"stop":1663044069000,"duration":0}},{"uid":"281176c9ef5bef16","status":"passed","time":{"start":1662348908000,"stop":1662348908250,"duration":250}},{"uid":"5346c58dccc993e2","status":"passed","time":{"start":1661844205000,"stop":1661844205000,"duration":0}},{"uid":"7180303146c9df90","status":"passed","time":{"start":1661754870000,"stop":1661754870001,"duration":1}},{"uid":"735860aba6a7704f","status":"passed","time":{"start":1661611962000,"stop":1661611962000,"duration":0}},{"uid":"27a3bd2828605a13","status":"passed","time":{"start":1661585077000,"stop":1661585077000,"duration":0}},{"uid":"bec373edda85cc56","status":"passed","time":{"start":1661584447000,"stop":1661584447000,"duration":0}},{"uid":"e53f2f1f54b87894","status":"passed","time":{"start":1661580045000,"stop":1661580045000,"duration":0}},{"uid":"a51311e79247276e","status":"passed","time":{"start":1661578505000,"stop":1661578505000,"duration":0}},{"uid":"5227ae2b1df15b1d","status":"passed","time":{"start":1661572479000,"stop":1661572479001,"duration":1}},{"uid":"55a39973d9ccaa63","status":"passed","time":{"start":1660832281000,"stop":1660832281005,"duration":5}},{"uid":"1e0fafdd28899c4a","status":"passed","time":{"start":1660808797000,"stop":1660808797001,"duration":1}},{"uid":"8d2bd0b889d12b51","status":"passed","time":{"start":1660299182000,"stop":1660299182001,"duration":1}},{"uid":"4d77327659a45351","status":"passed","time":{"start":1660295740000,"stop":1660295740000,"duration":0}}]},"Hub Suite:Hub Suite#[It] Cluster CR tests Cluster CR validation Register worker cluster":{"statistic":{"failed":59,"broken":0,"skipped":11,"passed":149,"unknown":0,"total":219},"items":[{"uid":"eb702843cb10cdd8","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000636018>: {\n        Underlying: <*exec.ExitError | 0xc00071a000>{\n            ProcessState: {\n                pid: 6728,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 2, Usec: 140545},\n                    Stime: {Sec: 0, Usec: 247767},\n                    Maxrss: 82452,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 11018,\n                    Majflt: 1,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14808,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 10906,\n                    Nivcsw: 3839,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.5.2.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:165: [debug] Clearing discovery cache\",\n                        \"wait.go:48: [debug] beginning wait for 5 resources with timeout of 1m0s\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:310: [debug] Starting delete for \\\"kubeslice-preinstall\\\" ServiceAccount\",\n                        \"client.go:339: [debug] serviceaccounts \\\"kubeslice-preinstall\\\" not found\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:310: [debug] Starting delete for \\\"kubeslice-preinstall-role\\\" ClusterRole\",\n                        \"client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \\\"kubeslice-preinstall-role\\\" not found\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:310: [debug] Starting delete for \\\"kubeslice-preinstall-rolebinding\\\" ClusterRoleBinding\",\n                        \"client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \\\"kubeslice-preinstall-rolebinding\\\" not found\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:310: [debug] Starting delete for \\\"kubeslice-worker-preinstall-configmap\\\" ConfigMap\",\n                        \"client.go:339: [debug] configmaps \\\"kubeslice-worker-preinstall-configmap\\\" not found\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:310: [debug] Starting delete for \\\"kubeslice-worker-preinstall-job\\\" Job\",\n                        \"client.go:339: [debug] jobs.batch \\\"kubeslice-worker-preinstall-job\\\" not found\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:540: [debug] Watching for changes to Job kubeslice-worker-preinstall-job with timeout of 5m0s\",\n                        \"client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: ADDED\",\n                        \"client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 0, jobs failed: 0, jobs succeeded: 0\",\n                        \"client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\",\n                        \"client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\",\n                        \"client.go:568: [...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.5.2.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:165: [debug] Clearing discovery cache\n    wait.go:48: [debug] beginning wait for 5 resources with timeout of 1m0s\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:310: [debug] Starting delete for \"kubeslice-preinstall\" ServiceAccount\n    client.go:339: [debug] serviceaccounts \"kubeslice-preinstall\" not found\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:310: [debug] Starting delete for \"kubeslice-preinstall-role\" ClusterRole\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-preinstall-role\" not found\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:310: [debug] Starting delete for \"kubeslice-preinstall-rolebinding\" ClusterRoleBinding\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-preinstall-rolebinding\" not found\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:310: [debug] Starting delete for \"kubeslice-worker-preinstall-configmap\" ConfigMap\n    client.go:339: [debug] configmaps \"kubeslice-worker-preinstall-configmap\" not found\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:310: [debug] Starting delete for \"kubeslice-worker-preinstall-job\" Job\n    client.go:339: [debug] jobs.batch \"kubeslice-worker-preinstall-job\" not found\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:540: [debug] Watching for changes to Job kubeslice-worker-preinstall-job with timeout of 5m0s\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: ADDED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 0, jobs failed: 0, jobs succeeded: 0\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 1, jobs succeeded: 0\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 2, jobs succeeded: 0\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 3, jobs succeeded: 0\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 4, jobs succeeded: 0\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 5, jobs succeeded: 0\n    client.go:310: [debug] Starting delete for \"kubeslice-worker-preinstall-job\" Job\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:310: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-svc\" Service\n    client.go:310: [debug] Starting delete for \"jaeger\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:339: [debug] services \"kubeslice-webhook-service\" not found\n    client.go:339: [debug] services \"nsm-admission-webhook-svc\" not found\n    client.go:339: [debug] services \"jaeger\" not found\n    client.go:339: [debug] services \"kubeslice-dns\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:310: [debug] Starting delete for \"jaeger\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook\" Deployment\n    client.go:310: [debug] Starting delete for \"prefix-service\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:339: [debug] deployments.apps \"prefix-service\" not found\n    client.go:339: [debug] deployments.apps \"kubeslice-dns\" not found\n    client.go:339: [debug] deployments.apps \"nsm-admission-webhook\" not found\n    client.go:339: [debug] deployments.apps \"kubeslice-operator\" not found\n    client.go:339: [debug] deployments.apps \"jaeger\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsm-kernel-forwarder\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:339: [debug] daemonsets.apps \"kubeslice-netop\" not found\n    client.go:339: [debug] daemonsets.apps \"nsm-kernel-forwarder\" not found\n    client.go:339: [debug] daemonsets.apps \"nsmgr\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:339: [debug] rolebindings.rbac.authorization.k8s.io \"kubeslice-leader-election-rolebinding\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:339: [debug] roles.rbac.authorization.k8s.io \"kubeslice-leader-election-role\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-kubernetes-dashboard\" not found\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-dns-rolebinding\" not found\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-manager-rolebinding\" not found\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"nsm-role-binding\" not found\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-proxy-rolebinding\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRole\n    client.go:310: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"aggregate-network-services-view\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"nsm-role\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-proxy-role\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-kubernetes-dashboard\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-manager-role\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-metrics-reader\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-dns-role\" not found\n    client.go:310: [debug] Starting delete for \"networkservices.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkserviceendpoints.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservicemanagers.networkservicemesh.io\" CustomResourceDefinition\n    client.go:339: [debug] customresourcedefinitions.apiextensions.k8s.io \"networkservices.networkservicemesh.io\" not found\n    client.go:339: [debug] customresourcedefinitions.apiextensions.k8s.io \"networkserviceendpoints.networkservicemesh.io\" not found\n    client.go:339: [debug] customresourcedefinitions.apiextensions.k8s.io \"networkservicemanagers.networkservicemesh.io\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:339: [debug] configmaps \"nsm-config\" not found\n    client.go:339: [debug] configmaps \"kubeslice-manager-config\" not found\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-image-pull-secret\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:339: [debug] secrets \"kubeslice-image-pull-secret\" not found\n    client.go:339: [debug] secrets \"kubeslice-hub\" not found\n    client.go:339: [debug] secrets \"nsm-admission-webhook-certs\" not found\n    client.go:339: [debug] secrets \"kubeslice-admission-webhook-certs\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:339: [debug] serviceaccounts \"kubeslice-kubernetes-dashboard\" not found\n    client.go:339: [debug] serviceaccounts \"kubeslice-dns\" not found\n    client.go:339: [debug] serviceaccounts \"nse-acc\" not found\n    client.go:339: [debug] serviceaccounts \"nsmgr-acc\" not found\n    client.go:339: [debug] serviceaccounts \"kubeslice-netop\" not found\n    client.go:339: [debug] serviceaccounts \"kubeslice-controller-manager\" not found\n    client.go:339: [debug] serviceaccounts \"vpn-gateway-server\" not found\n    client.go:339: [debug] serviceaccounts \"slice-router\" not found\n    client.go:339: [debug] serviceaccounts \"vpn-gateway-client\" not found\n    client.go:339: [debug] serviceaccounts \"forward-plane-acc\" not found\n    client.go:339: [debug] serviceaccounts \"nsc-acc\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-cfg\" MutatingWebhookConfiguration\n    client.go:339: [debug] mutatingwebhookconfigurations.admissionregistration.k8s.io \"kubeslice-mutating-webhook-configuration\" not found\n    client.go:339: [debug] mutatingwebhookconfigurations.admissionregistration.k8s.io \"nsm-admission-webhook-cfg\" not found\n    uninstall.go:144: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: failed pre-install: timed out waiting for the condition\n    helm.go:84: [debug] failed pre-install: timed out waiting for the condition\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:361\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1664275319000,"stop":1664275835032,"duration":516032}},{"uid":"6ff09dea4cefcec4","status":"passed","time":{"start":1664245203000,"stop":1664245261935,"duration":58935}},{"uid":"3482e54f5a72d163","status":"passed","time":{"start":1664160732000,"stop":1664160766633,"duration":34633}},{"uid":"cb820793fa51a03b","status":"passed","time":{"start":1664091725000,"stop":1664091770228,"duration":45228}},{"uid":"3d526fae60edef8a","status":"passed","time":{"start":1663841627000,"stop":1663841674901,"duration":47901}},{"uid":"c01645623159e5a1","status":"passed","time":{"start":1663827045000,"stop":1663827079951,"duration":34951}},{"uid":"890e27d22d759aa0","status":"passed","time":{"start":1663767974000,"stop":1663768008282,"duration":34282}},{"uid":"180e9c5cb05ede70","status":"passed","time":{"start":1663669677000,"stop":1663669722170,"duration":45170}},{"uid":"dc9e46eed13ec3e8","status":"passed","time":{"start":1663665323000,"stop":1663665361158,"duration":38158}},{"uid":"189fec76b820dc8d","status":"passed","time":{"start":1663665170000,"stop":1663665215961,"duration":45961}},{"uid":"e168cf6be10412c7","status":"passed","time":{"start":1663659067000,"stop":1663659103153,"duration":36153}},{"uid":"73d94feb28e91cc5","status":"passed","time":{"start":1663656809000,"stop":1663656853659,"duration":44659}},{"uid":"67f9039943112b5e","status":"passed","time":{"start":1663583877000,"stop":1663583937830,"duration":60830}},{"uid":"2a0c09549fc70e35","status":"passed","time":{"start":1663340950000,"stop":1663340986663,"duration":36663}},{"uid":"6e22cdd666d68348","status":"passed","time":{"start":1663340650000,"stop":1663340684684,"duration":34684}},{"uid":"19743371b3c4cdf0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663308473000,"stop":1663308473000,"duration":0}},{"uid":"f65909c509cfb27","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663304356000,"stop":1663304356000,"duration":0}},{"uid":"e5024cf90a7869a5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663302519000,"stop":1663302519000,"duration":0}},{"uid":"82679759828f3f19","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1663264756000,"stop":1663264756000,"duration":0}},{"uid":"8c64a72468a6ba44","status":"passed","time":{"start":1663228190000,"stop":1663228235354,"duration":45354}}]}}