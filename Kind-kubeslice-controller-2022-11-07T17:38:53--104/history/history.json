{"Worker Suite:Worker Suite#[It] Slice RBAC Tests SliceRoleTemplate Tests Create SliceRoleTempalate failure test: Creating SliceRoleTempalate read-only-role, missing resources field":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":2,"unknown":0,"total":2},"items":[{"uid":"f9bd4ab7e3a4a06","status":"passed","time":{"start":1667739621000,"stop":1667739621963,"duration":963}},{"uid":"8623ec965e2ebae4","status":"passed","time":{"start":1667458785000,"stop":1667458785786,"duration":786}}]},"Worker Suite:Worker Suite#[It] Slice Config Negative Scenario tests Slice Config Negative Scenario Update Should fail while updating sliceconfig with invalid clusters":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":5,"unknown":0,"total":6},"items":[{"uid":"aa6b06fdfc9a9122","status":"passed","time":{"start":1667837307000,"stop":1667837307561,"duration":561}},{"uid":"bd592f5b8138a32a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667826674000,"stop":1667826674000,"duration":0}},{"uid":"d3bf225c67a7cc24","status":"passed","time":{"start":1667823949000,"stop":1667823949281,"duration":281}},{"uid":"70608bcd413bb58f","status":"passed","time":{"start":1666674621000,"stop":1666674622072,"duration":1072}},{"uid":"84f10c4ed72fb9e5","status":"passed","time":{"start":1666674212000,"stop":1666674212776,"duration":776}},{"uid":"c35034f26287d252","status":"passed","time":{"start":1666665366000,"stop":1666665366788,"duration":788}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting worker-operator pod Check ping between iperf-server and iperf-client after worker-operator pod restart":{"statistic":{"failed":0,"broken":0,"skipped":5,"passed":1,"unknown":0,"total":6},"items":[{"uid":"21171b954f9916ac","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667837307000,"stop":1667837307000,"duration":0}},{"uid":"c7edc731b3ffc05","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667826674000,"stop":1667826674000,"duration":0}},{"uid":"8fce8cfd8910de60","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667823949000,"stop":1667823949000,"duration":0}},{"uid":"76bf3d51ba29f3bb","status":"passed","time":{"start":1666674621000,"stop":1666674625726,"duration":4726}},{"uid":"b39d901e5b0c6ed5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674212000,"stop":1666674212000,"duration":0}},{"uid":"4898b2e530b709f2","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666665366000,"stop":1666665366000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting vl3 pod Check ping between iperf-server and iperf-client after vl3 pod restart":{"statistic":{"failed":0,"broken":0,"skipped":5,"passed":1,"unknown":0,"total":6},"items":[{"uid":"e3c7bf3fdd8f9321","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667837307000,"stop":1667837307000,"duration":0}},{"uid":"40ae9360275e616","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667826674000,"stop":1667826674000,"duration":0}},{"uid":"e6288fa7764cbd4d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667823949000,"stop":1667823949000,"duration":0}},{"uid":"2c3c72791df4490e","status":"passed","time":{"start":1666674621000,"stop":1666674625862,"duration":4862}},{"uid":"d7528fabee8bf84","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674212000,"stop":1666674212000,"duration":0}},{"uid":"905687ccaf481ca4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666665366000,"stop":1666665366000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice resource quota Tests ResourceQuota creation tests Delete Sliceconfig fail test":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":3,"unknown":0,"total":3},"items":[{"uid":"66a62f53f6d0c0ea","status":"passed","time":{"start":1667739621000,"stop":1667739622895,"duration":1895}},{"uid":"e700e87a3543a5d0","status":"passed","time":{"start":1667458785000,"stop":1667458786716,"duration":1716}},{"uid":"b9c7557406b739b3","status":"passed","time":{"start":1667389661000,"stop":1667389662440,"duration":1440}}]},"Worker Suite:Worker Suite#[It] Hub netpol scenarios: Slice Configs Netpol scenarios Should create slice for valid namespace and valid clusters in applicationNamespaces of sliceconfigs manifest":{"statistic":{"failed":1,"broken":0,"skipped":0,"passed":5,"unknown":0,"total":6},"items":[{"uid":"3271f83ee17c40c5","status":"passed","time":{"start":1667837307000,"stop":1667837318272,"duration":11272}},{"uid":"d8dd2bd05deb5cfd","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0008b4690>: {\n        Underlying: <*exec.ExitError | 0xc0008de2a0>{\n            ProcessState: {\n                pid: 6227,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 244943},\n                    Stime: {Sec: 0, Usec: 67830},\n                    Maxrss: 107220,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 6746,\n                    Majflt: 4,\n                    Nswap: 0,\n                    Inblock: 696,\n                    Oublock: 19808,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 452,\n                    Nivcsw: 714,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"fireredslice\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\noccurred","time":{"start":1667826674000,"stop":1667826674302,"duration":302}},{"uid":"c06d836237271e07","status":"passed","time":{"start":1667823949000,"stop":1667823950421,"duration":1421}},{"uid":"fb69bba7b63ac860","status":"passed","time":{"start":1666674621000,"stop":1666674632231,"duration":11231}},{"uid":"ed8bcb05e1df738f","status":"passed","time":{"start":1666674212000,"stop":1666674223824,"duration":11824}},{"uid":"4106eeab77272c94","status":"passed","time":{"start":1666665366000,"stop":1666665366641,"duration":641}}]},"Worker Suite:Worker Suite#[It] Slice Config Negative Scenario tests Slice Config Negative Scenario Creation Should fail while creating sliceconfig with invalid clusters":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":6,"unknown":0,"total":6},"items":[{"uid":"f83d3f2ad53cb5c0","status":"passed","time":{"start":1667837307000,"stop":1667837307310,"duration":310}},{"uid":"5b7e38cb4df76ac9","status":"passed","time":{"start":1667826674000,"stop":1667826674332,"duration":332}},{"uid":"30f8c7216dcaf170","status":"passed","time":{"start":1667823949000,"stop":1667823949775,"duration":775}},{"uid":"22e006fa38119f89","status":"passed","time":{"start":1666674621000,"stop":1666674621465,"duration":465}},{"uid":"5d9d84c9ea40b9d4","status":"passed","time":{"start":1666674212000,"stop":1666674212669,"duration":669}},{"uid":"ff63a7543d631c85","status":"passed","time":{"start":1666665366000,"stop":1666665366514,"duration":514}}]},"Intracluster Suite:Intracluster Suite#[BeforeSuite]":{"statistic":{"failed":23,"broken":0,"skipped":0,"passed":4,"unknown":0,"total":27},"items":[{"uid":"844510ee79312085","status":"passed","time":{"start":1667842189000,"stop":1667842315122,"duration":126122}},{"uid":"aa07c190436b8beb","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004b8030>: {\n        Underlying: <*exec.ExitError | 0xc00049e000>{\n            ProcessState: {\n                pid: 6202,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 4, Usec: 624745},\n                    Stime: {Sec: 0, Usec: 589822},\n                    Maxrss: 110296,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 15588,\n                    Majflt: 12,\n                    Nswap: 0,\n                    Inblock: 1056,\n                    Oublock: 19952,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 49638,\n                    Nivcsw: 14054,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:192: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.4.7.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:165: [debug] Clearing discovery cache\",\n                        \"wait.go:66: [debug] beginning wait for 4 resources with timeout of 1m0s\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 48 resource(s)\",\n                        \"wait.go:66: [debug] beginning wait for 48 resources with timeout of 5m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 2 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 1 out of 2 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 1 out of 2 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 1 out of 2 expected pods are ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:192: [debug] Original chart version: \"\"\n    install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.4.7.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:165: [debug] Clearing discovery cache\n    wait.go:66: [debug] beginning wait for 4 resources with timeout of 1m0s\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 48 resource(s)\n    wait.go:66: [debug] beginning wait for 48 resources with timeout of 5m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 2 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 1 out of 2 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 1 out of 2 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 1 out of 2 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 1 out of 2 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 1 out of 2 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:310: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"jaeger\" Service\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-svc\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook\" Deployment\n    client.go:310: [debug] Starting delete for \"jaeger\" Deployment\n    client.go:310: [debug] Starting delete for \"prefix-service\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsm-kernel-forwarder\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRole\n    client.go:310: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"networkservices.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkserviceendpoints.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservicemanagers.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-cfg\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"nsm-webhook-high-priority\" PriorityClass\n    uninstall.go:148: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:141\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\noccurred","time":{"start":1667807977000,"stop":1667808343526,"duration":366526}},{"uid":"c82b244932d17cd4","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000128090>: {\n        Underlying: <*exec.ExitError | 0xc0009e4000>{\n            ProcessState: {\n                pid: 6159,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 3, Usec: 986174},\n                    Stime: {Sec: 0, Usec: 712388},\n                    Maxrss: 102264,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 5060,\n                    Majflt: 8,\n                    Nswap: 0,\n                    Inblock: 1472,\n                    Oublock: 20192,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 48974,\n                    Nivcsw: 12313,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:192: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.4.7.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:165: [debug] Clearing discovery cache\",\n                        \"wait.go:66: [debug] beginning wait for 4 resources with timeout of 1m0s\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 48 resource(s)\",\n                        \"wait.go:66: [debug] beginning wait for 48 resources with timeout of 5m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 2 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 2 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 2 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 2 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 2 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 2 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 2 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 2 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 2 expected pods are ready\",\n                        \"ready.go:277: [debug] Deploy...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:192: [debug] Original chart version: \"\"\n    install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.4.7.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:165: [debug] Clearing discovery cache\n    wait.go:66: [debug] beginning wait for 4 resources with timeout of 1m0s\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 48 resource(s)\n    wait.go:66: [debug] beginning wait for 48 resources with timeout of 5m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 2 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 2 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 2 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 2 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 2 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 2 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 2 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 2 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 2 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:310: [debug] Starting delete for \"jaeger\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-svc\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook\" Deployment\n    client.go:310: [debug] Starting delete for \"jaeger\" Deployment\n    client.go:310: [debug] Starting delete for \"prefix-service\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsm-kernel-forwarder\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRole\n    client.go:310: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"networkservices.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkserviceendpoints.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservicemanagers.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-cfg\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"nsm-webhook-high-priority\" PriorityClass\n    uninstall.go:148: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: client rate limiter Wait returned an error: context deadline exceeded\n    helm.go:84: [debug] client rate limiter Wait returned an error: context deadline exceeded\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:141\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\noccurred","time":{"start":1667536876000,"stop":1667537221652,"duration":345652}},{"uid":"d0a6d08155d31cba","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00035c228>: {\n        Underlying: <*exec.ExitError | 0xc0006cc000>{\n            ProcessState: {\n                pid: 6188,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 2, Usec: 584824},\n                    Stime: {Sec: 0, Usec: 363553},\n                    Maxrss: 98984,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 21999,\n                    Majflt: 9,\n                    Nswap: 0,\n                    Inblock: 1464,\n                    Oublock: 20872,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 6820,\n                    Nivcsw: 6683,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"install.go:192: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.4.4.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:165: [debug] Clearing discovery cache\",\n                        \"wait.go:66: [debug] beginning wait for 7 resources with timeout of 1m0s\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:310: [debug] Starting delete for \\\"kubeslice-preinstall\\\" ServiceAccount\",\n                        \"client.go:339: [debug] serviceaccounts \\\"kubeslice-preinstall\\\" not found\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:310: [debug] Starting delete for \\\"kubeslice-preinstall-role\\\" ClusterRole\",\n                        \"client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \\\"kubeslice-preinstall-role\\\" not found\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:310: [debug] Starting delete for \\\"kubeslice-preinstall-rolebinding\\\" ClusterRoleBinding\",\n                        \"client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \\\"kubeslice-preinstall-rolebinding\\\" not found\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:310: [debug] Starting delete for \\\"kubeslice-worker-preinstall-configmap\\\" ConfigMap\",\n                        \"client.go:339: [debug] configmaps \\\"kubeslice-worker-preinstall-configmap\\\" not found\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:310: [debug] Starting delete for \\\"kubeslice-worker-preinstall-job\\\" Job\",\n                        \"client.go:339: [debug] jobs.batch \\\"kubeslice-worker-preinstall-job\\\" not found\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:540: [debug] Watching for changes to Job kubeslice-worker-preinstall-job with timeout of 5m0s\",\n                        \"client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: ADDED\",\n                        \"client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 0, jobs failed: 0, jobs succeeded: 0\",\n                        \"client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\",\n                        \"c...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; install.go:192: [debug] Original chart version: \"\"\n    install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.4.4.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:165: [debug] Clearing discovery cache\n    wait.go:66: [debug] beginning wait for 7 resources with timeout of 1m0s\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:310: [debug] Starting delete for \"kubeslice-preinstall\" ServiceAccount\n    client.go:339: [debug] serviceaccounts \"kubeslice-preinstall\" not found\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:310: [debug] Starting delete for \"kubeslice-preinstall-role\" ClusterRole\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-preinstall-role\" not found\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:310: [debug] Starting delete for \"kubeslice-preinstall-rolebinding\" ClusterRoleBinding\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-preinstall-rolebinding\" not found\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:310: [debug] Starting delete for \"kubeslice-worker-preinstall-configmap\" ConfigMap\n    client.go:339: [debug] configmaps \"kubeslice-worker-preinstall-configmap\" not found\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:310: [debug] Starting delete for \"kubeslice-worker-preinstall-job\" Job\n    client.go:339: [debug] jobs.batch \"kubeslice-worker-preinstall-job\" not found\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:540: [debug] Watching for changes to Job kubeslice-worker-preinstall-job with timeout of 5m0s\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: ADDED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 0, jobs failed: 0, jobs succeeded: 0\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 1, jobs succeeded: 0\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 2, jobs succeeded: 0\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 3, jobs succeeded: 0\n    client.go:310: [debug] Starting delete for \"kubeslice-worker-preinstall-job\" Job\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:310: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"jaeger\" Service\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-svc\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:339: [debug] services \"kubeslice-webhook-service\" not found\n    client.go:339: [debug] services \"jaeger\" not found\n    client.go:339: [debug] services \"nsm-admission-webhook-svc\" not found\n    client.go:339: [debug] services \"kubeslice-dns\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook\" Deployment\n    client.go:310: [debug] Starting delete for \"jaeger\" Deployment\n    client.go:310: [debug] Starting delete for \"prefix-service\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:339: [debug] deployments.apps \"kubeslice-operator\" not found\n    client.go:339: [debug] deployments.apps \"nsm-admission-webhook\" not found\n    client.go:339: [debug] deployments.apps \"jaeger\" not found\n    client.go:339: [debug] deployments.apps \"kubeslice-dns\" not found\n    client.go:339: [debug] deployments.apps \"prefix-service\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsm-kernel-forwarder\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:339: [debug] daemonsets.apps \"nsmgr\" not found\n    client.go:339: [debug] daemonsets.apps \"kubeslice-netop\" not found\n    client.go:339: [debug] daemonsets.apps \"nsm-kernel-forwarder\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:339: [debug] rolebindings.rbac.authorization.k8s.io \"kubeslice-leader-election-rolebinding\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:339: [debug] roles.rbac.authorization.k8s.io \"kubeslice-leader-election-role\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-kubernetes-dashboard\" not found\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-dns-rolebinding\" not found\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-proxy-rolebinding\" not found\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"nsm-role-binding\" not found\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-manager-rolebinding\" not found\n    client.go:310: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"nsm-role\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"aggregate-network-services-view\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-metrics-reader\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-proxy-role\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-kubernetes-dashboard\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-manager-role\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-dns-role\" not found\n    client.go:310: [debug] Starting delete for \"networkservices.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkserviceendpoints.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservicemanagers.networkservicemesh.io\" CustomResourceDefinition\n    client.go:339: [debug] customresourcedefinitions.apiextensions.k8s.io \"networkserviceendpoints.networkservicemesh.io\" not found\n    client.go:339: [debug] customresourcedefinitions.apiextensions.k8s.io \"networkservicemanagers.networkservicemesh.io\" not found\n    client.go:339: [debug] customresourcedefinitions.apiextensions.k8s.io \"networkservices.networkservicemesh.io\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:339: [debug] configmaps \"kubeslice-manager-config\" not found\n    client.go:339: [debug] configmaps \"nsm-config\" not found\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-image-pull-secret\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:339: [debug] secrets \"kubeslice-hub\" not found\n    client.go:339: [debug] secrets \"kubeslice-image-pull-secret\" not found\n    client.go:339: [debug] secrets \"nsm-admission-webhook-certs\" not found\n    client.go:339: [debug] secrets \"kubeslice-admission-webhook-certs\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:339: [debug] serviceaccounts \"kubeslice-controller-manager\" not found\n    client.go:339: [debug] serviceaccounts \"nsmgr-acc\" not found\n    client.go:339: [debug] serviceaccounts \"kubeslice-kubernetes-dashboard\" not found\n    client.go:339: [debug] serviceaccounts \"kubeslice-dns\" not found\n    client.go:339: [debug] serviceaccounts \"nsc-acc\" not found\n    client.go:339: [debug] serviceaccounts \"nse-acc\" not found\n    client.go:339: [debug] serviceaccounts \"forward-plane-acc\" not found\n    client.go:339: [debug] serviceaccounts \"vpn-gateway-client\" not found\n    client.go:339: [debug] serviceaccounts \"vpn-gateway-server\" not found\n    client.go:339: [debug] serviceaccounts \"slice-router\" not found\n    client.go:339: [debug] serviceaccounts \"kubeslice-netop\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-cfg\" MutatingWebhookConfiguration\n    client.go:339: [debug] mutatingwebhookconfigurations.admissionregistration.k8s.io \"kubeslice-mutating-webhook-configuration\" not found\n    client.go:339: [debug] mutatingwebhookconfigurations.admissionregistration.k8s.io \"nsm-admission-webhook-cfg\" not found\n    uninstall.go:148: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: failed pre-install: timed out waiting for the condition\n    helm.go:84: [debug] failed pre-install: timed out waiting for the condition\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:361\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:141\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\noccurred","time":{"start":1667385281000,"stop":1667385652563,"duration":371563}},{"uid":"c284804455be4a69","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000128090>: {\n        Underlying: <*exec.ExitError | 0xc00072c000>{\n            ProcessState: {\n                pid: 6185,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 2, Usec: 52700},\n                    Stime: {Sec: 0, Usec: 244745},\n                    Maxrss: 104004,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 16755,\n                    Majflt: 2,\n                    Nswap: 0,\n                    Inblock: 240,\n                    Oublock: 20864,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 5019,\n                    Nivcsw: 5350,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"install.go:192: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.4.4.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:165: [debug] Clearing discovery cache\",\n                        \"wait.go:66: [debug] beginning wait for 7 resources with timeout of 1m0s\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:310: [debug] Starting delete for \\\"kubeslice-preinstall\\\" ServiceAccount\",\n                        \"client.go:339: [debug] serviceaccounts \\\"kubeslice-preinstall\\\" not found\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:310: [debug] Starting delete for \\\"kubeslice-preinstall-role\\\" ClusterRole\",\n                        \"client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \\\"kubeslice-preinstall-role\\\" not found\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:310: [debug] Starting delete for \\\"kubeslice-preinstall-rolebinding\\\" ClusterRoleBinding\",\n                        \"client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \\\"kubeslice-preinstall-rolebinding\\\" not found\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:310: [debug] Starting delete for \\\"kubeslice-worker-preinstall-configmap\\\" ConfigMap\",\n                        \"client.go:339: [debug] configmaps \\\"kubeslice-worker-preinstall-configmap\\\" not found\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:310: [debug] Starting delete for \\\"kubeslice-worker-preinstall-job\\\" Job\",\n                        \"client.go:339: [debug] jobs.batch \\\"kubeslice-worker-preinstall-job\\\" not found\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:540: [debug] Watching for changes to Job kubeslice-worker-preinstall-job with timeout of 5m0s\",\n                        \"client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: ADDED\",\n                        \"client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 0, jobs failed: 0, jobs succeeded: 0\",\n                        \"client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\",\n                        \"cl...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; install.go:192: [debug] Original chart version: \"\"\n    install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.4.4.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:165: [debug] Clearing discovery cache\n    wait.go:66: [debug] beginning wait for 7 resources with timeout of 1m0s\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:310: [debug] Starting delete for \"kubeslice-preinstall\" ServiceAccount\n    client.go:339: [debug] serviceaccounts \"kubeslice-preinstall\" not found\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:310: [debug] Starting delete for \"kubeslice-preinstall-role\" ClusterRole\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-preinstall-role\" not found\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:310: [debug] Starting delete for \"kubeslice-preinstall-rolebinding\" ClusterRoleBinding\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-preinstall-rolebinding\" not found\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:310: [debug] Starting delete for \"kubeslice-worker-preinstall-configmap\" ConfigMap\n    client.go:339: [debug] configmaps \"kubeslice-worker-preinstall-configmap\" not found\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:310: [debug] Starting delete for \"kubeslice-worker-preinstall-job\" Job\n    client.go:339: [debug] jobs.batch \"kubeslice-worker-preinstall-job\" not found\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:540: [debug] Watching for changes to Job kubeslice-worker-preinstall-job with timeout of 5m0s\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: ADDED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 0, jobs failed: 0, jobs succeeded: 0\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 1, jobs succeeded: 0\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 2, jobs succeeded: 0\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 3, jobs succeeded: 0\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 4, jobs succeeded: 0\n    client.go:310: [debug] Starting delete for \"kubeslice-worker-preinstall-job\" Job\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:310: [debug] Starting delete for \"jaeger\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-svc\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:339: [debug] services \"kubeslice-webhook-service\" not found\n    client.go:339: [debug] services \"nsm-admission-webhook-svc\" not found\n    client.go:339: [debug] services \"jaeger\" not found\n    client.go:339: [debug] services \"kubeslice-dns\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook\" Deployment\n    client.go:310: [debug] Starting delete for \"jaeger\" Deployment\n    client.go:310: [debug] Starting delete for \"prefix-service\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:339: [debug] deployments.apps \"kubeslice-operator\" not found\n    client.go:339: [debug] deployments.apps \"nsm-admission-webhook\" not found\n    client.go:339: [debug] deployments.apps \"kubeslice-dns\" not found\n    client.go:339: [debug] deployments.apps \"prefix-service\" not found\n    client.go:339: [debug] deployments.apps \"jaeger\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsm-kernel-forwarder\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:339: [debug] daemonsets.apps \"nsm-kernel-forwarder\" not found\n    client.go:339: [debug] daemonsets.apps \"kubeslice-netop\" not found\n    client.go:339: [debug] daemonsets.apps \"nsmgr\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:339: [debug] rolebindings.rbac.authorization.k8s.io \"kubeslice-leader-election-rolebinding\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:339: [debug] roles.rbac.authorization.k8s.io \"kubeslice-leader-election-role\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-kubernetes-dashboard\" not found\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"nsm-role-binding\" not found\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-proxy-rolebinding\" not found\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-manager-rolebinding\" not found\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-dns-rolebinding\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRole\n    client.go:310: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-kubernetes-dashboard\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-proxy-role\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-manager-role\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"nsm-role\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"aggregate-network-services-view\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-dns-role\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-metrics-reader\" not found\n    client.go:310: [debug] Starting delete for \"networkservices.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkserviceendpoints.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservicemanagers.networkservicemesh.io\" CustomResourceDefinition\n    client.go:339: [debug] customresourcedefinitions.apiextensions.k8s.io \"networkserviceendpoints.networkservicemesh.io\" not found\n    client.go:339: [debug] customresourcedefinitions.apiextensions.k8s.io \"networkservicemanagers.networkservicemesh.io\" not found\n    client.go:339: [debug] customresourcedefinitions.apiextensions.k8s.io \"networkservices.networkservicemesh.io\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:339: [debug] configmaps \"kubeslice-manager-config\" not found\n    client.go:339: [debug] configmaps \"nsm-config\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-image-pull-secret\" Secret\n    client.go:339: [debug] secrets \"kubeslice-admission-webhook-certs\" not found\n    client.go:339: [debug] secrets \"kubeslice-hub\" not found\n    client.go:339: [debug] secrets \"nsm-admission-webhook-certs\" not found\n    client.go:339: [debug] secrets \"kubeslice-image-pull-secret\" not found\n    client.go:310: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:339: [debug] serviceaccounts \"nse-acc\" not found\n    client.go:339: [debug] serviceaccounts \"nsc-acc\" not found\n    client.go:339: [debug] serviceaccounts \"kubeslice-kubernetes-dashboard\" not found\n    client.go:339: [debug] serviceaccounts \"kubeslice-controller-manager\" not found\n    client.go:339: [debug] serviceaccounts \"nsmgr-acc\" not found\n    client.go:339: [debug] serviceaccounts \"forward-plane-acc\" not found\n    client.go:339: [debug] serviceaccounts \"vpn-gateway-server\" not found\n    client.go:339: [debug] serviceaccounts \"kubeslice-dns\" not found\n    client.go:339: [debug] serviceaccounts \"vpn-gateway-client\" not found\n    client.go:339: [debug] serviceaccounts \"kubeslice-netop\" not found\n    client.go:339: [debug] serviceaccounts \"slice-router\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-cfg\" MutatingWebhookConfiguration\n    client.go:339: [debug] mutatingwebhookconfigurations.admissionregistration.k8s.io \"kubeslice-mutating-webhook-configuration\" not found\n    client.go:339: [debug] mutatingwebhookconfigurations.admissionregistration.k8s.io \"nsm-admission-webhook-cfg\" not found\n    uninstall.go:148: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: failed pre-install: timed out waiting for the condition\n    helm.go:84: [debug] failed pre-install: timed out waiting for the condition\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:361\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:141\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\noccurred","time":{"start":1667293744000,"stop":1667294105909,"duration":361909}},{"uid":"af00253644872206","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004a2060>: {\n        Underlying: <*exec.ExitError | 0xc0007a4000>{\n            ProcessState: {\n                pid: 6342,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 2, Usec: 896775},\n                    Stime: {Sec: 0, Usec: 226880},\n                    Maxrss: 87640,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 19350,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14816,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 5380,\n                    Nivcsw: 3685,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"install.go:192: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.5.6.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:165: [debug] Clearing discovery cache\",\n                        \"wait.go:66: [debug] beginning wait for 5 resources with timeout of 1m0s\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:310: [debug] Starting delete for \\\"kubeslice-preinstall\\\" ServiceAccount\",\n                        \"client.go:339: [debug] serviceaccounts \\\"kubeslice-preinstall\\\" not found\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:310: [debug] Starting delete for \\\"kubeslice-preinstall-role\\\" ClusterRole\",\n                        \"client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \\\"kubeslice-preinstall-role\\\" not found\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:310: [debug] Starting delete for \\\"kubeslice-preinstall-rolebinding\\\" ClusterRoleBinding\",\n                        \"client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \\\"kubeslice-preinstall-rolebinding\\\" not found\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:310: [debug] Starting delete for \\\"kubeslice-worker-preinstall-configmap\\\" ConfigMap\",\n                        \"client.go:339: [debug] configmaps \\\"kubeslice-worker-preinstall-configmap\\\" not found\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:310: [debug] Starting delete for \\\"kubeslice-worker-preinstall-job\\\" Job\",\n                        \"client.go:339: [debug] jobs.batch \\\"kubeslice-worker-preinstall-job\\\" not found\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:540: [debug] Watching for changes to Job kubeslice-worker-preinstall-job with timeout of 5m0s\",\n                        \"client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: ADDED\",\n                        \"client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 0, jobs failed: 0, jobs succeeded: 0\",\n                        \"client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\",\n                        \"client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\",\n                        \"client.go:568: [d...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; install.go:192: [debug] Original chart version: \"\"\n    install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.5.6.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:165: [debug] Clearing discovery cache\n    wait.go:66: [debug] beginning wait for 5 resources with timeout of 1m0s\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:310: [debug] Starting delete for \"kubeslice-preinstall\" ServiceAccount\n    client.go:339: [debug] serviceaccounts \"kubeslice-preinstall\" not found\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:310: [debug] Starting delete for \"kubeslice-preinstall-role\" ClusterRole\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-preinstall-role\" not found\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:310: [debug] Starting delete for \"kubeslice-preinstall-rolebinding\" ClusterRoleBinding\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-preinstall-rolebinding\" not found\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:310: [debug] Starting delete for \"kubeslice-worker-preinstall-configmap\" ConfigMap\n    client.go:339: [debug] configmaps \"kubeslice-worker-preinstall-configmap\" not found\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:310: [debug] Starting delete for \"kubeslice-worker-preinstall-job\" Job\n    client.go:339: [debug] jobs.batch \"kubeslice-worker-preinstall-job\" not found\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:540: [debug] Watching for changes to Job kubeslice-worker-preinstall-job with timeout of 5m0s\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: ADDED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 0, jobs failed: 0, jobs succeeded: 0\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 1, jobs succeeded: 0\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 2, jobs succeeded: 0\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 3, jobs succeeded: 0\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 4, jobs succeeded: 0\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 5, jobs succeeded: 0\n    client.go:310: [debug] Starting delete for \"kubeslice-worker-preinstall-job\" Job\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:310: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-svc\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:310: [debug] Starting delete for \"jaeger\" Service\n    client.go:339: [debug] services \"kubeslice-webhook-service\" not found\n    client.go:339: [debug] services \"kubeslice-dns\" not found\n    client.go:339: [debug] services \"jaeger\" not found\n    client.go:339: [debug] services \"nsm-admission-webhook-svc\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook\" Deployment\n    client.go:310: [debug] Starting delete for \"prefix-service\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:310: [debug] Starting delete for \"jaeger\" Deployment\n    client.go:339: [debug] deployments.apps \"kubeslice-operator\" not found\n    client.go:339: [debug] deployments.apps \"kubeslice-dns\" not found\n    client.go:339: [debug] deployments.apps \"jaeger\" not found\n    client.go:339: [debug] deployments.apps \"prefix-service\" not found\n    client.go:339: [debug] deployments.apps \"nsm-admission-webhook\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsm-kernel-forwarder\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:339: [debug] daemonsets.apps \"kubeslice-netop\" not found\n    client.go:339: [debug] daemonsets.apps \"nsmgr\" not found\n    client.go:339: [debug] daemonsets.apps \"nsm-kernel-forwarder\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:339: [debug] rolebindings.rbac.authorization.k8s.io \"kubeslice-leader-election-rolebinding\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:339: [debug] roles.rbac.authorization.k8s.io \"kubeslice-leader-election-role\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"nsm-role-binding\" not found\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-dns-rolebinding\" not found\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-proxy-rolebinding\" not found\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-kubernetes-dashboard\" not found\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-manager-rolebinding\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRole\n    client.go:310: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-proxy-role\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-kubernetes-dashboard\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-metrics-reader\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-manager-role\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"aggregate-network-services-view\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"nsm-role\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-dns-role\" not found\n    client.go:310: [debug] Starting delete for \"networkservices.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservicemanagers.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkserviceendpoints.networkservicemesh.io\" CustomResourceDefinition\n    client.go:339: [debug] customresourcedefinitions.apiextensions.k8s.io \"networkservicemanagers.networkservicemesh.io\" not found\n    client.go:339: [debug] customresourcedefinitions.apiextensions.k8s.io \"networkserviceendpoints.networkservicemesh.io\" not found\n    client.go:339: [debug] customresourcedefinitions.apiextensions.k8s.io \"networkservices.networkservicemesh.io\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:339: [debug] configmaps \"kubeslice-manager-config\" not found\n    client.go:339: [debug] configmaps \"nsm-config\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-image-pull-secret\" Secret\n    client.go:339: [debug] secrets \"nsm-admission-webhook-certs\" not found\n    client.go:339: [debug] secrets \"kubeslice-image-pull-secret\" not found\n    client.go:339: [debug] secrets \"kubeslice-admission-webhook-certs\" not found\n    client.go:339: [debug] secrets \"kubeslice-hub\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ServiceAccount\n    client.go:339: [debug] serviceaccounts \"kubeslice-controller-manager\" not found\n    client.go:339: [debug] serviceaccounts \"vpn-gateway-client\" not found\n    client.go:339: [debug] serviceaccounts \"kubeslice-kubernetes-dashboard\" not found\n    client.go:339: [debug] serviceaccounts \"forward-plane-acc\" not found\n    client.go:339: [debug] serviceaccounts \"kubeslice-dns\" not found\n    client.go:339: [debug] serviceaccounts \"slice-router\" not found\n    client.go:339: [debug] serviceaccounts \"nse-acc\" not found\n    client.go:339: [debug] serviceaccounts \"nsc-acc\" not found\n    client.go:339: [debug] serviceaccounts \"vpn-gateway-server\" not found\n    client.go:339: [debug] serviceaccounts \"kubeslice-netop\" not found\n    client.go:339: [debug] serviceaccounts \"nsmgr-acc\" not found\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-cfg\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:339: [debug] mutatingwebhookconfigurations.admissionregistration.k8s.io \"nsm-admission-webhook-cfg\" not found\n    client.go:339: [debug] mutatingwebhookconfigurations.admissionregistration.k8s.io \"kubeslice-mutating-webhook-configuration\" not found\n    uninstall.go:148: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: failed pre-install: timed out waiting for the condition\n    helm.go:84: [debug] failed pre-install: timed out waiting for the condition\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:361\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:141\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\noccurred","time":{"start":1666883986000,"stop":1666884409835,"duration":423835}},{"uid":"68bd6ea327dbc9bf","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004be7b0>: {\n        Underlying: <*exec.ExitError | 0xc0004a4640>{\n            ProcessState: {\n                pid: 6261,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 51520},\n                    Stime: {Sec: 0, Usec: 13738},\n                    Maxrss: 40936,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 1954,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 192,\n                    Nivcsw: 102,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"install.go:192: [debug] Original chart version: \\\"\\\"\",\n                        \"Error: INSTALLATION FAILED: looks like \\\"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages\\\" is not a valid chart repository or cannot be reached: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\",\n                        \"helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\",\n                        \"helm.sh/helm/v3/pkg/repo.(*ChartRepository).DownloadIndexFile\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:128\",\n                        \"helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:252\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:723\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:204\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:139\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:872\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:990\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:918\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:250\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1571\",\n                        \"looks like \\\"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages\\\" is not a valid chart repository or cannot be reached\",\n                        \"helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:254\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:723\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:204\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:139\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n           ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; install.go:192: [debug] Original chart version: \"\"\n    Error: INSTALLATION FAILED: looks like \"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages\" is not a valid chart repository or cannot be reached: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\n    helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\n    helm.sh/helm/v3/pkg/repo.(*ChartRepository).DownloadIndexFile\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:128\n    helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:252\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:723\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:204\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:139\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    looks like \"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages\" is not a valid chart repository or cannot be reached\n    helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:254\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:723\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:204\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:139\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:141\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\noccurred","time":{"start":1666754211000,"stop":1666754280721,"duration":69721}},{"uid":"e6dcdc36b131c9c7","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000455788>: {\n        Underlying: <*exec.ExitError | 0xc0000f3b40>{\n            ProcessState: {\n                pid: 6289,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 53323},\n                    Stime: {Sec: 0, Usec: 3808},\n                    Maxrss: 40404,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2433,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 181,\n                    Nivcsw: 77,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"install.go:192: [debug] Original chart version: \\\"\\\"\",\n                        \"Error: INSTALLATION FAILED: looks like \\\"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/\\\" is not a valid chart repository or cannot be reached: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\",\n                        \"helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\",\n                        \"helm.sh/helm/v3/pkg/repo.(*ChartRepository).DownloadIndexFile\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:128\",\n                        \"helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:252\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:723\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:204\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:139\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:872\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:990\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:918\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:250\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1571\",\n                        \"looks like \\\"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/\\\" is not a valid chart repository or cannot be reached\",\n                        \"helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:254\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:723\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:204\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:139\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n           ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; install.go:192: [debug] Original chart version: \"\"\n    Error: INSTALLATION FAILED: looks like \"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/\" is not a valid chart repository or cannot be reached: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\n    helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\n    helm.sh/helm/v3/pkg/repo.(*ChartRepository).DownloadIndexFile\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:128\n    helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:252\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:723\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:204\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:139\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    looks like \"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/\" is not a valid chart repository or cannot be reached\n    helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:254\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:723\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:204\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:139\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:141\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\noccurred","time":{"start":1666750446000,"stop":1666750520761,"duration":74761}},{"uid":"34769ef9da8aa68a","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004a8ff0>: {\n        Underlying: <*exec.ExitError | 0xc00044b500>{\n            ProcessState: {\n                pid: 6288,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 44185},\n                    Stime: {Sec: 0, Usec: 17674},\n                    Maxrss: 41000,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 1967,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 154,\n                    Nivcsw: 338,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"install.go:192: [debug] Original chart version: \\\"\\\"\",\n                        \"Error: INSTALLATION FAILED: looks like \\\"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/\\\" is not a valid chart repository or cannot be reached: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\",\n                        \"helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\",\n                        \"helm.sh/helm/v3/pkg/repo.(*ChartRepository).DownloadIndexFile\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:128\",\n                        \"helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:252\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:723\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:204\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:139\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:872\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:990\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:918\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:250\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1571\",\n                        \"looks like \\\"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/\\\" is not a valid chart repository or cannot be reached\",\n                        \"helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:254\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:723\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:204\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:139\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n         ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; install.go:192: [debug] Original chart version: \"\"\n    Error: INSTALLATION FAILED: looks like \"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/\" is not a valid chart repository or cannot be reached: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\n    helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\n    helm.sh/helm/v3/pkg/repo.(*ChartRepository).DownloadIndexFile\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:128\n    helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:252\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:723\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:204\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:139\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    looks like \"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/\" is not a valid chart repository or cannot be reached\n    helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:254\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:723\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:204\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:139\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:141\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\noccurred","time":{"start":1666747897000,"stop":1666747977149,"duration":80149}},{"uid":"b2c1c13abd69afd7","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000e0a8>: {\n        Underlying: <*exec.ExitError | 0xc0007a6040>{\n            ProcessState: {\n                pid: 6348,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 2, Usec: 87409},\n                    Stime: {Sec: 0, Usec: 218027},\n                    Maxrss: 83220,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 15533,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14816,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 4470,\n                    Nivcsw: 3063,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"install.go:192: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.5.6.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:165: [debug] Clearing discovery cache\",\n                        \"wait.go:66: [debug] beginning wait for 5 resources with timeout of 1m0s\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:310: [debug] Starting delete for \\\"kubeslice-preinstall\\\" ServiceAccount\",\n                        \"client.go:339: [debug] serviceaccounts \\\"kubeslice-preinstall\\\" not found\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:310: [debug] Starting delete for \\\"kubeslice-preinstall-role\\\" ClusterRole\",\n                        \"client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \\\"kubeslice-preinstall-role\\\" not found\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:310: [debug] Starting delete for \\\"kubeslice-preinstall-rolebinding\\\" ClusterRoleBinding\",\n                        \"client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \\\"kubeslice-preinstall-rolebinding\\\" not found\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:310: [debug] Starting delete for \\\"kubeslice-worker-preinstall-configmap\\\" ConfigMap\",\n                        \"client.go:339: [debug] configmaps \\\"kubeslice-worker-preinstall-configmap\\\" not found\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:310: [debug] Starting delete for \\\"kubeslice-worker-preinstall-job\\\" Job\",\n                        \"client.go:339: [debug] jobs.batch \\\"kubeslice-worker-preinstall-job\\\" not found\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:540: [debug] Watching for changes to Job kubeslice-worker-preinstall-job with timeout of 5m0s\",\n                        \"client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: ADDED\",\n                        \"client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 0, jobs failed: 0, jobs succeeded: 0\",\n                        \"client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\",\n                        \"client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\",\n                        \"client.go:568: [de...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; install.go:192: [debug] Original chart version: \"\"\n    install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.5.6.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:165: [debug] Clearing discovery cache\n    wait.go:66: [debug] beginning wait for 5 resources with timeout of 1m0s\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:310: [debug] Starting delete for \"kubeslice-preinstall\" ServiceAccount\n    client.go:339: [debug] serviceaccounts \"kubeslice-preinstall\" not found\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:310: [debug] Starting delete for \"kubeslice-preinstall-role\" ClusterRole\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-preinstall-role\" not found\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:310: [debug] Starting delete for \"kubeslice-preinstall-rolebinding\" ClusterRoleBinding\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-preinstall-rolebinding\" not found\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:310: [debug] Starting delete for \"kubeslice-worker-preinstall-configmap\" ConfigMap\n    client.go:339: [debug] configmaps \"kubeslice-worker-preinstall-configmap\" not found\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:310: [debug] Starting delete for \"kubeslice-worker-preinstall-job\" Job\n    client.go:339: [debug] jobs.batch \"kubeslice-worker-preinstall-job\" not found\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:540: [debug] Watching for changes to Job kubeslice-worker-preinstall-job with timeout of 5m0s\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: ADDED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 0, jobs failed: 0, jobs succeeded: 0\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 1, jobs succeeded: 0\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 2, jobs succeeded: 0\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 3, jobs succeeded: 0\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 4, jobs succeeded: 0\n    client.go:310: [debug] Starting delete for \"kubeslice-worker-preinstall-job\" Job\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:310: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"jaeger\" Service\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-svc\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:339: [debug] services \"kubeslice-webhook-service\" not found\n    client.go:339: [debug] services \"jaeger\" not found\n    client.go:339: [debug] services \"kubeslice-dns\" not found\n    client.go:339: [debug] services \"nsm-admission-webhook-svc\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook\" Deployment\n    client.go:310: [debug] Starting delete for \"jaeger\" Deployment\n    client.go:310: [debug] Starting delete for \"prefix-service\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:339: [debug] deployments.apps \"kubeslice-operator\" not found\n    client.go:339: [debug] deployments.apps \"kubeslice-dns\" not found\n    client.go:339: [debug] deployments.apps \"nsm-admission-webhook\" not found\n    client.go:339: [debug] deployments.apps \"jaeger\" not found\n    client.go:339: [debug] deployments.apps \"prefix-service\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsm-kernel-forwarder\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:339: [debug] daemonsets.apps \"kubeslice-netop\" not found\n    client.go:339: [debug] daemonsets.apps \"nsmgr\" not found\n    client.go:339: [debug] daemonsets.apps \"nsm-kernel-forwarder\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:339: [debug] rolebindings.rbac.authorization.k8s.io \"kubeslice-leader-election-rolebinding\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:339: [debug] roles.rbac.authorization.k8s.io \"kubeslice-leader-election-role\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-dns-rolebinding\" not found\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-kubernetes-dashboard\" not found\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-manager-rolebinding\" not found\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"nsm-role-binding\" not found\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-proxy-rolebinding\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"aggregate-network-services-view\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-dns-role\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-manager-role\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-kubernetes-dashboard\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-proxy-role\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"nsm-role\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-metrics-reader\" not found\n    client.go:310: [debug] Starting delete for \"networkservices.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkserviceendpoints.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservicemanagers.networkservicemesh.io\" CustomResourceDefinition\n    client.go:339: [debug] customresourcedefinitions.apiextensions.k8s.io \"networkserviceendpoints.networkservicemesh.io\" not found\n    client.go:339: [debug] customresourcedefinitions.apiextensions.k8s.io \"networkservices.networkservicemesh.io\" not found\n    client.go:339: [debug] customresourcedefinitions.apiextensions.k8s.io \"networkservicemanagers.networkservicemesh.io\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:339: [debug] configmaps \"nsm-config\" not found\n    client.go:339: [debug] configmaps \"kubeslice-manager-config\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-image-pull-secret\" Secret\n    client.go:339: [debug] secrets \"kubeslice-admission-webhook-certs\" not found\n    client.go:339: [debug] secrets \"kubeslice-image-pull-secret\" not found\n    client.go:339: [debug] secrets \"nsm-admission-webhook-certs\" not found\n    client.go:339: [debug] secrets \"kubeslice-hub\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:339: [debug] serviceaccounts \"slice-router\" not found\n    client.go:339: [debug] serviceaccounts \"nse-acc\" not found\n    client.go:339: [debug] serviceaccounts \"kubeslice-controller-manager\" not found\n    client.go:339: [debug] serviceaccounts \"nsc-acc\" not found\n    client.go:339: [debug] serviceaccounts \"kubeslice-kubernetes-dashboard\" not found\n    client.go:339: [debug] serviceaccounts \"kubeslice-netop\" not found\n    client.go:339: [debug] serviceaccounts \"vpn-gateway-server\" not found\n    client.go:339: [debug] serviceaccounts \"forward-plane-acc\" not found\n    client.go:339: [debug] serviceaccounts \"nsmgr-acc\" not found\n    client.go:339: [debug] serviceaccounts \"kubeslice-dns\" not found\n    client.go:339: [debug] serviceaccounts \"vpn-gateway-client\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-cfg\" MutatingWebhookConfiguration\n    client.go:339: [debug] mutatingwebhookconfigurations.admissionregistration.k8s.io \"nsm-admission-webhook-cfg\" not found\n    client.go:339: [debug] mutatingwebhookconfigurations.admissionregistration.k8s.io \"kubeslice-mutating-webhook-configuration\" not found\n    uninstall.go:148: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: failed pre-install: timed out waiting for the condition\n    helm.go:84: [debug] failed pre-install: timed out waiting for the condition\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:361\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:141\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\noccurred","time":{"start":1666706731000,"stop":1666707135483,"duration":404483}},{"uid":"dddf74b986714d21","status":"passed","time":{"start":1666674484000,"stop":1666674589437,"duration":105437}},{"uid":"757001286de8d90","status":"passed","time":{"start":1666673362000,"stop":1666673442347,"duration":80347}},{"uid":"341390dc3d91fc4","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000e0f0>: {\n        Underlying: <*exec.ExitError | 0xc0001668a0>{\n            ProcessState: {\n                pid: 6250,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 45106},\n                    Stime: {Sec: 0, Usec: 15035},\n                    Maxrss: 39928,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2308,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 253,\n                    Nivcsw: 246,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:192: [debug] Original chart version: \\\"\\\"\",\n                        \"Error: INSTALLATION FAILED: looks like \\\"https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/\\\" is not a valid chart repository or cannot be reached: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/index.yaml : 404 Not Found\",\n                        \"helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/index.yaml : 404 Not Found\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\",\n                        \"helm.sh/helm/v3/pkg/repo.(*ChartRepository).DownloadIndexFile\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:128\",\n                        \"helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:252\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:723\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:204\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:139\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:872\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:990\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:918\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:250\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1571\",\n                        \"looks like \\\"https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/\\\" is not a valid chart repository or cannot be reached\",\n                        \"helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:254\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:723\",\n                        \"main...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:192: [debug] Original chart version: \"\"\n    Error: INSTALLATION FAILED: looks like \"https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/\" is not a valid chart repository or cannot be reached: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/index.yaml : 404 Not Found\n    helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/index.yaml : 404 Not Found\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\n    helm.sh/helm/v3/pkg/repo.(*ChartRepository).DownloadIndexFile\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:128\n    helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:252\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:723\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:204\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:139\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    looks like \"https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/\" is not a valid chart repository or cannot be reached\n    helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:254\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:723\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:204\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:139\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:141\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\noccurred","time":{"start":1666668696000,"stop":1666668782021,"duration":86021}},{"uid":"c91e427d762d191b","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000128090>: {\n        Underlying: <*exec.ExitError | 0xc000774000>{\n            ProcessState: {\n                pid: 6107,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 3, Usec: 550293},\n                    Stime: {Sec: 0, Usec: 328211},\n                    Maxrss: 97520,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 15599,\n                    Majflt: 7,\n                    Nswap: 0,\n                    Inblock: 512,\n                    Oublock: 20168,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 5960,\n                    Nivcsw: 9372,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"install.go:192: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.4.3.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:165: [debug] Clearing discovery cache\",\n                        \"wait.go:66: [debug] beginning wait for 6 resources with timeout of 1m0s\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:310: [debug] Starting delete for \\\"kubeslice-preinstall\\\" ServiceAccount\",\n                        \"client.go:339: [debug] serviceaccounts \\\"kubeslice-preinstall\\\" not found\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:310: [debug] Starting delete for \\\"kubeslice-preinstall-role\\\" ClusterRole\",\n                        \"client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \\\"kubeslice-preinstall-role\\\" not found\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:310: [debug] Starting delete for \\\"kubeslice-preinstall-rolebinding\\\" ClusterRoleBinding\",\n                        \"client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \\\"kubeslice-preinstall-rolebinding\\\" not found\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:310: [debug] Starting delete for \\\"kubeslice-worker-preinstall-configmap\\\" ConfigMap\",\n                        \"client.go:339: [debug] configmaps \\\"kubeslice-worker-preinstall-configmap\\\" not found\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:310: [debug] Starting delete for \\\"kubeslice-worker-preinstall-job\\\" Job\",\n                        \"client.go:339: [debug] jobs.batch \\\"kubeslice-worker-preinstall-job\\\" not found\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:540: [debug] Watching for changes to Job kubeslice-worker-preinstall-job with timeout of 5m0s\",\n                        \"client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: ADDED\",\n                        \"client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 0, jobs failed: 0, jobs succeeded: 0\",\n                        \"client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\",\n                        \"client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, job...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; install.go:192: [debug] Original chart version: \"\"\n    install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.4.3.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:165: [debug] Clearing discovery cache\n    wait.go:66: [debug] beginning wait for 6 resources with timeout of 1m0s\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:310: [debug] Starting delete for \"kubeslice-preinstall\" ServiceAccount\n    client.go:339: [debug] serviceaccounts \"kubeslice-preinstall\" not found\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:310: [debug] Starting delete for \"kubeslice-preinstall-role\" ClusterRole\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-preinstall-role\" not found\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:310: [debug] Starting delete for \"kubeslice-preinstall-rolebinding\" ClusterRoleBinding\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-preinstall-rolebinding\" not found\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:310: [debug] Starting delete for \"kubeslice-worker-preinstall-configmap\" ConfigMap\n    client.go:339: [debug] configmaps \"kubeslice-worker-preinstall-configmap\" not found\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:310: [debug] Starting delete for \"kubeslice-worker-preinstall-job\" Job\n    client.go:339: [debug] jobs.batch \"kubeslice-worker-preinstall-job\" not found\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:540: [debug] Watching for changes to Job kubeslice-worker-preinstall-job with timeout of 5m0s\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: ADDED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 0, jobs failed: 0, jobs succeeded: 0\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 1, jobs succeeded: 0\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 2, jobs succeeded: 0\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 3, jobs succeeded: 0\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 4, jobs succeeded: 0\n    client.go:310: [debug] Starting delete for \"kubeslice-worker-preinstall-job\" Job\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:310: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-svc\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:310: [debug] Starting delete for \"jaeger\" Service\n    client.go:339: [debug] services \"kubeslice-webhook-service\" not found\n    client.go:339: [debug] services \"kubeslice-dns\" not found\n    client.go:339: [debug] services \"nsm-admission-webhook-svc\" not found\n    client.go:339: [debug] services \"jaeger\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook\" Deployment\n    client.go:310: [debug] Starting delete for \"jaeger\" Deployment\n    client.go:310: [debug] Starting delete for \"prefix-service\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:339: [debug] deployments.apps \"kubeslice-operator\" not found\n    client.go:339: [debug] deployments.apps \"jaeger\" not found\n    client.go:339: [debug] deployments.apps \"nsm-admission-webhook\" not found\n    client.go:339: [debug] deployments.apps \"prefix-service\" not found\n    client.go:339: [debug] deployments.apps \"kubeslice-dns\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsm-kernel-forwarder\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:339: [debug] daemonsets.apps \"kubeslice-netop\" not found\n    client.go:339: [debug] daemonsets.apps \"nsmgr\" not found\n    client.go:339: [debug] daemonsets.apps \"nsm-kernel-forwarder\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:339: [debug] rolebindings.rbac.authorization.k8s.io \"kubeslice-leader-election-rolebinding\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:339: [debug] roles.rbac.authorization.k8s.io \"kubeslice-leader-election-role\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"nsm-role-binding\" not found\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-dns-rolebinding\" not found\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-proxy-rolebinding\" not found\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-manager-rolebinding\" not found\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-kubernetes-dashboard\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRole\n    client.go:310: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-metrics-reader\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"nsm-role\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-kubernetes-dashboard\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"aggregate-network-services-view\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-dns-role\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-proxy-role\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-manager-role\" not found\n    client.go:310: [debug] Starting delete for \"networkservices.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkserviceendpoints.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservicemanagers.networkservicemesh.io\" CustomResourceDefinition\n    client.go:339: [debug] customresourcedefinitions.apiextensions.k8s.io \"networkservicemanagers.networkservicemesh.io\" not found\n    client.go:339: [debug] customresourcedefinitions.apiextensions.k8s.io \"networkserviceendpoints.networkservicemesh.io\" not found\n    client.go:339: [debug] customresourcedefinitions.apiextensions.k8s.io \"networkservices.networkservicemesh.io\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:339: [debug] configmaps \"kubeslice-manager-config\" not found\n    client.go:339: [debug] configmaps \"nsm-config\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-image-pull-secret\" Secret\n    client.go:339: [debug] secrets \"nsm-admission-webhook-certs\" not found\n    client.go:339: [debug] secrets \"kubeslice-hub\" not found\n    client.go:339: [debug] secrets \"kubeslice-admission-webhook-certs\" not found\n    client.go:339: [debug] secrets \"kubeslice-image-pull-secret\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:339: [debug] serviceaccounts \"kubeslice-controller-manager\" not found\n    client.go:339: [debug] serviceaccounts \"nse-acc\" not found\n    client.go:339: [debug] serviceaccounts \"kubeslice-netop\" not found\n    client.go:339: [debug] serviceaccounts \"nsc-acc\" not found\n    client.go:339: [debug] serviceaccounts \"kubeslice-kubernetes-dashboard\" not found\n    client.go:339: [debug] serviceaccounts \"vpn-gateway-server\" not found\n    client.go:339: [debug] serviceaccounts \"nsmgr-acc\" not found\n    client.go:339: [debug] serviceaccounts \"kubeslice-dns\" not found\n    client.go:339: [debug] serviceaccounts \"forward-plane-acc\" not found\n    client.go:339: [debug] serviceaccounts \"slice-router\" not found\n    client.go:339: [debug] serviceaccounts \"vpn-gateway-client\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-cfg\" MutatingWebhookConfiguration\n    client.go:339: [debug] mutatingwebhookconfigurations.admissionregistration.k8s.io \"nsm-admission-webhook-cfg\" not found\n    client.go:339: [debug] mutatingwebhookconfigurations.admissionregistration.k8s.io \"kubeslice-mutating-webhook-configuration\" not found\n    uninstall.go:148: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: failed pre-install: timed out waiting for the condition\n    helm.go:84: [debug] failed pre-install: timed out waiting for the condition\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:361\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:141\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\noccurred","time":{"start":1666667460000,"stop":1666667845764,"duration":385764}},{"uid":"a28b1f12333c838d","status":"passed","time":{"start":1666664909000,"stop":1666665007521,"duration":98521}},{"uid":"8bf37e4b8a17dd45","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000ef18>: {\n        Underlying: <*exec.ExitError | 0xc00015e920>{\n            ProcessState: {\n                pid: 6111,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 66684},\n                    Stime: {Sec: 0, Usec: 16671},\n                    Maxrss: 40824,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 1963,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 2240,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 851,\n                    Nivcsw: 783,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"install.go:192: [debug] Original chart version: \\\"1.7.0\\\"\",\n                        \"Error: INSTALLATION FAILED: looks like \\\"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/\\\" is not a valid chart repository or cannot be reached: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\",\n                        \"helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\",\n                        \"helm.sh/helm/v3/pkg/repo.(*ChartRepository).DownloadIndexFile\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:128\",\n                        \"helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:252\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:723\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:204\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:139\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:872\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:990\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:918\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:250\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1571\",\n                        \"looks like \\\"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/\\\" is not a valid chart repository or cannot be reached\",\n                        \"helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:254\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:723\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:204\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:139\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; install.go:192: [debug] Original chart version: \"1.7.0\"\n    Error: INSTALLATION FAILED: looks like \"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/\" is not a valid chart repository or cannot be reached: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\n    helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\n    helm.sh/helm/v3/pkg/repo.(*ChartRepository).DownloadIndexFile\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:128\n    helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:252\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:723\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:204\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:139\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    looks like \"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/\" is not a valid chart repository or cannot be reached\n    helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:254\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:723\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:204\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:139\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:141\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\noccurred","time":{"start":1666664766000,"stop":1666664766570,"duration":570}},{"uid":"645a236f3a2d9ff7","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000e750>: {\n        Underlying: <*exec.ExitError | 0xc0004530e0>{\n            ProcessState: {\n                pid: 6000,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 82558},\n                    Stime: {Sec: 0, Usec: 36692},\n                    Maxrss: 40852,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2453,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 2240,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 1098,\n                    Nivcsw: 725,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"install.go:192: [debug] Original chart version: \\\"1.7.0\\\"\",\n                        \"Error: INSTALLATION FAILED: looks like \\\"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/\\\" is not a valid chart repository or cannot be reached: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\",\n                        \"helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\",\n                        \"helm.sh/helm/v3/pkg/repo.(*ChartRepository).DownloadIndexFile\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:128\",\n                        \"helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:252\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:723\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:204\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:139\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:872\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:990\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:918\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:250\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1571\",\n                        \"looks like \\\"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/\\\" is not a valid chart repository or cannot be reached\",\n                        \"helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:254\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:723\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:204\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:139\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; install.go:192: [debug] Original chart version: \"1.7.0\"\n    Error: INSTALLATION FAILED: looks like \"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/\" is not a valid chart repository or cannot be reached: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\n    helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\n    helm.sh/helm/v3/pkg/repo.(*ChartRepository).DownloadIndexFile\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:128\n    helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:252\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:723\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:204\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:139\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    looks like \"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/\" is not a valid chart repository or cannot be reached\n    helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:254\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:723\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:204\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:139\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:141\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\noccurred","time":{"start":1666663248000,"stop":1666663248894,"duration":894}},{"uid":"c1ff94863976f176","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000e0c0>: {\n        Underlying: <*exec.ExitError | 0xc0009ea000>{\n            ProcessState: {\n                pid: 6173,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 4, Usec: 33369},\n                    Stime: {Sec: 0, Usec: 869942},\n                    Maxrss: 110972,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 8135,\n                    Majflt: 8,\n                    Nswap: 0,\n                    Inblock: 1304,\n                    Oublock: 19504,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 50842,\n                    Nivcsw: 13581,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:192: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.4.6.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:165: [debug] Clearing discovery cache\",\n                        \"wait.go:66: [debug] beginning wait for 4 resources with timeout of 1m0s\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 47 resource(s)\",\n                        \"wait.go:66: [debug] beginning wait for 47 resources with timeout of 5m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admiss...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:192: [debug] Original chart version: \"\"\n    install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.4.6.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:165: [debug] Clearing discovery cache\n    wait.go:66: [debug] beginning wait for 4 resources with timeout of 1m0s\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 47 resource(s)\n    wait.go:66: [debug] beginning wait for 47 resources with timeout of 5m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-svc\" Service\n    client.go:310: [debug] Starting delete for \"jaeger\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:310: [debug] Starting delete for \"jaeger\" Deployment\n    client.go:310: [debug] Starting delete for \"prefix-service\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsm-kernel-forwarder\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRole\n    client.go:310: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"networkservices.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkserviceendpoints.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservicemanagers.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-cfg\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    uninstall.go:148: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:141\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\noccurred","time":{"start":1666423616000,"stop":1666423979092,"duration":363092}},{"uid":"79cc1bc06ad67c4","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004b82a0>: {\n        Underlying: <*exec.ExitError | 0xc00049e200>{\n            ProcessState: {\n                pid: 6236,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 43305},\n                    Stime: {Sec: 0, Usec: 14435},\n                    Maxrss: 40652,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 1962,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 248,\n                    Nivcsw: 145,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:192: [debug] Original chart version: \\\"\\\"\",\n                        \"Error: INSTALLATION FAILED: looks like \\\"https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/\\\" is not a valid chart repository or cannot be reached: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/index.yaml : 404 Not Found\",\n                        \"helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/index.yaml : 404 Not Found\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\",\n                        \"helm.sh/helm/v3/pkg/repo.(*ChartRepository).DownloadIndexFile\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:128\",\n                        \"helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:252\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:723\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:204\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:139\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:872\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:990\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:918\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:250\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1571\",\n                        \"looks like \\\"https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/\\\" is not a valid chart repository or cannot be reached\",\n                        \"helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:254\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:723\",\n                        \"main...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:192: [debug] Original chart version: \"\"\n    Error: INSTALLATION FAILED: looks like \"https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/\" is not a valid chart repository or cannot be reached: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/index.yaml : 404 Not Found\n    helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/index.yaml : 404 Not Found\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\n    helm.sh/helm/v3/pkg/repo.(*ChartRepository).DownloadIndexFile\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:128\n    helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:252\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:723\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:204\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:139\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    looks like \"https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/\" is not a valid chart repository or cannot be reached\n    helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:254\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:723\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:204\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:139\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:141\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\noccurred","time":{"start":1666419243000,"stop":1666419329633,"duration":86633}},{"uid":"d78f6105072d48dc","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00027e048>: {\n        Underlying: <*exec.ExitError | 0xc0006a2000>{\n            ProcessState: {\n                pid: 6218,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 4, Usec: 894819},\n                    Stime: {Sec: 0, Usec: 741351},\n                    Maxrss: 99340,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 8093,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 17224,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 52372,\n                    Nivcsw: 12679,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:192: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.4.6.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:165: [debug] Clearing discovery cache\",\n                        \"wait.go:66: [debug] beginning wait for 4 resources with timeout of 1m0s\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 47 resource(s)\",\n                        \"wait.go:66: [debug] beginning wait for 47 resources with timeout of 5m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:192: [debug] Original chart version: \"\"\n    install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.4.6.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:165: [debug] Clearing discovery cache\n    wait.go:66: [debug] beginning wait for 4 resources with timeout of 1m0s\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 47 resource(s)\n    wait.go:66: [debug] beginning wait for 47 resources with timeout of 5m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:310: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"jaeger\" Service\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-svc\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:310: [debug] Starting delete for \"jaeger\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook\" Deployment\n    client.go:310: [debug] Starting delete for \"prefix-service\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsm-kernel-forwarder\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRole\n    client.go:310: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"networkservices.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkserviceendpoints.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservicemanagers.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-cfg\" MutatingWebhookConfiguration\n    uninstall.go:148: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:141\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\noccurred","time":{"start":1666338675000,"stop":1666339034374,"duration":359374}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests Iperf App connectivity Verify Connectivity between iperf-server and iperf-client":{"statistic":{"failed":0,"broken":0,"skipped":5,"passed":1,"unknown":0,"total":6},"items":[{"uid":"210713decb7a019b","status":"passed","time":{"start":1667837307000,"stop":1667837429738,"duration":122738}},{"uid":"80d110eb33b9fb08","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667826674000,"stop":1667826674000,"duration":0}},{"uid":"5eb3951bc3b8959","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667823949000,"stop":1667823949000,"duration":0}},{"uid":"8af4dab6d6f947","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674621000,"stop":1666674621000,"duration":0}},{"uid":"8753cd3b08bcb7af","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674212000,"stop":1666674212000,"duration":0}},{"uid":"7805020f88b3f0f7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666665366000,"stop":1666665366000,"duration":0}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests Iperf App connectivity should have gateway pod running":{"statistic":{"failed":2,"broken":0,"skipped":2,"passed":2,"unknown":0,"total":6},"items":[{"uid":"f10eb68ded87347","status":"passed","time":{"start":1667837307000,"stop":1667837307063,"duration":63}},{"uid":"2d794d6c7b3da060","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667826674000,"stop":1667826674000,"duration":0}},{"uid":"49bfa1f4e2cb39cd","status":"passed","time":{"start":1667823949000,"stop":1667823949119,"duration":119}},{"uid":"86dc75443902fa08","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1666674621000,"stop":1666674802755,"duration":181755}},{"uid":"bab3a388a5ecb9bd","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674212000,"stop":1666674212000,"duration":0}},{"uid":"4e0bc44d3f73fee","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1666665366000,"stop":1666665546702,"duration":180702}}]},"Worker Suite:Worker Suite#[It] Slice resource quota Tests ResourceQuota creation tests Create ResourceQuota fail test - sum(cluster.limit.mem) > slice.mem":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":3,"unknown":0,"total":3},"items":[{"uid":"53c706ab51c71034","status":"passed","time":{"start":1667739621000,"stop":1667739621966,"duration":966}},{"uid":"d682e722a64555dd","status":"passed","time":{"start":1667458785000,"stop":1667458786166,"duration":1166}},{"uid":"e0410f8d2181af0a","status":"passed","time":{"start":1667389661000,"stop":1667389661772,"duration":772}}]},"Worker Suite:Worker Suite#[It] Slice Config Negative Scenario tests Slice Config Negative Scenario Creation Should fail while creating sliceconfig with with invalid DSCP class in qosprofiledetails":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":6,"unknown":0,"total":6},"items":[{"uid":"9448a62d3494a619","status":"passed","time":{"start":1667837307000,"stop":1667837307361,"duration":361}},{"uid":"98271e42dc19be67","status":"passed","time":{"start":1667826674000,"stop":1667826674401,"duration":401}},{"uid":"3f0b68166fb7ea19","status":"passed","time":{"start":1667823949000,"stop":1667823949952,"duration":952}},{"uid":"e9bd73558e3feca3","status":"passed","time":{"start":1666674621000,"stop":1666674621516,"duration":516}},{"uid":"a6c11be7e96cf2fc","status":"passed","time":{"start":1666674212000,"stop":1666674212493,"duration":493}},{"uid":"eac4bdbdc0152d87","status":"passed","time":{"start":1666665366000,"stop":1666665366352,"duration":352}}]},"Worker Suite:Worker Suite#[It] Slice creation scenarios: when slice red is installed install slice blue workers should have vl3 routers from both slices":{"statistic":{"failed":0,"broken":0,"skipped":4,"passed":2,"unknown":0,"total":6},"items":[{"uid":"da11ab978c051bcd","status":"passed","time":{"start":1667837307000,"stop":1667837311236,"duration":4236}},{"uid":"a3a8cbf4fd0e1db3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667826674000,"stop":1667826674000,"duration":0}},{"uid":"f2c703b555861d60","status":"passed","time":{"start":1667823949000,"stop":1667823952526,"duration":3526}},{"uid":"a005e805a4b9d56b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674621000,"stop":1666674621000,"duration":0}},{"uid":"9177072624ebe51d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674212000,"stop":1666674212000,"duration":0}},{"uid":"a451ea5f6a43dee8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666665366000,"stop":1666665366000,"duration":0}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Update Should fail when Project is applied with service account name as blank in Write users":{"statistic":{"failed":2,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":3},"items":[{"uid":"b6e3551521ec354f","status":"passed","time":{"start":1667840900000,"stop":1667840905868,"duration":5868}},{"uid":"66f00ba1c6d81b44","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0001cf248>: {\n        Underlying: <*exec.ExitError | 0xc000829280>{\n            ProcessState: {\n                pid: 7667,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 201845},\n                    Stime: {Sec: 0, Usec: 60553},\n                    Maxrss: 77900,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 7913,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13760,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 445,\n                    Nivcsw: 917,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20applied%20with%20service%20account%20name%20as%20blank%20in%20Write%20users1600498833\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.94.3:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20applied%20with%20service%20account%20name%20as%20blank%20in%20Write%20users1600498833\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.94.3:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20applied%20with%20service%20account%20name%20as%20blank%20in%20Write%20users1600498833\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.94.3:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20applied%20with%20service%20account%20name%20as%20blank%20in%20Write%20users1600498833\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.94.3:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20applied%20with%20service%20account%20name%20as%20blank%20in%20Write%20users1600498833\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.94.3:443: connect: connection refused\noccurred","time":{"start":1665665955000,"stop":1665665955329,"duration":329}},{"uid":"c7904797d3391969","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000535c50>: {\n        Underlying: <*exec.ExitError | 0xc00077bc20>{\n            ProcessState: {\n                pid: 7243,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 219874},\n                    Stime: {Sec: 0, Usec: 82971},\n                    Maxrss: 89736,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 5453,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13760,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 348,\n                    Nivcsw: 284,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20applied%20with%20service%20account%20name%20as%20blank%20in%20Write%20users428461408\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.116.233:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20applied%20with%20service%20account%20name%20as%20blank%20in%20Write%20users428461408\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.116.233:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20applied%20with%20service%20account%20name%20as%20blank%20in%20Write%20users428461408\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.116.233:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20applied%20with%20service%20account%20name%20as%20blank%20in%20Write%20users428461408\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.116.233:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20applied%20with%20service%20account%20name%20as%20blank%20in%20Write%20users428461408\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.116.233:443: connect: connection refused\noccurred","time":{"start":1665666027000,"stop":1665666027386,"duration":386}}]},"13ebe318b750e99b58ecccc097a6ac75":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"3ed4c898ac054102","status":"passed","time":{"start":1664357313667,"stop":1664357345958,"duration":32291}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Deletion Should fail when deleting a project that does not exist":{"statistic":{"failed":2,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":3},"items":[{"uid":"82fa90c1ffccbd84","status":"passed","time":{"start":1667840900000,"stop":1667840907766,"duration":7766}},{"uid":"c5733e7e403a8e21","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0003880d8>: {\n        Underlying: <*exec.ExitError | 0xc000894000>{\n            ProcessState: {\n                pid: 7677,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 175839},\n                    Stime: {Sec: 0, Usec: 54426},\n                    Maxrss: 89052,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 11105,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13760,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 363,\n                    Nivcsw: 263,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Deletion%20Should%20fail%20when%20deleting%20a%20project%20that%20does%20not%20exist3953096897\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.94.3:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Deletion%20Should%20fail%20when%20deleting%20a%20project%20that%20does%20not%20exist3953096897\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.94.3:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Deletion%20Should%20fail%20when%20deleting%20a%20project%20that%20does%20not%20exist3953096897\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.94.3:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Deletion%20Should%20fail%20when%20deleting%20a%20project%20that%20does%20not%20exist3953096897\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.94.3:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Deletion%20Should%20fail%20when%20deleting%20a%20project%20that%20does%20not%20exist3953096897\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.94.3:443: connect: connection refused\noccurred","time":{"start":1665665955000,"stop":1665665955272,"duration":272}},{"uid":"b939f92de8e1b585","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <bool>: false\nto be true","time":{"start":1665666027000,"stop":1665666104854,"duration":77854}}]},"Worker Suite:Worker Suite#[It] Hub netpol scenarios: Slice Configs Netpol scenarios Should successfully deploy slice for valid namespace and valid clusters in allowedNamespaces of sliceconfigs manifest":{"statistic":{"failed":1,"broken":0,"skipped":1,"passed":4,"unknown":0,"total":6},"items":[{"uid":"114f8e0c747f650d","status":"passed","time":{"start":1667837307000,"stop":1667837307805,"duration":805}},{"uid":"ff7dda3a2a39ca73","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667826674000,"stop":1667826674000,"duration":0}},{"uid":"6b5dc87a42d9ad00","status":"failed","statusDetails":"Expected\n    <[]string | len:0, cap:0>: nil\nto equal\n    <[]string | len:1, cap:1>: [\"kube-system\"]","time":{"start":1667823949000,"stop":1667823950325,"duration":1325}},{"uid":"33b93a599a242e16","status":"passed","time":{"start":1666674621000,"stop":1666674622190,"duration":1190}},{"uid":"a24b9dd2853d10d1","status":"passed","time":{"start":1666674212000,"stop":1666674213183,"duration":1183}},{"uid":"c8cdc8892617b1ae","status":"passed","time":{"start":1666665366000,"stop":1666665367174,"duration":1174}}]},"Worker Suite:Worker Suite#[It] Slice RBAC Tests RBAC Common Tests SliceRoleBinding status test: SliceRoleBinding should have empty status for successful WorkerSliceRoleBinding generation":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":0,"unknown":0,"total":2},"items":[{"uid":"9f3b3fabedc5002b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667739621000,"stop":1667739621000,"duration":0}},{"uid":"1391a22fe21f1f5b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667458785000,"stop":1667458785000,"duration":0}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Update Should fail when Project is Applied with service account name as combination of special characters in Write users":{"statistic":{"failed":2,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":3},"items":[{"uid":"7afc541f8d120ab9","status":"passed","time":{"start":1667840900000,"stop":1667840904876,"duration":4876}},{"uid":"ab859194a1b3a2c3","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0006c3fc8>: {\n        Underlying: <*exec.ExitError | 0xc000529d00>{\n            ProcessState: {\n                pid: 7658,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 185920},\n                    Stime: {Sec: 0, Usec: 71203},\n                    Maxrss: 81272,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 7903,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13760,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 332,\n                    Nivcsw: 308,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Write%20users1908415316\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.94.3:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Write%20users1908415316\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.94.3:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Write%20users1908415316\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.94.3:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Write%20users1908415316\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.94.3:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Write%20users1908415316\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.94.3:443: connect: connection refused\noccurred","time":{"start":1665665955000,"stop":1665665955291,"duration":291}},{"uid":"a68bb7ac415a5254","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00061c060>: {\n        Underlying: <*exec.ExitError | 0xc0007a6980>{\n            ProcessState: {\n                pid: 7234,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 261896},\n                    Stime: {Sec: 0, Usec: 74827},\n                    Maxrss: 87332,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 6065,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13760,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 345,\n                    Nivcsw: 386,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Write%20users1169169401\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.116.233:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Write%20users1169169401\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.116.233:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Write%20users1169169401\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.116.233:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Write%20users1169169401\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.116.233:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Write%20users1169169401\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.116.233:443: connect: connection refused\noccurred","time":{"start":1665666027000,"stop":1665666027429,"duration":429}}]},"Worker Suite:Worker Suite#[It] Slice RBAC Tests RBAC Common Tests WorkerSliceRoleBinding data propagation test: Checking successful propagation of rules with namespace as asterisk":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":0,"unknown":0,"total":2},"items":[{"uid":"e815a3236f670eb5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667739621000,"stop":1667739621000,"duration":0}},{"uid":"96609e417cecc098","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667458785000,"stop":1667458785000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice Config Negative Scenario tests Slice Config Negative Scenario Update Should update while deploying sliceconfig with existing slice name":{"statistic":{"failed":1,"broken":0,"skipped":0,"passed":5,"unknown":0,"total":6},"items":[{"uid":"dad2ca82559a28e3","status":"passed","time":{"start":1667837307000,"stop":1667837308188,"duration":1188}},{"uid":"8e426ce3bfd3b42","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000912c00>: {\n        Underlying: <*exec.ExitError | 0xc00048b980>{\n            ProcessState: {\n                pid: 6491,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 228590},\n                    Stime: {Sec: 0, Usec: 47294},\n                    Maxrss: 102684,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4466,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 19808,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 369,\n                    Nivcsw: 689,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"fireredslice\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"fireredslice\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\noccurred","time":{"start":1667826674000,"stop":1667826674372,"duration":372}},{"uid":"6c1e10a2089463eb","status":"passed","time":{"start":1667823949000,"stop":1667823949902,"duration":902}},{"uid":"f79ee2283ca092b1","status":"passed","time":{"start":1666674621000,"stop":1666674622443,"duration":1443}},{"uid":"178c98571fc69a57","status":"passed","time":{"start":1666674212000,"stop":1666674213413,"duration":1413}},{"uid":"8070e92cd6447500","status":"passed","time":{"start":1666665366000,"stop":1666665367387,"duration":1387}}]},"Empty Suite:Empty Suite#[AfterSuite]":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":11,"unknown":0,"total":11},"items":[{"uid":"ed634703411033aa","status":"passed","time":{"start":1666883811000,"stop":1666883811000,"duration":0}},{"uid":"250910c86a48a626","status":"passed","time":{"start":1666754109000,"stop":1666754109000,"duration":0}},{"uid":"71d9d83711a7efc1","status":"passed","time":{"start":1666750338000,"stop":1666750338000,"duration":0}},{"uid":"8da04d8f1951558a","status":"passed","time":{"start":1666747791000,"stop":1666747791000,"duration":0}},{"uid":"15d4290774dff69b","status":"passed","time":{"start":1666706575000,"stop":1666706575000,"duration":0}},{"uid":"2eb8e75b5539d3c7","status":"passed","time":{"start":1666077791000,"stop":1666077791000,"duration":0}},{"uid":"a3426ccb20790311","status":"passed","time":{"start":1665944068000,"stop":1665944068000,"duration":0}},{"uid":"1e701ed935903b9c","status":"passed","time":{"start":1665665806000,"stop":1665665806000,"duration":0}},{"uid":"3b132a545dd9db2","status":"passed","time":{"start":1665665864000,"stop":1665665864000,"duration":0}},{"uid":"6089fc4ee091501d","status":"passed","time":{"start":1665552534000,"stop":1665552534000,"duration":0}},{"uid":"e5a9c1b71dd6c71b","status":"passed","time":{"start":1664798682000,"stop":1664798682000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice RBAC Tests SliceRoleBinding Tests Create SliceRoleBinding failure test: Creating SliceRoleBinding slice-red with empty namespace":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":2,"unknown":0,"total":2},"items":[{"uid":"ea6e707c1ac63a45","status":"passed","time":{"start":1667739621000,"stop":1667739621682,"duration":682}},{"uid":"951c05ab00888ab7","status":"passed","time":{"start":1667458785000,"stop":1667458785497,"duration":497}}]},"Worker Suite:Worker Suite#[It] Slice RBAC Tests RBAC Common Tests WorkerSliceRoleBinding data propagation test: Checking successful creation of labels":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":0,"unknown":0,"total":2},"items":[{"uid":"81afc451024d33c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667739621000,"stop":1667739621000,"duration":0}},{"uid":"f150bb966f18e5ab","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667458785000,"stop":1667458785000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice RBAC Tests SliceRoleBinding Tests Create SliceRoleBinding failure test: Creating SliceRoleBinding slice-red with empty subject":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":2,"unknown":0,"total":2},"items":[{"uid":"8a6fbde5f121a20a","status":"passed","time":{"start":1667739621000,"stop":1667739621975,"duration":975}},{"uid":"21d98fa7daf4a0e6","status":"passed","time":{"start":1667458785000,"stop":1667458785798,"duration":798}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests Iperf App connectivity Should install iperf-server on server cluster":{"statistic":{"failed":0,"broken":0,"skipped":5,"passed":1,"unknown":0,"total":6},"items":[{"uid":"bb1cf81d898428ca","status":"passed","time":{"start":1667837307000,"stop":1667837335338,"duration":28338}},{"uid":"ad41dc958b740292","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667826674000,"stop":1667826674000,"duration":0}},{"uid":"11c547169afc8b32","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667823949000,"stop":1667823949000,"duration":0}},{"uid":"ce39ab00c7139db8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674621000,"stop":1666674621000,"duration":0}},{"uid":"4f2e6a84f35172b1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674212000,"stop":1666674212000,"duration":0}},{"uid":"e13de42a2b3cee8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666665366000,"stop":1666665366000,"duration":0}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests slice with netpol should remove network policy resource from application namespace once the isolationEnabled is set to false":{"statistic":{"failed":0,"broken":0,"skipped":4,"passed":2,"unknown":0,"total":6},"items":[{"uid":"b8996b1d840700d3","status":"passed","time":{"start":1667837307000,"stop":1667837320602,"duration":13602}},{"uid":"ff8c136dd674e36a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667826674000,"stop":1667826674000,"duration":0}},{"uid":"ec753a0a9086d72b","status":"passed","time":{"start":1667823949000,"stop":1667823963516,"duration":14516}},{"uid":"bda93b900e2eda52","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674621000,"stop":1666674621000,"duration":0}},{"uid":"2150362fda73467c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674212000,"stop":1666674212000,"duration":0}},{"uid":"9976778184a80502","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666665366000,"stop":1666665366000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice RBAC Tests RBAC Common Tests SliceRoleBinding status test: SliceRoleBinding should have error status for role-ref not found":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":0,"unknown":0,"total":2},"items":[{"uid":"5c657bc968265d7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667739621000,"stop":1667739621000,"duration":0}},{"uid":"b2eb1d23fa79b665","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667458785000,"stop":1667458785000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice RBAC Tests SliceRoleBinding Tests Delete SliceRoleBinding success test: Deletion of SliceConfig should delete SliceRoleBinding":{"statistic":{"failed":1,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":2},"items":[{"uid":"21a2829740f2df5a","status":"passed","time":{"start":1667739621000,"stop":1667739625163,"duration":4163}},{"uid":"d85db4415055befa","status":"failed","statusDetails":"Expected an error to have occurred.  Got:\n    <nil>: nil","time":{"start":1667458785000,"stop":1667458795711,"duration":10711}}]},"Worker Suite:Worker Suite#[It] Slice RBAC Tests SliceRoleBinding Tests Create SliceRoleBinding failure test: Creating SliceRoleBinding slice-red with unsupported K8s Role":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":2,"unknown":0,"total":2},"items":[{"uid":"7e855ed477d28e3e","status":"passed","time":{"start":1667739621000,"stop":1667739621730,"duration":730}},{"uid":"a923a1deccebe9e4","status":"passed","time":{"start":1667458785000,"stop":1667458785910,"duration":910}}]},"Worker Suite:Worker Suite#[It] Slice resource quota Tests ResourceQuota creation tests Create ResourceQuota fail test - no resource specified in namespaceLevel quota  ":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":3,"unknown":0,"total":3},"items":[{"uid":"8c0625f6ba80aa38","status":"passed","time":{"start":1667739621000,"stop":1667739621856,"duration":856}},{"uid":"8a2d594348d76a8e","status":"passed","time":{"start":1667458785000,"stop":1667458785870,"duration":870}},{"uid":"c41b5f5cbf6eb607","status":"passed","time":{"start":1667389661000,"stop":1667389662273,"duration":1273}}]},"Worker Suite:Worker Suite#[It] Resource Quota tests iper connectivity Verify Iperf traffic":{"statistic":{"failed":0,"broken":0,"skipped":3,"passed":0,"unknown":0,"total":3},"items":[{"uid":"ed57b2925206ff9c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667739621000,"stop":1667739621000,"duration":0}},{"uid":"ca1c21a43d29594f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667458785000,"stop":1667458785000,"duration":0}},{"uid":"45761a2c3c7c784a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667389661000,"stop":1667389661000,"duration":0}}]},"Hub Suite:Hub Suite#[It] Cluster negative tests Worker Cluster Registration with Wrong endpoint":{"statistic":{"failed":2,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":3},"items":[{"uid":"9a72eed423d14f6c","status":"passed","time":{"start":1667840900000,"stop":1667841219988,"duration":319988}},{"uid":"1ac94a7e446ca4ba","status":"failed","statusDetails":"Timed out after 120.001s.\nExpected an error to have occurred.  Got:\n    <nil>: nil","time":{"start":1665665955000,"stop":1665666604859,"duration":649859}},{"uid":"d1b51cdd7eea210b","status":"failed","statusDetails":"Timed out after 120.000s.\nExpected an error to have occurred.  Got:\n    <nil>: nil","time":{"start":1665666027000,"stop":1665666762034,"duration":735034}}]},"Worker Suite:Worker Suite#[It] Hub netpol scenarios: Slice Configs Netpol scenarios Should successfully deploy for valid namespace creating clusters with * in both applicationNamespace and allowedNamespace":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":4,"unknown":0,"total":6},"items":[{"uid":"4d0cd087ed5a088d","status":"passed","time":{"start":1667837307000,"stop":1667837307704,"duration":704}},{"uid":"954d4324d25effac","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667826674000,"stop":1667826674000,"duration":0}},{"uid":"48eb34dbdf8ac954","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667823949000,"stop":1667823949000,"duration":0}},{"uid":"c2dcf50e3f16241a","status":"passed","time":{"start":1666674621000,"stop":1666674622235,"duration":1235}},{"uid":"bd9b235ca64e1ccb","status":"passed","time":{"start":1666674212000,"stop":1666674213004,"duration":1004}},{"uid":"8997a52ff303e9b4","status":"passed","time":{"start":1666665366000,"stop":1666665367138,"duration":1138}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Creation Should update Project while applying valid manifest with existing Project name":{"statistic":{"failed":2,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":3},"items":[{"uid":"c119f812e12aadfc","status":"passed","time":{"start":1667840900000,"stop":1667840904774,"duration":4774}},{"uid":"a27b21d15828dbff","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <bool>: false\nto be true","time":{"start":1665665955000,"stop":1665666031643,"duration":76643}},{"uid":"16fbcb1f534144ff","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00061c030>: {\n        Underlying: <*exec.ExitError | 0xc0007ac0a0>{\n            ProcessState: {\n                pid: 7127,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 280122},\n                    Stime: {Sec: 0, Usec: 62249},\n                    Maxrss: 87676,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 10855,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13760,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 476,\n                    Nivcsw: 544,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20update%20Project%20while%20applying%20valid%20manifest%20with%20existing%20Project%20name2294658713\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.116.233:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20update%20Project%20while%20applying%20valid%20manifest%20with%20existing%20Project%20name2294658713\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.116.233:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20update%20Project%20while%20applying%20valid%20manifest%20with%20existing%20Project%20name2294658713\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.116.233:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20update%20Project%20while%20applying%20valid%20manifest%20with%20existing%20Project%20name2294658713\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.116.233:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20update%20Project%20while%20applying%20valid%20manifest%20with%20existing%20Project%20name2294658713\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.116.233:443: connect: connection refused\noccurred","time":{"start":1665666027000,"stop":1665666027409,"duration":409}}]},"Worker Suite:Worker Suite#[BeforeSuite]":{"statistic":{"failed":36,"broken":0,"skipped":0,"passed":9,"unknown":0,"total":45},"items":[{"uid":"99c85a23e69637e8","status":"passed","time":{"start":1667837307000,"stop":1667837431115,"duration":124115}},{"uid":"98213d734a613290","status":"passed","time":{"start":1667826674000,"stop":1667826791721,"duration":117721}},{"uid":"82bddf487ca1c729","status":"passed","time":{"start":1667823949000,"stop":1667824063034,"duration":114034}},{"uid":"fcf32dc008edeb4b","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00089e138>: {\n        Underlying: <*exec.ExitError | 0xc000889e40>{\n            ProcessState: {\n                pid: 6171,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 1, Usec: 94828},\n                    Stime: {Sec: 0, Usec: 94517},\n                    Maxrss: 48004,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2581,\n                    Majflt: 6,\n                    Nswap: 0,\n                    Inblock: 912,\n                    Oublock: 1136,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 1814,\n                    Nivcsw: 2468,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:192: [debug] Original chart version: \\\"0.4.6\\\"\",\n                        \"install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.4.6.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:165: [debug] Clearing discovery cache\",\n                        \"wait.go:66: [debug] beginning wait for 4 resources with timeout of 1m0s\",\n                        \"coalesce.go:175: warning: skipped value for kubeslice-worker.imagePullSecrets: Not a table.\",\n                        \"Error: INSTALLATION FAILED: template: kubeslice-worker/templates/operator-deployment.yaml:85:20: executing \\\"kubeslice-worker/templates/operator-deployment.yaml\\\" at <first .Values.imagePullSecrets>: error calling first: Cannot find first on type map\",\n                        \"helm.go:84: [debug] template: kubeslice-worker/templates/operator-deployment.yaml:85:20: executing \\\"kubeslice-worker/templates/operator-deployment.yaml\\\" at <first .Values.imagePullSecrets>: error calling first: Cannot find first on type map\",\n                        \"INSTALLATION FAILED\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:141\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:872\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:990\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:918\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:250\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1571\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"install.go:192: [debug] Original chart version: \\\"0.4.6\\\"\",\n                  ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:192: [debug] Original chart version: \"0.4.6\"\n    install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.4.6.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:165: [debug] Clearing discovery cache\n    wait.go:66: [debug] beginning wait for 4 resources with timeout of 1m0s\n    coalesce.go:175: warning: skipped value for kubeslice-worker.imagePullSecrets: Not a table.\n    Error: INSTALLATION FAILED: template: kubeslice-worker/templates/operator-deployment.yaml:85:20: executing \"kubeslice-worker/templates/operator-deployment.yaml\" at <first .Values.imagePullSecrets>: error calling first: Cannot find first on type map\n    helm.go:84: [debug] template: kubeslice-worker/templates/operator-deployment.yaml:85:20: executing \"kubeslice-worker/templates/operator-deployment.yaml\" at <first .Values.imagePullSecrets>: error calling first: Cannot find first on type map\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:141\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\noccurred","time":{"start":1667819342000,"stop":1667819407881,"duration":65881}},{"uid":"f9b990b63056fd9e","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00096c030>: {\n        Underlying: <*exec.ExitError | 0xc0004a0000>{\n            ProcessState: {\n                pid: 6317,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 4, Usec: 250348},\n                    Stime: {Sec: 0, Usec: 509081},\n                    Maxrss: 111064,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 9064,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 19888,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 52525,\n                    Nivcsw: 14416,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:192: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.4.7.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD serviceexports.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD serviceimports.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD slice.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD slicegateways.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 48 resource(s)\",\n                        \"wait.go:66: [debug] beginning wait for 48 resources with timeout of 5m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 2 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 2 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 2 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 2 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 2 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 2 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 2 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 2 expected pods are ready\",\n                        \"rea...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:192: [debug] Original chart version: \"\"\n    install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.4.7.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD serviceexports.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD serviceimports.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD slice.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD slicegateways.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 48 resource(s)\n    wait.go:66: [debug] beginning wait for 48 resources with timeout of 5m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 2 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 2 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 2 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 2 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 2 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 2 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 2 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 2 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:310: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"jaeger\" Service\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-svc\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook\" Deployment\n    client.go:310: [debug] Starting delete for \"jaeger\" Deployment\n    client.go:310: [debug] Starting delete for \"prefix-service\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsm-kernel-forwarder\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRole\n    client.go:310: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"networkservices.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkserviceendpoints.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservicemanagers.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:310: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-cfg\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"nsm-webhook-high-priority\" PriorityClass\n    uninstall.go:148: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:141\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\noccurred","time":{"start":1667808371000,"stop":1667808778569,"duration":407569}},{"uid":"a3dad95eb2d6e082","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000e0c0>: {\n        Underlying: <*exec.ExitError | 0xc00073a000>{\n            ProcessState: {\n                pid: 6219,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 3, Usec: 966781},\n                    Stime: {Sec: 0, Usec: 616701},\n                    Maxrss: 108508,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 6020,\n                    Majflt: 10,\n                    Nswap: 0,\n                    Inblock: 1416,\n                    Oublock: 20192,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 46404,\n                    Nivcsw: 12319,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:192: [debug] Original chart version: \\\"0.4.6\\\"\",\n                        \"install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.4.6.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:165: [debug] Clearing discovery cache\",\n                        \"wait.go:66: [debug] beginning wait for 4 resources with timeout of 1m0s\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 47 resource(s)\",\n                        \"wait.go:66: [debug] beginning wait for 47 resources with timeout of 5m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubes...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:192: [debug] Original chart version: \"0.4.6\"\n    install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.4.6.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:165: [debug] Clearing discovery cache\n    wait.go:66: [debug] beginning wait for 4 resources with timeout of 1m0s\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 47 resource(s)\n    wait.go:66: [debug] beginning wait for 47 resources with timeout of 5m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:310: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-svc\" Service\n    client.go:310: [debug] Starting delete for \"jaeger\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:310: [debug] Starting delete for \"jaeger\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook\" Deployment\n    client.go:310: [debug] Starting delete for \"prefix-service\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsm-kernel-forwarder\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRole\n    client.go:310: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"networkservices.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkserviceendpoints.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservicemanagers.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-cfg\" MutatingWebhookConfiguration\n    uninstall.go:148: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: client rate limiter Wait returned an error: rate: Wait(n=1) would exceed context deadline\n    helm.go:84: [debug] client rate limiter Wait returned an error: rate: Wait(n=1) would exceed context deadline\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:141\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\noccurred","time":{"start":1667799859000,"stop":1667800204842,"duration":345842}},{"uid":"9a1fc3ddd66d2cdf","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00085c018>: {\n        Underlying: <*exec.ExitError | 0xc00096c000>{\n            ProcessState: {\n                pid: 6236,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 4, Usec: 158758},\n                    Stime: {Sec: 0, Usec: 575943},\n                    Maxrss: 111072,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 8548,\n                    Majflt: 12,\n                    Nswap: 0,\n                    Inblock: 1280,\n                    Oublock: 20192,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 47959,\n                    Nivcsw: 13024,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:192: [debug] Original chart version: \\\"0.4.6\\\"\",\n                        \"install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.4.6.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:165: [debug] Clearing discovery cache\",\n                        \"wait.go:66: [debug] beginning wait for 4 resources with timeout of 1m0s\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 47 resource(s)\",\n                        \"wait.go:66: [debug] beginning wait for 47 resources with timeout of 5m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslic...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:192: [debug] Original chart version: \"0.4.6\"\n    install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.4.6.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:165: [debug] Clearing discovery cache\n    wait.go:66: [debug] beginning wait for 4 resources with timeout of 1m0s\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 47 resource(s)\n    wait.go:66: [debug] beginning wait for 47 resources with timeout of 5m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:310: [debug] Starting delete for \"jaeger\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-svc\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:310: [debug] Starting delete for \"jaeger\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook\" Deployment\n    client.go:310: [debug] Starting delete for \"prefix-service\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsm-kernel-forwarder\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"networkservices.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkserviceendpoints.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservicemanagers.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:310: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-cfg\" MutatingWebhookConfiguration\n    uninstall.go:148: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:141\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\noccurred","time":{"start":1667796325000,"stop":1667796671154,"duration":346154}},{"uid":"c74e863cab05fbde","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004a0330>: {\n        Underlying: <*exec.ExitError | 0xc0007b2000>{\n            ProcessState: {\n                pid: 6216,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 4, Usec: 493944},\n                    Stime: {Sec: 0, Usec: 778747},\n                    Maxrss: 106320,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 11116,\n                    Majflt: 5,\n                    Nswap: 0,\n                    Inblock: 568,\n                    Oublock: 20416,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 52885,\n                    Nivcsw: 14774,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:192: [debug] Original chart version: \\\"0.4.6\\\"\",\n                        \"install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.4.6.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:165: [debug] Clearing discovery cache\",\n                        \"wait.go:66: [debug] beginning wait for 4 resources with timeout of 1m0s\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 47 resource(s)\",\n                        \"wait.go:66: [debug] beginning wait for 47 resources with timeout of 5m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] D...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:192: [debug] Original chart version: \"0.4.6\"\n    install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.4.6.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:165: [debug] Clearing discovery cache\n    wait.go:66: [debug] beginning wait for 4 resources with timeout of 1m0s\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 47 resource(s)\n    wait.go:66: [debug] beginning wait for 47 resources with timeout of 5m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:310: [debug] Starting delete for \"jaeger\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-svc\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:310: [debug] Starting delete for \"jaeger\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook\" Deployment\n    client.go:310: [debug] Starting delete for \"prefix-service\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsm-kernel-forwarder\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:310: [debug] Starting delete for \"networkserviceendpoints.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservices.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservicemanagers.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-cfg\" MutatingWebhookConfiguration\n    uninstall.go:148: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: client rate limiter Wait returned an error: context deadline exceeded\n    helm.go:84: [debug] client rate limiter Wait returned an error: context deadline exceeded\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:141\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\noccurred","time":{"start":1667793837000,"stop":1667794195983,"duration":358983}},{"uid":"27d539284d68951d","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004dc0a8>: {\n        Underlying: <*exec.ExitError | 0xc0006ee000>{\n            ProcessState: {\n                pid: 6202,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 4, Usec: 872205},\n                    Stime: {Sec: 1, Usec: 84888},\n                    Maxrss: 105696,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 11398,\n                    Majflt: 5,\n                    Nswap: 0,\n                    Inblock: 640,\n                    Oublock: 20656,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 51812,\n                    Nivcsw: 16358,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:192: [debug] Original chart version: \\\"0.4.6\\\"\",\n                        \"install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.4.6.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:165: [debug] Clearing discovery cache\",\n                        \"wait.go:66: [debug] beginning wait for 4 resources with timeout of 1m0s\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 47 resource(s)\",\n                        \"wait.go:66: [debug] beginning wait for 47 resources with timeout of 5m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 e...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:192: [debug] Original chart version: \"0.4.6\"\n    install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.4.6.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:165: [debug] Clearing discovery cache\n    wait.go:66: [debug] beginning wait for 4 resources with timeout of 1m0s\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 47 resource(s)\n    wait.go:66: [debug] beginning wait for 47 resources with timeout of 5m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:310: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-svc\" Service\n    client.go:310: [debug] Starting delete for \"jaeger\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:310: [debug] Starting delete for \"jaeger\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook\" Deployment\n    client.go:310: [debug] Starting delete for \"prefix-service\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsm-kernel-forwarder\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRole\n    client.go:310: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"networkservices.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkserviceendpoints.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservicemanagers.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-cfg\" MutatingWebhookConfiguration\n    uninstall.go:148: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:141\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\noccurred","time":{"start":1667766962000,"stop":1667767329131,"duration":367131}},{"uid":"bc454c932467b5c9","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000992018>: {\n        Underlying: <*exec.ExitError | 0xc000a00000>{\n            ProcessState: {\n                pid: 6165,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 5, Usec: 438091},\n                    Stime: {Sec: 0, Usec: 955729},\n                    Maxrss: 105140,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 13128,\n                    Majflt: 1,\n                    Nswap: 0,\n                    Inblock: 32,\n                    Oublock: 20192,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 52649,\n                    Nivcsw: 15131,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:192: [debug] Original chart version: \\\"0.4.6\\\"\",\n                        \"install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.4.6.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:165: [debug] Clearing discovery cache\",\n                        \"wait.go:66: [debug] beginning wait for 4 resources with timeout of 1m0s\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 47 resource(s)\",\n                        \"wait.go:66: [debug] beginning wait for 47 resources with timeout of 5m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:2...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:192: [debug] Original chart version: \"0.4.6\"\n    install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.4.6.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:165: [debug] Clearing discovery cache\n    wait.go:66: [debug] beginning wait for 4 resources with timeout of 1m0s\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 47 resource(s)\n    wait.go:66: [debug] beginning wait for 47 resources with timeout of 5m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:310: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-svc\" Service\n    client.go:310: [debug] Starting delete for \"jaeger\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:310: [debug] Starting delete for \"jaeger\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook\" Deployment\n    client.go:310: [debug] Starting delete for \"prefix-service\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsm-kernel-forwarder\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRole\n    client.go:310: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"networkservices.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkserviceendpoints.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservicemanagers.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-cfg\" MutatingWebhookConfiguration\n    uninstall.go:148: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: client rate limiter Wait returned an error: rate: Wait(n=1) would exceed context deadline\n    helm.go:84: [debug] client rate limiter Wait returned an error: rate: Wait(n=1) would exceed context deadline\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:141\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\noccurred","time":{"start":1667765147000,"stop":1667765515269,"duration":368269}},{"uid":"14a2820bc915e45e","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0006a2018>: {\n        Underlying: <*exec.ExitError | 0xc00072a000>{\n            ProcessState: {\n                pid: 6221,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 3, Usec: 470885},\n                    Stime: {Sec: 0, Usec: 640384},\n                    Maxrss: 100680,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 8384,\n                    Majflt: 13,\n                    Nswap: 0,\n                    Inblock: 1760,\n                    Oublock: 20192,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 47512,\n                    Nivcsw: 11273,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:192: [debug] Original chart version: \\\"0.4.6\\\"\",\n                        \"install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.4.6.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:165: [debug] Clearing discovery cache\",\n                        \"wait.go:66: [debug] beginning wait for 4 resources with timeout of 1m0s\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 47 resource(s)\",\n                        \"wait.go:66: [debug] beginning wait for 47 resources with timeout of 5m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is n...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:192: [debug] Original chart version: \"0.4.6\"\n    install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.4.6.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:165: [debug] Clearing discovery cache\n    wait.go:66: [debug] beginning wait for 4 resources with timeout of 1m0s\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 47 resource(s)\n    wait.go:66: [debug] beginning wait for 47 resources with timeout of 5m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:310: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-svc\" Service\n    client.go:310: [debug] Starting delete for \"jaeger\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:310: [debug] Starting delete for \"jaeger\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook\" Deployment\n    client.go:310: [debug] Starting delete for \"prefix-service\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsm-kernel-forwarder\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRole\n    client.go:310: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"networkservices.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkserviceendpoints.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservicemanagers.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-cfg\" MutatingWebhookConfiguration\n    uninstall.go:148: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: client rate limiter Wait returned an error: context deadline exceeded\n    helm.go:84: [debug] client rate limiter Wait returned an error: context deadline exceeded\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:141\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\noccurred","time":{"start":1667763515000,"stop":1667763859306,"duration":344306}},{"uid":"cd3da2c0102d20bf","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000830018>: {\n        Underlying: <*exec.ExitError | 0xc000934040>{\n            ProcessState: {\n                pid: 6225,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 3, Usec: 983770},\n                    Stime: {Sec: 0, Usec: 753909},\n                    Maxrss: 103244,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 6807,\n                    Majflt: 10,\n                    Nswap: 0,\n                    Inblock: 968,\n                    Oublock: 20192,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 48819,\n                    Nivcsw: 11783,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:192: [debug] Original chart version: \\\"0.4.6\\\"\",\n                        \"install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.4.6.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:165: [debug] Clearing discovery cache\",\n                        \"wait.go:66: [debug] beginning wait for 4 resources with timeout of 1m0s\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 47 resource(s)\",\n                        \"wait.go:66: [debug] beginning wait for 47 resources with timeout of 5m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not r...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:192: [debug] Original chart version: \"0.4.6\"\n    install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.4.6.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:165: [debug] Clearing discovery cache\n    wait.go:66: [debug] beginning wait for 4 resources with timeout of 1m0s\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 47 resource(s)\n    wait.go:66: [debug] beginning wait for 47 resources with timeout of 5m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:310: [debug] Starting delete for \"jaeger\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-svc\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:310: [debug] Starting delete for \"jaeger\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook\" Deployment\n    client.go:310: [debug] Starting delete for \"prefix-service\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsm-kernel-forwarder\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRole\n    client.go:310: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"networkservices.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkserviceendpoints.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservicemanagers.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-cfg\" MutatingWebhookConfiguration\n    uninstall.go:148: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:141\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\noccurred","time":{"start":1667755854000,"stop":1667756198049,"duration":344049}},{"uid":"e0bfaeaf9694d3c9","status":"passed","time":{"start":1667739621000,"stop":1667739916217,"duration":295217}},{"uid":"6d0dfaf5cc06294c","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0001fa018>: {\n        Underlying: <*exec.ExitError | 0xc0001b4000>{\n            ProcessState: {\n                pid: 6251,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 4, Usec: 227405},\n                    Stime: {Sec: 0, Usec: 580232},\n                    Maxrss: 113984,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 7253,\n                    Majflt: 7,\n                    Nswap: 0,\n                    Inblock: 856,\n                    Oublock: 20192,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 48237,\n                    Nivcsw: 11040,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:192: [debug] Original chart version: \\\"0.4.6\\\"\",\n                        \"install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.4.6.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:165: [debug] Clearing discovery cache\",\n                        \"wait.go:66: [debug] beginning wait for 4 resources with timeout of 1m0s\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 47 resource(s)\",\n                        \"wait.go:66: [debug] beginning wait for 47 resources with timeout of 5m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubesli...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:192: [debug] Original chart version: \"0.4.6\"\n    install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.4.6.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:165: [debug] Clearing discovery cache\n    wait.go:66: [debug] beginning wait for 4 resources with timeout of 1m0s\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 47 resource(s)\n    wait.go:66: [debug] beginning wait for 47 resources with timeout of 5m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:310: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"jaeger\" Service\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-svc\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook\" Deployment\n    client.go:310: [debug] Starting delete for \"jaeger\" Deployment\n    client.go:310: [debug] Starting delete for \"prefix-service\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsm-kernel-forwarder\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRole\n    client.go:310: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"networkservices.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkserviceendpoints.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservicemanagers.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-cfg\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    uninstall.go:148: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: client rate limiter Wait returned an error: context deadline exceeded\n    helm.go:84: [debug] client rate limiter Wait returned an error: context deadline exceeded\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:141\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\noccurred","time":{"start":1667738525000,"stop":1667738870883,"duration":345883}},{"uid":"c828da859f24a4b6","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004ae060>: {\n        Underlying: <*exec.ExitError | 0xc0004c4000>{\n            ProcessState: {\n                pid: 5888,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 6, Usec: 234086},\n                    Stime: {Sec: 0, Usec: 568300},\n                    Maxrss: 91480,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 12064,\n                    Majflt: 120,\n                    Nswap: 0,\n                    Inblock: 15480,\n                    Oublock: 15640,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 53932,\n                    Nivcsw: 17680,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"install.go:192: [debug] Original chart version: \\\"1.7.0\\\"\",\n                        \"install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.7.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 45 resource(s)\",\n                        \"wait.go:66: [debug] beginning wait for 45 resources with timeout of 5m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; install.go:192: [debug] Original chart version: \"1.7.0\"\n    install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.7.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 45 resource(s)\n    wait.go:66: [debug] beginning wait for 45 resources with timeout of 5m0s\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-cainjector. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: cert-manager/cert-manager-webhook. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting cert-manager\n    client.go:310: [debug] Starting delete for \"cert-manager\" Service\n    client.go:310: [debug] Starting delete for \"cert-manager-webhook\" Service\n    client.go:310: [debug] Starting delete for \"cert-manager-cainjector\" Deployment\n    client.go:310: [debug] Starting delete for \"cert-manager\" Deployment\n    client.go:310: [debug] Starting delete for \"cert-manager-webhook\" Deployment\n    client.go:310: [debug] Starting delete for \"cert-manager-webhook:dynamic-serving\" RoleBinding\n    client.go:310: [debug] Starting delete for \"cert-manager-cainjector:leaderelection\" RoleBinding\n    client.go:310: [debug] Starting delete for \"cert-manager:leaderelection\" RoleBinding\n    client.go:310: [debug] Starting delete for \"cert-manager-webhook:dynamic-serving\" Role\n    client.go:310: [debug] Starting delete for \"cert-manager-cainjector:leaderelection\" Role\n    client.go:310: [debug] Starting delete for \"cert-manager:leaderelection\" Role\n    client.go:310: [debug] Starting delete for \"cert-manager-webhook:subjectaccessreviews\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"cert-manager-cainjector\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"cert-manager-controller-issuers\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"cert-manager-controller-clusterissuers\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"cert-manager-controller-certificates\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"cert-manager-controller-orders\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"cert-manager-controller-challenges\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"cert-manager-controller-ingress-shim\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"cert-manager-controller-approve:cert-manager-io\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"cert-manager-controller-certificatesigningrequests\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"cert-manager-webhook:subjectaccessreviews\" ClusterRole\n    client.go:310: [debug] Starting delete for \"cert-manager-cainjector\" ClusterRole\n    client.go:310: [debug] Starting delete for \"cert-manager-controller-issuers\" ClusterRole\n    client.go:310: [debug] Starting delete for \"cert-manager-controller-clusterissuers\" ClusterRole\n    client.go:310: [debug] Starting delete for \"cert-manager-controller-certificates\" ClusterRole\n    client.go:310: [debug] Starting delete for \"cert-manager-controller-orders\" ClusterRole\n    client.go:310: [debug] Starting delete for \"cert-manager-controller-challenges\" ClusterRole\n    client.go:310: [debug] Starting delete for \"cert-manager-controller-ingress-shim\" ClusterRole\n    client.go:310: [debug] Starting delete for \"cert-manager-view\" ClusterRole\n    client.go:310: [debug] Starting delete for \"cert-manager-edit\" ClusterRole\n    client.go:310: [debug] Starting delete for \"cert-manager-controller-approve:cert-manager-io\" ClusterRole\n    client.go:310: [debug] Starting delete for \"cert-manager-controller-certificatesigningrequests\" ClusterRole\n    client.go:310: [debug] Starting delete for \"challenges.acme.cert-manager.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"orders.acme.cert-manager.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"clusterissuers.cert-manager.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"issuers.cert-manager.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"certificates.cert-manager.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"certificaterequests.cert-manager.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"cert-manager-webhook\" ConfigMap\n    client.go:310: [debug] Starting delete for \"cert-manager-cainjector\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"cert-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"cert-manager-webhook\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"cert-manager-webhook\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"cert-manager-webhook\" ValidatingWebhookConfiguration\n    uninstall.go:148: [debug] purge requested for cert-manager\n    Error: INSTALLATION FAILED: release cert-manager failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release cert-manager failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:141\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\noccurred","time":{"start":1667737440000,"stop":1667737744183,"duration":304183}},{"uid":"3030aa2f87f9e529","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00041a108>: {\n        Underlying: <*exec.ExitError | 0xc0008fc000>{\n            ProcessState: {\n                pid: 5915,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 2, Usec: 545246},\n                    Stime: {Sec: 0, Usec: 235375},\n                    Maxrss: 110564,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 15332,\n                    Majflt: 4,\n                    Nswap: 0,\n                    Inblock: 488,\n                    Oublock: 21592,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 4876,\n                    Nivcsw: 5138,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"install.go:192: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.4.5.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:165: [debug] Clearing discovery cache\",\n                        \"wait.go:66: [debug] beginning wait for 6 resources with timeout of 1m0s\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:310: [debug] Starting delete for \\\"kubeslice-preinstall\\\" ServiceAccount\",\n                        \"client.go:339: [debug] serviceaccounts \\\"kubeslice-preinstall\\\" not found\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:310: [debug] Starting delete for \\\"kubeslice-preinstall-role\\\" ClusterRole\",\n                        \"client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \\\"kubeslice-preinstall-role\\\" not found\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:310: [debug] Starting delete for \\\"kubeslice-preinstall-rolebinding\\\" ClusterRoleBinding\",\n                        \"client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \\\"kubeslice-preinstall-rolebinding\\\" not found\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:310: [debug] Starting delete for \\\"kubeslice-worker-preinstall-configmap\\\" ConfigMap\",\n                        \"client.go:339: [debug] configmaps \\\"kubeslice-worker-preinstall-configmap\\\" not found\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:310: [debug] Starting delete for \\\"kubeslice-worker-preinstall-job\\\" Job\",\n                        \"client.go:339: [debug] jobs.batch \\\"kubeslice-worker-preinstall-job\\\" not found\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:540: [debug] Watching for changes to Job kubeslice-worker-preinstall-job with timeout of 5m0s\",\n                        \"client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: ADDED\",\n                        \"client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 0, jobs failed: 0, jobs succeeded: 0\",\n                        \"client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\",\n                        \"client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jo...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; install.go:192: [debug] Original chart version: \"\"\n    install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.4.5.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:165: [debug] Clearing discovery cache\n    wait.go:66: [debug] beginning wait for 6 resources with timeout of 1m0s\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:310: [debug] Starting delete for \"kubeslice-preinstall\" ServiceAccount\n    client.go:339: [debug] serviceaccounts \"kubeslice-preinstall\" not found\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:310: [debug] Starting delete for \"kubeslice-preinstall-role\" ClusterRole\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-preinstall-role\" not found\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:310: [debug] Starting delete for \"kubeslice-preinstall-rolebinding\" ClusterRoleBinding\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-preinstall-rolebinding\" not found\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:310: [debug] Starting delete for \"kubeslice-worker-preinstall-configmap\" ConfigMap\n    client.go:339: [debug] configmaps \"kubeslice-worker-preinstall-configmap\" not found\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:310: [debug] Starting delete for \"kubeslice-worker-preinstall-job\" Job\n    client.go:339: [debug] jobs.batch \"kubeslice-worker-preinstall-job\" not found\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:540: [debug] Watching for changes to Job kubeslice-worker-preinstall-job with timeout of 5m0s\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: ADDED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 0, jobs failed: 0, jobs succeeded: 0\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 1, jobs succeeded: 0\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 2, jobs succeeded: 0\n    client.go:310: [debug] Starting delete for \"kubeslice-worker-preinstall-job\" Job\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:310: [debug] Starting delete for \"jaeger\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-svc\" Service\n    client.go:339: [debug] services \"jaeger\" not found\n    client.go:339: [debug] services \"kubeslice-dns\" not found\n    client.go:339: [debug] services \"kubeslice-webhook-service\" not found\n    client.go:339: [debug] services \"nsm-admission-webhook-svc\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook\" Deployment\n    client.go:310: [debug] Starting delete for \"jaeger\" Deployment\n    client.go:310: [debug] Starting delete for \"prefix-service\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:339: [debug] deployments.apps \"nsm-admission-webhook\" not found\n    client.go:339: [debug] deployments.apps \"kubeslice-dns\" not found\n    client.go:339: [debug] deployments.apps \"kubeslice-operator\" not found\n    client.go:339: [debug] deployments.apps \"prefix-service\" not found\n    client.go:339: [debug] deployments.apps \"jaeger\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsm-kernel-forwarder\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:339: [debug] daemonsets.apps \"nsmgr\" not found\n    client.go:339: [debug] daemonsets.apps \"nsm-kernel-forwarder\" not found\n    client.go:339: [debug] daemonsets.apps \"kubeslice-netop\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:339: [debug] rolebindings.rbac.authorization.k8s.io \"kubeslice-leader-election-rolebinding\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:339: [debug] roles.rbac.authorization.k8s.io \"kubeslice-leader-election-role\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-manager-rolebinding\" not found\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-proxy-rolebinding\" not found\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"nsm-role-binding\" not found\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-dns-rolebinding\" not found\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-kubernetes-dashboard\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRole\n    client.go:310: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"aggregate-network-services-view\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-proxy-role\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-metrics-reader\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-kubernetes-dashboard\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"nsm-role\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-dns-role\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-manager-role\" not found\n    client.go:310: [debug] Starting delete for \"networkservices.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkserviceendpoints.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservicemanagers.networkservicemesh.io\" CustomResourceDefinition\n    client.go:339: [debug] customresourcedefinitions.apiextensions.k8s.io \"networkservices.networkservicemesh.io\" not found\n    client.go:339: [debug] customresourcedefinitions.apiextensions.k8s.io \"networkserviceendpoints.networkservicemesh.io\" not found\n    client.go:339: [debug] customresourcedefinitions.apiextensions.k8s.io \"networkservicemanagers.networkservicemesh.io\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:339: [debug] configmaps \"nsm-config\" not found\n    client.go:339: [debug] configmaps \"kubeslice-manager-config\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:339: [debug] secrets \"kubeslice-admission-webhook-certs\" not found\n    client.go:339: [debug] secrets \"nsm-admission-webhook-certs\" not found\n    client.go:339: [debug] secrets \"kubeslice-hub\" not found\n    client.go:310: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:339: [debug] serviceaccounts \"kubeslice-controller-manager\" not found\n    client.go:339: [debug] serviceaccounts \"nse-acc\" not found\n    client.go:339: [debug] serviceaccounts \"nsc-acc\" not found\n    client.go:339: [debug] serviceaccounts \"nsmgr-acc\" not found\n    client.go:339: [debug] serviceaccounts \"kubeslice-dns\" not found\n    client.go:339: [debug] serviceaccounts \"kubeslice-netop\" not found\n    client.go:339: [debug] serviceaccounts \"slice-router\" not found\n    client.go:339: [debug] serviceaccounts \"forward-plane-acc\" not found\n    client.go:339: [debug] serviceaccounts \"kubeslice-kubernetes-dashboard\" not found\n    client.go:339: [debug] serviceaccounts \"vpn-gateway-client\" not found\n    client.go:339: [debug] serviceaccounts \"vpn-gateway-server\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-cfg\" MutatingWebhookConfiguration\n    client.go:339: [debug] mutatingwebhookconfigurations.admissionregistration.k8s.io \"nsm-admission-webhook-cfg\" not found\n    client.go:339: [debug] mutatingwebhookconfigurations.admissionregistration.k8s.io \"kubeslice-mutating-webhook-configuration\" not found\n    uninstall.go:148: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: failed pre-install: timed out waiting for the condition\n    helm.go:84: [debug] failed pre-install: timed out waiting for the condition\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:361\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:141\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\noccurred","time":{"start":1667651299000,"stop":1667651660171,"duration":361171}},{"uid":"bec9acd7fe46f947","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000456078>: {\n        Underlying: <*exec.ExitError | 0xc0007be000>{\n            ProcessState: {\n                pid: 6169,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 5, Usec: 879859},\n                    Stime: {Sec: 1, Usec: 303417},\n                    Maxrss: 110680,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 15006,\n                    Majflt: 2,\n                    Nswap: 0,\n                    Inblock: 112,\n                    Oublock: 20432,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 54809,\n                    Nivcsw: 16391,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:192: [debug] Original chart version: \\\"0.4.6\\\"\",\n                        \"install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.4.6.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:165: [debug] Clearing discovery cache\",\n                        \"wait.go:66: [debug] beginning wait for 4 resources with timeout of 1m0s\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 47 resource(s)\",\n                        \"wait.go:66: [debug] beginning wait for 47 resources with timeout of 5m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n    ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:192: [debug] Original chart version: \"0.4.6\"\n    install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.4.6.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:165: [debug] Clearing discovery cache\n    wait.go:66: [debug] beginning wait for 4 resources with timeout of 1m0s\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 47 resource(s)\n    wait.go:66: [debug] beginning wait for 47 resources with timeout of 5m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:310: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-svc\" Service\n    client.go:310: [debug] Starting delete for \"jaeger\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook\" Deployment\n    client.go:310: [debug] Starting delete for \"jaeger\" Deployment\n    client.go:310: [debug] Starting delete for \"prefix-service\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsm-kernel-forwarder\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRole\n    client.go:310: [debug] Starting delete for \"networkservices.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkserviceendpoints.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservicemanagers.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-cfg\" MutatingWebhookConfiguration\n    uninstall.go:148: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: client rate limiter Wait returned an error: rate: Wait(n=1) would exceed context deadline\n    helm.go:84: [debug] client rate limiter Wait returned an error: rate: Wait(n=1) would exceed context deadline\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:141\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\noccurred","time":{"start":1667595443000,"stop":1667595808921,"duration":365921}},{"uid":"34173798a534d24f","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00093a018>: {\n        Underlying: <*exec.ExitError | 0xc000680000>{\n            ProcessState: {\n                pid: 6267,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 4, Usec: 48212},\n                    Stime: {Sec: 0, Usec: 647394},\n                    Maxrss: 104892,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 6157,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 20592,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 49457,\n                    Nivcsw: 12592,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:192: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.4.7.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD serviceexports.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD serviceimports.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD slice.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD slicegateways.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 48 resource(s)\",\n                        \"wait.go:66: [debug] beginning wait for 48 resources with timeout of 5m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 2 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 2 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 2 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 2 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 2 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 2 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 2 expected pods are ready\",\n                        \"ready.go:277: [debu...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:192: [debug] Original chart version: \"\"\n    install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.4.7.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD serviceexports.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD serviceimports.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD slice.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD slicegateways.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 48 resource(s)\n    wait.go:66: [debug] beginning wait for 48 resources with timeout of 5m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 2 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 2 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 2 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 2 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 2 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 2 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 2 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 2 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 2 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:310: [debug] Starting delete for \"jaeger\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-svc\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:310: [debug] Starting delete for \"jaeger\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook\" Deployment\n    client.go:310: [debug] Starting delete for \"prefix-service\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsm-kernel-forwarder\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRole\n    client.go:310: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"networkservices.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkserviceendpoints.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservicemanagers.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-cfg\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"nsm-webhook-high-priority\" PriorityClass\n    uninstall.go:148: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: client rate limiter Wait returned an error: rate: Wait(n=1) would exceed context deadline\n    helm.go:84: [debug] client rate limiter Wait returned an error: rate: Wait(n=1) would exceed context deadline\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:141\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\noccurred","time":{"start":1667537247000,"stop":1667537663848,"duration":416848}},{"uid":"9309e461adca2c29","status":"passed","time":{"start":1667458785000,"stop":1667459039258,"duration":254258}},{"uid":"9f73749105a056a3","status":"passed","time":{"start":1667389661000,"stop":1667389899306,"duration":238306}}]},"Worker Suite:Worker Suite#[It] Slice creation scenarios: when slice red is installed workers should have gateway pod running":{"statistic":{"failed":2,"broken":0,"skipped":2,"passed":2,"unknown":0,"total":6},"items":[{"uid":"6f2492f909f7cff","status":"passed","time":{"start":1667837307000,"stop":1667837389325,"duration":82325}},{"uid":"febec7667bd4cd8b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667826674000,"stop":1667826674000,"duration":0}},{"uid":"5d8950d5868d74b4","status":"passed","time":{"start":1667823949000,"stop":1667824031325,"duration":82325}},{"uid":"5ba607b19aee5a44","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1666674621000,"stop":1666674801289,"duration":180289}},{"uid":"62395ab83a88eb26","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674212000,"stop":1666674212000,"duration":0}},{"uid":"a2d4d7b1db13c5c6","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1666665366000,"stop":1666665546268,"duration":180268}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting iperf-server pod Check ping between iperf-server and iperf-client after iperf-server pod restart":{"statistic":{"failed":0,"broken":0,"skipped":5,"passed":1,"unknown":0,"total":6},"items":[{"uid":"b7592cd1c02795fa","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667837307000,"stop":1667837307000,"duration":0}},{"uid":"2007a6e8bf7be4e8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667826674000,"stop":1667826674000,"duration":0}},{"uid":"360939c98560968a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667823949000,"stop":1667823949000,"duration":0}},{"uid":"e02d61d62900444","status":"passed","time":{"start":1666674621000,"stop":1666674640845,"duration":19845}},{"uid":"d0538f9e6780eb4d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674212000,"stop":1666674212000,"duration":0}},{"uid":"27cdd0b237858abb","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666665366000,"stop":1666665366000,"duration":0}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Update Should update successfully when Project Applied while removing a service account name in Read users":{"statistic":{"failed":2,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":3},"items":[{"uid":"6978e3fa0fe8a5d4","status":"passed","time":{"start":1667840900000,"stop":1667840904695,"duration":4695}},{"uid":"dd294f9cb0ee0bf6","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000e3d8>: {\n        Underlying: <*exec.ExitError | 0xc000749920>{\n            ProcessState: {\n                pid: 7621,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 181810},\n                    Stime: {Sec: 0, Usec: 69260},\n                    Maxrss: 81532,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 8120,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13760,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 481,\n                    Nivcsw: 364,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Read%20users2994065585\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.94.3:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Read%20users2994065585\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.94.3:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Read%20users2994065585\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.94.3:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Read%20users2994065585\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.94.3:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Read%20users2994065585\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.94.3:443: connect: connection refused\noccurred","time":{"start":1665665955000,"stop":1665665955314,"duration":314}},{"uid":"5c3b6c0b44b85332","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0005340a8>: {\n        Underlying: <*exec.ExitError | 0xc00088c8e0>{\n            ProcessState: {\n                pid: 7194,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 272305},\n                    Stime: {Sec: 0, Usec: 54461},\n                    Maxrss: 76384,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 6138,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13760,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 293,\n                    Nivcsw: 407,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Read%20users2184618495\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.116.233:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Read%20users2184618495\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.116.233:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Read%20users2184618495\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.116.233:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Read%20users2184618495\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.116.233:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Read%20users2184618495\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.116.233:443: connect: connection refused\noccurred","time":{"start":1665666027000,"stop":1665666027417,"duration":417}}]},"Worker Suite:Worker Suite#[It] Slice resource quota Tests ResourceQuota creation tests Create ResourceQuota fail test - not applied in project namespace  ":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":3,"unknown":0,"total":3},"items":[{"uid":"f607ad4d9fdf2ede","status":"passed","time":{"start":1667739621000,"stop":1667739621807,"duration":807}},{"uid":"c3ddd8c81e234a09","status":"passed","time":{"start":1667458785000,"stop":1667458785803,"duration":803}},{"uid":"311ebc7c101545e2","status":"passed","time":{"start":1667389661000,"stop":1667389661917,"duration":917}}]},"Worker Suite:Worker Suite#[It] Hub netpol scenarios: Slice Configs Netpol scenarios Should fail while Deleting Slice without removing the namespaceIsolationProfile part in slice configs":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":4,"unknown":0,"total":6},"items":[{"uid":"253ccdc322f997b6","status":"passed","time":{"start":1667837307000,"stop":1667837307114,"duration":114}},{"uid":"638c159067a18e1f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667826674000,"stop":1667826674000,"duration":0}},{"uid":"618057cc7076175a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667823949000,"stop":1667823949000,"duration":0}},{"uid":"743d14dad1ce46e6","status":"passed","time":{"start":1666674621000,"stop":1666674621123,"duration":123}},{"uid":"518eecbaf5d57e51","status":"passed","time":{"start":1666674212000,"stop":1666674212127,"duration":127}},{"uid":"ae82511a404d76ea","status":"passed","time":{"start":1666665366000,"stop":1666665366072,"duration":72}}]},"Worker Suite:Worker Suite#[It] Resource Quota tests slice resource quota enforcement mode should not create only Memory limits in rq in app namespace-1":{"statistic":{"failed":0,"broken":0,"skipped":3,"passed":0,"unknown":0,"total":3},"items":[{"uid":"547f13018ef3350","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667739621000,"stop":1667739621000,"duration":0}},{"uid":"1ae2f59997e63525","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667458785000,"stop":1667458785000,"duration":0}},{"uid":"fd825d5134a205a1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667389661000,"stop":1667389661000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Resource Quota tests slice resource quota enforcement mode should create limit range in application namespaces":{"statistic":{"failed":0,"broken":0,"skipped":3,"passed":0,"unknown":0,"total":3},"items":[{"uid":"d0cb3b38ae74aaeb","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667739621000,"stop":1667739621000,"duration":0}},{"uid":"e67ddad53fac0f04","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667458785000,"stop":1667458785000,"duration":0}},{"uid":"1182cca7abb9fa8b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667389661000,"stop":1667389661000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice RBAC Tests SliceRoleTemplate Tests Create SliceRoleTempalate failure test: Creating SliceRoleTempalate read-only-role, missing apiGroups field":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":2,"unknown":0,"total":2},"items":[{"uid":"deee77990b01203c","status":"passed","time":{"start":1667739621000,"stop":1667739621816,"duration":816}},{"uid":"a2e014464057b22c","status":"passed","time":{"start":1667458785000,"stop":1667458786242,"duration":1242}}]},"Worker Suite:Worker Suite#[It] Slice resource quota Tests ResourceQuota creation tests Create ResourceQuota fail test - sum(cluster.limit.podcount) > slice.podcount":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":3,"unknown":0,"total":3},"items":[{"uid":"88c95dfa5ddaa81c","status":"passed","time":{"start":1667739621000,"stop":1667739622541,"duration":1541}},{"uid":"909b9f4207ae7545","status":"passed","time":{"start":1667458785000,"stop":1667458786296,"duration":1296}},{"uid":"1858a5eb99fc88e0","status":"passed","time":{"start":1667389661000,"stop":1667389661833,"duration":833}}]},"Worker Suite:Worker Suite#[It] Slice deletion tests: Slice is installed & validated Slice object is deleted from hub should remove vl3 router from spoke":{"statistic":{"failed":4,"broken":0,"skipped":0,"passed":2,"unknown":0,"total":6},"items":[{"uid":"d55adbdf7ec35496","status":"passed","time":{"start":1667837307000,"stop":1667837398478,"duration":91478}},{"uid":"79271874d6058d61","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0008b4468>: {\n        Underlying: <*exec.ExitError | 0xc000893d40>{\n            ProcessState: {\n                pid: 6221,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 264918},\n                    Stime: {Sec: 0, Usec: 228793},\n                    Maxrss: 98640,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 7288,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 9120,\n                    Oublock: 19808,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 676,\n                    Nivcsw: 1016,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-red\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\noccurred","time":{"start":1667826674000,"stop":1667826675894,"duration":1894}},{"uid":"12f092abf84977a2","status":"passed","time":{"start":1667823949000,"stop":1667824040128,"duration":91128}},{"uid":"bf752ce981917883","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1666674621000,"stop":1666674807506,"duration":186506}},{"uid":"315eb90523ec71e9","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1666674212000,"stop":1666674394862,"duration":182862}},{"uid":"9e0f64da63ebebd6","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1666665366000,"stop":1666665551829,"duration":185829}}]},"Worker Suite:Worker Suite#[It] Resource Quota tests iper connectivity Verify Connectivity between iperf-server and iperf-client":{"statistic":{"failed":0,"broken":0,"skipped":3,"passed":0,"unknown":0,"total":3},"items":[{"uid":"98a06a2024ac7efd","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667739621000,"stop":1667739621000,"duration":0}},{"uid":"e60b3ea9b4944845","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667458785000,"stop":1667458785000,"duration":0}},{"uid":"471c19f6407f1448","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667389661000,"stop":1667389661000,"duration":0}}]},"Worker Suite:Worker Suite#[It]  QosProfile tests QosProfile Create validation Create qos profile- validation of BandwidthCeilingKbps > BandwidthGuaranteedKbps":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":6,"unknown":0,"total":6},"items":[{"uid":"9093a9d73f12bce5","status":"passed","time":{"start":1667837307000,"stop":1667837307809,"duration":809}},{"uid":"889cd31abc1b86af","status":"passed","time":{"start":1667826674000,"stop":1667826674271,"duration":271}},{"uid":"a3904be3805efaab","status":"passed","time":{"start":1667823949000,"stop":1667823949320,"duration":320}},{"uid":"9671b111380c118","status":"passed","time":{"start":1666674621000,"stop":1666674621857,"duration":857}},{"uid":"cf2b5923f1b10871","status":"passed","time":{"start":1666674212000,"stop":1666674212445,"duration":445}},{"uid":"5b41accb7aae5cdb","status":"passed","time":{"start":1666665366000,"stop":1666665366479,"duration":479}}]},"Worker Suite:Worker Suite#[It] Slice resource quota Tests ResourceQuota creation tests Create ResourceQuota fail test- Request.Mem > Limit.Mem at slice level":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":3,"unknown":0,"total":3},"items":[{"uid":"f953c26013d8e239","status":"passed","time":{"start":1667739621000,"stop":1667739622111,"duration":1111}},{"uid":"7e7cfa1ed4dacd8c","status":"passed","time":{"start":1667458785000,"stop":1667458785741,"duration":741}},{"uid":"b174361441ca14ee","status":"passed","time":{"start":1667389661000,"stop":1667389661625,"duration":625}}]},"Worker Suite:Worker Suite#[It] Slice RBAC Tests SliceRoleTemplate Tests Delete SliceRoleTempalate failure test: Deleting SliceRoleTempalate read-only-role participating in a SliceRoleBinding":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":2,"unknown":0,"total":2},"items":[{"uid":"e58d871b72422806","status":"passed","time":{"start":1667739621000,"stop":1667739625530,"duration":4530}},{"uid":"53a43803980b8705","status":"passed","time":{"start":1667458785000,"stop":1667458806246,"duration":21246}}]},"Worker Suite:Worker Suite#[It] Hub netpol scenarios: Slice Configs Netpol scenarios Should fail when deploying with invalid namespace in both applicationNamespace and allowedNamespace":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":4,"unknown":0,"total":6},"items":[{"uid":"90ace7522bf59287","status":"passed","time":{"start":1667837307000,"stop":1667837308023,"duration":1023}},{"uid":"9bdff31951be37ce","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667826674000,"stop":1667826674000,"duration":0}},{"uid":"68ce74d08b6c8acc","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667823949000,"stop":1667823949000,"duration":0}},{"uid":"3aefd471e51691b8","status":"passed","time":{"start":1666674621000,"stop":1666674622116,"duration":1116}},{"uid":"fa09d3209c896077","status":"passed","time":{"start":1666674212000,"stop":1666674213180,"duration":1180}},{"uid":"e5ae6dfbfc0f153b","status":"passed","time":{"start":1666665366000,"stop":1666665367310,"duration":1310}}]},"Hub Suite:Hub Suite#[AfterSuite]":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":5,"unknown":0,"total":5},"items":[{"uid":"c7d8180b23b49de6","status":"passed","time":{"start":1667840900000,"stop":1667840902401,"duration":2401}},{"uid":"93cfa4ef0cf37672","status":"passed","time":{"start":1666668667000,"stop":1666668669056,"duration":2056}},{"uid":"7ae6eb20bb60067d","status":"passed","time":{"start":1666419212000,"stop":1666419214361,"duration":2361}},{"uid":"e98a559a225fcce3","status":"passed","time":{"start":1665665955000,"stop":1665666257388,"duration":302388}},{"uid":"8bbbfaab38a08bb","status":"passed","time":{"start":1665666027000,"stop":1665666031529,"duration":4529}}]},"Hub Suite:Hub Suite#[It] Cluster CR tests Cluster CR validation Creates cluster secrets":{"statistic":{"failed":2,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":3},"items":[{"uid":"d7cd2925cb920345","status":"passed","time":{"start":1667840900000,"stop":1667840903610,"duration":3610}},{"uid":"316bf71cda515d92","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0006c22e8>: {\n        Underlying: <*exec.ExitError | 0xc0005feac0>{\n            ProcessState: {\n                pid: 7591,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 202287},\n                    Stime: {Sec: 0, Usec: 47597},\n                    Maxrss: 82156,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 13136,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13760,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 540,\n                    Nivcsw: 485,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Cluster%20CR%20tests%20Cluster%20CR%20validation%20Creates%20cluster%20secrets61365964\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.94.3:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Cluster%20CR%20tests%20Cluster%20CR%20validation%20Creates%20cluster%20secrets61365964\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.94.3:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Cluster%20CR%20tests%20Cluster%20CR%20validation%20Creates%20cluster%20secrets61365964\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.94.3:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Cluster%20CR%20tests%20Cluster%20CR%20validation%20Creates%20cluster%20secrets61365964\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.94.3:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Cluster%20CR%20tests%20Cluster%20CR%20validation%20Creates%20cluster%20secrets61365964\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.94.3:443: connect: connection refused\noccurred","time":{"start":1665665955000,"stop":1665665955341,"duration":341}},{"uid":"25ff6d4442713bdd","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000535008>: {\n        Underlying: <*exec.ExitError | 0xc0006af1e0>{\n            ProcessState: {\n                pid: 7108,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 282963},\n                    Stime: {Sec: 0, Usec: 59126},\n                    Maxrss: 73396,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 6185,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13760,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 705,\n                    Nivcsw: 648,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Cluster%20CR%20tests%20Cluster%20CR%20validation%20Creates%20cluster%20secrets3063442492\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.116.233:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Cluster%20CR%20tests%20Cluster%20CR%20validation%20Creates%20cluster%20secrets3063442492\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.116.233:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Cluster%20CR%20tests%20Cluster%20CR%20validation%20Creates%20cluster%20secrets3063442492\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.116.233:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Cluster%20CR%20tests%20Cluster%20CR%20validation%20Creates%20cluster%20secrets3063442492\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.116.233:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Cluster%20CR%20tests%20Cluster%20CR%20validation%20Creates%20cluster%20secrets3063442492\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.116.233:443: connect: connection refused\noccurred","time":{"start":1665666027000,"stop":1665666027478,"duration":478}}]},"Worker Suite:Worker Suite#[It] Resource Quota tests slice resource quota enforcement mode should not create Memory requests in limits in app namespace-1":{"statistic":{"failed":0,"broken":0,"skipped":3,"passed":0,"unknown":0,"total":3},"items":[{"uid":"2a08df01384570e3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667739621000,"stop":1667739621000,"duration":0}},{"uid":"e7db3fd81f7903f3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667458785000,"stop":1667458785000,"duration":0}},{"uid":"cbe9f3cc9c9025cc","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667389661000,"stop":1667389661000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice resource quota Tests ResourceQuota creation tests Create ResourceQuota fail test-cluster.limit.mem negative":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":3,"unknown":0,"total":3},"items":[{"uid":"e88f60eb502e7064","status":"passed","time":{"start":1667739621000,"stop":1667739622435,"duration":1435}},{"uid":"57d24b5dafd5140f","status":"passed","time":{"start":1667458785000,"stop":1667458786214,"duration":1214}},{"uid":"a7dc5bec1d9e9693","status":"passed","time":{"start":1667389661000,"stop":1667389661815,"duration":815}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Iperf connectivity should onboard deployment in application Namespaces without slice annotations":{"statistic":{"failed":5,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":6},"items":[{"uid":"bd1ce327a660df97","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0009e1e00>: {\n        Underlying: <*exec.ExitError | 0xc00046c540>{\n            ProcessState: {\n                pid: 7240,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 269782},\n                    Stime: {Sec: 0, Usec: 102449},\n                    Maxrss: 116012,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 12584,\n                    Majflt: 1,\n                    Nswap: 0,\n                    Inblock: 40,\n                    Oublock: 19808,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 1281,\n                    Nivcsw: 1135,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.NamespaceIsolationProfile.ApplicationNamespaces: Invalid value: \\\"iperf\\\": The given namespace: iperf in cluster cluster1 is already acquired by other slice: blue\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.NamespaceIsolationProfile.ApplicationNamespaces: Invalid value: \\\"iperf\\\": The given namespace: iperf in cluster cluster1 is already acquired by other slice: blue\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.NamespaceIsolationProfile.ApplicationNamespaces: Invalid value: \\\"iperf\\\": The given namespace: iperf in cluster cluster1 is already acquired by other slice: blue\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.NamespaceIsolationProfile.ApplicationNamespaces: Invalid value: \\\"iperf\\\": The given namespace: iperf in cluster cluster1 is already acquired by other slice: blue\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.NamespaceIsolationProfile.ApplicationNamespaces: Invalid value: \"iperf\": The given namespace: iperf in cluster cluster1 is already acquired by other slice: blue\noccurred","time":{"start":1667837307000,"stop":1667837312837,"duration":5837}},{"uid":"2a051b44d351cd1c","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0009130b0>: {\n        Underlying: <*exec.ExitError | 0xc0008de900>{\n            ProcessState: {\n                pid: 6505,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 251973},\n                    Stime: {Sec: 0, Usec: 56897},\n                    Maxrss: 95144,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3731,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 19808,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 261,\n                    Nivcsw: 823,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\noccurred","time":{"start":1667826674000,"stop":1667826674729,"duration":729}},{"uid":"1f901eb5cf6c78c1","status":"failed","statusDetails":"Unexpected error:\n    <*errors.StatusError | 0xc00027f540>: {\n        ErrStatus: {\n            TypeMeta: {Kind: \"\", APIVersion: \"\"},\n            ListMeta: {\n                SelfLink: \"\",\n                ResourceVersion: \"\",\n                Continue: \"\",\n                RemainingItemCount: nil,\n            },\n            Status: \"Failure\",\n            Message: \"namespaces \\\"iperf\\\" already exists\",\n            Reason: \"AlreadyExists\",\n            Details: {Name: \"iperf\", Group: \"\", Kind: \"namespaces\", UID: \"\", Causes: nil, RetryAfterSeconds: 0},\n            Code: 409,\n        },\n    }\n    namespaces \"iperf\" already exists\noccurred","time":{"start":1667823949000,"stop":1667823950959,"duration":1959}},{"uid":"d07fb62c0db03ebe","status":"passed","time":{"start":1666674621000,"stop":1666674639275,"duration":18275}},{"uid":"c0582ded4629581e","status":"failed","statusDetails":"Unexpected error:\n    <*errors.StatusError | 0xc0008fa1e0>: {\n        ErrStatus: {\n            TypeMeta: {Kind: \"\", APIVersion: \"\"},\n            ListMeta: {\n                SelfLink: \"\",\n                ResourceVersion: \"\",\n                Continue: \"\",\n                RemainingItemCount: nil,\n            },\n            Status: \"Failure\",\n            Message: \"namespaces \\\"iperf\\\" already exists\",\n            Reason: \"AlreadyExists\",\n            Details: {Name: \"iperf\", Group: \"\", Kind: \"namespaces\", UID: \"\", Causes: nil, RetryAfterSeconds: 0},\n            Code: 409,\n        },\n    }\n    namespaces \"iperf\" already exists\noccurred","time":{"start":1666674212000,"stop":1666674216632,"duration":4632}},{"uid":"ddf8427c2f145b9a","status":"failed","statusDetails":"Unexpected error:\n    <*errors.StatusError | 0xc0007dca00>: {\n        ErrStatus: {\n            TypeMeta: {Kind: \"\", APIVersion: \"\"},\n            ListMeta: {\n                SelfLink: \"\",\n                ResourceVersion: \"\",\n                Continue: \"\",\n                RemainingItemCount: nil,\n            },\n            Status: \"Failure\",\n            Message: \"namespaces \\\"iperf\\\" already exists\",\n            Reason: \"AlreadyExists\",\n            Details: {Name: \"iperf\", Group: \"\", Kind: \"namespaces\", UID: \"\", Causes: nil, RetryAfterSeconds: 0},\n            Code: 409,\n        },\n    }\n    namespaces \"iperf\" already exists\noccurred","time":{"start":1666665366000,"stop":1666665367449,"duration":1449}}]},"Worker Suite:Worker Suite#[It] Namespace reconcile test scenarios: Offboard remove label from app ns":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":6,"unknown":0,"total":6},"items":[{"uid":"120ee36a7e88bb87","status":"passed","time":{"start":1667837307000,"stop":1667837307282,"duration":282}},{"uid":"5a4972a7ebf0ec20","status":"passed","time":{"start":1667826674000,"stop":1667826674171,"duration":171}},{"uid":"d1436b8df75e4b84","status":"passed","time":{"start":1667823949000,"stop":1667823949462,"duration":462}},{"uid":"1c8d5d6edc84399e","status":"passed","time":{"start":1666674621000,"stop":1666674621254,"duration":254}},{"uid":"3ee9b2c994816320","status":"passed","time":{"start":1666674212000,"stop":1666674212206,"duration":206}},{"uid":"d4d8cf18c2977cf9","status":"passed","time":{"start":1666665366000,"stop":1666665366351,"duration":351}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Creation Should fail while creating Project with  project name as blank":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":3,"unknown":0,"total":3},"items":[{"uid":"d47d0eeae3655d4d","status":"passed","time":{"start":1667840900000,"stop":1667840900246,"duration":246}},{"uid":"5f480c4f8ff748c4","status":"passed","time":{"start":1665665955000,"stop":1665665955295,"duration":295}},{"uid":"d0471f0f8c91f192","status":"passed","time":{"start":1665666027000,"stop":1665666027370,"duration":370}}]},"Istio Suite:Istio Suite#[AfterSuite]":{"statistic":{"failed":6,"broken":0,"skipped":0,"passed":0,"unknown":0,"total":6},"items":[{"uid":"ebb3d6d8b92be2b","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0006e6690>: {\n        Underlying: <*exec.ExitError | 0xc0000b20c0>{\n            ProcessState: {\n                pid: 6400,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 40920},\n                    Stime: {Sec: 0, Usec: 13640},\n                    Maxrss: 41460,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 1852,\n                    Majflt: 1,\n                    Nswap: 0,\n                    Inblock: 128,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 188,\n                    Nivcsw: 68,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error: uninstall: Release not loaded: istiod: release: not found\noccurred","time":{"start":1666884621000,"stop":1666884621262,"duration":262}},{"uid":"dab26f55a74be52e","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004f5e00>: {\n        Underlying: <*exec.ExitError | 0xc00068fd60>{\n            ProcessState: {\n                pid: 6348,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 43436},\n                    Stime: {Sec: 0, Usec: 4343},\n                    Maxrss: 39780,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 1840,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 239,\n                    Nivcsw: 79,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error: uninstall: Release not loaded: istiod: release: not found\noccurred","time":{"start":1666747979000,"stop":1666747979193,"duration":193}},{"uid":"695120cec797eec","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000e288>: {\n        Underlying: <*exec.ExitError | 0xc00014eda0>{\n            ProcessState: {\n                pid: 6401,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 31761},\n                    Stime: {Sec: 0, Usec: 9074},\n                    Maxrss: 40464,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 1863,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 172,\n                    Nivcsw: 39,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error: uninstall: Release not loaded: istiod: release: not found\noccurred","time":{"start":1666707346000,"stop":1666707346156,"duration":156}},{"uid":"5e8ad4941508d071","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004e87c8>: {\n        Underlying: <*exec.ExitError | 0xc000b82b80>{\n            ProcessState: {\n                pid: 6477,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 46324},\n                    Stime: {Sec: 0, Usec: 12633},\n                    Maxrss: 44792,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 1890,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 258,\n                    Nivcsw: 256,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    Error: uninstall: Release not loaded: istiod: release: not found\noccurred","time":{"start":1665944636000,"stop":1665944637564,"duration":1564}},{"uid":"9f2405fb1ee1ac56","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00035ca50>: {\n        Underlying: <*exec.ExitError | 0xc00045ec60>{\n            ProcessState: {\n                pid: 6141,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 50829},\n                    Stime: {Sec: 0, Usec: 23104},\n                    Maxrss: 42920,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2355,\n                    Majflt: 3,\n                    Nswap: 0,\n                    Inblock: 744,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 240,\n                    Nivcsw: 120,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    Error: uninstall: Release not loaded: istiod: release: not found\noccurred","time":{"start":1665552694000,"stop":1665552695741,"duration":1741}},{"uid":"b75c881d493b2392","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000fc98>: {\n        Underlying: <*exec.ExitError | 0xc00066c3a0>{\n            ProcessState: {\n                pid: 6121,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 39076},\n                    Stime: {Sec: 0, Usec: 21709},\n                    Maxrss: 41528,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 1876,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 231,\n                    Nivcsw: 82,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error: uninstall: Release not loaded: istiod: release: not found\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error: uninstall: Release not loaded: istiod: release: not found\noccurred","time":{"start":1664798844000,"stop":1664798844352,"duration":352}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests Iperf App connectivity Verify Iperf traffic":{"statistic":{"failed":0,"broken":0,"skipped":5,"passed":1,"unknown":0,"total":6},"items":[{"uid":"237240c30cdd3334","status":"passed","time":{"start":1667837307000,"stop":1667837368358,"duration":61358}},{"uid":"b35c8cd0b92c140b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667826674000,"stop":1667826674000,"duration":0}},{"uid":"767f6c42b12fe6b6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667823949000,"stop":1667823949000,"duration":0}},{"uid":"5887988a96c33f2d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674621000,"stop":1666674621000,"duration":0}},{"uid":"4b2cfbea69e94410","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674212000,"stop":1666674212000,"duration":0}},{"uid":"2cb3e5edae769987","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666665366000,"stop":1666665366000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Iperf connectivity Verify Connectivity between iperf-server and iperf-client":{"statistic":{"failed":0,"broken":0,"skipped":5,"passed":1,"unknown":0,"total":6},"items":[{"uid":"795cd15763f6af21","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667837307000,"stop":1667837307000,"duration":0}},{"uid":"cc740ac41e409be8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667826674000,"stop":1667826674000,"duration":0}},{"uid":"910208efd683bf9a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667823949000,"stop":1667823949000,"duration":0}},{"uid":"747276eef9474554","status":"passed","time":{"start":1666674621000,"stop":1666674641477,"duration":20477}},{"uid":"94aeb7ae13deafed","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674212000,"stop":1666674212000,"duration":0}},{"uid":"499b43cd310d1e3d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666665366000,"stop":1666665366000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice RBAC Tests RBAC Common Tests SliceRoleBinding status test: SliceRoleBinding should have error status for invalid namespace for custom roles":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":0,"unknown":0,"total":2},"items":[{"uid":"85bb5349f1eafb2a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667739621000,"stop":1667739621000,"duration":0}},{"uid":"d990d2c9e71bf373","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667458785000,"stop":1667458785000,"duration":0}}]},"Worker Suite:Worker Suite#[It]  QosProfile tests QosProfile delete validation should remove vl3 router from spoke":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":5,"unknown":0,"total":6},"items":[{"uid":"a3ed089fc6db1cc6","status":"passed","time":{"start":1667837307000,"stop":1667837310030,"duration":3030}},{"uid":"3989b49530708de","status":"passed","time":{"start":1667826674000,"stop":1667826677027,"duration":3027}},{"uid":"714baaca9a8ecd4a","status":"passed","time":{"start":1667823949000,"stop":1667823952037,"duration":3037}},{"uid":"5f455b19d468b248","status":"passed","time":{"start":1666674621000,"stop":1666674624030,"duration":3030}},{"uid":"379d60762346e10c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674212000,"stop":1666674212000,"duration":0}},{"uid":"95037a4e90452294","status":"passed","time":{"start":1666665366000,"stop":1666665369053,"duration":3053}}]},"Hub Suite:Hub Suite#[It] Cluster negative tests Worker Cluster Registration with Wrong token":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":1,"unknown":0,"total":3},"items":[{"uid":"f2e43241fe9af6df","status":"passed","time":{"start":1667840900000,"stop":1667841214669,"duration":314669}},{"uid":"5871eb0969debaeb","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1665665955000,"stop":1665665955000,"duration":0}},{"uid":"3f4ee082fcfa2773","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1665666027000,"stop":1665666027000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice Creation scenarios Slice with MEDIUM QOS should have vl3 router running":{"statistic":{"failed":2,"broken":0,"skipped":0,"passed":4,"unknown":0,"total":6},"items":[{"uid":"63e3c3d1fdba5183","status":"passed","time":{"start":1667837307000,"stop":1667837312090,"duration":5090}},{"uid":"a2fb34666a30f0a4","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0007cd7d0>: {\n        Underlying: <*exec.ExitError | 0xc0009aa780>{\n            ProcessState: {\n                pid: 6452,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 235006},\n                    Stime: {Sec: 0, Usec: 79663},\n                    Maxrss: 111044,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 5643,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 19808,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 442,\n                    Nivcsw: 596,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicemediumqos\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicemediumqos\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\noccurred","time":{"start":1667826674000,"stop":1667826674376,"duration":376}},{"uid":"fe8a88abd7246213","status":"passed","time":{"start":1667823949000,"stop":1667823956417,"duration":7417}},{"uid":"98b50f35c1c0cd85","status":"passed","time":{"start":1666674621000,"stop":1666674628968,"duration":7968}},{"uid":"7a97c764f0e1c175","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1666674212000,"stop":1666674393343,"duration":181343}},{"uid":"f6bc733bdd5c0180","status":"passed","time":{"start":1666665366000,"stop":1666665371877,"duration":5877}}]},"Worker Suite:Worker Suite#[It] Slice creation scenarios: when slice red is installed workers should have vl3 router pods running":{"statistic":{"failed":1,"broken":0,"skipped":1,"passed":4,"unknown":0,"total":6},"items":[{"uid":"616bca48fb2e5926","status":"passed","time":{"start":1667837307000,"stop":1667837309026,"duration":2026}},{"uid":"85c256bf4ef6751b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667826674000,"stop":1667826674000,"duration":0}},{"uid":"7ac344e93852294d","status":"passed","time":{"start":1667823949000,"stop":1667823951054,"duration":2054}},{"uid":"a96cae28fd8f6de","status":"passed","time":{"start":1666674621000,"stop":1666674625080,"duration":4080}},{"uid":"b0e1d9fbdbfedfef","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1666674212000,"stop":1666674392286,"duration":180286}},{"uid":"159d9a6b5efbb829","status":"passed","time":{"start":1666665366000,"stop":1666665366079,"duration":79}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests Iperf App connectivity Application namespaces should be isolated":{"statistic":{"failed":0,"broken":0,"skipped":5,"passed":1,"unknown":0,"total":6},"items":[{"uid":"723fec1f493a98ec","status":"passed","time":{"start":1667837307000,"stop":1667837366988,"duration":59988}},{"uid":"5a7db1b377af42c0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667826674000,"stop":1667826674000,"duration":0}},{"uid":"e46d3b7db504222f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667823949000,"stop":1667823949000,"duration":0}},{"uid":"4f82d386c9db9523","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674621000,"stop":1666674621000,"duration":0}},{"uid":"7465603fff4565c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674212000,"stop":1666674212000,"duration":0}},{"uid":"f62db318d3c0396c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666665366000,"stop":1666665366000,"duration":0}}]},"Hub Suite:Hub Suite#[It] Cluster negative tests Worker Cluster Registration with Wrong ca.cert":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":1,"unknown":0,"total":3},"items":[{"uid":"523827fccc0495c9","status":"passed","time":{"start":1667840900000,"stop":1667841215531,"duration":315531}},{"uid":"dc025b001ebe1660","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1665665955000,"stop":1665665955000,"duration":0}},{"uid":"b77399730a713ae4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1665666027000,"stop":1665666027000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice resource quota Tests ResourceQuota creation tests Create ResourceQuota fail test - sum(namespace.limit.mem) > slice.mem":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":3,"unknown":0,"total":3},"items":[{"uid":"ab274d83cbc29f17","status":"passed","time":{"start":1667739621000,"stop":1667739621721,"duration":721}},{"uid":"b86b64be46c69b0b","status":"passed","time":{"start":1667458785000,"stop":1667458785756,"duration":756}},{"uid":"57d8cfb7992b39e0","status":"passed","time":{"start":1667389661000,"stop":1667389661598,"duration":598}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting nsm-manager pod Check ping between iperf-server and iperf-client after nsm-manager pod restart":{"statistic":{"failed":0,"broken":0,"skipped":5,"passed":1,"unknown":0,"total":6},"items":[{"uid":"84500d893acca019","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667837307000,"stop":1667837307000,"duration":0}},{"uid":"79b61b27a9d1aa07","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667826674000,"stop":1667826674000,"duration":0}},{"uid":"8925c13fb4d3a24f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667823949000,"stop":1667823949000,"duration":0}},{"uid":"f1207f9e73d89d9b","status":"passed","time":{"start":1666674621000,"stop":1666674625708,"duration":4708}},{"uid":"a7b071c77f2df46d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674212000,"stop":1666674212000,"duration":0}},{"uid":"226443f6860d9d22","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666665366000,"stop":1666665366000,"duration":0}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Creation Should fail while creating Project with project name as combination of special characters":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":3,"unknown":0,"total":3},"items":[{"uid":"e3027eb996e83d61","status":"passed","time":{"start":1667840900000,"stop":1667840900251,"duration":251}},{"uid":"a212a4bd2972ea3f","status":"passed","time":{"start":1665665955000,"stop":1665665955271,"duration":271}},{"uid":"c09aa1ec9b1fdf56","status":"passed","time":{"start":1665666027000,"stop":1665666027398,"duration":398}}]},"Worker Suite:Worker Suite#[It] Hub netpol scenarios: Slice Configs Netpol scenarios Should fail while creating namespaces with same name in both allowedNamespace and applicationNamespace":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":4,"unknown":0,"total":6},"items":[{"uid":"a78403faaec621e1","status":"passed","time":{"start":1667837307000,"stop":1667837307348,"duration":348}},{"uid":"7668d2e1dd94a266","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667826674000,"stop":1667826674000,"duration":0}},{"uid":"b0cf27440dbc0db3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667823949000,"stop":1667823949000,"duration":0}},{"uid":"6f4256c7e807e45f","status":"passed","time":{"start":1666674621000,"stop":1666674621532,"duration":532}},{"uid":"43da8a7d4a93be05","status":"passed","time":{"start":1666674212000,"stop":1666674212563,"duration":563}},{"uid":"24a30c1be75ce70d","status":"passed","time":{"start":1666665366000,"stop":1666665366526,"duration":526}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting iperf-server pod Should restart iperf-server pod":{"statistic":{"failed":0,"broken":0,"skipped":5,"passed":1,"unknown":0,"total":6},"items":[{"uid":"bfca6797cd2b0f91","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667837307000,"stop":1667837307000,"duration":0}},{"uid":"6f550e54af91e4d4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667826674000,"stop":1667826674000,"duration":0}},{"uid":"1ce1704cb32ea679","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667823949000,"stop":1667823949000,"duration":0}},{"uid":"9ea533a13ebfe64f","status":"passed","time":{"start":1666674621000,"stop":1666674652477,"duration":31477}},{"uid":"872f6ddd85cb8ed8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674212000,"stop":1666674212000,"duration":0}},{"uid":"5fc874abecddad35","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666665366000,"stop":1666665366000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting worker-operator pod Should restart worker-operator pod":{"statistic":{"failed":0,"broken":0,"skipped":5,"passed":1,"unknown":0,"total":6},"items":[{"uid":"d7c132e4323bd6a3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667837307000,"stop":1667837307000,"duration":0}},{"uid":"fc7845c2ea41ba36","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667826674000,"stop":1667826674000,"duration":0}},{"uid":"664087d8e51165ec","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667823949000,"stop":1667823949000,"duration":0}},{"uid":"b7509070cb74cbf8","status":"passed","time":{"start":1666674621000,"stop":1666674643216,"duration":22216}},{"uid":"f71193e860bca40b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674212000,"stop":1666674212000,"duration":0}},{"uid":"a0b320a56a2f42c3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666665366000,"stop":1666665366000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice Creation scenarios Slice with LOW QOS should have vl3 router running":{"statistic":{"failed":2,"broken":0,"skipped":0,"passed":4,"unknown":0,"total":6},"items":[{"uid":"8f1a70ffdfd695d9","status":"passed","time":{"start":1667837307000,"stop":1667837311324,"duration":4324}},{"uid":"5b3ee333d3690652","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000912570>: {\n        Underlying: <*exec.ExitError | 0xc00048a3a0>{\n            ProcessState: {\n                pid: 6443,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 226444},\n                    Stime: {Sec: 0, Usec: 56611},\n                    Maxrss: 96860,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4874,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 19808,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 432,\n                    Nivcsw: 536,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicelowqos\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicelowqos\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\noccurred","time":{"start":1667826674000,"stop":1667826674357,"duration":357}},{"uid":"f13d2d9f190e87ff","status":"passed","time":{"start":1667823949000,"stop":1667823953362,"duration":4362}},{"uid":"3e6209431d484280","status":"passed","time":{"start":1666674621000,"stop":1666674627356,"duration":6356}},{"uid":"344f8969a594d284","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1666674212000,"stop":1666674392690,"duration":180690}},{"uid":"e2b1aef43bd99b87","status":"passed","time":{"start":1666665366000,"stop":1666665370530,"duration":4530}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests slice with netpol should install slice on each worker cluster with correct namespaceisolationprofile":{"statistic":{"failed":0,"broken":0,"skipped":4,"passed":2,"unknown":0,"total":6},"items":[{"uid":"e23a0353f1bf4879","status":"passed","time":{"start":1667837307000,"stop":1667837307303,"duration":303}},{"uid":"e3b42a88189f1d4a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667826674000,"stop":1667826674000,"duration":0}},{"uid":"2c9eea4f880f2d87","status":"passed","time":{"start":1667823949000,"stop":1667823949189,"duration":189}},{"uid":"5afe915ec4ef443","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674621000,"stop":1666674621000,"duration":0}},{"uid":"7a40e03716975881","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674212000,"stop":1666674212000,"duration":0}},{"uid":"9953240102436e6c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666665366000,"stop":1666665366000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice resource quota Tests ResourceQuota creation tests Create ResourceQuota fail test - EnforceQuota value true tests":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":3,"unknown":0,"total":3},"items":[{"uid":"21cb939479ed5eb6","status":"passed","time":{"start":1667739621000,"stop":1667739622108,"duration":1108}},{"uid":"ed1c98109d51d90b","status":"passed","time":{"start":1667458785000,"stop":1667458786300,"duration":1300}},{"uid":"f80e6c2a70e37dcf","status":"passed","time":{"start":1667389661000,"stop":1667389662355,"duration":1355}}]},"Worker Suite:Worker Suite#[It] Namespace reconcile test scenarios: Delete deboarded app ns gets deleted":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":6,"unknown":0,"total":6},"items":[{"uid":"d8e292ef7f42e6c0","status":"passed","time":{"start":1667837307000,"stop":1667837318124,"duration":11124}},{"uid":"8c9aa6d20c93c862","status":"passed","time":{"start":1667826674000,"stop":1667826684635,"duration":10635}},{"uid":"8a1a28e3279f7eef","status":"passed","time":{"start":1667823949000,"stop":1667823959773,"duration":10773}},{"uid":"c20f35687316643a","status":"passed","time":{"start":1666674621000,"stop":1666674631907,"duration":10907}},{"uid":"7a71374e3d7029d7","status":"passed","time":{"start":1666674212000,"stop":1666674222865,"duration":10865}},{"uid":"f0ae2683397d6f12","status":"passed","time":{"start":1666665366000,"stop":1666665376691,"duration":10691}}]},"Worker Suite:Worker Suite#[It] Slice RBAC Tests SliceRoleBinding Tests Create SliceRoleBinding failure test: Creating SliceRoleBinding slice-red with duplicate Kubernetes Roles":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":2,"unknown":0,"total":2},"items":[{"uid":"610d12b537a58e2c","status":"passed","time":{"start":1667739621000,"stop":1667739621996,"duration":996}},{"uid":"f6d039bf443ce638","status":"passed","time":{"start":1667458785000,"stop":1667458785438,"duration":438}}]},"Worker Suite:Worker Suite#[It] Slice resource quota Tests ResourceQuota creation tests Create ResourceQuota fail test - slice.limit.podcount > sum(namespace.limit.podcount)":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":3,"unknown":0,"total":3},"items":[{"uid":"2dd8e15ad845e3e2","status":"passed","time":{"start":1667739621000,"stop":1667739621921,"duration":921}},{"uid":"89763fc3f86a3d5d","status":"passed","time":{"start":1667458785000,"stop":1667458786102,"duration":1102}},{"uid":"18d4e45956b1f2cf","status":"passed","time":{"start":1667389661000,"stop":1667389661880,"duration":880}}]},"Worker Suite:Worker Suite#[It] Slice resource quota Tests ResourceQuota creation tests Create ResourceQuota fail test-cluster.limit.cpu negative":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":3,"unknown":0,"total":3},"items":[{"uid":"4f7138f36b8fb483","status":"passed","time":{"start":1667739621000,"stop":1667739622261,"duration":1261}},{"uid":"ad187924d2ec176a","status":"passed","time":{"start":1667458785000,"stop":1667458786022,"duration":1022}},{"uid":"1477482cee73976b","status":"passed","time":{"start":1667389661000,"stop":1667389661722,"duration":722}}]},"Worker Suite:Worker Suite#[It] Slice creation scenarios: when slice red is installed install slice blue workers should get attached to slice blue":{"statistic":{"failed":0,"broken":0,"skipped":4,"passed":2,"unknown":0,"total":6},"items":[{"uid":"c5ffaa6fc12f6f72","status":"passed","time":{"start":1667837307000,"stop":1667837307925,"duration":925}},{"uid":"311a3c498f2a5fdb","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667826674000,"stop":1667826674000,"duration":0}},{"uid":"95e06ebcf6f2cdf2","status":"passed","time":{"start":1667823949000,"stop":1667823950321,"duration":1321}},{"uid":"623b6283143d6cb9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674621000,"stop":1666674621000,"duration":0}},{"uid":"ccc53c3bbcf15049","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674212000,"stop":1666674212000,"duration":0}},{"uid":"6a21071cef435bc3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666665366000,"stop":1666665366000,"duration":0}}]},"Intracluster Suite:Intracluster Suite#[AfterSuite]":{"statistic":{"failed":5,"broken":0,"skipped":0,"passed":22,"unknown":0,"total":27},"items":[{"uid":"77b1a525c13e16e9","status":"passed","time":{"start":1667842189000,"stop":1667842374648,"duration":185648}},{"uid":"8422fe20a178b6aa","status":"passed","time":{"start":1667807977000,"stop":1667808003910,"duration":26910}},{"uid":"df490afefe92c44e","status":"passed","time":{"start":1667536876000,"stop":1667536901318,"duration":25318}},{"uid":"1d03fa2ccf97a621","status":"failed","statusDetails":"Timed out after 210.009s.\nExpected an error to have occurred.  Got:\n    <nil>: nil","time":{"start":1667385281000,"stop":1667385491521,"duration":210521}},{"uid":"1513134f22bebd78","status":"failed","statusDetails":"Timed out after 210.009s.\nExpected an error to have occurred.  Got:\n    <nil>: nil","time":{"start":1667293744000,"stop":1667293954508,"duration":210508}},{"uid":"c3c47eaeae3d9d21","status":"failed","statusDetails":"Timed out after 210.009s.\nExpected an error to have occurred.  Got:\n    <nil>: nil","time":{"start":1666883986000,"stop":1666884196589,"duration":210589}},{"uid":"97ee1cbc244fb1bb","status":"passed","time":{"start":1666754211000,"stop":1666754212807,"duration":1807}},{"uid":"7bdc992f5ad72069","status":"passed","time":{"start":1666750446000,"stop":1666750447645,"duration":1645}},{"uid":"4bba68cf13487624","status":"passed","time":{"start":1666747897000,"stop":1666747898936,"duration":1936}},{"uid":"daf83020bac04de3","status":"failed","statusDetails":"Timed out after 210.010s.\nExpected an error to have occurred.  Got:\n    <nil>: nil","time":{"start":1666706731000,"stop":1666706941464,"duration":210464}},{"uid":"55e1507f521d9efa","status":"passed","time":{"start":1666674484000,"stop":1666674512118,"duration":28118}},{"uid":"d915bc794f9d7c72","status":"passed","time":{"start":1666673362000,"stop":1666673548752,"duration":186752}},{"uid":"ca3635dd15d53390","status":"passed","time":{"start":1666668696000,"stop":1666668698208,"duration":2208}},{"uid":"38543d1808ce6315","status":"failed","statusDetails":"Timed out after 210.008s.\nExpected an error to have occurred.  Got:\n    <nil>: nil","time":{"start":1666667460000,"stop":1666667670705,"duration":210705}},{"uid":"b67e1956960b2662","status":"passed","time":{"start":1666664909000,"stop":1666665094865,"duration":185865}},{"uid":"885a5e878e87f36","status":"passed","time":{"start":1666664766000,"stop":1666664767161,"duration":1161}},{"uid":"b33ecf433d5b3f0a","status":"passed","time":{"start":1666663248000,"stop":1666663249494,"duration":1494}},{"uid":"db4d8f02e6fa7849","status":"passed","time":{"start":1666423616000,"stop":1666423642847,"duration":26847}},{"uid":"60be839a65868056","status":"passed","time":{"start":1666419243000,"stop":1666419245822,"duration":2822}},{"uid":"e6cd9679bc94a2c0","status":"passed","time":{"start":1666338675000,"stop":1666338700065,"duration":25065}}]},"Empty Suite:Empty Suite#[BeforeSuite]":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":11,"unknown":0,"total":11},"items":[{"uid":"5bfccb4ef5b4d40a","status":"passed","time":{"start":1666883811000,"stop":1666883811000,"duration":0}},{"uid":"c02da9839f226eb6","status":"passed","time":{"start":1666754109000,"stop":1666754109000,"duration":0}},{"uid":"73307383b3a2ce0e","status":"passed","time":{"start":1666750338000,"stop":1666750338000,"duration":0}},{"uid":"95e2e9774a76b04a","status":"passed","time":{"start":1666747791000,"stop":1666747791000,"duration":0}},{"uid":"88dbcfb7b5612b32","status":"passed","time":{"start":1666706575000,"stop":1666706575000,"duration":0}},{"uid":"a5f573464ee8bfec","status":"passed","time":{"start":1666077791000,"stop":1666077791000,"duration":0}},{"uid":"a1f151e7e4d89ce","status":"passed","time":{"start":1665944068000,"stop":1665944068000,"duration":0}},{"uid":"263b687c417bf2c4","status":"passed","time":{"start":1665665806000,"stop":1665665806000,"duration":0}},{"uid":"db912ffd987e6511","status":"passed","time":{"start":1665665864000,"stop":1665665864000,"duration":0}},{"uid":"9846427ee6b049a","status":"passed","time":{"start":1665552534000,"stop":1665552534000,"duration":0}},{"uid":"ae06bc63f9c946bf","status":"passed","time":{"start":1664798682000,"stop":1664798682000,"duration":0}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Update Should fail when Project is Applied with service account name as blank in Read users":{"statistic":{"failed":2,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":3},"items":[{"uid":"97625a2bd8f92924","status":"passed","time":{"start":1667840900000,"stop":1667840904883,"duration":4883}},{"uid":"4e017b39e0e50605","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000ec90>: {\n        Underlying: <*exec.ExitError | 0xc0006be860>{\n            ProcessState: {\n                pid: 7649,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 176035},\n                    Stime: {Sec: 0, Usec: 58678},\n                    Maxrss: 84564,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 10099,\n                    Majflt: 1,\n                    Nswap: 0,\n                    Inblock: 232,\n                    Oublock: 13760,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 401,\n                    Nivcsw: 339,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20blank%20in%20Read%20users1275205082\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.94.3:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20blank%20in%20Read%20users1275205082\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.94.3:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20blank%20in%20Read%20users1275205082\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.94.3:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20blank%20in%20Read%20users1275205082\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.94.3:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20blank%20in%20Read%20users1275205082\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.94.3:443: connect: connection refused\noccurred","time":{"start":1665665955000,"stop":1665665955295,"duration":295}},{"uid":"fffc98573e9a4dea","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0001a5698>: {\n        Underlying: <*exec.ExitError | 0xc0007a6140>{\n            ProcessState: {\n                pid: 7224,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 236514},\n                    Stime: {Sec: 0, Usec: 81556},\n                    Maxrss: 86132,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 6310,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13760,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 501,\n                    Nivcsw: 493,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20blank%20in%20Read%20users137744948\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.116.233:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20blank%20in%20Read%20users137744948\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.116.233:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20blank%20in%20Read%20users137744948\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.116.233:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20blank%20in%20Read%20users137744948\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.116.233:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20blank%20in%20Read%20users137744948\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.116.233:443: connect: connection refused\noccurred","time":{"start":1665666027000,"stop":1665666027486,"duration":486}}]},"Worker Suite:Worker Suite#[It] Slice resource quota Tests ResourceQuota creation tests Create ResourceQuota successful test":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":3,"unknown":0,"total":3},"items":[{"uid":"5520024e7d358236","status":"passed","time":{"start":1667739621000,"stop":1667739626037,"duration":5037}},{"uid":"fb1043a4ac61d9ea","status":"passed","time":{"start":1667458785000,"stop":1667458789477,"duration":4477}},{"uid":"c8dbe3c8d7e02e2d","status":"passed","time":{"start":1667389661000,"stop":1667389664723,"duration":3723}}]},"Worker Suite:Worker Suite#[It] Namespace reconcile test scenarios: Delete deleted app ns entry should get removed from cluster objs":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":6,"unknown":0,"total":6},"items":[{"uid":"79859ad3776bf05d","status":"passed","time":{"start":1667837307000,"stop":1667837318883,"duration":11883}},{"uid":"b4d21bb255237a45","status":"passed","time":{"start":1667826674000,"stop":1667826685308,"duration":11308}},{"uid":"11de3ef73e2fb558","status":"passed","time":{"start":1667823949000,"stop":1667823960376,"duration":11376}},{"uid":"a2429bb1269b6565","status":"passed","time":{"start":1666674621000,"stop":1666674632581,"duration":11581}},{"uid":"9ea2e74fafb33f2d","status":"passed","time":{"start":1666674212000,"stop":1666674223479,"duration":11479}},{"uid":"2918e9e1d827a43d","status":"passed","time":{"start":1666665366000,"stop":1666665377246,"duration":11246}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting nsm-manager pod Should restart nsm-manager pod":{"statistic":{"failed":0,"broken":0,"skipped":5,"passed":1,"unknown":0,"total":6},"items":[{"uid":"800565e93d9ead","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667837307000,"stop":1667837307000,"duration":0}},{"uid":"a58a59dd158e7126","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667826674000,"stop":1667826674000,"duration":0}},{"uid":"94ec55aab666dd6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667823949000,"stop":1667823949000,"duration":0}},{"uid":"23059a6586789f86","status":"passed","time":{"start":1666674621000,"stop":1666674652344,"duration":31344}},{"uid":"840e42421136972a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674212000,"stop":1666674212000,"duration":0}},{"uid":"711680b9e167e1dc","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666665366000,"stop":1666665366000,"duration":0}}]},"Intracluster Suite:Intracluster Suite#[It] Intracluster slice tests Slice with netpol should remove network policy resource from application namespace once the isolationEnabled is set to false":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":2,"unknown":0,"total":4},"items":[{"uid":"639eba81687a45b3","status":"passed","time":{"start":1667842189000,"stop":1667842198220,"duration":9220}},{"uid":"45d160ebb6bd26e8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674484000,"stop":1666674484000,"duration":0}},{"uid":"2c13d4b5b15275f1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666673362000,"stop":1666673362000,"duration":0}},{"uid":"4c01f25e6fd89cb2","status":"passed","time":{"start":1666664909000,"stop":1666664916324,"duration":7324}}]},"Worker Suite:Worker Suite#[It] Slice resource quota Tests ResourceQuota creation tests Create ResourceQuota fail test":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":3,"unknown":0,"total":3},"items":[{"uid":"fa3ede32ddfee8e6","status":"passed","time":{"start":1667739621000,"stop":1667739623600,"duration":2600}},{"uid":"63e918789e9f708d","status":"passed","time":{"start":1667458785000,"stop":1667458790344,"duration":5344}},{"uid":"c58a4ac9c7db1382","status":"passed","time":{"start":1667389661000,"stop":1667389665237,"duration":4237}}]},"Intracluster Suite:Intracluster Suite#[It] Intracluster slice tests Slice with netpol should reconcile netpol config":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":2,"unknown":0,"total":4},"items":[{"uid":"477e7ccc57346318","status":"passed","time":{"start":1667842189000,"stop":1667842199673,"duration":10673}},{"uid":"46a72b00f117dc2e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674484000,"stop":1666674484000,"duration":0}},{"uid":"a6f9755d28e3c8e4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666673362000,"stop":1666673362000,"duration":0}},{"uid":"5af15f7d7437ec3f","status":"passed","time":{"start":1666664909000,"stop":1666664917518,"duration":8518}}]},"Worker Suite:Worker Suite#[It] Slice RBAC Tests RBAC Common Tests WorkerSliceRoleBinding generation test: WorkerSliceRoleBinding with kubernetes role should be generated automatically when forcefully deleted":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":1,"unknown":0,"total":2},"items":[{"uid":"bd5d10cdf769924b","status":"passed","time":{"start":1667739621000,"stop":1667739801773,"duration":180773}},{"uid":"bc13df2ca04864c3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667458785000,"stop":1667458785000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice Creation scenarios Slice with LOW QOS should have gateway pod running":{"statistic":{"failed":3,"broken":0,"skipped":2,"passed":1,"unknown":0,"total":6},"items":[{"uid":"b6bdc853f7d5c86b","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1667837307000,"stop":1667837487217,"duration":180217}},{"uid":"c32f4003ee02ef34","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667826674000,"stop":1667826674000,"duration":0}},{"uid":"a78b11d4218aed66","status":"passed","time":{"start":1667823949000,"stop":1667823994117,"duration":45117}},{"uid":"22998905a43b780b","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1666674621000,"stop":1666674801318,"duration":180318}},{"uid":"9cbe6c65d6e1fe78","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674212000,"stop":1666674212000,"duration":0}},{"uid":"c06d0109c7a7aa0f","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1666665366000,"stop":1666665546332,"duration":180332}}]},"Worker Suite:Worker Suite#[It] Slice resource quota Tests ResourceQuota creation tests Create ResourceQuota fail test - sum(namespace.limit.cpu) > slice.cpu":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":3,"unknown":0,"total":3},"items":[{"uid":"4b26475ca8e35692","status":"passed","time":{"start":1667739621000,"stop":1667739621827,"duration":827}},{"uid":"a17e0c489d62664e","status":"passed","time":{"start":1667458785000,"stop":1667458785794,"duration":794}},{"uid":"eec5420a60afed8d","status":"passed","time":{"start":1667389661000,"stop":1667389661852,"duration":852}}]},"Worker Suite:Worker Suite#[It] Slice RBAC Tests RBAC Common Tests WorkerSliceRoleBinding generation test: Creation of SliceRoleBinding with custom roles and k8s roles should generate two WorkerSliceRoleBindings":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":1,"unknown":0,"total":2},"items":[{"uid":"213d72a0a3e4173a","status":"passed","time":{"start":1667739621000,"stop":1667739623428,"duration":2428}},{"uid":"bc938a6eaf548005","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667458785000,"stop":1667458785000,"duration":0}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Creation Should create Project while using valid manifest":{"statistic":{"failed":2,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":3},"items":[{"uid":"749ddbf6c381b4de","status":"passed","time":{"start":1667840900000,"stop":1667840908454,"duration":8454}},{"uid":"aca79bd4327c5a13","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <bool>: false\nto be true","time":{"start":1665665955000,"stop":1665666032287,"duration":77287}},{"uid":"e467bfa0d24c5ff8","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000535140>: {\n        Underlying: <*exec.ExitError | 0xc0006af720>{\n            ProcessState: {\n                pid: 7118,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 253446},\n                    Stime: {Sec: 0, Usec: 76436},\n                    Maxrss: 85476,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 9921,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13760,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 353,\n                    Nivcsw: 488,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20create%20Project%20while%20using%20valid%20manifest672581222\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.116.233:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20create%20Project%20while%20using%20valid%20manifest672581222\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.116.233:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20create%20Project%20while%20using%20valid%20manifest672581222\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.116.233:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20create%20Project%20while%20using%20valid%20manifest672581222\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.116.233:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20create%20Project%20while%20using%20valid%20manifest672581222\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.116.233:443: connect: connection refused\noccurred","time":{"start":1665666027000,"stop":1665666027426,"duration":426}}]},"Worker Suite:Worker Suite#[It] Slice resource quota Tests ResourceQuota creation tests Create ResourceQuota fail test- Request.Mem > Limit.Mem at cluster level":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":3,"unknown":0,"total":3},"items":[{"uid":"6e6b4cdba9946582","status":"passed","time":{"start":1667739621000,"stop":1667739622059,"duration":1059}},{"uid":"4c165963785bfe2d","status":"passed","time":{"start":1667458785000,"stop":1667458785674,"duration":674}},{"uid":"4c86db3cfb7db455","status":"passed","time":{"start":1667389661000,"stop":1667389661531,"duration":531}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting iperf-client pod Should restart iperf-client pod":{"statistic":{"failed":0,"broken":0,"skipped":5,"passed":1,"unknown":0,"total":6},"items":[{"uid":"190bf814bae8d960","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667837307000,"stop":1667837307000,"duration":0}},{"uid":"dd9677b876739f3d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667826674000,"stop":1667826674000,"duration":0}},{"uid":"1475f1085320439c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667823949000,"stop":1667823949000,"duration":0}},{"uid":"791a66f069e235d8","status":"passed","time":{"start":1666674621000,"stop":1666674652327,"duration":31327}},{"uid":"3db089b9132eea90","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674212000,"stop":1666674212000,"duration":0}},{"uid":"91e74953dc450680","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666665366000,"stop":1666665366000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice resource quota Tests ResourceQuota creation tests Create ResourceQuota fail test-namespace not part of application namespaces":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":3,"unknown":0,"total":3},"items":[{"uid":"d464b414dc59121","status":"passed","time":{"start":1667739621000,"stop":1667739622074,"duration":1074}},{"uid":"c691e990ff7c583a","status":"passed","time":{"start":1667458785000,"stop":1667458785912,"duration":912}},{"uid":"7d1d4173d649e381","status":"passed","time":{"start":1667389661000,"stop":1667389661660,"duration":660}}]},"Worker Suite:Worker Suite#[It] Namespace reconcile test scenarios: Onboard cluster objs should have app ns & attached slice entry":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":6,"unknown":0,"total":6},"items":[{"uid":"89bcd13c4ff7ed40","status":"passed","time":{"start":1667837307000,"stop":1667837307331,"duration":331}},{"uid":"d1425f3daefab394","status":"passed","time":{"start":1667826674000,"stop":1667826674168,"duration":168}},{"uid":"d31961bc9f303067","status":"passed","time":{"start":1667823949000,"stop":1667823949485,"duration":485}},{"uid":"cd301e835b009d37","status":"passed","time":{"start":1666674621000,"stop":1666674621306,"duration":306}},{"uid":"9c5c3913635f3cd5","status":"passed","time":{"start":1666674212000,"stop":1666674212207,"duration":207}},{"uid":"94b5cbfcb105208a","status":"passed","time":{"start":1666665366000,"stop":1666665366286,"duration":286}}]},"c3e9b1aabfe279c6b2b3003fd9939e51":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"227fdc514f111a0","status":"passed","time":{"start":1664357346059,"stop":1664357350079,"duration":4020}}]},"Worker Suite:Worker Suite#[It] Slice creation scenarios: when slice red is installed install slice blue workers should have gateway pods from both slices":{"statistic":{"failed":0,"broken":0,"skipped":4,"passed":2,"unknown":0,"total":6},"items":[{"uid":"8c3bf78d1e09c437","status":"passed","time":{"start":1667837307000,"stop":1667837349891,"duration":42891}},{"uid":"f90197f16c8d083f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667826674000,"stop":1667826674000,"duration":0}},{"uid":"61eb18fd1931572f","status":"passed","time":{"start":1667823949000,"stop":1667823992551,"duration":43551}},{"uid":"c67b5a27f94ff67e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674621000,"stop":1666674621000,"duration":0}},{"uid":"3402f9fda3978c9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674212000,"stop":1666674212000,"duration":0}},{"uid":"bedc8846a08107c9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666665366000,"stop":1666665366000,"duration":0}}]},"80cad514533480e0d3e6606cdbfcee2c":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"414ad2a49316c72e","status":"passed","time":{"start":1664357288257,"stop":1664357426728,"duration":138471}}]},"Worker Suite:Worker Suite#[It] Resource Quota tests slice resource quota enforcement mode should create resource quota in application namespaces":{"statistic":{"failed":0,"broken":0,"skipped":3,"passed":0,"unknown":0,"total":3},"items":[{"uid":"1d9a08d20718ab73","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667739621000,"stop":1667739621000,"duration":0}},{"uid":"9a70beb66c32d1e8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667458785000,"stop":1667458785000,"duration":0}},{"uid":"2eb94f384afbbb12","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667389661000,"stop":1667389661000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice RBAC Tests SliceRoleBinding Tests Create SliceRoleBinding failure test: Creating SliceRoleBinding slice-red with invalid subject kind":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":2,"unknown":0,"total":2},"items":[{"uid":"f9f19b17be354e7","status":"passed","time":{"start":1667739621000,"stop":1667739621911,"duration":911}},{"uid":"b00c3e81fec52402","status":"passed","time":{"start":1667458785000,"stop":1667458785467,"duration":467}}]},"Worker Suite:Worker Suite#[It] Resource Quota tests slice resource quota monitoring mode Should install iperf-server on cluster":{"statistic":{"failed":1,"broken":0,"skipped":0,"passed":2,"unknown":0,"total":3},"items":[{"uid":"a91b12073a0918bb","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00048ab40>: {\n        Underlying: <*exec.ExitError | 0xc000914f20>{\n            ProcessState: {\n                pid: 6660,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 95900},\n                    Stime: {Sec: 0, Usec: 35518},\n                    Maxrss: 44324,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2096,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 276,\n                    Nivcsw: 540,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slice-rq\\\" not found\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slice-rq\\\" not found\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slice-rq\\\" not found\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (NotFound): slice.networking.kubeslice.io \\\"slice-rq\\\" not found\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (NotFound): slice.networking.kubeslice.io \"slice-rq\" not found\noccurred","time":{"start":1667739621000,"stop":1667739657921,"duration":36921}},{"uid":"828f57ef6887fc95","status":"passed","time":{"start":1667458785000,"stop":1667458800417,"duration":15417}},{"uid":"e077fdd64da5fea9","status":"passed","time":{"start":1667389661000,"stop":1667389670672,"duration":9672}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting vl3 pod Should restart vl3 pod":{"statistic":{"failed":0,"broken":0,"skipped":5,"passed":1,"unknown":0,"total":6},"items":[{"uid":"6752bd8f3b2ce287","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667837307000,"stop":1667837307000,"duration":0}},{"uid":"fa07630b58dff84c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667826674000,"stop":1667826674000,"duration":0}},{"uid":"fea53f0d588a331a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667823949000,"stop":1667823949000,"duration":0}},{"uid":"8ff721e33bb67d1d","status":"passed","time":{"start":1666674621000,"stop":1666674633462,"duration":12462}},{"uid":"4708c4f2fe47db98","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674212000,"stop":1666674212000,"duration":0}},{"uid":"41a75758cd238df9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666665366000,"stop":1666665366000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Resource Quota tests slice resource quota monitoring mode increasing load on namespace should log the usage on sliceresourcequota":{"statistic":{"failed":0,"broken":0,"skipped":3,"passed":0,"unknown":0,"total":3},"items":[{"uid":"bc04e7cb2f595ee0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667739621000,"stop":1667739621000,"duration":0}},{"uid":"3da4e911eddd7f8b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667458785000,"stop":1667458785000,"duration":0}},{"uid":"a22ccb83046ac51d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667389661000,"stop":1667389661000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice Config Negative Scenario tests Slice Config Negative Scenario Creation Should fail while creating sliceconfig with invalid values on slice name":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":6,"unknown":0,"total":6},"items":[{"uid":"9f28412472e70cac","status":"passed","time":{"start":1667837307000,"stop":1667837307571,"duration":571}},{"uid":"403eebaf913e6873","status":"passed","time":{"start":1667826674000,"stop":1667826674555,"duration":555}},{"uid":"c854d6d94b63fafd","status":"passed","time":{"start":1667823949000,"stop":1667823950872,"duration":1872}},{"uid":"d037f9d8da359aa","status":"passed","time":{"start":1666674621000,"stop":1666674623116,"duration":2116}},{"uid":"d235284037950804","status":"passed","time":{"start":1666674212000,"stop":1666674213728,"duration":1728}},{"uid":"98b1e77f602283ff","status":"passed","time":{"start":1666665366000,"stop":1666665367446,"duration":1446}}]},"Worker Suite:Worker Suite#[It] Slice RBAC Tests SliceRoleBinding Tests Create SliceRoleBinding successful test: Creating SliceRoleBinding slice-red with Kubernetes Roles":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":2,"unknown":0,"total":2},"items":[{"uid":"3a4cebe3b21c3382","status":"passed","time":{"start":1667739621000,"stop":1667739622410,"duration":1410}},{"uid":"17b32abe171e621d","status":"passed","time":{"start":1667458785000,"stop":1667458786789,"duration":1789}}]},"Worker Suite:Worker Suite#[It] Slice resource quota Tests ResourceQuota creation tests Create ResourceQuota fail test- Request.EphemeralStorage > Limit.EphemeralStorage at namespace level":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":3,"unknown":0,"total":3},"items":[{"uid":"a37831a3f59bdd4a","status":"passed","time":{"start":1667739621000,"stop":1667739622080,"duration":1080}},{"uid":"d3a69eae6ec33108","status":"passed","time":{"start":1667458785000,"stop":1667458785707,"duration":707}},{"uid":"336062ca1874efe9","status":"passed","time":{"start":1667389661000,"stop":1667389661532,"duration":532}}]},"Worker Suite:Worker Suite#[It] Slice resource quota Tests ResourceQuota creation tests Create ResourceQuota fail test-namespace.limit.mem negative":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":3,"unknown":0,"total":3},"items":[{"uid":"744f04bb16d90c7c","status":"passed","time":{"start":1667739621000,"stop":1667739622308,"duration":1308}},{"uid":"da97e33695253676","status":"passed","time":{"start":1667458785000,"stop":1667458786019,"duration":1019}},{"uid":"49ca7725b443c299","status":"passed","time":{"start":1667389661000,"stop":1667389661994,"duration":994}}]},"Worker Suite:Worker Suite#[It] Slice RBAC Tests SliceRoleTemplate Tests Create SliceRoleTempalate failure test: Creating SliceRoleTempalate read-only-role, missing verbs field":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":2,"unknown":0,"total":2},"items":[{"uid":"ee627e1bb6e82e5c","status":"passed","time":{"start":1667739621000,"stop":1667739621933,"duration":933}},{"uid":"37b0c40a71b841ee","status":"passed","time":{"start":1667458785000,"stop":1667458785960,"duration":960}}]},"Worker Suite:Worker Suite#[It] Resource Quota tests slice resource quota enforcement mode should not create Memory limits in limits in app namespace-1":{"statistic":{"failed":0,"broken":0,"skipped":3,"passed":0,"unknown":0,"total":3},"items":[{"uid":"5d6eed6be57e7625","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667739621000,"stop":1667739621000,"duration":0}},{"uid":"4ebb09b197442416","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667458785000,"stop":1667458785000,"duration":0}},{"uid":"b07c252af0763d3e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667389661000,"stop":1667389661000,"duration":0}}]},"Worker Suite:Worker Suite#[It]  QosProfile tests QosProfile Create validation Creates qosprofile":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":6,"unknown":0,"total":6},"items":[{"uid":"44cb9368faf3465e","status":"passed","time":{"start":1667837307000,"stop":1667837308267,"duration":1267}},{"uid":"2c766f328883499d","status":"passed","time":{"start":1667826674000,"stop":1667826674493,"duration":493}},{"uid":"3ae55c88eb6395cf","status":"passed","time":{"start":1667823949000,"stop":1667823949912,"duration":912}},{"uid":"55f47c2336ca7fe1","status":"passed","time":{"start":1666674621000,"stop":1666674622956,"duration":1956}},{"uid":"c55b727e64a02ab8","status":"passed","time":{"start":1666674212000,"stop":1666674212718,"duration":718}},{"uid":"cb11b99efdd6e4e5","status":"passed","time":{"start":1666665366000,"stop":1666665366761,"duration":761}}]},"Worker Suite:Worker Suite#[It] Slice resource quota Tests ResourceQuota creation tests Create ResourceQuota fail test- Request.EphemeralStorage > Limit.EphemeralStorage at slice level":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":3,"unknown":0,"total":3},"items":[{"uid":"12c8936381f5e84d","status":"passed","time":{"start":1667739621000,"stop":1667739623760,"duration":2760}},{"uid":"f87735a11c412d27","status":"passed","time":{"start":1667458785000,"stop":1667458788534,"duration":3534}},{"uid":"63709bedcabadd32","status":"passed","time":{"start":1667389661000,"stop":1667389662692,"duration":1692}}]},"Worker Suite:Worker Suite#[It] Slice resource quota Tests ResourceQuota creation tests Create ResourceQuota fail test-pod count empty":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":3,"unknown":0,"total":3},"items":[{"uid":"1a6c845c7fd7cc9a","status":"passed","time":{"start":1667739621000,"stop":1667739622793,"duration":1793}},{"uid":"40ac45bcd844f239","status":"passed","time":{"start":1667458785000,"stop":1667458787115,"duration":2115}},{"uid":"3781e4c967f4a6f1","status":"passed","time":{"start":1667389661000,"stop":1667389662198,"duration":1198}}]},"Empty Suite:Empty Suite#[It] Hub Creation tests Hub Installtion Test Senarios Should fail if Hub is installed again in the same cluster":{"statistic":{"failed":0,"broken":0,"skipped":3,"passed":8,"unknown":0,"total":11},"items":[{"uid":"e792a36d7fcf78cf","status":"passed","time":{"start":1666883811000,"stop":1666883814977,"duration":3977}},{"uid":"dcaf77d6b45865ae","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666754109000,"stop":1666754109000,"duration":0}},{"uid":"47878eddb234efd6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666750338000,"stop":1666750338000,"duration":0}},{"uid":"6b273de3091448f7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666747791000,"stop":1666747791000,"duration":0}},{"uid":"6a4771d068040c66","status":"passed","time":{"start":1666706575000,"stop":1666706577594,"duration":2594}},{"uid":"f786878ad4cc8479","status":"passed","time":{"start":1666077791000,"stop":1666077794637,"duration":3637}},{"uid":"4b68e88e3ae83824","status":"passed","time":{"start":1665944068000,"stop":1665944070736,"duration":2736}},{"uid":"77b9b280c0736c51","status":"passed","time":{"start":1665665806000,"stop":1665665809254,"duration":3254}},{"uid":"2a1a418d7648ae4e","status":"passed","time":{"start":1665665864000,"stop":1665665868992,"duration":4992}},{"uid":"7e4d26d002f77153","status":"passed","time":{"start":1665552534000,"stop":1665552537501,"duration":3501}},{"uid":"c68491335cba1a0a","status":"passed","time":{"start":1664798682000,"stop":1664798687418,"duration":5418}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests slice with netpol Should contain application namespaces":{"statistic":{"failed":0,"broken":0,"skipped":4,"passed":2,"unknown":0,"total":6},"items":[{"uid":"89da7a0605ccec65","status":"passed","time":{"start":1667837307000,"stop":1667837307165,"duration":165}},{"uid":"8ff3f08652f83be0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667826674000,"stop":1667826674000,"duration":0}},{"uid":"ecb8a0c4766fbe64","status":"passed","time":{"start":1667823949000,"stop":1667823949156,"duration":156}},{"uid":"e55dbe285f60aa89","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674621000,"stop":1666674621000,"duration":0}},{"uid":"f2abe3b2b1676f3b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674212000,"stop":1666674212000,"duration":0}},{"uid":"c862e7d3fb3e3d7c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666665366000,"stop":1666665366000,"duration":0}}]},"Intracluster Suite:Intracluster Suite#[It] Intracluster slice tests Iperf connectivity Verify Iperf traffic":{"statistic":{"failed":0,"broken":0,"skipped":3,"passed":1,"unknown":0,"total":4},"items":[{"uid":"b746441cd3b56814","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667842189000,"stop":1667842189000,"duration":0}},{"uid":"b009f60bf683ac0a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674484000,"stop":1666674484000,"duration":0}},{"uid":"15546daac538cbb6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666673362000,"stop":1666673362000,"duration":0}},{"uid":"248a82cfbcff4d6","status":"passed","time":{"start":1666664909000,"stop":1666664978880,"duration":69880}}]},"6d3ae00abbd534b9ffed53e43b8ae06d":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"d0e00832a80fa9e2","status":"passed","time":{"start":1664356969024,"stop":1664357288256,"duration":319232}}]},"Worker Suite:Worker Suite#[It] Slice Config Negative Scenario tests Slice Config Negative Scenario Update Should fail while updating sliceconfig with invalid values on slice subnet":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":5,"unknown":0,"total":6},"items":[{"uid":"67b5a2f6ede3d5a3","status":"passed","time":{"start":1667837307000,"stop":1667837307590,"duration":590}},{"uid":"47edf015ee9ec7e7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667826674000,"stop":1667826674000,"duration":0}},{"uid":"d2cf166bb72a9bcd","status":"passed","time":{"start":1667823949000,"stop":1667823949300,"duration":300}},{"uid":"cfa480eca67c09bd","status":"passed","time":{"start":1666674621000,"stop":1666674622259,"duration":1259}},{"uid":"748d0f2ea2f4eac0","status":"passed","time":{"start":1666674212000,"stop":1666674212766,"duration":766}},{"uid":"6e8fcd08ba97a742","status":"passed","time":{"start":1666665366000,"stop":1666665366986,"duration":986}}]},"Worker Suite:Worker Suite#[It] Slice RBAC Tests RBAC Common Tests WorkerSliceRoleBinding generation test: Creation of SliceRoleBinding with k8s roles should generate WorkerSliceRoleBinding":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":1,"unknown":0,"total":2},"items":[{"uid":"473eaf8794993ec","status":"passed","time":{"start":1667739621000,"stop":1667739622713,"duration":1713}},{"uid":"220b8ed5d3b28e9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667458785000,"stop":1667458785000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice resource quota Tests ResourceQuota creation tests Create ResourceQuota fail test - slice.limit.CPU negative  ":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":3,"unknown":0,"total":3},"items":[{"uid":"e8fd50ea24c8bde1","status":"passed","time":{"start":1667739621000,"stop":1667739621854,"duration":854}},{"uid":"cd7da04b054fa6a3","status":"passed","time":{"start":1667458785000,"stop":1667458785760,"duration":760}},{"uid":"1124c4d5ea177dcb","status":"passed","time":{"start":1667389661000,"stop":1667389661991,"duration":991}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests Iperf App connectivity Should install iperf-sleep on client cluster":{"statistic":{"failed":1,"broken":0,"skipped":4,"passed":1,"unknown":0,"total":6},"items":[{"uid":"90b4d2a3c578cdb7","status":"passed","time":{"start":1667837307000,"stop":1667837341370,"duration":34370}},{"uid":"72800e850a33bda4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667826674000,"stop":1667826674000,"duration":0}},{"uid":"a60e57ca0ca9e0d2","status":"failed","statusDetails":"Timed out after 120.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1667823949000,"stop":1667824073637,"duration":124637}},{"uid":"ad4b4288cc9f5e38","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674621000,"stop":1666674621000,"duration":0}},{"uid":"2099b56525520370","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674212000,"stop":1666674212000,"duration":0}},{"uid":"8d82d4dbf82bc739","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666665366000,"stop":1666665366000,"duration":0}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests slice with netpol should contain the allowed namespaces":{"statistic":{"failed":0,"broken":0,"skipped":4,"passed":2,"unknown":0,"total":6},"items":[{"uid":"89824019a17b0871","status":"passed","time":{"start":1667837307000,"stop":1667837307135,"duration":135}},{"uid":"c88d7409ca46efa1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667826674000,"stop":1667826674000,"duration":0}},{"uid":"fd58d4db769afaf5","status":"passed","time":{"start":1667823949000,"stop":1667823949155,"duration":155}},{"uid":"9a8f311edefb50a5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674621000,"stop":1666674621000,"duration":0}},{"uid":"485aeafe466aec00","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674212000,"stop":1666674212000,"duration":0}},{"uid":"6c6fa4606e59af11","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666665366000,"stop":1666665366000,"duration":0}}]},"Empty Suite:Empty Suite#[It] Hub Creation tests Hub Installtion Test Senarios Should successfully pass if Cert is installed first and then Hub":{"statistic":{"failed":3,"broken":0,"skipped":0,"passed":8,"unknown":0,"total":11},"items":[{"uid":"96125eb2607d263f","status":"passed","time":{"start":1666883811000,"stop":1666883860192,"duration":49192}},{"uid":"c023445878a526ee","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000e858>: {\n        Underlying: <*exec.ExitError | 0xc0000784c0>{\n            ProcessState: {\n                pid: 6219,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 55605},\n                    Stime: {Sec: 0, Usec: 3971},\n                    Maxrss: 40616,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 1979,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 171,\n                    Nivcsw: 51,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"install.go:192: [debug] Original chart version: \\\"\\\"\",\n                        \"Error: INSTALLATION FAILED: looks like \\\"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages\\\" is not a valid chart repository or cannot be reached: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\",\n                        \"helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\",\n                        \"helm.sh/helm/v3/pkg/repo.(*ChartRepository).DownloadIndexFile\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:128\",\n                        \"helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:252\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:723\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:204\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:139\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:872\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:990\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:918\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:250\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1571\",\n                        \"looks like \\\"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages\\\" is not a valid chart repository or cannot be reached\",\n                        \"helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:254\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:723\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:204\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:139\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n             ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; install.go:192: [debug] Original chart version: \"\"\n    Error: INSTALLATION FAILED: looks like \"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages\" is not a valid chart repository or cannot be reached: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\n    helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\n    helm.sh/helm/v3/pkg/repo.(*ChartRepository).DownloadIndexFile\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:128\n    helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:252\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:723\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:204\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:139\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    looks like \"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages\" is not a valid chart repository or cannot be reached\n    helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:254\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:723\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:204\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:139\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:141\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\noccurred","time":{"start":1666754109000,"stop":1666754182119,"duration":73119}},{"uid":"db203ea6a32bf38e","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000fba8>: {\n        Underlying: <*exec.ExitError | 0xc00049eae0>{\n            ProcessState: {\n                pid: 6249,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 53756},\n                    Stime: {Sec: 0, Usec: 3839},\n                    Maxrss: 40884,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 1953,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 317,\n                    Nivcsw: 100,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"install.go:192: [debug] Original chart version: \\\"\\\"\",\n                        \"Error: INSTALLATION FAILED: looks like \\\"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/\\\" is not a valid chart repository or cannot be reached: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\",\n                        \"helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\",\n                        \"helm.sh/helm/v3/pkg/repo.(*ChartRepository).DownloadIndexFile\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:128\",\n                        \"helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:252\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:723\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:204\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:139\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:872\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:990\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:918\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:250\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1571\",\n                        \"looks like \\\"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/\\\" is not a valid chart repository or cannot be reached\",\n                        \"helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:254\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:723\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:204\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:139\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n          ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; install.go:192: [debug] Original chart version: \"\"\n    Error: INSTALLATION FAILED: looks like \"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/\" is not a valid chart repository or cannot be reached: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\n    helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\n    helm.sh/helm/v3/pkg/repo.(*ChartRepository).DownloadIndexFile\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:128\n    helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:252\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:723\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:204\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:139\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    looks like \"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/\" is not a valid chart repository or cannot be reached\n    helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:254\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:723\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:204\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:139\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:141\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\noccurred","time":{"start":1666750338000,"stop":1666750420125,"duration":82125}},{"uid":"879cdb92d0b023c4","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004ea030>: {\n        Underlying: <*exec.ExitError | 0xc0004fc040>{\n            ProcessState: {\n                pid: 6249,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 32423},\n                    Stime: {Sec: 0, Usec: 32423},\n                    Maxrss: 42520,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 1887,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 234,\n                    Nivcsw: 223,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"install.go:192: [debug] Original chart version: \\\"\\\"\",\n                        \"Error: INSTALLATION FAILED: looks like \\\"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/\\\" is not a valid chart repository or cannot be reached: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\",\n                        \"helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\",\n                        \"helm.sh/helm/v3/pkg/repo.(*ChartRepository).DownloadIndexFile\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:128\",\n                        \"helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:252\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:723\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:204\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:139\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:872\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:990\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:918\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:250\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1571\",\n                        \"looks like \\\"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/\\\" is not a valid chart repository or cannot be reached\",\n                        \"helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:254\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:723\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:204\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:139\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n         ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; install.go:192: [debug] Original chart version: \"\"\n    Error: INSTALLATION FAILED: looks like \"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/\" is not a valid chart repository or cannot be reached: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\n    helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\n    helm.sh/helm/v3/pkg/repo.(*ChartRepository).DownloadIndexFile\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:128\n    helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:252\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:723\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:204\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:139\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    looks like \"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/\" is not a valid chart repository or cannot be reached\n    helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:254\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:723\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:204\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:139\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:141\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\noccurred","time":{"start":1666747791000,"stop":1666747873444,"duration":82444}},{"uid":"cdda694b72ba4cea","status":"passed","time":{"start":1666706575000,"stop":1666706616181,"duration":41181}},{"uid":"fc8d3acd092c2a38","status":"passed","time":{"start":1666077791000,"stop":1666077840157,"duration":49157}},{"uid":"4447fd1392ccf9e0","status":"passed","time":{"start":1665944068000,"stop":1665944101892,"duration":33892}},{"uid":"87e6803e8f374867","status":"passed","time":{"start":1665665806000,"stop":1665665897869,"duration":91869}},{"uid":"6b893f602b35a010","status":"passed","time":{"start":1665665864000,"stop":1665665953615,"duration":89615}},{"uid":"f900bca47743b0b6","status":"passed","time":{"start":1665552534000,"stop":1665552625973,"duration":91973}},{"uid":"bac5ee86c5d7e1f8","status":"passed","time":{"start":1664798682000,"stop":1664798770633,"duration":88633}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Creation Should create Multiple Projects in controller using valid manifest":{"statistic":{"failed":2,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":3},"items":[{"uid":"1f07ff56a8135645","status":"passed","time":{"start":1667840900000,"stop":1667840907767,"duration":7767}},{"uid":"18a00444b0304a98","status":"failed","statusDetails":"Timed out after 60.001s.\nExpected\n    <bool>: false\nto be true","time":{"start":1665665955000,"stop":1665666022315,"duration":67315}},{"uid":"bf3055a3cef5b761","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0005346c0>: {\n        Underlying: <*exec.ExitError | 0xc0006a2640>{\n            ProcessState: {\n                pid: 7137,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 240403},\n                    Stime: {Sec: 0, Usec: 80134},\n                    Maxrss: 83008,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 9342,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13760,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 454,\n                    Nivcsw: 440,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20create%20Multiple%20Projects%20in%20controller%20using%20valid%20manifest3011383695\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.116.233:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20create%20Multiple%20Projects%20in%20controller%20using%20valid%20manifest3011383695\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.116.233:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20create%20Multiple%20Projects%20in%20controller%20using%20valid%20manifest3011383695\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.116.233:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20create%20Multiple%20Projects%20in%20controller%20using%20valid%20manifest3011383695\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.116.233:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Creation%20Should%20create%20Multiple%20Projects%20in%20controller%20using%20valid%20manifest3011383695\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.116.233:443: connect: connection refused\noccurred","time":{"start":1665666027000,"stop":1665666027378,"duration":378}}]},"Worker Suite:Worker Suite#[It] Slice Config Negative Scenario tests Slice Config Negative Scenario Creation Should fail while creating sliceconfig with invalid values on slice subnet":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":6,"unknown":0,"total":6},"items":[{"uid":"17e9fb898ce939c4","status":"passed","time":{"start":1667837307000,"stop":1667837307355,"duration":355}},{"uid":"2b37985545b69abe","status":"passed","time":{"start":1667826674000,"stop":1667826674381,"duration":381}},{"uid":"953afc8a1e49527b","status":"passed","time":{"start":1667823949000,"stop":1667823949855,"duration":855}},{"uid":"bcecfa51f4e7d1d7","status":"passed","time":{"start":1666674621000,"stop":1666674621799,"duration":799}},{"uid":"9a88a7fc19149015","status":"passed","time":{"start":1666674212000,"stop":1666674212546,"duration":546}},{"uid":"3f5d64f31b148fcf","status":"passed","time":{"start":1666665366000,"stop":1666665366729,"duration":729}}]},"Worker Suite:Worker Suite#[It] Slice RBAC Tests RBAC Common Tests SliceRoleBinding status test: SliceRoleBinding should have error status for invalid namespace and invalid roles":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":0,"unknown":0,"total":2},"items":[{"uid":"376a789a1c76d9f4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667739621000,"stop":1667739621000,"duration":0}},{"uid":"f94e8ca8730872f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667458785000,"stop":1667458785000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Attach/Deattach Scenario Attach cluster from slice  when Deattach cluster from slice  should have vl3 router running on attached cluster":{"statistic":{"failed":0,"broken":0,"skipped":4,"passed":2,"unknown":0,"total":6},"items":[{"uid":"17717da0dc838e31","status":"passed","time":{"start":1667837307000,"stop":1667837307006,"duration":6}},{"uid":"7c2b666e9ac915e0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667826674000,"stop":1667826674000,"duration":0}},{"uid":"4e3c53cde2a589fa","status":"passed","time":{"start":1667823949000,"stop":1667823949006,"duration":6}},{"uid":"3b6ad7c9fbced854","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674621000,"stop":1666674621000,"duration":0}},{"uid":"9f016eb4ad998901","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674212000,"stop":1666674212000,"duration":0}},{"uid":"7658f4a34a207a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666665366000,"stop":1666665366000,"duration":0}}]},"Intracluster Suite:Intracluster Suite#[It] Intracluster slice tests Slice with netpol should reconcile netpol in app NS":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":2,"unknown":0,"total":4},"items":[{"uid":"1cfe5d011df63112","status":"passed","time":{"start":1667842189000,"stop":1667842189220,"duration":220}},{"uid":"6e51e580580a3c8f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674484000,"stop":1666674484000,"duration":0}},{"uid":"cfc2715f0ae3ab33","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666673362000,"stop":1666673362000,"duration":0}},{"uid":"fc14330795da52d2","status":"passed","time":{"start":1666664909000,"stop":1666664909161,"duration":161}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Deletion Should Delete an existing project successfully":{"statistic":{"failed":2,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":3},"items":[{"uid":"ecd7f050d1aa67b6","status":"passed","time":{"start":1667840900000,"stop":1667840904745,"duration":4745}},{"uid":"969fe0e2722e37b5","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0001cf2d8>: {\n        Underlying: <*exec.ExitError | 0xc0003db1e0>{\n            ProcessState: {\n                pid: 7686,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 151589},\n                    Stime: {Sec: 0, Usec: 60635},\n                    Maxrss: 89192,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 10071,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13760,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 488,\n                    Nivcsw: 245,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Deletion%20Should%20Delete%20an%20existing%20project%20successfully865657031\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.94.3:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Deletion%20Should%20Delete%20an%20existing%20project%20successfully865657031\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.94.3:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Deletion%20Should%20Delete%20an%20existing%20project%20successfully865657031\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.94.3:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Deletion%20Should%20Delete%20an%20existing%20project%20successfully865657031\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.94.3:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Deletion%20Should%20Delete%20an%20existing%20project%20successfully865657031\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.94.3:443: connect: connection refused\noccurred","time":{"start":1665665955000,"stop":1665665955271,"duration":271}},{"uid":"bfac7adfb3190871","status":"failed","statusDetails":"Timed out after 60.000s.\nExpected\n    <bool>: false\nto be true","time":{"start":1665666027000,"stop":1665666104746,"duration":77746}}]},"Worker Suite:Worker Suite#[It] Slice RBAC Tests RBAC Common Tests WorkerSliceRoleBinding generation test: WorkerSliceRoleBinding with custom role should be generated automatically when forcefully deleted":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":1,"unknown":0,"total":2},"items":[{"uid":"c92f494c865d101c","status":"passed","time":{"start":1667739621000,"stop":1667739770372,"duration":149372}},{"uid":"e959087353be8cc7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667458785000,"stop":1667458785000,"duration":0}}]},"Worker Suite:Worker Suite#[AfterSuite]":{"statistic":{"failed":10,"broken":0,"skipped":0,"passed":35,"unknown":0,"total":45},"items":[{"uid":"b40246df5eae34d8","status":"failed","statusDetails":"Timed out after 210.009s.\nExpected an error to have occurred.  Got:\n    <nil>: nil","time":{"start":1667837307000,"stop":1667837525367,"duration":218367}},{"uid":"4be9d988440181a","status":"passed","time":{"start":1667826674000,"stop":1667826704803,"duration":30803}},{"uid":"cbfa6e8a937e1af5","status":"failed","statusDetails":"Timed out after 210.009s.\nExpected an error to have occurred.  Got:\n    <nil>: nil","time":{"start":1667823949000,"stop":1667824168346,"duration":219346}},{"uid":"9b315654092f4af1","status":"passed","time":{"start":1667819342000,"stop":1667819368624,"duration":26624}},{"uid":"3b57e3ea9aade971","status":"passed","time":{"start":1667808371000,"stop":1667808398058,"duration":27058}},{"uid":"65d4eea9d841c3fc","status":"passed","time":{"start":1667799859000,"stop":1667799883828,"duration":24828}},{"uid":"fe77153ef000bb2f","status":"passed","time":{"start":1667796325000,"stop":1667796350474,"duration":25474}},{"uid":"7aa2f02afcd62505","status":"passed","time":{"start":1667793837000,"stop":1667793863131,"duration":26131}},{"uid":"f918e2839e9e6e08","status":"passed","time":{"start":1667766962000,"stop":1667766989466,"duration":27466}},{"uid":"456bda4c396e2a26","status":"passed","time":{"start":1667765147000,"stop":1667765175490,"duration":28490}},{"uid":"ffdcfc931d597f77","status":"passed","time":{"start":1667763515000,"stop":1667763540580,"duration":25580}},{"uid":"c77431a50006e359","status":"passed","time":{"start":1667755854000,"stop":1667755880205,"duration":26205}},{"uid":"cca3e59bb1a56b4d","status":"failed","statusDetails":"Timed out after 210.010s.\nExpected an error to have occurred.  Got:\n    <nil>: nil","time":{"start":1667739621000,"stop":1667739844551,"duration":223551}},{"uid":"9f7bc1280ba56ab7","status":"passed","time":{"start":1667738525000,"stop":1667738551504,"duration":26504}},{"uid":"d5235c2e2922ee5d","status":"passed","time":{"start":1667737440000,"stop":1667737441304,"duration":1304}},{"uid":"5dfb49874888e8db","status":"failed","statusDetails":"Timed out after 210.009s.\nExpected an error to have occurred.  Got:\n    <nil>: nil","time":{"start":1667651299000,"stop":1667651509734,"duration":210734}},{"uid":"de40bd9c0171b7d9","status":"passed","time":{"start":1667595443000,"stop":1667595472030,"duration":29030}},{"uid":"1f04a06d2379bb2a","status":"passed","time":{"start":1667537247000,"stop":1667537273596,"duration":26596}},{"uid":"110b8f4440c31f16","status":"failed","statusDetails":"Timed out after 210.010s.\nExpected an error to have occurred.  Got:\n    <nil>: nil","time":{"start":1667458785000,"stop":1667459009473,"duration":224473}},{"uid":"bdafa3d5dc9fd36c","status":"failed","statusDetails":"Timed out after 210.009s.\nExpected an error to have occurred.  Got:\n    <nil>: nil","time":{"start":1667389661000,"stop":1667389881007,"duration":220007}}]},"Worker Suite:Worker Suite#[It]  QosProfile tests QosProfile delete validation Delete qos profile - negative scenario":{"statistic":{"failed":1,"broken":0,"skipped":0,"passed":5,"unknown":0,"total":6},"items":[{"uid":"5d1992020e48a2a4","status":"passed","time":{"start":1667837307000,"stop":1667837310419,"duration":3419}},{"uid":"5e218a0ad35582a0","status":"passed","time":{"start":1667826674000,"stop":1667826689298,"duration":15298}},{"uid":"9d0ed8ca0e2427fd","status":"passed","time":{"start":1667823949000,"stop":1667823952640,"duration":3640}},{"uid":"cbffd0e9e52ef374","status":"passed","time":{"start":1666674621000,"stop":1666674625946,"duration":4946}},{"uid":"4b4ddb288b571f42","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1666674212000,"stop":1666674393217,"duration":181217}},{"uid":"f337dcbbf9532b4f","status":"passed","time":{"start":1666665366000,"stop":1666665384537,"duration":18537}}]},"Worker Suite:Worker Suite#[It] Attach/Deattach Scenario Attach cluster from slice  when Deattach cluster from slice  should have gateway pod deleted from deattached cluster":{"statistic":{"failed":0,"broken":0,"skipped":4,"passed":2,"unknown":0,"total":6},"items":[{"uid":"8479fd4cd7ffa17c","status":"passed","time":{"start":1667837307000,"stop":1667837307003,"duration":3}},{"uid":"a5473280a9d0628a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667826674000,"stop":1667826674000,"duration":0}},{"uid":"cafc1d97a49c113d","status":"passed","time":{"start":1667823949000,"stop":1667823949004,"duration":4}},{"uid":"5e5e35003ef16f39","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674621000,"stop":1666674621000,"duration":0}},{"uid":"67757b5e779af27a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674212000,"stop":1666674212000,"duration":0}},{"uid":"b54032c048100938","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666665366000,"stop":1666665366000,"duration":0}}]},"Intracluster Suite:Intracluster Suite#[It] Intracluster slice tests Slice with netpol should label app ns with kubeslice label":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":2,"unknown":0,"total":4},"items":[{"uid":"99d183444eda3910","status":"passed","time":{"start":1667842189000,"stop":1667842189087,"duration":87}},{"uid":"bb79fed2ac699425","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674484000,"stop":1666674484000,"duration":0}},{"uid":"5ada287d7b42dc39","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666673362000,"stop":1666673362000,"duration":0}},{"uid":"9c8cc9655051d6b0","status":"passed","time":{"start":1666664909000,"stop":1666664909069,"duration":69}}]},"Empty Suite:Empty Suite#[It] Hub Creation tests Hub Installtion Test Senarios Should fail if Hub is installed before installing Cert":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":11,"unknown":0,"total":11},"items":[{"uid":"4e300ac6dbd64a3d","status":"passed","time":{"start":1666883811000,"stop":1666883815793,"duration":4793}},{"uid":"1a1a62f8873062dc","status":"passed","time":{"start":1666754109000,"stop":1666754109127,"duration":127}},{"uid":"79e38ad599cebaea","status":"passed","time":{"start":1666750338000,"stop":1666750338199,"duration":199}},{"uid":"d6665c7282b614d4","status":"passed","time":{"start":1666747791000,"stop":1666747791119,"duration":119}},{"uid":"c35498da5817c514","status":"passed","time":{"start":1666706575000,"stop":1666706577377,"duration":2377}},{"uid":"a1074016246462e0","status":"passed","time":{"start":1666077791000,"stop":1666077793178,"duration":2178}},{"uid":"fbb081059b6bfa36","status":"passed","time":{"start":1665944068000,"stop":1665944069639,"duration":1639}},{"uid":"1d85de33c3e4740e","status":"passed","time":{"start":1665665806000,"stop":1665665807706,"duration":1706}},{"uid":"21d2a7a6d80aaa8a","status":"passed","time":{"start":1665665864000,"stop":1665665866652,"duration":2652}},{"uid":"1865cca59b350f33","status":"passed","time":{"start":1665552534000,"stop":1665552536076,"duration":2076}},{"uid":"23d69bd3d795b0b5","status":"passed","time":{"start":1664798682000,"stop":1664798684713,"duration":2713}}]},"Worker Suite:Worker Suite#[It] Slice RBAC Tests SliceRoleBinding Tests Create SliceRoleBinding failure test: Creating SliceRoleBinding slice-red with invalid ApiVersion in Role":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":2,"unknown":0,"total":2},"items":[{"uid":"ea149b1f5cea0997","status":"passed","time":{"start":1667739621000,"stop":1667739621873,"duration":873}},{"uid":"80e094e53e3719fb","status":"passed","time":{"start":1667458785000,"stop":1667458786225,"duration":1225}}]},"Worker Suite:Worker Suite#[It] Slice creation scenarios: when slice red is installed workers should get attached to slice":{"statistic":{"failed":1,"broken":0,"skipped":0,"passed":5,"unknown":0,"total":6},"items":[{"uid":"5a7b4c87817f92f6","status":"passed","time":{"start":1667837307000,"stop":1667837310122,"duration":3122}},{"uid":"89ab664efa9f57bf","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0007cc198>: {\n        Underlying: <*exec.ExitError | 0xc000797040>{\n            ProcessState: {\n                pid: 6232,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 227780},\n                    Stime: {Sec: 0, Usec: 57980},\n                    Maxrss: 102116,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 5681,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 19808,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 519,\n                    Nivcsw: 566,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-red\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-red\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\noccurred","time":{"start":1667826674000,"stop":1667826674294,"duration":294}},{"uid":"abb0f3d495524c0b","status":"passed","time":{"start":1667823949000,"stop":1667823952030,"duration":3030}},{"uid":"e412274f0fb2f9ab","status":"passed","time":{"start":1666674621000,"stop":1666674621949,"duration":949}},{"uid":"be442b6438ae3f99","status":"passed","time":{"start":1666674212000,"stop":1666674213252,"duration":1252}},{"uid":"d9bf91241ac31196","status":"passed","time":{"start":1666665366000,"stop":1666665367572,"duration":1572}}]},"Worker Suite:Worker Suite#[It] Slice resource quota Tests ResourceQuota creation tests Create ResourceQuota fail test-cluster not part":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":3,"unknown":0,"total":3},"items":[{"uid":"6989f912a9176d1","status":"passed","time":{"start":1667739621000,"stop":1667739622023,"duration":1023}},{"uid":"6754eef044eb87b","status":"passed","time":{"start":1667458785000,"stop":1667458785801,"duration":801}},{"uid":"4f91c12f82ea3d50","status":"passed","time":{"start":1667389661000,"stop":1667389661689,"duration":689}}]},"Worker Suite:Worker Suite#[It] Resource Quota tests iper connectivity should onboard deployment in application Namespaces without slice annotations":{"statistic":{"failed":0,"broken":0,"skipped":3,"passed":0,"unknown":0,"total":3},"items":[{"uid":"fae05cd6385b4158","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667739621000,"stop":1667739621000,"duration":0}},{"uid":"202faa228de62b3c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667458785000,"stop":1667458785000,"duration":0}},{"uid":"927ce2740b361f9f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667389661000,"stop":1667389661000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Namespace reconcile test scenarios: Offboard slice get detached from app ns in cluster objects":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":6,"unknown":0,"total":6},"items":[{"uid":"6567ffcdc0010326","status":"passed","time":{"start":1667837307000,"stop":1667837307286,"duration":286}},{"uid":"b50c5be730dd1936","status":"passed","time":{"start":1667826674000,"stop":1667826674181,"duration":181}},{"uid":"d26d03e52a309fbc","status":"passed","time":{"start":1667823949000,"stop":1667823949314,"duration":314}},{"uid":"92609a3466218b08","status":"passed","time":{"start":1666674621000,"stop":1666674621247,"duration":247}},{"uid":"633e652e2d2109ae","status":"passed","time":{"start":1666674212000,"stop":1666674212236,"duration":236}},{"uid":"d11d50f51f780330","status":"passed","time":{"start":1666665366000,"stop":1666665366360,"duration":360}}]},"Worker Suite:Worker Suite#[It] Slice Creation scenarios Slice with LOW QOS should reconcile low qos slice":{"statistic":{"failed":0,"broken":0,"skipped":5,"passed":1,"unknown":0,"total":6},"items":[{"uid":"aa64ecc695bdb3f8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667837307000,"stop":1667837307000,"duration":0}},{"uid":"60510d1dda17e254","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667826674000,"stop":1667826674000,"duration":0}},{"uid":"b9da9e8f27cf68f8","status":"passed","time":{"start":1667823949000,"stop":1667823949467,"duration":467}},{"uid":"ae432458b80bb5c7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674621000,"stop":1666674621000,"duration":0}},{"uid":"a542914546ca85c8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674212000,"stop":1666674212000,"duration":0}},{"uid":"d19a6f77a628bfd2","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666665366000,"stop":1666665366000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice resource quota Tests ResourceQuota creation tests Create ResourceQuota fail test - slice.limit.mem negative  ":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":3,"unknown":0,"total":3},"items":[{"uid":"c0b8f7c1354994d5","status":"passed","time":{"start":1667739621000,"stop":1667739622205,"duration":1205}},{"uid":"f982aadfcbcb20a8","status":"passed","time":{"start":1667458785000,"stop":1667458785905,"duration":905}},{"uid":"15350ea2b8c857b","status":"passed","time":{"start":1667389661000,"stop":1667389661685,"duration":685}}]},"Worker Suite:Worker Suite#[It] NodeRestartSlice Testing node restart should re-establish connection on node restart":{"statistic":{"failed":1,"broken":0,"skipped":5,"passed":0,"unknown":0,"total":6},"items":[{"uid":"661e42839b298c0a","status":"failed","statusDetails":"Timed out after 60.009s.\nExpected\n    <bool>: false\nto equal\n    <bool>: true","time":{"start":1667837307000,"stop":1667837397664,"duration":90664}},{"uid":"73a683abaac8e07f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667826674000,"stop":1667826674000,"duration":0}},{"uid":"7e753c24ad97a9bc","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667823949000,"stop":1667823949000,"duration":0}},{"uid":"23530269308a5a66","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674621000,"stop":1666674621000,"duration":0}},{"uid":"afb1deea97f77557","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674212000,"stop":1666674212000,"duration":0}},{"uid":"863e91df5fd13522","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666665366000,"stop":1666665366000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice RBAC Tests SliceRoleBinding Tests Create SliceRoleBinding successful test: Creating SliceRoleBinding slice-red with SliceRoleTemplate":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":2,"unknown":0,"total":2},"items":[{"uid":"9832e99786826ec8","status":"passed","time":{"start":1667739621000,"stop":1667739623207,"duration":2207}},{"uid":"ef3515cf8a7528d7","status":"passed","time":{"start":1667458785000,"stop":1667458787923,"duration":2923}}]},"Worker Suite:Worker Suite#[It]  QosProfile tests QosProfile delete validation should remove gateway pod from spoke":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":5,"unknown":0,"total":6},"items":[{"uid":"76672e369536e446","status":"passed","time":{"start":1667837307000,"stop":1667837307009,"duration":9}},{"uid":"82abc4e29cece3f0","status":"passed","time":{"start":1667826674000,"stop":1667826674006,"duration":6}},{"uid":"2c7a6c4daafb7b6f","status":"passed","time":{"start":1667823949000,"stop":1667823949004,"duration":4}},{"uid":"7eaf0620d59e96ac","status":"passed","time":{"start":1666674621000,"stop":1666674621005,"duration":5}},{"uid":"bba4d4cc33518b8d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674212000,"stop":1666674212000,"duration":0}},{"uid":"c65d1b30856f0561","status":"passed","time":{"start":1666665366000,"stop":1666665366007,"duration":7}}]},"Intracluster Suite:Intracluster Suite#[It] Intracluster slice tests Iperf connectivity Verify Connectivity between iperf-server and iperf-client":{"statistic":{"failed":1,"broken":0,"skipped":2,"passed":1,"unknown":0,"total":4},"items":[{"uid":"9559c8bd79da511f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667842189000,"stop":1667842189000,"duration":0}},{"uid":"728112fd2bc3c4e9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674484000,"stop":1666674484000,"duration":0}},{"uid":"fcc62dcd713f4eb5","status":"failed","statusDetails":"Timed out after 240.001s.\nExpected\n    <bool>: false\nto be true","time":{"start":1666673362000,"stop":1666673610529,"duration":248529}},{"uid":"d66eaf4cd5c83125","status":"passed","time":{"start":1666664909000,"stop":1666664930257,"duration":21257}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests slice with netpol should have gateway pod running":{"statistic":{"failed":2,"broken":0,"skipped":2,"passed":2,"unknown":0,"total":6},"items":[{"uid":"111842e80ae7050e","status":"passed","time":{"start":1667837307000,"stop":1667837379249,"duration":72249}},{"uid":"a5bd4a44e66210ec","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667826674000,"stop":1667826674000,"duration":0}},{"uid":"a72faa9b4075c023","status":"passed","time":{"start":1667823949000,"stop":1667823993436,"duration":44436}},{"uid":"52d3f76573dd67c9","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1666674621000,"stop":1666674814549,"duration":193549}},{"uid":"6e0670b861134228","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674212000,"stop":1666674212000,"duration":0}},{"uid":"c28104908256c621","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1666665366000,"stop":1666665633328,"duration":267328}}]},"Worker Suite:Worker Suite#[It] Resource Quota tests slice resource quota enforcement mode should not create limits in application ns namespace-3 with no enforcement mode":{"statistic":{"failed":0,"broken":0,"skipped":3,"passed":0,"unknown":0,"total":3},"items":[{"uid":"7f750ec52c795efa","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667739621000,"stop":1667739621000,"duration":0}},{"uid":"c123328359f8ad05","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667458785000,"stop":1667458785000,"duration":0}},{"uid":"ec16c524a248439a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667389661000,"stop":1667389661000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice Config Negative Scenario tests Slice Config Negative Scenario Update Should fail while updating sliceconfig with with invalid DSCP class in qosprofiledetails":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":5,"unknown":0,"total":6},"items":[{"uid":"a8648c50f5d41a13","status":"passed","time":{"start":1667837307000,"stop":1667837307868,"duration":868}},{"uid":"e8ccacb401542ad2","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667826674000,"stop":1667826674000,"duration":0}},{"uid":"c7954b6ada91063e","status":"passed","time":{"start":1667823949000,"stop":1667823949625,"duration":625}},{"uid":"93efa331b6fd9539","status":"passed","time":{"start":1666674621000,"stop":1666674622750,"duration":1750}},{"uid":"bb073809c7b7117c","status":"passed","time":{"start":1666674212000,"stop":1666674213045,"duration":1045}},{"uid":"2b98eb78128d016f","status":"passed","time":{"start":1666665366000,"stop":1666665366659,"duration":659}}]},"Worker Suite:Worker Suite#[It] Slice resource quota Tests ResourceQuota creation tests Create ResourceQuota fail test - sum(namespace.limit.cpu) > cluster.cpu":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":3,"unknown":0,"total":3},"items":[{"uid":"3c03015e8dbe0d3e","status":"passed","time":{"start":1667739621000,"stop":1667739621879,"duration":879}},{"uid":"228771937b0d7974","status":"passed","time":{"start":1667458785000,"stop":1667458785870,"duration":870}},{"uid":"84d8b5642dad9d99","status":"passed","time":{"start":1667389661000,"stop":1667389662244,"duration":1244}}]},"Empty Suite:Empty Suite#[It] Hub Deletion tests Hub Uninstalltion Test Scenarios Should pass if project is uninstalled first and then hub":{"statistic":{"failed":3,"broken":0,"skipped":0,"passed":8,"unknown":0,"total":11},"items":[{"uid":"1fe2d47ee0023794","status":"passed","time":{"start":1666883811000,"stop":1666883928221,"duration":117221}},{"uid":"66de5df2f5972e23","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000f6c8>: {\n        Underlying: <*exec.ExitError | 0xc0000788e0>{\n            ProcessState: {\n                pid: 6191,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 49732},\n                    Stime: {Sec: 0, Usec: 13563},\n                    Maxrss: 40736,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 1959,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 282,\n                    Nivcsw: 110,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"install.go:192: [debug] Original chart version: \\\"\\\"\",\n                        \"Error: INSTALLATION FAILED: looks like \\\"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages\\\" is not a valid chart repository or cannot be reached: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\",\n                        \"helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\",\n                        \"helm.sh/helm/v3/pkg/repo.(*ChartRepository).DownloadIndexFile\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:128\",\n                        \"helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:252\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:723\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:204\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:139\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:872\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:990\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:918\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:250\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1571\",\n                        \"looks like \\\"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages\\\" is not a valid chart repository or cannot be reached\",\n                        \"helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:254\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:723\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:204\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:139\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n           ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; install.go:192: [debug] Original chart version: \"\"\n    Error: INSTALLATION FAILED: looks like \"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages\" is not a valid chart repository or cannot be reached: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\n    helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\n    helm.sh/helm/v3/pkg/repo.(*ChartRepository).DownloadIndexFile\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:128\n    helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:252\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:723\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:204\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:139\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    looks like \"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages\" is not a valid chart repository or cannot be reached\n    helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:254\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:723\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:204\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:139\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:141\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\noccurred","time":{"start":1666754109000,"stop":1666754137162,"duration":28162}},{"uid":"5f434379a6eed139","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000e720>: {\n        Underlying: <*exec.ExitError | 0xc00049e060>{\n            ProcessState: {\n                pid: 6221,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 41391},\n                    Stime: {Sec: 0, Usec: 16556},\n                    Maxrss: 40680,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 1982,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 328,\n                    Nivcsw: 119,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"install.go:192: [debug] Original chart version: \\\"\\\"\",\n                        \"Error: INSTALLATION FAILED: looks like \\\"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/\\\" is not a valid chart repository or cannot be reached: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\",\n                        \"helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\",\n                        \"helm.sh/helm/v3/pkg/repo.(*ChartRepository).DownloadIndexFile\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:128\",\n                        \"helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:252\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:723\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:204\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:139\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:872\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:990\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:918\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:250\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1571\",\n                        \"looks like \\\"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/\\\" is not a valid chart repository or cannot be reached\",\n                        \"helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:254\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:723\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:204\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:139\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n         ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; install.go:192: [debug] Original chart version: \"\"\n    Error: INSTALLATION FAILED: looks like \"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/\" is not a valid chart repository or cannot be reached: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\n    helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\n    helm.sh/helm/v3/pkg/repo.(*ChartRepository).DownloadIndexFile\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:128\n    helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:252\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:723\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:204\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:139\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    looks like \"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/\" is not a valid chart repository or cannot be reached\n    helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:254\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:723\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:204\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:139\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:141\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\noccurred","time":{"start":1666750338000,"stop":1666750364302,"duration":26302}},{"uid":"d549f70f77d90c0d","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000e3a8>: {\n        Underlying: <*exec.ExitError | 0xc0005227e0>{\n            ProcessState: {\n                pid: 6220,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 35149},\n                    Stime: {Sec: 0, Usec: 24604},\n                    Maxrss: 40492,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 1942,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 236,\n                    Nivcsw: 49,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"install.go:192: [debug] Original chart version: \\\"\\\"\",\n                        \"Error: INSTALLATION FAILED: looks like \\\"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/\\\" is not a valid chart repository or cannot be reached: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\",\n                        \"helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\",\n                        \"helm.sh/helm/v3/pkg/repo.(*ChartRepository).DownloadIndexFile\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:128\",\n                        \"helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:252\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:723\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:204\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:139\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:872\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:990\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:918\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:250\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1571\",\n                        \"looks like \\\"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/\\\" is not a valid chart repository or cannot be reached\",\n                        \"helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:254\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:723\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:204\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:139\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n          ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; install.go:192: [debug] Original chart version: \"\"\n    Error: INSTALLATION FAILED: looks like \"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/\" is not a valid chart repository or cannot be reached: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\n    helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\n    helm.sh/helm/v3/pkg/repo.(*ChartRepository).DownloadIndexFile\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:128\n    helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:252\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:723\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:204\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:139\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    looks like \"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/\" is not a valid chart repository or cannot be reached\n    helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:254\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:723\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:204\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:139\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:141\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\noccurred","time":{"start":1666747791000,"stop":1666747814524,"duration":23524}},{"uid":"2127ac8734f2f1aa","status":"passed","time":{"start":1666706575000,"stop":1666706684410,"duration":109410}},{"uid":"887a3add36672719","status":"passed","time":{"start":1666077791000,"stop":1666077906721,"duration":115721}},{"uid":"90d2ff014c6fd9a4","status":"passed","time":{"start":1665944068000,"stop":1665944170151,"duration":102151}},{"uid":"3c3e4f3a1ced3ca6","status":"passed","time":{"start":1665665806000,"stop":1665665858357,"duration":52357}},{"uid":"4c03dc2cef90ec98","status":"passed","time":{"start":1665665864000,"stop":1665665929453,"duration":65453}},{"uid":"a8aa711dae237e","status":"passed","time":{"start":1665552534000,"stop":1665552596081,"duration":62081}},{"uid":"9bf04857a45b9492","status":"passed","time":{"start":1664798682000,"stop":1664798747095,"duration":65095}}]},"Worker Suite:Worker Suite#[It] Slice RBAC Tests SliceRoleTemplate Tests Create SliceRoleTempalate successful test: Creating SliceRoleTempalate read-only-role":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":2,"unknown":0,"total":2},"items":[{"uid":"df03e88e18c93eab","status":"passed","time":{"start":1667739621000,"stop":1667739625083,"duration":4083}},{"uid":"153c455ba05f757e","status":"passed","time":{"start":1667458785000,"stop":1667458787477,"duration":2477}}]},"Worker Suite:Worker Suite#[It] Hub netpol scenarios: Slice Configs Netpol scenarios Should fail when deploying with invalid clusters in both applicationNamespace and allowedNamespace":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":4,"unknown":0,"total":6},"items":[{"uid":"977c73a3643fb916","status":"passed","time":{"start":1667837307000,"stop":1667837308366,"duration":1366}},{"uid":"a705ea8729d70c9d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667826674000,"stop":1667826674000,"duration":0}},{"uid":"718fb2b0722bd6e7","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667823949000,"stop":1667823949000,"duration":0}},{"uid":"6b81fb4b54c12076","status":"passed","time":{"start":1666674621000,"stop":1666674622591,"duration":1591}},{"uid":"374a98a82b77e6c0","status":"passed","time":{"start":1666674212000,"stop":1666674213970,"duration":1970}},{"uid":"edd5d107323ba0dd","status":"passed","time":{"start":1666665366000,"stop":1666665367477,"duration":1477}}]},"Worker Suite:Worker Suite#[It] Slice resource quota Tests ResourceQuota creation tests Create ResourceQuota fail test - slice.limit.podcount negative  ":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":3,"unknown":0,"total":3},"items":[{"uid":"a3f89a5b182c59bf","status":"passed","time":{"start":1667739621000,"stop":1667739621731,"duration":731}},{"uid":"2af8f377a1b3c745","status":"passed","time":{"start":1667458785000,"stop":1667458785836,"duration":836}},{"uid":"f6740ed20bf60e7b","status":"passed","time":{"start":1667389661000,"stop":1667389661947,"duration":947}}]},"Worker Suite:Worker Suite#[It] Hub netpol scenarios: Slice Configs Netpol scenarios Should remove successfully while Deleting Slice after removing the applicationNamespace in namespaceIsolationProfile part in slice configs":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":4,"unknown":0,"total":6},"items":[{"uid":"d28b23224f7ae60e","status":"passed","time":{"start":1667837307000,"stop":1667837317916,"duration":10916}},{"uid":"adbff6a2336c9341","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667826674000,"stop":1667826674000,"duration":0}},{"uid":"5533438c91695eb0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667823949000,"stop":1667823949000,"duration":0}},{"uid":"72f5821dc3c138ef","status":"passed","time":{"start":1666674621000,"stop":1666674631986,"duration":10986}},{"uid":"80b0945b01f4e470","status":"passed","time":{"start":1666674212000,"stop":1666674223466,"duration":11466}},{"uid":"db37d1839d1fe2e1","status":"passed","time":{"start":1666665366000,"stop":1666665376901,"duration":10901}}]},"Worker Suite:Worker Suite#[It] Slice resource quota Tests ResourceQuota creation tests Create ResourceQuota fail test- Request.CPU > Limit.CPU at namespace level":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":3,"unknown":0,"total":3},"items":[{"uid":"cbbd469f3f98872f","status":"passed","time":{"start":1667739621000,"stop":1667739622008,"duration":1008}},{"uid":"fefb662a1652b3a8","status":"passed","time":{"start":1667458785000,"stop":1667458785773,"duration":773}},{"uid":"e6a4487dd9d81d10","status":"passed","time":{"start":1667389661000,"stop":1667389661516,"duration":516}}]},"Worker Suite:Worker Suite#[It] Slice resource quota Tests ResourceQuota creation tests Create ResourceQuota fail test- Request.CPU > Limit.CPU at slice level":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":3,"unknown":0,"total":3},"items":[{"uid":"2ca4b808fabf8752","status":"passed","time":{"start":1667739621000,"stop":1667739622139,"duration":1139}},{"uid":"d30192cc423ac4f0","status":"passed","time":{"start":1667458785000,"stop":1667458785786,"duration":786}},{"uid":"760f279907d608b3","status":"passed","time":{"start":1667389661000,"stop":1667389661588,"duration":588}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting nsm-kernel-forwarder pod Check ping between iperf-server and iperf-client after nsm-kernel-forwarder pod restart":{"statistic":{"failed":0,"broken":0,"skipped":5,"passed":1,"unknown":0,"total":6},"items":[{"uid":"696aa1ee5aed5261","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667837307000,"stop":1667837307000,"duration":0}},{"uid":"a3fb001bbb3fb057","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667826674000,"stop":1667826674000,"duration":0}},{"uid":"3985dc3a0903cda8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667823949000,"stop":1667823949000,"duration":0}},{"uid":"b7834886f14f67c7","status":"passed","time":{"start":1666674621000,"stop":1666674638563,"duration":17563}},{"uid":"5501783c25343284","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674212000,"stop":1666674212000,"duration":0}},{"uid":"45e80585dafa0e71","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666665366000,"stop":1666665366000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice Creation scenarios Slice with MEDIUM QOS should have gateway pod running":{"statistic":{"failed":2,"broken":0,"skipped":2,"passed":2,"unknown":0,"total":6},"items":[{"uid":"3798b466e4a7432c","status":"passed","time":{"start":1667837307000,"stop":1667837472318,"duration":165318}},{"uid":"f054cdc084563d6f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667826674000,"stop":1667826674000,"duration":0}},{"uid":"33a9da2a373f530e","status":"passed","time":{"start":1667823949000,"stop":1667824034173,"duration":85173}},{"uid":"666eeac12bb122a5","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1666674621000,"stop":1666674801316,"duration":180316}},{"uid":"63e3a30eccfe903b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674212000,"stop":1666674212000,"duration":0}},{"uid":"a1bc22d70c959661","status":"failed","statusDetails":"Timed out after 180.003s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1666665366000,"stop":1666665711545,"duration":345545}}]},"Worker Suite:Worker Suite#[It] Slice resource quota Tests ResourceQuota creation tests Create ResourceQuota fail test - cluster.limit.podcount > sum(namespace.limit.podcount)":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":3,"unknown":0,"total":3},"items":[{"uid":"582c2809f1c8ebc0","status":"passed","time":{"start":1667739621000,"stop":1667739621844,"duration":844}},{"uid":"1dd9e82b8cf4af0","status":"passed","time":{"start":1667458785000,"stop":1667458786138,"duration":1138}},{"uid":"aea38c227f8234d8","status":"passed","time":{"start":1667389661000,"stop":1667389661940,"duration":940}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests slice with netpol Should label all application namespaces with kubeslice namespace":{"statistic":{"failed":0,"broken":0,"skipped":4,"passed":2,"unknown":0,"total":6},"items":[{"uid":"1eff9ef5bd1f5e91","status":"passed","time":{"start":1667837307000,"stop":1667837307147,"duration":147}},{"uid":"53618ce75af95878","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667826674000,"stop":1667826674000,"duration":0}},{"uid":"d104fe55217f2ea9","status":"passed","time":{"start":1667823949000,"stop":1667823949175,"duration":175}},{"uid":"e9d9f2c9875ebf75","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674621000,"stop":1666674621000,"duration":0}},{"uid":"6879cada6e4a679","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674212000,"stop":1666674212000,"duration":0}},{"uid":"3235e2c9beb61035","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666665366000,"stop":1666665366000,"duration":0}}]},"Worker Suite:Worker Suite#[It] NodeRestartSlice Testing node restart should have vl3 router running":{"statistic":{"failed":4,"broken":0,"skipped":0,"passed":2,"unknown":0,"total":6},"items":[{"uid":"c974508034ec5bc7","status":"passed","time":{"start":1667837307000,"stop":1667837311920,"duration":4920}},{"uid":"197c97f20246f62f","status":"failed","statusDetails":"Expected success, but got an error:\n    <*errors.StatusError | 0xc00022cd20>: {\n        ErrStatus: {\n            TypeMeta: {Kind: \"\", APIVersion: \"\"},\n            ListMeta: {\n                SelfLink: \"\",\n                ResourceVersion: \"\",\n                Continue: \"\",\n                RemainingItemCount: nil,\n            },\n            Status: \"Failure\",\n            Message: \"namespaces \\\"iperf\\\" already exists\",\n            Reason: \"AlreadyExists\",\n            Details: {Name: \"iperf\", Group: \"\", Kind: \"namespaces\", UID: \"\", Causes: nil, RetryAfterSeconds: 0},\n            Code: 409,\n        },\n    }\n    namespaces \"iperf\" already exists","time":{"start":1667826674000,"stop":1667826674333,"duration":333}},{"uid":"ac917d18fa93b357","status":"failed","statusDetails":"Expected success, but got an error:\n    <*errors.StatusError | 0xc00089adc0>: {\n        ErrStatus: {\n            TypeMeta: {Kind: \"\", APIVersion: \"\"},\n            ListMeta: {\n                SelfLink: \"\",\n                ResourceVersion: \"\",\n                Continue: \"\",\n                RemainingItemCount: nil,\n            },\n            Status: \"Failure\",\n            Message: \"namespaces \\\"iperf\\\" already exists\",\n            Reason: \"AlreadyExists\",\n            Details: {Name: \"iperf\", Group: \"\", Kind: \"namespaces\", UID: \"\", Causes: nil, RetryAfterSeconds: 0},\n            Code: 409,\n        },\n    }\n    namespaces \"iperf\" already exists","time":{"start":1667823949000,"stop":1667823952965,"duration":3965}},{"uid":"4aee7225483306fd","status":"passed","time":{"start":1666674621000,"stop":1666674658994,"duration":37994}},{"uid":"adbe46e736e25e06","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1666674212000,"stop":1666674395919,"duration":183919}},{"uid":"73cdf85f03daf134","status":"failed","statusDetails":"Expected success, but got an error:\n    <*errors.StatusError | 0xc000a7c280>: {\n        ErrStatus: {\n            TypeMeta: {Kind: \"\", APIVersion: \"\"},\n            ListMeta: {\n                SelfLink: \"\",\n                ResourceVersion: \"\",\n                Continue: \"\",\n                RemainingItemCount: nil,\n            },\n            Status: \"Failure\",\n            Message: \"namespaces \\\"iperf\\\" already exists\",\n            Reason: \"AlreadyExists\",\n            Details: {Name: \"iperf\", Group: \"\", Kind: \"namespaces\", UID: \"\", Causes: nil, RetryAfterSeconds: 0},\n            Code: 409,\n        },\n    }\n    namespaces \"iperf\" already exists","time":{"start":1666665366000,"stop":1666665369025,"duration":3025}}]},"Worker Suite:Worker Suite#[It] Slice RBAC Tests SliceRoleBinding Tests Create SliceRoleBinding successful test: Checking Owner Reference set to SliceConfig":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":2,"unknown":0,"total":2},"items":[{"uid":"1a85fc089a626d10","status":"passed","time":{"start":1667739621000,"stop":1667739622622,"duration":1622}},{"uid":"a9cf93955210034a","status":"passed","time":{"start":1667458785000,"stop":1667458787280,"duration":2280}}]},"Worker Suite:Worker Suite#[It] Slice Creation scenarios Slice with MEDIUM QOS should reconcile medium qos slice":{"statistic":{"failed":0,"broken":0,"skipped":4,"passed":2,"unknown":0,"total":6},"items":[{"uid":"f24d855cba2480cb","status":"passed","time":{"start":1667837307000,"stop":1667837307404,"duration":404}},{"uid":"276b0485ee210521","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667826674000,"stop":1667826674000,"duration":0}},{"uid":"82c6d72b0ec46758","status":"passed","time":{"start":1667823949000,"stop":1667823949443,"duration":443}},{"uid":"beaa45a83cf70abc","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674621000,"stop":1666674621000,"duration":0}},{"uid":"45b24f5667fc8084","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674212000,"stop":1666674212000,"duration":0}},{"uid":"f6a951ff7df2ca28","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666665366000,"stop":1666665366000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Resource Quota tests slice resource quota monitoring mode should have sliceResourceQuota Usage":{"statistic":{"failed":2,"broken":0,"skipped":1,"passed":0,"unknown":0,"total":3},"items":[{"uid":"5571173980d0c183","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667739621000,"stop":1667739621000,"duration":0}},{"uid":"71d77f28c0aa0559","status":"failed","statusDetails":"Timed out after 300.001s.\nExpected\n    <bool>: false\nto be true","time":{"start":1667458785000,"stop":1667459089268,"duration":304268}},{"uid":"c15c72a5cf30ad93","status":"failed","statusDetails":"Timed out after 160.002s.\nExpected\n    <bool>: false\nto be true","time":{"start":1667389661000,"stop":1667389825268,"duration":164268}}]},"Intracluster Suite:Intracluster Suite#[It] Intracluster slice tests Slice with netpol should have vl3 router running":{"statistic":{"failed":2,"broken":0,"skipped":0,"passed":2,"unknown":0,"total":4},"items":[{"uid":"e275722612528081","status":"passed","time":{"start":1667842189000,"stop":1667842199161,"duration":10161}},{"uid":"98c0dc18170eddb2","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00098e930>: {\n        Underlying: <*exec.ExitError | 0xc0009bada0>{\n            ProcessState: {\n                pid: 6163,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 339794},\n                    Stime: {Sec: 0, Usec: 226529},\n                    Maxrss: 104624,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 12818,\n                    Majflt: 2,\n                    Nswap: 0,\n                    Inblock: 232,\n                    Oublock: 19152,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 922,\n                    Nivcsw: 1706,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1666674484000,"stop":1666674486433,"duration":2433}},{"uid":"efb4067803ceb3fd","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1666673362000,"stop":1666673544862,"duration":182862}},{"uid":"88b6b18ca036e926","status":"passed","time":{"start":1666664909000,"stop":1666664927991,"duration":18991}}]},"Worker Suite:Worker Suite#[It] Slice RBAC Tests SliceRoleBinding Tests Create SliceRoleBinding successful test: Creating SliceRoleBinding slice-red with both SliceRoleTemplate and Kubernetes Roles":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":2,"unknown":0,"total":2},"items":[{"uid":"d1293f9c7f6ee8eb","status":"passed","time":{"start":1667739621000,"stop":1667739622304,"duration":1304}},{"uid":"4327d66f6133276c","status":"passed","time":{"start":1667458785000,"stop":1667458786364,"duration":1364}}]},"Worker Suite:Worker Suite#[It] Slice RBAC Tests SliceRoleBinding Tests Create SliceRoleBinding failure test: Creating SliceRoleBinding slice-red with empty subject name":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":2,"unknown":0,"total":2},"items":[{"uid":"6b35c4d4bc5bfbd","status":"passed","time":{"start":1667739621000,"stop":1667739621863,"duration":863}},{"uid":"3ed004e926826062","status":"passed","time":{"start":1667458785000,"stop":1667458785544,"duration":544}}]},"Worker Suite:Worker Suite#[It] Slice resource quota Tests ResourceQuota creation tests Create ResourceQuota fail test - negative pod cound":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":3,"unknown":0,"total":3},"items":[{"uid":"2804675851a6b22e","status":"passed","time":{"start":1667739621000,"stop":1667739621850,"duration":850}},{"uid":"bfa6c06cde0003fa","status":"passed","time":{"start":1667458785000,"stop":1667458785807,"duration":807}},{"uid":"db56544784d5f705","status":"passed","time":{"start":1667389661000,"stop":1667389661604,"duration":604}}]},"Worker Suite:Worker Suite#[It] NodeRestartSlice Testing node restart iperf connectivity across multi cluster":{"statistic":{"failed":1,"broken":0,"skipped":4,"passed":1,"unknown":0,"total":6},"items":[{"uid":"2bc87bea426e75b4","status":"passed","time":{"start":1667837307000,"stop":1667837543828,"duration":236828}},{"uid":"94361b64da96fbc0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667826674000,"stop":1667826674000,"duration":0}},{"uid":"b79adeaee4626dd0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667823949000,"stop":1667823949000,"duration":0}},{"uid":"9124d72031fd7cd8","status":"failed","statusDetails":"Timed out after 271.905s.\nExpected\n    <bool>: false\nto equal\n    <bool>: true","time":{"start":1666674621000,"stop":1666675082895,"duration":461895}},{"uid":"1eb79d9a76cba23d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674212000,"stop":1666674212000,"duration":0}},{"uid":"23b366779109dbbc","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666665366000,"stop":1666665366000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Iperf connectivity Verify Iperf traffic":{"statistic":{"failed":0,"broken":0,"skipped":5,"passed":1,"unknown":0,"total":6},"items":[{"uid":"bbfceeb82c6c9e10","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667837307000,"stop":1667837307000,"duration":0}},{"uid":"926167d42f4419b3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667826674000,"stop":1667826674000,"duration":0}},{"uid":"28117554d4fcd769","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667823949000,"stop":1667823949000,"duration":0}},{"uid":"6743e34473b8ec6b","status":"passed","time":{"start":1666674621000,"stop":1666674683569,"duration":62569}},{"uid":"c93629510176a4c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674212000,"stop":1666674212000,"duration":0}},{"uid":"9990e928d55743a3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666665366000,"stop":1666665366000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Attach/Deattach Scenario Attach cluster from slice  when Deattach cluster from slice  should have vl3 router deleted from deattach cluster":{"statistic":{"failed":4,"broken":0,"skipped":0,"passed":2,"unknown":0,"total":6},"items":[{"uid":"172f60490011f4c5","status":"passed","time":{"start":1667837307000,"stop":1667837398505,"duration":91505}},{"uid":"7a82073cfa948dd1","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00047d8a8>: {\n        Underlying: <*exec.ExitError | 0xc0009abea0>{\n            ProcessState: {\n                pid: 6500,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 244742},\n                    Stime: {Sec: 0, Usec: 49713},\n                    Maxrss: 105156,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3741,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 19808,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 449,\n                    Nivcsw: 626,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slicetwocluster\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slicetwocluster\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\noccurred","time":{"start":1667826674000,"stop":1667826674320,"duration":320}},{"uid":"4335b26cfaae269","status":"passed","time":{"start":1667823949000,"stop":1667824015412,"duration":66412}},{"uid":"6790aa0b69862346","status":"failed","statusDetails":"Timed out after 180.004s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1666674621000,"stop":1666674808118,"duration":187118}},{"uid":"22afc2fa8a4c50ac","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1666674212000,"stop":1666674393851,"duration":181851}},{"uid":"a286c88e5be4c754","status":"failed","statusDetails":"Timed out after 180.001s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1666665366000,"stop":1666665595804,"duration":229804}}]},"Hub Suite:Hub Suite#[BeforeSuite]":{"statistic":{"failed":2,"broken":0,"skipped":0,"passed":3,"unknown":0,"total":5},"items":[{"uid":"46e3dbeba2977ccf","status":"passed","time":{"start":1667840900000,"stop":1667840928987,"duration":28987}},{"uid":"121eeb38b5d77fb7","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000f770>: {\n        Underlying: <*exec.ExitError | 0xc000078580>{\n            ProcessState: {\n                pid: 6216,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 43392},\n                    Stime: {Sec: 0, Usec: 20027},\n                    Maxrss: 40456,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2447,\n                    Majflt: 7,\n                    Nswap: 0,\n                    Inblock: 1576,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 373,\n                    Nivcsw: 135,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:192: [debug] Original chart version: \\\"\\\"\",\n                        \"Error: INSTALLATION FAILED: looks like \\\"https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/\\\" is not a valid chart repository or cannot be reached: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/index.yaml : 404 Not Found\",\n                        \"helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/index.yaml : 404 Not Found\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\",\n                        \"helm.sh/helm/v3/pkg/repo.(*ChartRepository).DownloadIndexFile\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:128\",\n                        \"helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:252\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:723\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:204\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:139\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:872\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:990\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:918\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:250\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1571\",\n                        \"looks like \\\"https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/\\\" is not a valid chart repository or cannot be reached\",\n                        \"helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:254\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:723\",\n                        \"m...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:192: [debug] Original chart version: \"\"\n    Error: INSTALLATION FAILED: looks like \"https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/\" is not a valid chart repository or cannot be reached: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/index.yaml : 404 Not Found\n    helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/index.yaml : 404 Not Found\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\n    helm.sh/helm/v3/pkg/repo.(*ChartRepository).DownloadIndexFile\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:128\n    helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:252\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:723\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:204\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:139\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    looks like \"https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/\" is not a valid chart repository or cannot be reached\n    helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:254\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:723\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:204\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:139\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:141\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\noccurred","time":{"start":1666668667000,"stop":1666668694019,"duration":27019}},{"uid":"3e541c77607c27a0","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000361020>: {\n        Underlying: <*exec.ExitError | 0xc0000b4420>{\n            ProcessState: {\n                pid: 6200,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 40289},\n                    Stime: {Sec: 0, Usec: 14389},\n                    Maxrss: 40516,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 1457,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 16,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 224,\n                    Nivcsw: 210,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:192: [debug] Original chart version: \\\"\\\"\",\n                        \"Error: INSTALLATION FAILED: looks like \\\"https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/\\\" is not a valid chart repository or cannot be reached: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/index.yaml : 404 Not Found\",\n                        \"helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/index.yaml : 404 Not Found\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\",\n                        \"helm.sh/helm/v3/pkg/repo.(*ChartRepository).DownloadIndexFile\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:128\",\n                        \"helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:252\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:723\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:204\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:139\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:872\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:990\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:918\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:250\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1571\",\n                        \"looks like \\\"https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/\\\" is not a valid chart repository or cannot be reached\",\n                        \"helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:254\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:723\",\n                        \"mai...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:192: [debug] Original chart version: \"\"\n    Error: INSTALLATION FAILED: looks like \"https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/\" is not a valid chart repository or cannot be reached: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/index.yaml : 404 Not Found\n    helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/index.yaml : 404 Not Found\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\n    helm.sh/helm/v3/pkg/repo.(*ChartRepository).DownloadIndexFile\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:128\n    helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:252\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:723\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:204\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:139\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    looks like \"https://raw.githubusercontent.com/kubeslice/dev-charts/gh-pages/\" is not a valid chart repository or cannot be reached\n    helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:254\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:723\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:204\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:139\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:141\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\noccurred","time":{"start":1666419212000,"stop":1666419239715,"duration":27715}},{"uid":"818d51499a0ff4b","status":"passed","time":{"start":1665665955000,"stop":1665666052660,"duration":97660}},{"uid":"871ebd3fd7df8e48","status":"passed","time":{"start":1665666027000,"stop":1665666126756,"duration":99756}}]},"Worker Suite:Worker Suite#[It] Slice resource quota Tests ResourceQuota creation tests Create ResourceQuota fail test- Request.CPU > Limit.CPU at cluster level":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":3,"unknown":0,"total":3},"items":[{"uid":"2e37ad4bd0b2d711","status":"passed","time":{"start":1667739621000,"stop":1667739622152,"duration":1152}},{"uid":"65c6c569e62fccf1","status":"passed","time":{"start":1667458785000,"stop":1667458785821,"duration":821}},{"uid":"93b61713a273806b","status":"passed","time":{"start":1667389661000,"stop":1667389661661,"duration":661}}]},"Worker Suite:Worker Suite#[It] Slice resource quota Tests ResourceQuota creation tests Delete resource quota test":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":3,"unknown":0,"total":3},"items":[{"uid":"3985c1dbc45b0ca7","status":"passed","time":{"start":1667739621000,"stop":1667739623585,"duration":2585}},{"uid":"79953be8aa441019","status":"passed","time":{"start":1667458785000,"stop":1667458787023,"duration":2023}},{"uid":"209a08889304cbeb","status":"passed","time":{"start":1667389661000,"stop":1667389662089,"duration":1089}}]},"Worker Suite:Worker Suite#[It] Slice deletion tests: Slice is installed & validated Slice object is deleted from hub should remove gateway pod from spoke":{"statistic":{"failed":0,"broken":0,"skipped":4,"passed":2,"unknown":0,"total":6},"items":[{"uid":"973df4d2003c2517","status":"passed","time":{"start":1667837307000,"stop":1667837307008,"duration":8}},{"uid":"2a3b2cde0c861c3c","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667826674000,"stop":1667826674000,"duration":0}},{"uid":"b36dc7bc6e41905a","status":"passed","time":{"start":1667823949000,"stop":1667823979170,"duration":30170}},{"uid":"ac8356a8d805ccb2","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674621000,"stop":1666674621000,"duration":0}},{"uid":"96f4e754da2dd82d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674212000,"stop":1666674212000,"duration":0}},{"uid":"a8621fd1814b0e8f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666665366000,"stop":1666665366000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice resource quota Tests ResourceQuota creation tests Create ResourceQuota fail test - sum(cluster.limit.CPU) > slice.CPU":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":3,"unknown":0,"total":3},"items":[{"uid":"2c02ef0497bd9002","status":"passed","time":{"start":1667739621000,"stop":1667739621881,"duration":881}},{"uid":"7ae90aa322fce21","status":"passed","time":{"start":1667458785000,"stop":1667458785846,"duration":846}},{"uid":"a3c980ddfb27e187","status":"passed","time":{"start":1667389661000,"stop":1667389661668,"duration":668}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Creation Should fail while creating service accounts as combination of special characters":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":3,"unknown":0,"total":3},"items":[{"uid":"cc924caf3c91d83","status":"passed","time":{"start":1667840900000,"stop":1667840900286,"duration":286}},{"uid":"8fa10de8639757a4","status":"passed","time":{"start":1665665955000,"stop":1665665956631,"duration":1631}},{"uid":"60eb76f15b503998","status":"passed","time":{"start":1665666027000,"stop":1665666027381,"duration":381}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting nsm-kernel-forwarder pod Should restart nsm-kernel-forwarder pod":{"statistic":{"failed":0,"broken":0,"skipped":5,"passed":1,"unknown":0,"total":6},"items":[{"uid":"13953dbccda438f3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667837307000,"stop":1667837307000,"duration":0}},{"uid":"d3a72a011da219cb","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667826674000,"stop":1667826674000,"duration":0}},{"uid":"b7cbead7f795e487","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667823949000,"stop":1667823949000,"duration":0}},{"uid":"7dc02fd6d532bdab","status":"passed","time":{"start":1666674621000,"stop":1666674642362,"duration":21362}},{"uid":"14e4112042d871d1","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674212000,"stop":1666674212000,"duration":0}},{"uid":"5e46fe7a3387993a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666665366000,"stop":1666665366000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice resource quota Tests ResourceQuota creation tests Create ResourceQuota fail test-namespace.limit.cpu negative":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":3,"unknown":0,"total":3},"items":[{"uid":"194c466bf71d0f91","status":"passed","time":{"start":1667739621000,"stop":1667739622344,"duration":1344}},{"uid":"e0c3b61eaf28b58f","status":"passed","time":{"start":1667458785000,"stop":1667458786177,"duration":1177}},{"uid":"30bda1caa2f84ce8","status":"passed","time":{"start":1667389661000,"stop":1667389662048,"duration":1048}}]},"Worker Suite:Worker Suite#[It] Resource Quota tests slice resource quota monitoring mode should have CPU metrics in sliceResourceQuota Usage":{"statistic":{"failed":0,"broken":0,"skipped":3,"passed":0,"unknown":0,"total":3},"items":[{"uid":"7dcf3286062f5291","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667739621000,"stop":1667739621000,"duration":0}},{"uid":"89d363342eafee2","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667458785000,"stop":1667458785000,"duration":0}},{"uid":"d25a2384721ad155","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667389661000,"stop":1667389661000,"duration":0}}]},"Worker Suite:Worker Suite#[It]  QosProfile tests QosProfile delete validation Delete qosProfile positive scenario":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":5,"unknown":0,"total":6},"items":[{"uid":"38580716f2c71c64","status":"passed","time":{"start":1667837307000,"stop":1667837307550,"duration":550}},{"uid":"d4d0dbaea190f7f0","status":"passed","time":{"start":1667826674000,"stop":1667826674582,"duration":582}},{"uid":"635e4e2e4d845f6b","status":"passed","time":{"start":1667823949000,"stop":1667823949544,"duration":544}},{"uid":"c94780116c11541b","status":"passed","time":{"start":1666674621000,"stop":1666674621619,"duration":619}},{"uid":"34e1c340a6b1ba67","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674212000,"stop":1666674212000,"duration":0}},{"uid":"f8c96d4446700153","status":"passed","time":{"start":1666665366000,"stop":1666665366600,"duration":600}}]},"Worker Suite:Worker Suite#[It] Slice RBAC Tests SliceRoleBinding Tests Create SliceRoleBinding failure test: Creating SliceRoleBinding slice-red with duplicate SliceRoleTemplate":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":2,"unknown":0,"total":2},"items":[{"uid":"b4629a9d362d491c","status":"passed","time":{"start":1667739621000,"stop":1667739622158,"duration":1158}},{"uid":"53f92512989847ef","status":"passed","time":{"start":1667458785000,"stop":1667458785613,"duration":613}}]},"Worker Suite:Worker Suite#[It] Slice resource quota Tests ResourceQuota creation tests Create ResourceQuota fail test- Request.EphemeralStorage > Limit.EphemeralStorage at cluster level":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":3,"unknown":0,"total":3},"items":[{"uid":"ae35d4871ec87dd2","status":"passed","time":{"start":1667739621000,"stop":1667739622106,"duration":1106}},{"uid":"7ed484bfece80560","status":"passed","time":{"start":1667458785000,"stop":1667458785750,"duration":750}},{"uid":"7c917c52c9daca6f","status":"passed","time":{"start":1667389661000,"stop":1667389661481,"duration":481}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting mesh-dns pod Check ping between iperf-server and iperf-client after mesh-dns pod restart":{"statistic":{"failed":0,"broken":0,"skipped":5,"passed":1,"unknown":0,"total":6},"items":[{"uid":"fdecdce1bf71f6b5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667837307000,"stop":1667837307000,"duration":0}},{"uid":"2e64940e44fe6be6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667826674000,"stop":1667826674000,"duration":0}},{"uid":"6f8ad34e15ecf0c9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667823949000,"stop":1667823949000,"duration":0}},{"uid":"795604882163a276","status":"passed","time":{"start":1666674621000,"stop":1666674625979,"duration":4979}},{"uid":"5fc987fdc73cb652","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674212000,"stop":1666674212000,"duration":0}},{"uid":"3126fbb1c95819d3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666665366000,"stop":1666665366000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice resource quota Tests ResourceQuota creation tests Create ResourceQuota fail test- Request.Mem > Limit.Mem at namespace level":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":3,"unknown":0,"total":3},"items":[{"uid":"68e7f3f6174065a7","status":"passed","time":{"start":1667739621000,"stop":1667739621936,"duration":936}},{"uid":"969274898b99796a","status":"passed","time":{"start":1667458785000,"stop":1667458785723,"duration":723}},{"uid":"7ff6f4929b06a81c","status":"passed","time":{"start":1667389661000,"stop":1667389661663,"duration":663}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests slice with netpol should have vl3 router running":{"statistic":{"failed":2,"broken":0,"skipped":0,"passed":4,"unknown":0,"total":6},"items":[{"uid":"1bdf8f5e61340c29","status":"passed","time":{"start":1667837307000,"stop":1667837334917,"duration":27917}},{"uid":"e0ede224b69bd351","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0008b4198>: {\n        Underlying: <*exec.ExitError | 0xc000b1bcc0>{\n            ProcessState: {\n                pid: 6356,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 250794},\n                    Stime: {Sec: 0, Usec: 72811},\n                    Maxrss: 102600,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 3669,\n                    Majflt: 1,\n                    Nswap: 0,\n                    Inblock: 8,\n                    Oublock: 19808,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 468,\n                    Nivcsw: 798,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\noccurred","time":{"start":1667826674000,"stop":1667826674713,"duration":713}},{"uid":"c5472b7fe3439038","status":"passed","time":{"start":1667823949000,"stop":1667823955473,"duration":6473}},{"uid":"ca083833bbd2c51b","status":"passed","time":{"start":1666674621000,"stop":1666674630435,"duration":9435}},{"uid":"7bce7e2e0613f981","status":"failed","statusDetails":"Unexpected error:\n    <*errors.StatusError | 0xc000620d20>: {\n        ErrStatus: {\n            TypeMeta: {Kind: \"\", APIVersion: \"\"},\n            ListMeta: {\n                SelfLink: \"\",\n                ResourceVersion: \"\",\n                Continue: \"\",\n                RemainingItemCount: nil,\n            },\n            Status: \"Failure\",\n            Message: \"namespaces \\\"iperf\\\" already exists\",\n            Reason: \"AlreadyExists\",\n            Details: {Name: \"iperf\", Group: \"\", Kind: \"namespaces\", UID: \"\", Causes: nil, RetryAfterSeconds: 0},\n            Code: 409,\n        },\n    }\n    namespaces \"iperf\" already exists\noccurred","time":{"start":1666674212000,"stop":1666674227920,"duration":15920}},{"uid":"bf8ec6b81b7b173c","status":"passed","time":{"start":1666665366000,"stop":1666665390279,"duration":24279}}]},"Worker Suite:Worker Suite#[It] Resource Quota tests slice resource quota enforcement mode should not create resource quota in application ns namespace-3 with no enforcement mode":{"statistic":{"failed":0,"broken":0,"skipped":3,"passed":0,"unknown":0,"total":3},"items":[{"uid":"b00a07666f5838c0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667739621000,"stop":1667739621000,"duration":0}},{"uid":"c0d543353e075386","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667458785000,"stop":1667458785000,"duration":0}},{"uid":"f117ca29a075ac36","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667389661000,"stop":1667389661000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice resource quota Tests ResourceQuota creation tests Create ResourceQuota fail test - sum(namespace.limit.mem) > cluster.mem":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":3,"unknown":0,"total":3},"items":[{"uid":"25ab4600ab0f7223","status":"passed","time":{"start":1667739621000,"stop":1667739622252,"duration":1252}},{"uid":"c909f9e547550db9","status":"passed","time":{"start":1667458785000,"stop":1667458785805,"duration":805}},{"uid":"e720fa3f34a75c46","status":"passed","time":{"start":1667389661000,"stop":1667389661873,"duration":873}}]},"Hub Suite:Hub Suite#[It] Cluster negative tests Worker Cluster Registration with Wrong clustername":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":1,"unknown":0,"total":3},"items":[{"uid":"a283da596d80516","status":"passed","time":{"start":1667840900000,"stop":1667841087942,"duration":187942}},{"uid":"a768a6dadd8b6676","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1665665955000,"stop":1665665955000,"duration":0}},{"uid":"76fe2cb00584df6a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1665666027000,"stop":1665666027000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Resource Quota tests slice resource quota enforcement mode should create CPU limits/requests in limits in app namespace-1":{"statistic":{"failed":0,"broken":0,"skipped":3,"passed":0,"unknown":0,"total":3},"items":[{"uid":"e6ec0668e3e455a8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667739621000,"stop":1667739621000,"duration":0}},{"uid":"3a2929670e0ebd0a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667458785000,"stop":1667458785000,"duration":0}},{"uid":"adf15f09b887516d","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667389661000,"stop":1667389661000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Resource Quota tests slice resource quota monitoring mode should have pod count metrics in sliceResourceQuota Usage":{"statistic":{"failed":0,"broken":0,"skipped":3,"passed":0,"unknown":0,"total":3},"items":[{"uid":"7dcbc4bdd3b37604","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667739621000,"stop":1667739621000,"duration":0}},{"uid":"17764aae42f23fc","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667458785000,"stop":1667458785000,"duration":0}},{"uid":"54741f5a4b3591c9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667389661000,"stop":1667389661000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting mesh-dns pod Should restart mesh-dns pod":{"statistic":{"failed":0,"broken":0,"skipped":5,"passed":1,"unknown":0,"total":6},"items":[{"uid":"50df151fe14f1ae3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667837307000,"stop":1667837307000,"duration":0}},{"uid":"d610e2c7ee7a711f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667826674000,"stop":1667826674000,"duration":0}},{"uid":"815a4f5638c1dd81","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667823949000,"stop":1667823949000,"duration":0}},{"uid":"aa3187964e003665","status":"passed","time":{"start":1666674621000,"stop":1666674632214,"duration":11214}},{"uid":"92096330ef90d831","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674212000,"stop":1666674212000,"duration":0}},{"uid":"f32accac86600d0b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666665366000,"stop":1666665366000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice RBAC Tests RBAC Common Tests WorkerSliceRoleBinding removal test: If roles are removed from SliceRolebinding, WorkerSliceRoleBinding should be removed":{"statistic":{"failed":1,"broken":0,"skipped":1,"passed":0,"unknown":0,"total":2},"items":[{"uid":"da42d1d4f69b6c36","status":"failed","statusDetails":"Timed out after 5.001s.\nExpected an error to have occurred.  Got:\n    <nil>: nil","time":{"start":1667739621000,"stop":1667739629811,"duration":8811}},{"uid":"76b7c3fb27b80931","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667458785000,"stop":1667458785000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice RBAC Tests RBAC Common Tests WorkerSliceRoleBinding data propagation test: Checking successful propagation of rules":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":0,"unknown":0,"total":2},"items":[{"uid":"beb96b928f513df9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667739621000,"stop":1667739621000,"duration":0}},{"uid":"ea36b158677db96","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667458785000,"stop":1667458785000,"duration":0}}]},"Empty Suite:Empty Suite#[It] Hub Creation tests Hub Installtion Test Senarios Should fail if Cert is installed again in the same cluster":{"statistic":{"failed":0,"broken":0,"skipped":3,"passed":8,"unknown":0,"total":11},"items":[{"uid":"81b3bd7d97fb5ed7","status":"passed","time":{"start":1666883811000,"stop":1666883811309,"duration":309}},{"uid":"f7dc1aaaa73f047b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666754109000,"stop":1666754109000,"duration":0}},{"uid":"aa7d9fbc6ee10cf9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666750338000,"stop":1666750338000,"duration":0}},{"uid":"538dd2490b295a64","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666747791000,"stop":1666747791000,"duration":0}},{"uid":"ebf087016e9d1d6f","status":"passed","time":{"start":1666706575000,"stop":1666706575360,"duration":360}},{"uid":"832ff072c9ead5c8","status":"passed","time":{"start":1666077791000,"stop":1666077791293,"duration":293}},{"uid":"da31357bb6b8729b","status":"passed","time":{"start":1665944068000,"stop":1665944068319,"duration":319}},{"uid":"2136aec3c7584a18","status":"passed","time":{"start":1665665806000,"stop":1665665806326,"duration":326}},{"uid":"50a28c6e8cd5f2b","status":"passed","time":{"start":1665665864000,"stop":1665665864306,"duration":306}},{"uid":"1acade2afe20c6ea","status":"passed","time":{"start":1665552534000,"stop":1665552534405,"duration":405}},{"uid":"a9628127a3b7968f","status":"passed","time":{"start":1664798682000,"stop":1664798682296,"duration":296}}]},"36e5cc2aa1b3fe54ab1d2ed6d47adaa2":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":1},"items":[{"uid":"93867d9c48a2e27c","status":"passed","time":{"start":1664357350099,"stop":1664357352298,"duration":2199}}]},"Worker Suite:Worker Suite#[It] Resource Quota tests slice resource quota enforcement mode should create only CPU limits in rq in app namespace-1":{"statistic":{"failed":0,"broken":0,"skipped":3,"passed":0,"unknown":0,"total":3},"items":[{"uid":"b337e572cef441c6","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667739621000,"stop":1667739621000,"duration":0}},{"uid":"e3da38e7a63b3e7f","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667458785000,"stop":1667458785000,"duration":0}},{"uid":"5a1bcaabef986cf0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667389661000,"stop":1667389661000,"duration":0}}]},"Intracluster Suite:Intracluster Suite#[It] Intracluster slice tests Iperf connectivity should onboard deployment in application Namespaces without slice annotations":{"statistic":{"failed":2,"broken":0,"skipped":0,"passed":2,"unknown":0,"total":4},"items":[{"uid":"33152c9231e73182","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 2","time":{"start":1667842189000,"stop":1667842379508,"duration":190508}},{"uid":"86e64c68bb6fafe9","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00098e018>: {\n        Underlying: <*exec.ExitError | 0xc0009ba040>{\n            ProcessState: {\n                pid: 6173,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 239894},\n                    Stime: {Sec: 0, Usec: 110720},\n                    Maxrss: 108920,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 12618,\n                    Majflt: 3,\n                    Nswap: 0,\n                    Inblock: 448,\n                    Oublock: 19152,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 527,\n                    Nivcsw: 677,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIPs: Required value: for cluster cluster1\noccurred","time":{"start":1666674484000,"stop":1666674484731,"duration":731}},{"uid":"54a20ccea13007ee","status":"passed","time":{"start":1666673362000,"stop":1666673513871,"duration":151871}},{"uid":"237cb8d32ad34257","status":"passed","time":{"start":1666664909000,"stop":1666664955391,"duration":46391}}]},"Worker Suite:Worker Suite#[It] Resource Quota tests slice resource quota monitoring mode should have Memory metrics in sliceResourceQuota Usage":{"statistic":{"failed":0,"broken":0,"skipped":3,"passed":0,"unknown":0,"total":3},"items":[{"uid":"bdd9613a5c5048f5","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667739621000,"stop":1667739621000,"duration":0}},{"uid":"5cf79d0928ccfd46","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667458785000,"stop":1667458785000,"duration":0}},{"uid":"5a5b16bfbbd8ee68","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667389661000,"stop":1667389661000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Pod-restart test cases Restarting iperf-client pod Check ping between iperf-server and iperf-client after iperf-client pod restart":{"statistic":{"failed":0,"broken":0,"skipped":5,"passed":1,"unknown":0,"total":6},"items":[{"uid":"90ad45c5baf02bfd","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667837307000,"stop":1667837307000,"duration":0}},{"uid":"25090278060fcbe8","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667826674000,"stop":1667826674000,"duration":0}},{"uid":"c906f1d72c137f9e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667823949000,"stop":1667823949000,"duration":0}},{"uid":"37a3970ca3e3d958","status":"passed","time":{"start":1666674621000,"stop":1666674640806,"duration":19806}},{"uid":"131e56b48f1142c3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674212000,"stop":1666674212000,"duration":0}},{"uid":"c467e55dfbe9bd2e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666665366000,"stop":1666665366000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice RBAC Tests SliceRoleBinding Tests Create SliceRoleBinding failure test: Creating SliceRoleBinding slice-red with invalid RoleRef Name":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":2,"unknown":0,"total":2},"items":[{"uid":"c4ecf3d1d714405e","status":"passed","time":{"start":1667739621000,"stop":1667739621639,"duration":639}},{"uid":"379e36707f46657c","status":"passed","time":{"start":1667458785000,"stop":1667458785446,"duration":446}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests slice with netpol Should install networkpolicies in all application namespaces":{"statistic":{"failed":0,"broken":0,"skipped":4,"passed":2,"unknown":0,"total":6},"items":[{"uid":"9ec89829b8576f63","status":"passed","time":{"start":1667837307000,"stop":1667837307283,"duration":283}},{"uid":"c7bd3322fd60424","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667826674000,"stop":1667826674000,"duration":0}},{"uid":"db77c46565241335","status":"passed","time":{"start":1667823949000,"stop":1667823949585,"duration":585}},{"uid":"aa0b085c594c305e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674621000,"stop":1666674621000,"duration":0}},{"uid":"a4f35d06f14f6c6b","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674212000,"stop":1666674212000,"duration":0}},{"uid":"6bf9603fb3166cba","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666665366000,"stop":1666665366000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Hub netpol scenarios: Slice Configs Netpol scenarios Should fail when deploying clusters in namespaceisolationprofile with * and a cluster name in both applicationNamespace and allowedNamespace":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":4,"unknown":0,"total":6},"items":[{"uid":"6c335f94855d5cc7","status":"passed","time":{"start":1667837307000,"stop":1667837307683,"duration":683}},{"uid":"7058b2bf1c98824e","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667826674000,"stop":1667826674000,"duration":0}},{"uid":"d4ca6563f14e83e4","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667823949000,"stop":1667823949000,"duration":0}},{"uid":"21728e0aaddf2122","status":"passed","time":{"start":1666674621000,"stop":1666674621879,"duration":879}},{"uid":"779e8a84065c7cab","status":"passed","time":{"start":1666674212000,"stop":1666674212710,"duration":710}},{"uid":"8eb2c1751fc46a5e","status":"passed","time":{"start":1666665366000,"stop":1666665366953,"duration":953}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Update Should update successfully when Project applied with valid service account name in Write users":{"statistic":{"failed":2,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":3},"items":[{"uid":"dcc03f15b405c41d","status":"passed","time":{"start":1667840900000,"stop":1667840905231,"duration":5231}},{"uid":"eedaa7cb93302999","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000e030>: {\n        Underlying: <*exec.ExitError | 0xc0007493e0>{\n            ProcessState: {\n                pid: 7612,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 245108},\n                    Stime: {Sec: 0, Usec: 60334},\n                    Maxrss: 90508,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 11054,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13760,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 486,\n                    Nivcsw: 428,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20applied%20with%20valid%20service%20account%20name%20in%20Write%20users3736952544\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.94.3:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20applied%20with%20valid%20service%20account%20name%20in%20Write%20users3736952544\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.94.3:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20applied%20with%20valid%20service%20account%20name%20in%20Write%20users3736952544\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.94.3:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20applied%20with%20valid%20service%20account%20name%20in%20Write%20users3736952544\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.94.3:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20applied%20with%20valid%20service%20account%20name%20in%20Write%20users3736952544\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.94.3:443: connect: connection refused\noccurred","time":{"start":1665665955000,"stop":1665665955335,"duration":335}},{"uid":"92a1cf6290c73497","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000534bb8>: {\n        Underlying: <*exec.ExitError | 0xc0006a3980>{\n            ProcessState: {\n                pid: 7184,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 242814},\n                    Stime: {Sec: 0, Usec: 80938},\n                    Maxrss: 85764,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 9341,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13760,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 503,\n                    Nivcsw: 352,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20applied%20with%20valid%20service%20account%20name%20in%20Write%20users1327815431\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.116.233:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20applied%20with%20valid%20service%20account%20name%20in%20Write%20users1327815431\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.116.233:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20applied%20with%20valid%20service%20account%20name%20in%20Write%20users1327815431\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.116.233:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20applied%20with%20valid%20service%20account%20name%20in%20Write%20users1327815431\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.116.233:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20applied%20with%20valid%20service%20account%20name%20in%20Write%20users1327815431\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.116.233:443: connect: connection refused\noccurred","time":{"start":1665666027000,"stop":1665666027431,"duration":431}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Update Should fail when Project is Applied with service account name as combination of special characters in Read users":{"statistic":{"failed":2,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":3},"items":[{"uid":"1ef6961fe6c5a864","status":"passed","time":{"start":1667840900000,"stop":1667840905971,"duration":5971}},{"uid":"e5f6a8f076b4ecb2","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000e990>: {\n        Underlying: <*exec.ExitError | 0xc0006be320>{\n            ProcessState: {\n                pid: 7640,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 202709},\n                    Stime: {Sec: 0, Usec: 38982},\n                    Maxrss: 84408,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 10340,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13760,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 632,\n                    Nivcsw: 379,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Read%20users647910301\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.94.3:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Read%20users647910301\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.94.3:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Read%20users647910301\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.94.3:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Read%20users647910301\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.94.3:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Read%20users647910301\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.94.3:443: connect: connection refused\noccurred","time":{"start":1665665955000,"stop":1665665955277,"duration":277}},{"uid":"42bade3689220f35","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0001a52d8>: {\n        Underlying: <*exec.ExitError | 0xc0004d9b40>{\n            ProcessState: {\n                pid: 7214,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 256881},\n                    Stime: {Sec: 0, Usec: 58956},\n                    Maxrss: 79860,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 7483,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13760,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 338,\n                    Nivcsw: 443,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Read%20users286678817\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.116.233:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Read%20users286678817\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.116.233:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Read%20users286678817\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.116.233:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Read%20users286678817\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.116.233:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Update%20Should%20fail%20when%20Project%20is%20Applied%20with%20service%20account%20name%20as%20combination%20of%20special%20characters%20in%20Read%20users286678817\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.116.233:443: connect: connection refused\noccurred","time":{"start":1665666027000,"stop":1665666027384,"duration":384}}]},"Worker Suite:Worker Suite#[It] Slice RBAC Tests RBAC Common Tests WorkerSliceRoleBinding generation test: Creation of SliceRoleBinding with custom roles should generate WorkerSliceRoleBinding":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":1,"unknown":0,"total":2},"items":[{"uid":"30190f546e91183f","status":"passed","time":{"start":1667739621000,"stop":1667739625442,"duration":4442}},{"uid":"b31af1eccdb7cdca","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667458785000,"stop":1667458785000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Namespace reconcile test scenarios: Onboard label app ns on workers":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":6,"unknown":0,"total":6},"items":[{"uid":"6be1a5788c7c6d39","status":"passed","time":{"start":1667837307000,"stop":1667837308935,"duration":1935}},{"uid":"d6d38f10bff3eacf","status":"passed","time":{"start":1667826674000,"stop":1667826674898,"duration":898}},{"uid":"fac4c77f5e194543","status":"passed","time":{"start":1667823949000,"stop":1667823951436,"duration":2436}},{"uid":"cb9225af274f22f6","status":"passed","time":{"start":1666674621000,"stop":1666674622425,"duration":1425}},{"uid":"57556dc69643b8de","status":"passed","time":{"start":1666674212000,"stop":1666674212960,"duration":960}},{"uid":"212b3a8a57a83a6a","status":"passed","time":{"start":1666665366000,"stop":1666665367771,"duration":1771}}]},"Worker Suite:Worker Suite#[It] NetworkPolicy tests Iperf App connectivity should have vl3 router running":{"statistic":{"failed":2,"broken":0,"skipped":0,"passed":4,"unknown":0,"total":6},"items":[{"uid":"29d1f36222e246b0","status":"passed","time":{"start":1667837307000,"stop":1667837308144,"duration":1144}},{"uid":"125426daff1038d","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0008b42d0>: {\n        Underlying: <*exec.ExitError | 0xc0009e8a40>{\n            ProcessState: {\n                pid: 6366,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 253071},\n                    Stime: {Sec: 0, Usec: 59313},\n                    Maxrss: 113540,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 4547,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 19808,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 558,\n                    Nivcsw: 651,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"The SliceConfig \\\"slice-netpol\\\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; The SliceConfig \"slice-netpol\" is invalid: Spec.Clusters.NodeIP: Required value: for cluster cluster1\noccurred","time":{"start":1667826674000,"stop":1667826674612,"duration":612}},{"uid":"5f756aedcc81f54d","status":"passed","time":{"start":1667823949000,"stop":1667823950171,"duration":1171}},{"uid":"2b17f3ce2005aff0","status":"passed","time":{"start":1666674621000,"stop":1666674622652,"duration":1652}},{"uid":"317698ebeb157f11","status":"failed","statusDetails":"Timed out after 180.000s.\nExpected\n    <int>: 0\nto equal\n    <int>: 1","time":{"start":1666674212000,"stop":1666674396467,"duration":184467}},{"uid":"9fd549b7e93f3113","status":"passed","time":{"start":1666665366000,"stop":1666665366968,"duration":968}}]},"Worker Suite:Worker Suite#[It] Slice RBAC Tests SliceRoleBinding Tests Create SliceRoleBinding failure test: Creating SliceRoleBinding slice-red with invalid subject name":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":2,"unknown":0,"total":2},"items":[{"uid":"349e8781f8f7ab9","status":"passed","time":{"start":1667739621000,"stop":1667739622477,"duration":1477}},{"uid":"18a13a73aa5acd79","status":"passed","time":{"start":1667458785000,"stop":1667458785477,"duration":477}}]},"Worker Suite:Worker Suite#[It] Slice RBAC Tests SliceRoleBinding Tests Create SliceRoleBinding failure test: Creating SliceRoleBinding slice-red with empty RoleRef Name":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":2,"unknown":0,"total":2},"items":[{"uid":"b4a538ecc6a7d0d","status":"passed","time":{"start":1667739621000,"stop":1667739621632,"duration":632}},{"uid":"dde73c340aff5aff","status":"passed","time":{"start":1667458785000,"stop":1667458785735,"duration":735}}]},"Istio Suite:Istio Suite#[BeforeSuite]":{"statistic":{"failed":6,"broken":0,"skipped":0,"passed":0,"unknown":0,"total":6},"items":[{"uid":"5614166770d5903d","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0001db1b8>: {\n        Underlying: <*exec.ExitError | 0xc000488540>{\n            ProcessState: {\n                pid: 6384,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 167887},\n                    Stime: {Sec: 0, Usec: 18149},\n                    Maxrss: 50604,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2828,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 360,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 550,\n                    Nivcsw: 178,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"install.go:192: [debug] Original chart version: \\\"v1.8.0\\\"\",\n                        \"install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\",\n                        \"\",\n                        \"Error: INSTALLATION FAILED: cannot re-use a name that is still in use\",\n                        \"helm.go:84: [debug] cannot re-use a name that is still in use\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).availableName\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:483\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:197\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:278\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:139\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:872\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:990\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:918\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:250\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1571\",\n                        \"INSTALLATION FAILED\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:141\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:872\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:990\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:918\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:250\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1571\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"install.go:192: [debug] Original chart version: \\\"v1.8.0\\\"\",\n                    \"install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\",\n                    \"\",\n                    \"Error: INSTALLATION FAILED: cannot re-use a name that is still in use\",\n                    \"helm.go:84: [debug] cannot re-use a name that is still in ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; install.go:192: [debug] Original chart version: \"v1.8.0\"\n    install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\n    \n    Error: INSTALLATION FAILED: cannot re-use a name that is still in use\n    helm.go:84: [debug] cannot re-use a name that is still in use\n    helm.sh/helm/v3/pkg/action.(*Install).availableName\n    \thelm.sh/helm/v3/pkg/action/install.go:483\n    helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\n    \thelm.sh/helm/v3/pkg/action/install.go:197\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:278\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:139\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:141\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\noccurred","time":{"start":1666884621000,"stop":1666884621480,"duration":480}},{"uid":"d80bc857d1050672","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004789c0>: {\n        Underlying: <*exec.ExitError | 0xc00048b660>{\n            ProcessState: {\n                pid: 6336,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 57756},\n                    Stime: {Sec: 0, Usec: 13328},\n                    Maxrss: 40484,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 1941,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 0,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 250,\n                    Nivcsw: 278,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"install.go:192: [debug] Original chart version: \\\"\\\"\",\n                        \"Error: INSTALLATION FAILED: looks like \\\"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/\\\" is not a valid chart repository or cannot be reached: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\",\n                        \"helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\",\n                        \"helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\",\n                        \"\\thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\",\n                        \"helm.sh/helm/v3/pkg/repo.(*ChartRepository).DownloadIndexFile\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:128\",\n                        \"helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:252\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:723\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:204\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:139\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:872\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:990\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:918\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:250\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1571\",\n                        \"looks like \\\"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/\\\" is not a valid chart repository or cannot be reached\",\n                        \"helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\",\n                        \"\\thelm.sh/helm/v3/pkg/repo/chartrepo.go:254\",\n                        \"helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:723\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:204\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:139\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n         ...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; install.go:192: [debug] Original chart version: \"\"\n    Error: INSTALLATION FAILED: looks like \"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/\" is not a valid chart repository or cannot be reached: failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\n    helm.go:84: [debug] failed to fetch https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/index.yaml : 404 Not Found\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:92\n    helm.sh/helm/v3/pkg/getter.(*HTTPGetter).Get\n    \thelm.sh/helm/v3/pkg/getter/httpgetter.go:45\n    helm.sh/helm/v3/pkg/repo.(*ChartRepository).DownloadIndexFile\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:128\n    helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:252\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:723\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:204\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:139\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    looks like \"https://raw.githubusercontent.com/kubeslice/dev-charts-ent/gh-pages/\" is not a valid chart repository or cannot be reached\n    helm.sh/helm/v3/pkg/repo.FindChartInAuthAndTLSAndPassRepoURL\n    \thelm.sh/helm/v3/pkg/repo/chartrepo.go:254\n    helm.sh/helm/v3/pkg/action.(*ChartPathOptions).LocateChart\n    \thelm.sh/helm/v3/pkg/action/install.go:723\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:204\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:139\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:141\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\noccurred","time":{"start":1666747979000,"stop":1666748074368,"duration":95368}},{"uid":"1cde8c1efc6df270","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004dc318>: {\n        Underlying: <*exec.ExitError | 0xc0004f0220>{\n            ProcessState: {\n                pid: 6386,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 122377},\n                    Stime: {Sec: 0, Usec: 16317},\n                    Maxrss: 51728,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 2121,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 24,\n                    Oublock: 360,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 688,\n                    Nivcsw: 222,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"install.go:192: [debug] Original chart version: \\\"v1.8.0\\\"\",\n                        \"install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\",\n                        \"\",\n                        \"Error: INSTALLATION FAILED: cannot re-use a name that is still in use\",\n                        \"helm.go:84: [debug] cannot re-use a name that is still in use\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).availableName\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:483\",\n                        \"helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\",\n                        \"\\thelm.sh/helm/v3/pkg/action/install.go:197\",\n                        \"main.runInstall\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:278\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:139\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:872\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:990\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:918\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:250\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1571\",\n                        \"INSTALLATION FAILED\",\n                        \"main.newInstallCmd.func2\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/install.go:141\",\n                        \"github.com/spf13/cobra.(*Command).execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:872\",\n                        \"github.com/spf13/cobra.(*Command).ExecuteC\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:990\",\n                        \"github.com/spf13/cobra.(*Command).Execute\",\n                        \"\\tgithub.com/spf13/cobra@v1.5.0/command.go:918\",\n                        \"main.main\",\n                        \"\\thelm.sh/helm/v3/cmd/helm/helm.go:83\",\n                        \"runtime.main\",\n                        \"\\truntime/proc.go:250\",\n                        \"runtime.goexit\",\n                        \"\\truntime/asm_amd64.s:1571\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"install.go:192: [debug] Original chart version: \\\"v1.8.0\\\"\",\n                    \"install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\",\n                    \"\",\n                    \"Error: INSTALLATION FAILED: cannot re-use a name that is still in use\",\n                    \"helm.go:84: [debug] cannot re-use a name that is still in...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; install.go:192: [debug] Original chart version: \"v1.8.0\"\n    install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/cert-manager-v1.8.0.tgz\n    \n    Error: INSTALLATION FAILED: cannot re-use a name that is still in use\n    helm.go:84: [debug] cannot re-use a name that is still in use\n    helm.sh/helm/v3/pkg/action.(*Install).availableName\n    \thelm.sh/helm/v3/pkg/action/install.go:483\n    helm.sh/helm/v3/pkg/action.(*Install).RunWithContext\n    \thelm.sh/helm/v3/pkg/action/install.go:197\n    main.runInstall\n    \thelm.sh/helm/v3/cmd/helm/install.go:278\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:139\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:141\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\noccurred","time":{"start":1666707346000,"stop":1666707346388,"duration":388}},{"uid":"ead93f6718764b08","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0004e8060>: {\n        Underlying: <*exec.ExitError | 0xc0001c0000>{\n            ProcessState: {\n                pid: 6437,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 4, Usec: 39204},\n                    Stime: {Sec: 0, Usec: 608010},\n                    Maxrss: 90856,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 5495,\n                    Majflt: 1,\n                    Nswap: 0,\n                    Inblock: 160,\n                    Oublock: 14288,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 49764,\n                    Nivcsw: 9910,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:192: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.4.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD serviceexports.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD serviceimports.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD slice.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:151: [debug] CRD slicegateways.networking.kubeslice.io is already present. Skipping.\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 47 resource(s)\",\n                        \"wait.go:66: [debug] beginning wait for 47 resources with timeout of 5m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debu...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:192: [debug] Original chart version: \"\"\n    install.go:209: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.4.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD serviceexports.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD serviceimports.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD slice.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:151: [debug] CRD slicegateways.networking.kubeslice.io is already present. Skipping.\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 47 resource(s)\n    wait.go:66: [debug] beginning wait for 47 resources with timeout of 5m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:310: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"jaeger\" Service\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-svc\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook\" Deployment\n    client.go:310: [debug] Starting delete for \"jaeger\" Deployment\n    client.go:310: [debug] Starting delete for \"prefix-service\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsm-kernel-forwarder\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRole\n    client.go:310: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"networkserviceendpoints.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservices.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservicemanagers.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-cfg\" MutatingWebhookConfiguration\n    uninstall.go:148: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: timed out waiting for the condition\n    helm.go:84: [debug] timed out waiting for the condition\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:141\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:872\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:990\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.5.0/command.go:918\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:250\n    runtime.goexit\n    \truntime/asm_amd64.s:1571\noccurred","time":{"start":1665944636000,"stop":1665945054369,"duration":418369}},{"uid":"4177292bb4210012","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000132450>: {\n        Underlying: <*exec.ExitError | 0xc00014e000>{\n            ProcessState: {\n                pid: 6098,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 4, Usec: 887993},\n                    Stime: {Sec: 0, Usec: 813208},\n                    Maxrss: 81628,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 9418,\n                    Majflt: 4,\n                    Nswap: 0,\n                    Inblock: 720,\n                    Oublock: 13888,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 52079,\n                    Nivcsw: 10073,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\",\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.3.0.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:165: [debug] Clearing discovery cache\",\n                        \"wait.go:48: [debug] beginning wait for 4 resources with timeout of 1m0s\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 44 resource(s)\",\n                        \"wait.go:48: [debug] beginning wait for 44 resources with timeout of 5m0s\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\",\n                        \"ready.go:277: [debug] Deployment is not ready: kubeslice-sys...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /e2e/assets/kubeconfig/kinde2e.yaml\n    install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.3.0.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:165: [debug] Clearing discovery cache\n    wait.go:48: [debug] beginning wait for 4 resources with timeout of 1m0s\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 44 resource(s)\n    wait.go:48: [debug] beginning wait for 44 resources with timeout of 5m0s\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/jaeger. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/nsm-admission-webhook. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    ready.go:277: [debug] Deployment is not ready: kubeslice-system/kubeslice-operator. 0 out of 1 expected pods are ready\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:310: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-svc\" Service\n    client.go:310: [debug] Starting delete for \"jaeger\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:310: [debug] Starting delete for \"jaeger\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook\" Deployment\n    client.go:310: [debug] Starting delete for \"prefix-service\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsm-kernel-forwarder\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:310: [debug] Starting delete for \"networkservices.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkserviceendpoints.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservicemanagers.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-cfg\" MutatingWebhookConfiguration\n    uninstall.go:144: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: client rate limiter Wait returned an error: rate: Wait(n=1) would exceed context deadline\n    helm.go:84: [debug] client rate limiter Wait returned an error: rate: Wait(n=1) would exceed context deadline\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:389\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1665552694000,"stop":1665553100419,"duration":406419}},{"uid":"42fef9a0774140ea","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00034e030>: {\n        Underlying: <*exec.ExitError | 0xc000422000>{\n            ProcessState: {\n                pid: 6102,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 2, Usec: 819215},\n                    Stime: {Sec: 0, Usec: 383113},\n                    Maxrss: 86208,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 16079,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 14208,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 13521,\n                    Nivcsw: 4184,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"install.go:178: [debug] Original chart version: \\\"\\\"\",\n                        \"install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.5.5.tgz\",\n                        \"\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"install.go:165: [debug] Clearing discovery cache\",\n                        \"wait.go:48: [debug] beginning wait for 5 resources with timeout of 1m0s\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:310: [debug] Starting delete for \\\"kubeslice-preinstall\\\" ServiceAccount\",\n                        \"client.go:339: [debug] serviceaccounts \\\"kubeslice-preinstall\\\" not found\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:310: [debug] Starting delete for \\\"kubeslice-preinstall-role\\\" ClusterRole\",\n                        \"client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \\\"kubeslice-preinstall-role\\\" not found\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:310: [debug] Starting delete for \\\"kubeslice-preinstall-rolebinding\\\" ClusterRoleBinding\",\n                        \"client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \\\"kubeslice-preinstall-rolebinding\\\" not found\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:310: [debug] Starting delete for \\\"kubeslice-worker-preinstall-configmap\\\" ConfigMap\",\n                        \"client.go:339: [debug] configmaps \\\"kubeslice-worker-preinstall-configmap\\\" not found\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:310: [debug] Starting delete for \\\"kubeslice-worker-preinstall-job\\\" Job\",\n                        \"client.go:339: [debug] jobs.batch \\\"kubeslice-worker-preinstall-job\\\" not found\",\n                        \"client.go:128: [debug] creating 1 resource(s)\",\n                        \"client.go:540: [debug] Watching for changes to Job kubeslice-worker-preinstall-job with timeout of 5m0s\",\n                        \"client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: ADDED\",\n                        \"client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 0, jobs failed: 0, jobs succeeded: 0\",\n                        \"client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\",\n                        \"client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\",\n                        \"client.go:568: [...\n\nGomega truncated this representation as it exceeds 'format.MaxLength'.\nConsider having the object provide a custom 'GomegaStringer' representation\nor adjust the parameters in Gomega's 'format' package.\n\nLearn more here: https://onsi.github.io/gomega/#adjusting-output\n\n    error while running command: exit status 1; install.go:178: [debug] Original chart version: \"\"\n    install.go:195: [debug] CHART PATH: /root/.cache/helm/repository/kubeslice-worker-0.5.5.tgz\n    \n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:128: [debug] creating 1 resource(s)\n    install.go:165: [debug] Clearing discovery cache\n    wait.go:48: [debug] beginning wait for 5 resources with timeout of 1m0s\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:310: [debug] Starting delete for \"kubeslice-preinstall\" ServiceAccount\n    client.go:339: [debug] serviceaccounts \"kubeslice-preinstall\" not found\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:310: [debug] Starting delete for \"kubeslice-preinstall-role\" ClusterRole\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-preinstall-role\" not found\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:310: [debug] Starting delete for \"kubeslice-preinstall-rolebinding\" ClusterRoleBinding\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-preinstall-rolebinding\" not found\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:310: [debug] Starting delete for \"kubeslice-worker-preinstall-configmap\" ConfigMap\n    client.go:339: [debug] configmaps \"kubeslice-worker-preinstall-configmap\" not found\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:310: [debug] Starting delete for \"kubeslice-worker-preinstall-job\" Job\n    client.go:339: [debug] jobs.batch \"kubeslice-worker-preinstall-job\" not found\n    client.go:128: [debug] creating 1 resource(s)\n    client.go:540: [debug] Watching for changes to Job kubeslice-worker-preinstall-job with timeout of 5m0s\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: ADDED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 0, jobs failed: 0, jobs succeeded: 0\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 0, jobs succeeded: 0\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 1, jobs succeeded: 0\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 2, jobs succeeded: 0\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 3, jobs succeeded: 0\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 4, jobs succeeded: 0\n    client.go:568: [debug] Add/Modify event for kubeslice-worker-preinstall-job: MODIFIED\n    client.go:607: [debug] kubeslice-worker-preinstall-job: Jobs active: 1, jobs failed: 5, jobs succeeded: 0\n    client.go:310: [debug] Starting delete for \"kubeslice-worker-preinstall-job\" Job\n    install.go:441: [debug] Install failed and atomic is set, uninstalling release\n    uninstall.go:95: [debug] uninstall: Deleting kubeslice-worker\n    client.go:310: [debug] Starting delete for \"kubeslice-webhook-service\" Service\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-svc\" Service\n    client.go:310: [debug] Starting delete for \"jaeger\" Service\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Service\n    client.go:339: [debug] services \"kubeslice-webhook-service\" not found\n    client.go:339: [debug] services \"nsm-admission-webhook-svc\" not found\n    client.go:339: [debug] services \"kubeslice-dns\" not found\n    client.go:339: [debug] services \"jaeger\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-operator\" Deployment\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook\" Deployment\n    client.go:310: [debug] Starting delete for \"jaeger\" Deployment\n    client.go:310: [debug] Starting delete for \"prefix-service\" Deployment\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" Deployment\n    client.go:339: [debug] deployments.apps \"jaeger\" not found\n    client.go:339: [debug] deployments.apps \"kubeslice-dns\" not found\n    client.go:339: [debug] deployments.apps \"prefix-service\" not found\n    client.go:339: [debug] deployments.apps \"kubeslice-operator\" not found\n    client.go:339: [debug] deployments.apps \"nsm-admission-webhook\" not found\n    client.go:310: [debug] Starting delete for \"nsm-kernel-forwarder\" DaemonSet\n    client.go:310: [debug] Starting delete for \"nsmgr\" DaemonSet\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" DaemonSet\n    client.go:339: [debug] daemonsets.apps \"kubeslice-netop\" not found\n    client.go:339: [debug] daemonsets.apps \"nsm-kernel-forwarder\" not found\n    client.go:339: [debug] daemonsets.apps \"nsmgr\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-rolebinding\" RoleBinding\n    client.go:339: [debug] rolebindings.rbac.authorization.k8s.io \"kubeslice-leader-election-rolebinding\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-leader-election-role\" Role\n    client.go:339: [debug] roles.rbac.authorization.k8s.io \"kubeslice-leader-election-role\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"nsm-role-binding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-rolebinding\" ClusterRoleBinding\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-rolebinding\" ClusterRoleBinding\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-manager-rolebinding\" not found\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-kubernetes-dashboard\" not found\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-dns-rolebinding\" not found\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"kubeslice-proxy-rolebinding\" not found\n    client.go:339: [debug] clusterrolebindings.rbac.authorization.k8s.io \"nsm-role-binding\" not found\n    client.go:310: [debug] Starting delete for \"nsm-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"aggregate-network-services-view\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-dns-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-metrics-reader\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-role\" ClusterRole\n    client.go:310: [debug] Starting delete for \"kubeslice-proxy-role\" ClusterRole\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-manager-role\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-proxy-role\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"nsm-role\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-metrics-reader\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-kubernetes-dashboard\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"aggregate-network-services-view\" not found\n    client.go:339: [debug] clusterroles.rbac.authorization.k8s.io \"kubeslice-dns-role\" not found\n    client.go:310: [debug] Starting delete for \"networkservices.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkserviceendpoints.networkservicemesh.io\" CustomResourceDefinition\n    client.go:310: [debug] Starting delete for \"networkservicemanagers.networkservicemesh.io\" CustomResourceDefinition\n    client.go:339: [debug] customresourcedefinitions.apiextensions.k8s.io \"networkserviceendpoints.networkservicemesh.io\" not found\n    client.go:339: [debug] customresourcedefinitions.apiextensions.k8s.io \"networkservices.networkservicemesh.io\" not found\n    client.go:339: [debug] customresourcedefinitions.apiextensions.k8s.io \"networkservicemanagers.networkservicemesh.io\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-manager-config\" ConfigMap\n    client.go:310: [debug] Starting delete for \"nsm-config\" ConfigMap\n    client.go:339: [debug] configmaps \"nsm-config\" not found\n    client.go:339: [debug] configmaps \"kubeslice-manager-config\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-certs\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-hub\" Secret\n    client.go:310: [debug] Starting delete for \"kubeslice-image-pull-secret\" Secret\n    client.go:339: [debug] secrets \"kubeslice-hub\" not found\n    client.go:339: [debug] secrets \"nsm-admission-webhook-certs\" not found\n    client.go:339: [debug] secrets \"kubeslice-admission-webhook-certs\" not found\n    client.go:339: [debug] secrets \"kubeslice-image-pull-secret\" not found\n    client.go:310: [debug] Starting delete for \"nsmgr-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nse-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"nsc-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-kubernetes-dashboard\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-dns\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"forward-plane-acc\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-netop\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-server\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"kubeslice-controller-manager\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"vpn-gateway-client\" ServiceAccount\n    client.go:310: [debug] Starting delete for \"slice-router\" ServiceAccount\n    client.go:339: [debug] serviceaccounts \"kubeslice-controller-manager\" not found\n    client.go:339: [debug] serviceaccounts \"slice-router\" not found\n    client.go:339: [debug] serviceaccounts \"vpn-gateway-client\" not found\n    client.go:339: [debug] serviceaccounts \"forward-plane-acc\" not found\n    client.go:339: [debug] serviceaccounts \"nsc-acc\" not found\n    client.go:339: [debug] serviceaccounts \"kubeslice-kubernetes-dashboard\" not found\n    client.go:339: [debug] serviceaccounts \"vpn-gateway-server\" not found\n    client.go:339: [debug] serviceaccounts \"nsmgr-acc\" not found\n    client.go:339: [debug] serviceaccounts \"nse-acc\" not found\n    client.go:339: [debug] serviceaccounts \"kubeslice-netop\" not found\n    client.go:339: [debug] serviceaccounts \"kubeslice-dns\" not found\n    client.go:310: [debug] Starting delete for \"kubeslice-mutating-webhook-configuration\" MutatingWebhookConfiguration\n    client.go:310: [debug] Starting delete for \"nsm-admission-webhook-cfg\" MutatingWebhookConfiguration\n    client.go:339: [debug] mutatingwebhookconfigurations.admissionregistration.k8s.io \"kubeslice-mutating-webhook-configuration\" not found\n    client.go:339: [debug] mutatingwebhookconfigurations.admissionregistration.k8s.io \"nsm-admission-webhook-cfg\" not found\n    uninstall.go:144: [debug] purge requested for kubeslice-worker\n    Error: INSTALLATION FAILED: release kubeslice-worker failed, and has been uninstalled due to atomic being set: failed pre-install: timed out waiting for the condition\n    helm.go:84: [debug] failed pre-install: timed out waiting for the condition\n    release kubeslice-worker failed, and has been uninstalled due to atomic being set\n    helm.sh/helm/v3/pkg/action.(*Install).failRelease\n    \thelm.sh/helm/v3/pkg/action/install.go:449\n    helm.sh/helm/v3/pkg/action.(*Install).reportToRun\n    \thelm.sh/helm/v3/pkg/action/install.go:433\n    helm.sh/helm/v3/pkg/action.(*Install).performInstall\n    \thelm.sh/helm/v3/pkg/action/install.go:361\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\n    INSTALLATION FAILED\n    main.newInstallCmd.func2\n    \thelm.sh/helm/v3/cmd/helm/install.go:127\n    github.com/spf13/cobra.(*Command).execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:856\n    github.com/spf13/cobra.(*Command).ExecuteC\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:974\n    github.com/spf13/cobra.(*Command).Execute\n    \tgithub.com/spf13/cobra@v1.4.0/command.go:902\n    main.main\n    \thelm.sh/helm/v3/cmd/helm/helm.go:83\n    runtime.main\n    \truntime/proc.go:255\n    runtime.goexit\n    \truntime/asm_amd64.s:1581\noccurred","time":{"start":1664798844000,"stop":1664799263439,"duration":419439}}]},"Worker Suite:Worker Suite#[It] Slice Config Negative Scenario tests Slice Config Negative Scenario Update Should fail while updating sliceconfig with bandwithguaranteedkbps more than bandwidthceilingkbps in qosprofiledetailsnvalid ":{"statistic":{"failed":0,"broken":0,"skipped":1,"passed":5,"unknown":0,"total":6},"items":[{"uid":"e952566662c6edf5","status":"passed","time":{"start":1667837307000,"stop":1667837307659,"duration":659}},{"uid":"923eff41645a7152","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667826674000,"stop":1667826674000,"duration":0}},{"uid":"a45350fa4663635a","status":"passed","time":{"start":1667823949000,"stop":1667823949291,"duration":291}},{"uid":"18754e6bb183d749","status":"passed","time":{"start":1666674621000,"stop":1666674621849,"duration":849}},{"uid":"e89b4c70ea2e1335","status":"passed","time":{"start":1666674212000,"stop":1666674212838,"duration":838}},{"uid":"e8eaaf620a90cba8","status":"passed","time":{"start":1666665366000,"stop":1666665366722,"duration":722}}]},"Worker Suite:Worker Suite#[It] Slice RBAC Tests RBAC Common Tests WorkerSliceRoleBinding removal test: WorkerSliceRoleBinding should not be generated if SliceRoleTempate does not exit.":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":0,"unknown":0,"total":2},"items":[{"uid":"d9617ba56a60bcdc","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667739621000,"stop":1667739621000,"duration":0}},{"uid":"1654ce280f8b9bb","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667458785000,"stop":1667458785000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice RBAC Tests SliceRoleBinding Tests Create SliceRoleBinding failure test: Creating SliceRoleBinding slice-green with name not same as Slice slice-red":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":2,"unknown":0,"total":2},"items":[{"uid":"b2779f60f1c6589e","status":"passed","time":{"start":1667739621000,"stop":1667739621735,"duration":735}},{"uid":"dd9c920821f7b423","status":"passed","time":{"start":1667458785000,"stop":1667458785487,"duration":487}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Update Should update successfully when Project Applied while removing a service account name in Write users":{"statistic":{"failed":2,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":3},"items":[{"uid":"ce4c513de2f55480","status":"passed","time":{"start":1667840900000,"stop":1667840904760,"duration":4760}},{"uid":"c0ab472777ffde5d","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc00000e678>: {\n        Underlying: <*exec.ExitError | 0xc000749de0>{\n            ProcessState: {\n                pid: 7631,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 180466},\n                    Stime: {Sec: 0, Usec: 52636},\n                    Maxrss: 78780,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 7687,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13760,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 268,\n                    Nivcsw: 299,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Write%20users2716222324\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.94.3:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Write%20users2716222324\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.94.3:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Write%20users2716222324\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.94.3:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Write%20users2716222324\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.94.3:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Write%20users2716222324\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.94.3:443: connect: connection refused\noccurred","time":{"start":1665665955000,"stop":1665665955282,"duration":282}},{"uid":"7ff32a597a002292","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0001a4e40>: {\n        Underlying: <*exec.ExitError | 0xc0004d9440>{\n            ProcessState: {\n                pid: 7204,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 281212},\n                    Stime: {Sec: 0, Usec: 44190},\n                    Maxrss: 76648,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 6306,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13760,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 278,\n                    Nivcsw: 417,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Write%20users807127199\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.116.233:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Write%20users807127199\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.116.233:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Write%20users807127199\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.116.233:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Write%20users807127199\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.116.233:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Update%20Should%20update%20successfully%20when%20Project%20Applied%20while%20removing%20a%20service%20account%20name%20in%20Write%20users807127199\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.116.233:443: connect: connection refused\noccurred","time":{"start":1665666027000,"stop":1665666027427,"duration":427}}]},"Hub Suite:Hub Suite#[It] Project CR test Project Update Should Update successfullyy when Project is Applied with valid service account name in Read users":{"statistic":{"failed":2,"broken":0,"skipped":0,"passed":1,"unknown":0,"total":3},"items":[{"uid":"d2704907a93902f3","status":"passed","time":{"start":1667840900000,"stop":1667840909124,"duration":9124}},{"uid":"6389cde96aed0647","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc0006c2468>: {\n        Underlying: <*exec.ExitError | 0xc0005ff000>{\n            ProcessState: {\n                pid: 7602,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 184388},\n                    Stime: {Sec: 0, Usec: 52682},\n                    Maxrss: 83012,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 10255,\n                    Majflt: 0,\n                    Nswap: 0,\n                    Inblock: 0,\n                    Oublock: 13760,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 339,\n                    Nivcsw: 487,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20Update%20successfullyy%20when%20Project%20is%20Applied%20with%20valid%20service%20account%20name%20in%20Read%20users3082760587\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.94.3:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20Update%20successfullyy%20when%20Project%20is%20Applied%20with%20valid%20service%20account%20name%20in%20Read%20users3082760587\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.94.3:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20Update%20successfullyy%20when%20Project%20is%20Applied%20with%20valid%20service%20account%20name%20in%20Read%20users3082760587\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.94.3:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20Update%20successfullyy%20when%20Project%20is%20Applied%20with%20valid%20service%20account%20name%20in%20Read%20users3082760587\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.94.3:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Update%20Should%20Update%20successfullyy%20when%20Project%20is%20Applied%20with%20valid%20service%20account%20name%20in%20Read%20users3082760587\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.94.3:443: connect: connection refused\noccurred","time":{"start":1665665955000,"stop":1665665955287,"duration":287}},{"uid":"3bb7dbc920d9f1a9","status":"failed","statusDetails":"Unexpected error:\n    <*shell.ErrWithCmdOutput | 0xc000534a98>: {\n        Underlying: <*exec.ExitError | 0xc0006a3440>{\n            ProcessState: {\n                pid: 7174,\n                status: 256,\n                rusage: {\n                    Utime: {Sec: 0, Usec: 245374},\n                    Stime: {Sec: 0, Usec: 78073},\n                    Maxrss: 83984,\n                    Ixrss: 0,\n                    Idrss: 0,\n                    Isrss: 0,\n                    Minflt: 9060,\n                    Majflt: 1,\n                    Nswap: 0,\n                    Inblock: 256,\n                    Oublock: 13760,\n                    Msgsnd: 0,\n                    Msgrcv: 0,\n                    Nsignals: 0,\n                    Nvcsw: 490,\n                    Nivcsw: 392,\n                },\n            },\n            Stderr: nil,\n        },\n        Output: {\n            stdout: {\n                Lines: nil,\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20Update%20successfullyy%20when%20Project%20is%20Applied%20with%20valid%20service%20account%20name%20in%20Read%20users3706711320\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.116.233:443: connect: connection refused\",\n                    ],\n                },\n            },\n            stderr: {\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20Update%20successfullyy%20when%20Project%20is%20Applied%20with%20valid%20service%20account%20name%20in%20Read%20users3706711320\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.116.233:443: connect: connection refused\",\n                ],\n                merged: {\n                    Mutex: {state: 0, sema: 0},\n                    Lines: [\n                        \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20Update%20successfullyy%20when%20Project%20is%20Applied%20with%20valid%20service%20account%20name%20in%20Read%20users3706711320\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.116.233:443: connect: connection refused\",\n                    ],\n                },\n            },\n            merged: {\n                Mutex: {state: 0, sema: 0},\n                Lines: [\n                    \"Error from server (InternalError): error when creating \\\"/tmp/Project%20CR%20test%20Project%20Update%20Should%20Update%20successfullyy%20when%20Project%20is%20Applied%20with%20valid%20service%20account%20name%20in%20Read%20users3706711320\\\": Internal error occurred: failed calling webhook \\\"mproject.kb.io\\\": failed to call webhook: Post \\\"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\\\": dial tcp 10.96.116.233:443: connect: connection refused\",\n                ],\n            },\n        },\n    }\n    error while running command: exit status 1; Error from server (InternalError): error when creating \"/tmp/Project%20CR%20test%20Project%20Update%20Should%20Update%20successfullyy%20when%20Project%20is%20Applied%20with%20valid%20service%20account%20name%20in%20Read%20users3706711320\": Internal error occurred: failed calling webhook \"mproject.kb.io\": failed to call webhook: Post \"https://kubeslice-controller-webhook-service.kubeslice-controller.svc:443/mutate-controller-kubeslice-io-v1alpha1-project?timeout=10s\": dial tcp 10.96.116.233:443: connect: connection refused\noccurred","time":{"start":1665666027000,"stop":1665666027426,"duration":426}}]},"Worker Suite:Worker Suite#[It] Slice RBAC Tests RBAC Common Tests WorkerSliceRoleBinding data propagation test: Checking successful propagation of ApplyTo":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":0,"unknown":0,"total":2},"items":[{"uid":"588be9f25d859410","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667739621000,"stop":1667739621000,"duration":0}},{"uid":"ade174cdab6a9225","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667458785000,"stop":1667458785000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice RBAC Tests RBAC Common Tests SliceRoleBinding status test: SliceRoleBinding should have error status for invalid namespace for k8s roles":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":0,"unknown":0,"total":2},"items":[{"uid":"954cf8e4507da4b9","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667739621000,"stop":1667739621000,"duration":0}},{"uid":"5afaecfa0bf9b7b0","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667458785000,"stop":1667458785000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Attach/Deattach Scenario Attach cluster from slice  when Deattach cluster from slice  should have gateway pod running on attached cluster":{"statistic":{"failed":0,"broken":0,"skipped":4,"passed":2,"unknown":0,"total":6},"items":[{"uid":"e182ec67faadc863","status":"passed","time":{"start":1667837307000,"stop":1667837307187,"duration":187}},{"uid":"5c614ef0c51c8590","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1667826674000,"stop":1667826674000,"duration":0}},{"uid":"28372a1e022f04f9","status":"passed","time":{"start":1667823949000,"stop":1667823949210,"duration":210}},{"uid":"31867e62064ae7da","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674621000,"stop":1666674621000,"duration":0}},{"uid":"687de1346083d599","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666674212000,"stop":1666674212000,"duration":0}},{"uid":"490422f1f91df45a","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1666665366000,"stop":1666665366000,"duration":0}}]},"Worker Suite:Worker Suite#[It] Slice Config Negative Scenario tests Slice Config Negative Scenario Creation Should fail while creating sliceconfig with bandwithguaranteedkbps more than bandwidthceilingkbps in qosprofiledetailsnvalid ":{"statistic":{"failed":0,"broken":0,"skipped":0,"passed":6,"unknown":0,"total":6},"items":[{"uid":"1980051f9e3c1704","status":"passed","time":{"start":1667837307000,"stop":1667837307369,"duration":369}},{"uid":"998100a0ef3cc0b1","status":"passed","time":{"start":1667826674000,"stop":1667826674000,"duration":0}},{"uid":"34b514958ccdc175","status":"passed","time":{"start":1667823949000,"stop":1667823949001,"duration":1}},{"uid":"62103481a17cebb2","status":"passed","time":{"start":1666674621000,"stop":1666674621392,"duration":392}},{"uid":"d9c5d1edbaa0c22a","status":"passed","time":{"start":1666674212000,"stop":1666674212000,"duration":0}},{"uid":"4e93f2484f1bdf25","status":"passed","time":{"start":1666665366000,"stop":1666665366385,"duration":385}}]},"Hub Suite:Hub Suite#[It] Cluster CR tests Cluster CR validation Register worker cluster":{"statistic":{"failed":0,"broken":0,"skipped":2,"passed":1,"unknown":0,"total":3},"items":[{"uid":"8ce7bd68bd11691c","status":"passed","time":{"start":1667840900000,"stop":1667840935127,"duration":35127}},{"uid":"8f887bf1c7923eb3","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1665665955000,"stop":1665665955000,"duration":0}},{"uid":"428962ac3a190439","status":"skipped","statusDetails":"skipped - Spec skipped because an earlier spec in an ordered container failed","time":{"start":1665666027000,"stop":1665666027000,"duration":0}}]}}